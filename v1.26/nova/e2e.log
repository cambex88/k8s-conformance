I0123 13:21:25.527286      22 e2e.go:126] Starting e2e run "13937306-003f-4b12-8d30-d166651e4e5b" on Ginkgo node 1
Jan 23 13:21:25.539: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1706016084 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan 23 13:21:25.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 13:21:25.641: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0123 13:21:25.641750      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jan 23 13:21:25.654: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 23 13:21:25.680: INFO: 22 / 22 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 23 13:21:25.680: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
Jan 23 13:21:25.680: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 23 13:21:25.684: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 23 13:21:25.684: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 23 13:21:25.684: INFO: e2e test version: v1.26.8
Jan 23 13:21:25.685: INFO: kube-apiserver version: v1.26.8
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan 23 13:21:25.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 13:21:25.688: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.048 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan 23 13:21:25.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 13:21:25.641: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0123 13:21:25.641750      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Jan 23 13:21:25.654: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan 23 13:21:25.680: INFO: 22 / 22 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan 23 13:21:25.680: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
    Jan 23 13:21:25.680: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan 23 13:21:25.684: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Jan 23 13:21:25.684: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jan 23 13:21:25.684: INFO: e2e test version: v1.26.8
    Jan 23 13:21:25.685: INFO: kube-apiserver version: v1.26.8
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan 23 13:21:25.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 13:21:25.688: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:21:25.707
Jan 23 13:21:25.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename pods 01/23/24 13:21:25.708
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:21:25.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:21:25.719
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 01/23/24 13:21:25.72
Jan 23 13:21:25.732: INFO: Waiting up to 5m0s for pod "pod-mkv82" in namespace "pods-2204" to be "running"
Jan 23 13:21:25.734: INFO: Pod "pod-mkv82": Phase="Pending", Reason="", readiness=false. Elapsed: 1.988926ms
Jan 23 13:21:27.737: INFO: Pod "pod-mkv82": Phase="Running", Reason="", readiness=true. Elapsed: 2.005085995s
Jan 23 13:21:27.737: INFO: Pod "pod-mkv82" satisfied condition "running"
STEP: patching /status 01/23/24 13:21:27.737
Jan 23 13:21:27.764: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 23 13:21:27.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2204" for this suite. 01/23/24 13:21:27.767
------------------------------
• [2.078 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:21:25.707
    Jan 23 13:21:25.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename pods 01/23/24 13:21:25.708
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:21:25.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:21:25.719
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 01/23/24 13:21:25.72
    Jan 23 13:21:25.732: INFO: Waiting up to 5m0s for pod "pod-mkv82" in namespace "pods-2204" to be "running"
    Jan 23 13:21:25.734: INFO: Pod "pod-mkv82": Phase="Pending", Reason="", readiness=false. Elapsed: 1.988926ms
    Jan 23 13:21:27.737: INFO: Pod "pod-mkv82": Phase="Running", Reason="", readiness=true. Elapsed: 2.005085995s
    Jan 23 13:21:27.737: INFO: Pod "pod-mkv82" satisfied condition "running"
    STEP: patching /status 01/23/24 13:21:27.737
    Jan 23 13:21:27.764: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:21:27.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2204" for this suite. 01/23/24 13:21:27.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSE0123 13:21:27.785854      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:21:27.786
Jan 23 13:21:27.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename init-container 01/23/24 13:21:27.787
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:21:27.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:21:27.847
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 01/23/24 13:21:27.86
Jan 23 13:21:27.861: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:21:32.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1519" for this suite. 01/23/24 13:21:32.694
------------------------------
• [4.912 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:21:27.786
    Jan 23 13:21:27.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename init-container 01/23/24 13:21:27.787
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:21:27.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:21:27.847
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 01/23/24 13:21:27.86
    Jan 23 13:21:27.861: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:21:32.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1519" for this suite. 01/23/24 13:21:32.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:21:32.698
Jan 23 13:21:32.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename var-expansion 01/23/24 13:21:32.699
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:21:32.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:21:32.708
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 01/23/24 13:21:32.71
STEP: waiting for pod running 01/23/24 13:21:32.723
Jan 23 13:21:32.723: INFO: Waiting up to 2m0s for pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39" in namespace "var-expansion-1196" to be "running"
Jan 23 13:21:32.726: INFO: Pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.564282ms
Jan 23 13:21:34.728: INFO: Pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39": Phase="Running", Reason="", readiness=true. Elapsed: 2.004823861s
Jan 23 13:21:34.728: INFO: Pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39" satisfied condition "running"
STEP: creating a file in subpath 01/23/24 13:21:34.728
Jan 23 13:21:34.729: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1196 PodName:var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 13:21:34.729: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 13:21:34.730: INFO: ExecWithOptions: Clientset creation
Jan 23 13:21:34.730: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-1196/pods/var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/23/24 13:21:34.802
Jan 23 13:21:34.804: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1196 PodName:var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 13:21:34.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 13:21:34.805: INFO: ExecWithOptions: Clientset creation
Jan 23 13:21:34.805: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-1196/pods/var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/23/24 13:21:34.861
Jan 23 13:21:35.369: INFO: Successfully updated pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39"
STEP: waiting for annotated pod running 01/23/24 13:21:35.369
Jan 23 13:21:35.369: INFO: Waiting up to 2m0s for pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39" in namespace "var-expansion-1196" to be "running"
Jan 23 13:21:35.371: INFO: Pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39": Phase="Running", Reason="", readiness=true. Elapsed: 1.521298ms
Jan 23 13:21:35.371: INFO: Pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39" satisfied condition "running"
STEP: deleting the pod gracefully 01/23/24 13:21:35.371
Jan 23 13:21:35.371: INFO: Deleting pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39" in namespace "var-expansion-1196"
Jan 23 13:21:35.374: INFO: Wait up to 5m0s for pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 23 13:22:09.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1196" for this suite. 01/23/24 13:22:09.383
------------------------------
• [SLOW TEST] [36.688 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:21:32.698
    Jan 23 13:21:32.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename var-expansion 01/23/24 13:21:32.699
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:21:32.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:21:32.708
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 01/23/24 13:21:32.71
    STEP: waiting for pod running 01/23/24 13:21:32.723
    Jan 23 13:21:32.723: INFO: Waiting up to 2m0s for pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39" in namespace "var-expansion-1196" to be "running"
    Jan 23 13:21:32.726: INFO: Pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.564282ms
    Jan 23 13:21:34.728: INFO: Pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39": Phase="Running", Reason="", readiness=true. Elapsed: 2.004823861s
    Jan 23 13:21:34.728: INFO: Pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39" satisfied condition "running"
    STEP: creating a file in subpath 01/23/24 13:21:34.728
    Jan 23 13:21:34.729: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1196 PodName:var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 13:21:34.729: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 13:21:34.730: INFO: ExecWithOptions: Clientset creation
    Jan 23 13:21:34.730: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-1196/pods/var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/23/24 13:21:34.802
    Jan 23 13:21:34.804: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1196 PodName:var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 13:21:34.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 13:21:34.805: INFO: ExecWithOptions: Clientset creation
    Jan 23 13:21:34.805: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-1196/pods/var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/23/24 13:21:34.861
    Jan 23 13:21:35.369: INFO: Successfully updated pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39"
    STEP: waiting for annotated pod running 01/23/24 13:21:35.369
    Jan 23 13:21:35.369: INFO: Waiting up to 2m0s for pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39" in namespace "var-expansion-1196" to be "running"
    Jan 23 13:21:35.371: INFO: Pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39": Phase="Running", Reason="", readiness=true. Elapsed: 1.521298ms
    Jan 23 13:21:35.371: INFO: Pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39" satisfied condition "running"
    STEP: deleting the pod gracefully 01/23/24 13:21:35.371
    Jan 23 13:21:35.371: INFO: Deleting pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39" in namespace "var-expansion-1196"
    Jan 23 13:21:35.374: INFO: Wait up to 5m0s for pod "var-expansion-2c67674d-3e03-4885-8ff6-27ae1b4d7b39" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:22:09.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1196" for this suite. 01/23/24 13:22:09.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:22:09.386
Jan 23 13:22:09.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename podtemplate 01/23/24 13:22:09.387
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:22:09.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:22:09.398
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/23/24 13:22:09.399
Jan 23 13:22:09.402: INFO: created test-podtemplate-1
Jan 23 13:22:09.405: INFO: created test-podtemplate-2
Jan 23 13:22:09.408: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/23/24 13:22:09.408
STEP: delete collection of pod templates 01/23/24 13:22:09.409
Jan 23 13:22:09.409: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/23/24 13:22:09.416
Jan 23 13:22:09.416: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 23 13:22:09.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8191" for this suite. 01/23/24 13:22:09.419
------------------------------
• [0.035 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:22:09.386
    Jan 23 13:22:09.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename podtemplate 01/23/24 13:22:09.387
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:22:09.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:22:09.398
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/23/24 13:22:09.399
    Jan 23 13:22:09.402: INFO: created test-podtemplate-1
    Jan 23 13:22:09.405: INFO: created test-podtemplate-2
    Jan 23 13:22:09.408: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/23/24 13:22:09.408
    STEP: delete collection of pod templates 01/23/24 13:22:09.409
    Jan 23 13:22:09.409: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/23/24 13:22:09.416
    Jan 23 13:22:09.416: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:22:09.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8191" for this suite. 01/23/24 13:22:09.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:22:09.422
Jan 23 13:22:09.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubectl 01/23/24 13:22:09.422
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:22:09.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:22:09.431
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/23/24 13:22:09.432
Jan 23 13:22:09.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4255 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 23 13:22:09.742: INFO: stderr: ""
Jan 23 13:22:09.742: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/23/24 13:22:09.742
Jan 23 13:22:09.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4255 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Jan 23 13:22:11.558: INFO: stderr: ""
Jan 23 13:22:11.558: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/23/24 13:22:11.558
Jan 23 13:22:11.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4255 delete pods e2e-test-httpd-pod'
Jan 23 13:22:14.833: INFO: stderr: ""
Jan 23 13:22:14.834: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 23 13:22:14.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4255" for this suite. 01/23/24 13:22:14.838
------------------------------
• [SLOW TEST] [5.420 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:22:09.422
    Jan 23 13:22:09.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubectl 01/23/24 13:22:09.422
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:22:09.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:22:09.431
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/23/24 13:22:09.432
    Jan 23 13:22:09.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4255 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 23 13:22:09.742: INFO: stderr: ""
    Jan 23 13:22:09.742: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/23/24 13:22:09.742
    Jan 23 13:22:09.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4255 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Jan 23 13:22:11.558: INFO: stderr: ""
    Jan 23 13:22:11.558: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/23/24 13:22:11.558
    Jan 23 13:22:11.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4255 delete pods e2e-test-httpd-pod'
    Jan 23 13:22:14.833: INFO: stderr: ""
    Jan 23 13:22:14.834: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:22:14.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4255" for this suite. 01/23/24 13:22:14.838
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:22:14.842
Jan 23 13:22:14.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubectl 01/23/24 13:22:14.843
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:22:14.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:22:14.855
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 01/23/24 13:22:14.858
Jan 23 13:22:14.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 create -f -'
Jan 23 13:22:15.230: INFO: stderr: ""
Jan 23 13:22:15.230: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/23/24 13:22:15.23
Jan 23 13:22:15.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 23 13:22:15.501: INFO: stderr: ""
Jan 23 13:22:15.501: INFO: stdout: "update-demo-nautilus-67gr4 update-demo-nautilus-hzd7v "
Jan 23 13:22:15.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 get pods update-demo-nautilus-67gr4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 23 13:22:15.870: INFO: stderr: ""
Jan 23 13:22:15.870: INFO: stdout: ""
Jan 23 13:22:15.870: INFO: update-demo-nautilus-67gr4 is created but not running
Jan 23 13:22:20.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 23 13:22:21.015: INFO: stderr: ""
Jan 23 13:22:21.015: INFO: stdout: "update-demo-nautilus-67gr4 update-demo-nautilus-hzd7v "
Jan 23 13:22:21.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 get pods update-demo-nautilus-67gr4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 23 13:22:21.159: INFO: stderr: ""
Jan 23 13:22:21.159: INFO: stdout: "true"
Jan 23 13:22:21.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 get pods update-demo-nautilus-67gr4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 23 13:22:21.309: INFO: stderr: ""
Jan 23 13:22:21.309: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 23 13:22:21.309: INFO: validating pod update-demo-nautilus-67gr4
Jan 23 13:22:21.313: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 13:22:21.313: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 13:22:21.313: INFO: update-demo-nautilus-67gr4 is verified up and running
Jan 23 13:22:21.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 get pods update-demo-nautilus-hzd7v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 23 13:22:21.466: INFO: stderr: ""
Jan 23 13:22:21.466: INFO: stdout: "true"
Jan 23 13:22:21.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 get pods update-demo-nautilus-hzd7v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 23 13:22:21.602: INFO: stderr: ""
Jan 23 13:22:21.602: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 23 13:22:21.602: INFO: validating pod update-demo-nautilus-hzd7v
Jan 23 13:22:21.606: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 13:22:21.606: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 13:22:21.606: INFO: update-demo-nautilus-hzd7v is verified up and running
STEP: using delete to clean up resources 01/23/24 13:22:21.606
Jan 23 13:22:21.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 delete --grace-period=0 --force -f -'
Jan 23 13:22:21.689: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 13:22:21.689: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 23 13:22:21.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 get rc,svc -l name=update-demo --no-headers'
Jan 23 13:22:21.893: INFO: stderr: "No resources found in kubectl-3528 namespace.\n"
Jan 23 13:22:21.893: INFO: stdout: ""
Jan 23 13:22:21.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 23 13:22:22.034: INFO: stderr: ""
Jan 23 13:22:22.034: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 23 13:22:22.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3528" for this suite. 01/23/24 13:22:22.037
------------------------------
• [SLOW TEST] [7.198 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:22:14.842
    Jan 23 13:22:14.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubectl 01/23/24 13:22:14.843
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:22:14.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:22:14.855
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 01/23/24 13:22:14.858
    Jan 23 13:22:14.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 create -f -'
    Jan 23 13:22:15.230: INFO: stderr: ""
    Jan 23 13:22:15.230: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/23/24 13:22:15.23
    Jan 23 13:22:15.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 23 13:22:15.501: INFO: stderr: ""
    Jan 23 13:22:15.501: INFO: stdout: "update-demo-nautilus-67gr4 update-demo-nautilus-hzd7v "
    Jan 23 13:22:15.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 get pods update-demo-nautilus-67gr4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 23 13:22:15.870: INFO: stderr: ""
    Jan 23 13:22:15.870: INFO: stdout: ""
    Jan 23 13:22:15.870: INFO: update-demo-nautilus-67gr4 is created but not running
    Jan 23 13:22:20.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 23 13:22:21.015: INFO: stderr: ""
    Jan 23 13:22:21.015: INFO: stdout: "update-demo-nautilus-67gr4 update-demo-nautilus-hzd7v "
    Jan 23 13:22:21.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 get pods update-demo-nautilus-67gr4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 23 13:22:21.159: INFO: stderr: ""
    Jan 23 13:22:21.159: INFO: stdout: "true"
    Jan 23 13:22:21.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 get pods update-demo-nautilus-67gr4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 23 13:22:21.309: INFO: stderr: ""
    Jan 23 13:22:21.309: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 23 13:22:21.309: INFO: validating pod update-demo-nautilus-67gr4
    Jan 23 13:22:21.313: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 23 13:22:21.313: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 23 13:22:21.313: INFO: update-demo-nautilus-67gr4 is verified up and running
    Jan 23 13:22:21.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 get pods update-demo-nautilus-hzd7v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 23 13:22:21.466: INFO: stderr: ""
    Jan 23 13:22:21.466: INFO: stdout: "true"
    Jan 23 13:22:21.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 get pods update-demo-nautilus-hzd7v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 23 13:22:21.602: INFO: stderr: ""
    Jan 23 13:22:21.602: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 23 13:22:21.602: INFO: validating pod update-demo-nautilus-hzd7v
    Jan 23 13:22:21.606: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 23 13:22:21.606: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 23 13:22:21.606: INFO: update-demo-nautilus-hzd7v is verified up and running
    STEP: using delete to clean up resources 01/23/24 13:22:21.606
    Jan 23 13:22:21.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 delete --grace-period=0 --force -f -'
    Jan 23 13:22:21.689: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 23 13:22:21.689: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 23 13:22:21.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 get rc,svc -l name=update-demo --no-headers'
    Jan 23 13:22:21.893: INFO: stderr: "No resources found in kubectl-3528 namespace.\n"
    Jan 23 13:22:21.893: INFO: stdout: ""
    Jan 23 13:22:21.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3528 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 23 13:22:22.034: INFO: stderr: ""
    Jan 23 13:22:22.034: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:22:22.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3528" for this suite. 01/23/24 13:22:22.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:22:22.041
Jan 23 13:22:22.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename var-expansion 01/23/24 13:22:22.042
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:22:22.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:22:22.052
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 01/23/24 13:22:22.054
Jan 23 13:22:22.066: INFO: Waiting up to 5m0s for pod "var-expansion-30bb2f72-67f1-47a4-a9f9-37367b99c743" in namespace "var-expansion-7498" to be "Succeeded or Failed"
Jan 23 13:22:22.068: INFO: Pod "var-expansion-30bb2f72-67f1-47a4-a9f9-37367b99c743": Phase="Pending", Reason="", readiness=false. Elapsed: 1.424371ms
Jan 23 13:22:24.070: INFO: Pod "var-expansion-30bb2f72-67f1-47a4-a9f9-37367b99c743": Phase="Running", Reason="", readiness=true. Elapsed: 2.003366614s
Jan 23 13:22:26.071: INFO: Pod "var-expansion-30bb2f72-67f1-47a4-a9f9-37367b99c743": Phase="Running", Reason="", readiness=false. Elapsed: 4.004668178s
Jan 23 13:22:28.072: INFO: Pod "var-expansion-30bb2f72-67f1-47a4-a9f9-37367b99c743": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005303087s
STEP: Saw pod success 01/23/24 13:22:28.072
Jan 23 13:22:28.072: INFO: Pod "var-expansion-30bb2f72-67f1-47a4-a9f9-37367b99c743" satisfied condition "Succeeded or Failed"
Jan 23 13:22:28.073: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod var-expansion-30bb2f72-67f1-47a4-a9f9-37367b99c743 container dapi-container: <nil>
STEP: delete the pod 01/23/24 13:22:28.084
Jan 23 13:22:28.090: INFO: Waiting for pod var-expansion-30bb2f72-67f1-47a4-a9f9-37367b99c743 to disappear
Jan 23 13:22:28.092: INFO: Pod var-expansion-30bb2f72-67f1-47a4-a9f9-37367b99c743 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 23 13:22:28.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7498" for this suite. 01/23/24 13:22:28.094
------------------------------
• [SLOW TEST] [6.057 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:22:22.041
    Jan 23 13:22:22.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename var-expansion 01/23/24 13:22:22.042
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:22:22.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:22:22.052
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 01/23/24 13:22:22.054
    Jan 23 13:22:22.066: INFO: Waiting up to 5m0s for pod "var-expansion-30bb2f72-67f1-47a4-a9f9-37367b99c743" in namespace "var-expansion-7498" to be "Succeeded or Failed"
    Jan 23 13:22:22.068: INFO: Pod "var-expansion-30bb2f72-67f1-47a4-a9f9-37367b99c743": Phase="Pending", Reason="", readiness=false. Elapsed: 1.424371ms
    Jan 23 13:22:24.070: INFO: Pod "var-expansion-30bb2f72-67f1-47a4-a9f9-37367b99c743": Phase="Running", Reason="", readiness=true. Elapsed: 2.003366614s
    Jan 23 13:22:26.071: INFO: Pod "var-expansion-30bb2f72-67f1-47a4-a9f9-37367b99c743": Phase="Running", Reason="", readiness=false. Elapsed: 4.004668178s
    Jan 23 13:22:28.072: INFO: Pod "var-expansion-30bb2f72-67f1-47a4-a9f9-37367b99c743": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005303087s
    STEP: Saw pod success 01/23/24 13:22:28.072
    Jan 23 13:22:28.072: INFO: Pod "var-expansion-30bb2f72-67f1-47a4-a9f9-37367b99c743" satisfied condition "Succeeded or Failed"
    Jan 23 13:22:28.073: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod var-expansion-30bb2f72-67f1-47a4-a9f9-37367b99c743 container dapi-container: <nil>
    STEP: delete the pod 01/23/24 13:22:28.084
    Jan 23 13:22:28.090: INFO: Waiting for pod var-expansion-30bb2f72-67f1-47a4-a9f9-37367b99c743 to disappear
    Jan 23 13:22:28.092: INFO: Pod var-expansion-30bb2f72-67f1-47a4-a9f9-37367b99c743 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:22:28.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7498" for this suite. 01/23/24 13:22:28.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:22:28.098
Jan 23 13:22:28.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename events 01/23/24 13:22:28.099
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:22:28.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:22:28.106
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/23/24 13:22:28.108
Jan 23 13:22:28.110: INFO: created test-event-1
Jan 23 13:22:28.113: INFO: created test-event-2
Jan 23 13:22:28.116: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/23/24 13:22:28.116
STEP: delete collection of events 01/23/24 13:22:28.117
Jan 23 13:22:28.117: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/23/24 13:22:28.125
Jan 23 13:22:28.125: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan 23 13:22:28.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1472" for this suite. 01/23/24 13:22:28.129
------------------------------
• [0.034 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:22:28.098
    Jan 23 13:22:28.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename events 01/23/24 13:22:28.099
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:22:28.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:22:28.106
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/23/24 13:22:28.108
    Jan 23 13:22:28.110: INFO: created test-event-1
    Jan 23 13:22:28.113: INFO: created test-event-2
    Jan 23 13:22:28.116: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/23/24 13:22:28.116
    STEP: delete collection of events 01/23/24 13:22:28.117
    Jan 23 13:22:28.117: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/23/24 13:22:28.125
    Jan 23 13:22:28.125: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:22:28.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1472" for this suite. 01/23/24 13:22:28.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:22:28.133
Jan 23 13:22:28.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 13:22:28.134
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:22:28.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:22:28.144
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 01/23/24 13:22:28.146
Jan 23 13:22:28.160: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b8a2bb1-4fa2-49d9-bb12-0dbbda5ad8a3" in namespace "projected-373" to be "Succeeded or Failed"
Jan 23 13:22:28.162: INFO: Pod "downwardapi-volume-7b8a2bb1-4fa2-49d9-bb12-0dbbda5ad8a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.542595ms
Jan 23 13:22:30.165: INFO: Pod "downwardapi-volume-7b8a2bb1-4fa2-49d9-bb12-0dbbda5ad8a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004579105s
Jan 23 13:22:32.165: INFO: Pod "downwardapi-volume-7b8a2bb1-4fa2-49d9-bb12-0dbbda5ad8a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004959447s
STEP: Saw pod success 01/23/24 13:22:32.165
Jan 23 13:22:32.166: INFO: Pod "downwardapi-volume-7b8a2bb1-4fa2-49d9-bb12-0dbbda5ad8a3" satisfied condition "Succeeded or Failed"
Jan 23 13:22:32.167: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-7b8a2bb1-4fa2-49d9-bb12-0dbbda5ad8a3 container client-container: <nil>
STEP: delete the pod 01/23/24 13:22:32.172
Jan 23 13:22:32.183: INFO: Waiting for pod downwardapi-volume-7b8a2bb1-4fa2-49d9-bb12-0dbbda5ad8a3 to disappear
Jan 23 13:22:32.184: INFO: Pod downwardapi-volume-7b8a2bb1-4fa2-49d9-bb12-0dbbda5ad8a3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 23 13:22:32.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-373" for this suite. 01/23/24 13:22:32.186
------------------------------
• [4.056 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:22:28.133
    Jan 23 13:22:28.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 13:22:28.134
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:22:28.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:22:28.144
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 01/23/24 13:22:28.146
    Jan 23 13:22:28.160: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b8a2bb1-4fa2-49d9-bb12-0dbbda5ad8a3" in namespace "projected-373" to be "Succeeded or Failed"
    Jan 23 13:22:28.162: INFO: Pod "downwardapi-volume-7b8a2bb1-4fa2-49d9-bb12-0dbbda5ad8a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.542595ms
    Jan 23 13:22:30.165: INFO: Pod "downwardapi-volume-7b8a2bb1-4fa2-49d9-bb12-0dbbda5ad8a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004579105s
    Jan 23 13:22:32.165: INFO: Pod "downwardapi-volume-7b8a2bb1-4fa2-49d9-bb12-0dbbda5ad8a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004959447s
    STEP: Saw pod success 01/23/24 13:22:32.165
    Jan 23 13:22:32.166: INFO: Pod "downwardapi-volume-7b8a2bb1-4fa2-49d9-bb12-0dbbda5ad8a3" satisfied condition "Succeeded or Failed"
    Jan 23 13:22:32.167: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-7b8a2bb1-4fa2-49d9-bb12-0dbbda5ad8a3 container client-container: <nil>
    STEP: delete the pod 01/23/24 13:22:32.172
    Jan 23 13:22:32.183: INFO: Waiting for pod downwardapi-volume-7b8a2bb1-4fa2-49d9-bb12-0dbbda5ad8a3 to disappear
    Jan 23 13:22:32.184: INFO: Pod downwardapi-volume-7b8a2bb1-4fa2-49d9-bb12-0dbbda5ad8a3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:22:32.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-373" for this suite. 01/23/24 13:22:32.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:22:32.19
Jan 23 13:22:32.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename webhook 01/23/24 13:22:32.191
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:22:32.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:22:32.201
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/23/24 13:22:32.211
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 13:22:32.526
STEP: Deploying the webhook pod 01/23/24 13:22:32.531
STEP: Wait for the deployment to be ready 01/23/24 13:22:32.54
Jan 23 13:22:32.542: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/23/24 13:22:34.549
STEP: Verifying the service has paired with the endpoint 01/23/24 13:22:34.557
Jan 23 13:22:35.558: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/23/24 13:22:35.56
STEP: Registering slow webhook via the AdmissionRegistration API 01/23/24 13:22:35.56
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/23/24 13:22:35.571
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/23/24 13:22:36.578
STEP: Registering slow webhook via the AdmissionRegistration API 01/23/24 13:22:36.578
STEP: Having no error when timeout is longer than webhook latency 01/23/24 13:22:37.595
STEP: Registering slow webhook via the AdmissionRegistration API 01/23/24 13:22:37.595
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/23/24 13:22:42.618
STEP: Registering slow webhook via the AdmissionRegistration API 01/23/24 13:22:42.618
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:22:47.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4799" for this suite. 01/23/24 13:22:47.66
STEP: Destroying namespace "webhook-4799-markers" for this suite. 01/23/24 13:22:47.666
------------------------------
• [SLOW TEST] [15.482 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:22:32.19
    Jan 23 13:22:32.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename webhook 01/23/24 13:22:32.191
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:22:32.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:22:32.201
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/23/24 13:22:32.211
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 13:22:32.526
    STEP: Deploying the webhook pod 01/23/24 13:22:32.531
    STEP: Wait for the deployment to be ready 01/23/24 13:22:32.54
    Jan 23 13:22:32.542: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/23/24 13:22:34.549
    STEP: Verifying the service has paired with the endpoint 01/23/24 13:22:34.557
    Jan 23 13:22:35.558: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/23/24 13:22:35.56
    STEP: Registering slow webhook via the AdmissionRegistration API 01/23/24 13:22:35.56
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/23/24 13:22:35.571
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/23/24 13:22:36.578
    STEP: Registering slow webhook via the AdmissionRegistration API 01/23/24 13:22:36.578
    STEP: Having no error when timeout is longer than webhook latency 01/23/24 13:22:37.595
    STEP: Registering slow webhook via the AdmissionRegistration API 01/23/24 13:22:37.595
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/23/24 13:22:42.618
    STEP: Registering slow webhook via the AdmissionRegistration API 01/23/24 13:22:42.618
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:22:47.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4799" for this suite. 01/23/24 13:22:47.66
    STEP: Destroying namespace "webhook-4799-markers" for this suite. 01/23/24 13:22:47.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:22:47.673
Jan 23 13:22:47.673: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename disruption 01/23/24 13:22:47.674
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:22:47.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:22:47.684
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 01/23/24 13:22:47.686
STEP: Waiting for the pdb to be processed 01/23/24 13:22:47.699
STEP: First trying to evict a pod which shouldn't be evictable 01/23/24 13:22:49.709
STEP: Waiting for all pods to be running 01/23/24 13:22:49.709
Jan 23 13:22:49.711: INFO: pods: 0 < 3
STEP: locating a running pod 01/23/24 13:22:51.715
STEP: Updating the pdb to allow a pod to be evicted 01/23/24 13:22:51.72
STEP: Waiting for the pdb to be processed 01/23/24 13:22:51.726
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/23/24 13:22:53.73
STEP: Waiting for all pods to be running 01/23/24 13:22:53.73
STEP: Waiting for the pdb to observed all healthy pods 01/23/24 13:22:53.735
STEP: Patching the pdb to disallow a pod to be evicted 01/23/24 13:22:53.749
STEP: Waiting for the pdb to be processed 01/23/24 13:22:53.76
STEP: Waiting for all pods to be running 01/23/24 13:22:55.767
Jan 23 13:22:55.769: INFO: running pods: 2 < 3
STEP: locating a running pod 01/23/24 13:22:57.773
STEP: Deleting the pdb to allow a pod to be evicted 01/23/24 13:22:57.777
STEP: Waiting for the pdb to be deleted 01/23/24 13:22:57.78
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/23/24 13:22:57.781
STEP: Waiting for all pods to be running 01/23/24 13:22:57.781
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 23 13:22:57.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7599" for this suite. 01/23/24 13:22:57.793
------------------------------
• [SLOW TEST] [10.124 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:22:47.673
    Jan 23 13:22:47.673: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename disruption 01/23/24 13:22:47.674
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:22:47.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:22:47.684
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 01/23/24 13:22:47.686
    STEP: Waiting for the pdb to be processed 01/23/24 13:22:47.699
    STEP: First trying to evict a pod which shouldn't be evictable 01/23/24 13:22:49.709
    STEP: Waiting for all pods to be running 01/23/24 13:22:49.709
    Jan 23 13:22:49.711: INFO: pods: 0 < 3
    STEP: locating a running pod 01/23/24 13:22:51.715
    STEP: Updating the pdb to allow a pod to be evicted 01/23/24 13:22:51.72
    STEP: Waiting for the pdb to be processed 01/23/24 13:22:51.726
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/23/24 13:22:53.73
    STEP: Waiting for all pods to be running 01/23/24 13:22:53.73
    STEP: Waiting for the pdb to observed all healthy pods 01/23/24 13:22:53.735
    STEP: Patching the pdb to disallow a pod to be evicted 01/23/24 13:22:53.749
    STEP: Waiting for the pdb to be processed 01/23/24 13:22:53.76
    STEP: Waiting for all pods to be running 01/23/24 13:22:55.767
    Jan 23 13:22:55.769: INFO: running pods: 2 < 3
    STEP: locating a running pod 01/23/24 13:22:57.773
    STEP: Deleting the pdb to allow a pod to be evicted 01/23/24 13:22:57.777
    STEP: Waiting for the pdb to be deleted 01/23/24 13:22:57.78
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/23/24 13:22:57.781
    STEP: Waiting for all pods to be running 01/23/24 13:22:57.781
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:22:57.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7599" for this suite. 01/23/24 13:22:57.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:22:57.797
Jan 23 13:22:57.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename job 01/23/24 13:22:57.798
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:22:57.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:22:57.809
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 01/23/24 13:22:57.811
STEP: Ensure pods equal to parallelism count is attached to the job 01/23/24 13:22:57.815
STEP: patching /status 01/23/24 13:23:01.819
STEP: updating /status 01/23/24 13:23:01.824
STEP: get /status 01/23/24 13:23:01.829
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 23 13:23:01.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2358" for this suite. 01/23/24 13:23:01.833
------------------------------
• [4.040 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:22:57.797
    Jan 23 13:22:57.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename job 01/23/24 13:22:57.798
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:22:57.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:22:57.809
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 01/23/24 13:22:57.811
    STEP: Ensure pods equal to parallelism count is attached to the job 01/23/24 13:22:57.815
    STEP: patching /status 01/23/24 13:23:01.819
    STEP: updating /status 01/23/24 13:23:01.824
    STEP: get /status 01/23/24 13:23:01.829
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:23:01.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2358" for this suite. 01/23/24 13:23:01.833
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:23:01.838
Jan 23 13:23:01.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename subpath 01/23/24 13:23:01.838
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:23:01.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:23:01.848
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/23/24 13:23:01.85
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-wzds 01/23/24 13:23:01.859
STEP: Creating a pod to test atomic-volume-subpath 01/23/24 13:23:01.859
Jan 23 13:23:01.872: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-wzds" in namespace "subpath-8322" to be "Succeeded or Failed"
Jan 23 13:23:01.873: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Pending", Reason="", readiness=false. Elapsed: 1.28575ms
Jan 23 13:23:03.877: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 2.005328641s
Jan 23 13:23:05.877: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 4.004788497s
Jan 23 13:23:07.877: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 6.005087685s
Jan 23 13:23:09.878: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 8.005686512s
Jan 23 13:23:11.877: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 10.00465508s
Jan 23 13:23:13.878: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 12.005947794s
Jan 23 13:23:15.878: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 14.005733159s
Jan 23 13:23:17.877: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 16.005422926s
Jan 23 13:23:19.878: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 18.005976815s
Jan 23 13:23:21.877: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 20.004981555s
Jan 23 13:23:23.878: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 22.005544616s
Jan 23 13:23:25.877: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=false. Elapsed: 24.005277733s
Jan 23 13:23:27.878: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.006108076s
STEP: Saw pod success 01/23/24 13:23:27.878
Jan 23 13:23:27.878: INFO: Pod "pod-subpath-test-projected-wzds" satisfied condition "Succeeded or Failed"
Jan 23 13:23:27.880: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-subpath-test-projected-wzds container test-container-subpath-projected-wzds: <nil>
STEP: delete the pod 01/23/24 13:23:27.885
Jan 23 13:23:27.892: INFO: Waiting for pod pod-subpath-test-projected-wzds to disappear
Jan 23 13:23:27.894: INFO: Pod pod-subpath-test-projected-wzds no longer exists
STEP: Deleting pod pod-subpath-test-projected-wzds 01/23/24 13:23:27.894
Jan 23 13:23:27.894: INFO: Deleting pod "pod-subpath-test-projected-wzds" in namespace "subpath-8322"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 23 13:23:27.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8322" for this suite. 01/23/24 13:23:27.898
------------------------------
• [SLOW TEST] [26.063 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:23:01.838
    Jan 23 13:23:01.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename subpath 01/23/24 13:23:01.838
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:23:01.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:23:01.848
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/23/24 13:23:01.85
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-wzds 01/23/24 13:23:01.859
    STEP: Creating a pod to test atomic-volume-subpath 01/23/24 13:23:01.859
    Jan 23 13:23:01.872: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-wzds" in namespace "subpath-8322" to be "Succeeded or Failed"
    Jan 23 13:23:01.873: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Pending", Reason="", readiness=false. Elapsed: 1.28575ms
    Jan 23 13:23:03.877: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 2.005328641s
    Jan 23 13:23:05.877: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 4.004788497s
    Jan 23 13:23:07.877: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 6.005087685s
    Jan 23 13:23:09.878: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 8.005686512s
    Jan 23 13:23:11.877: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 10.00465508s
    Jan 23 13:23:13.878: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 12.005947794s
    Jan 23 13:23:15.878: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 14.005733159s
    Jan 23 13:23:17.877: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 16.005422926s
    Jan 23 13:23:19.878: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 18.005976815s
    Jan 23 13:23:21.877: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 20.004981555s
    Jan 23 13:23:23.878: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=true. Elapsed: 22.005544616s
    Jan 23 13:23:25.877: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Running", Reason="", readiness=false. Elapsed: 24.005277733s
    Jan 23 13:23:27.878: INFO: Pod "pod-subpath-test-projected-wzds": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.006108076s
    STEP: Saw pod success 01/23/24 13:23:27.878
    Jan 23 13:23:27.878: INFO: Pod "pod-subpath-test-projected-wzds" satisfied condition "Succeeded or Failed"
    Jan 23 13:23:27.880: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-subpath-test-projected-wzds container test-container-subpath-projected-wzds: <nil>
    STEP: delete the pod 01/23/24 13:23:27.885
    Jan 23 13:23:27.892: INFO: Waiting for pod pod-subpath-test-projected-wzds to disappear
    Jan 23 13:23:27.894: INFO: Pod pod-subpath-test-projected-wzds no longer exists
    STEP: Deleting pod pod-subpath-test-projected-wzds 01/23/24 13:23:27.894
    Jan 23 13:23:27.894: INFO: Deleting pod "pod-subpath-test-projected-wzds" in namespace "subpath-8322"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:23:27.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8322" for this suite. 01/23/24 13:23:27.898
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:23:27.901
Jan 23 13:23:27.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename taint-multiple-pods 01/23/24 13:23:27.903
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:23:27.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:23:27.917
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Jan 23 13:23:27.919: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 23 13:24:27.957: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Jan 23 13:24:27.960: INFO: Starting informer...
STEP: Starting pods... 01/23/24 13:24:27.96
Jan 23 13:24:28.176: INFO: Pod1 is running on node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local. Tainting Node
Jan 23 13:24:28.389: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-2637" to be "running"
Jan 23 13:24:28.390: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.495697ms
Jan 23 13:24:30.394: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004965422s
Jan 23 13:24:30.394: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan 23 13:24:30.394: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-2637" to be "running"
Jan 23 13:24:30.396: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.844167ms
Jan 23 13:24:30.396: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan 23 13:24:30.396: INFO: Pod2 is running on node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local. Tainting Node
STEP: Trying to apply a taint on the Node 01/23/24 13:24:30.396
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/23/24 13:24:30.404
STEP: Waiting for Pod1 and Pod2 to be deleted 01/23/24 13:24:30.407
Jan 23 13:24:37.196: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 23 13:24:57.243: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/23/24 13:24:57.252
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:24:57.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-2637" for this suite. 01/23/24 13:24:57.256
------------------------------
• [SLOW TEST] [89.359 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:23:27.901
    Jan 23 13:23:27.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename taint-multiple-pods 01/23/24 13:23:27.903
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:23:27.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:23:27.917
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Jan 23 13:23:27.919: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 23 13:24:27.957: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Jan 23 13:24:27.960: INFO: Starting informer...
    STEP: Starting pods... 01/23/24 13:24:27.96
    Jan 23 13:24:28.176: INFO: Pod1 is running on node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local. Tainting Node
    Jan 23 13:24:28.389: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-2637" to be "running"
    Jan 23 13:24:28.390: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.495697ms
    Jan 23 13:24:30.394: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004965422s
    Jan 23 13:24:30.394: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan 23 13:24:30.394: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-2637" to be "running"
    Jan 23 13:24:30.396: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.844167ms
    Jan 23 13:24:30.396: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan 23 13:24:30.396: INFO: Pod2 is running on node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local. Tainting Node
    STEP: Trying to apply a taint on the Node 01/23/24 13:24:30.396
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/23/24 13:24:30.404
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/23/24 13:24:30.407
    Jan 23 13:24:37.196: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan 23 13:24:57.243: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/23/24 13:24:57.252
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:24:57.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-2637" for this suite. 01/23/24 13:24:57.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:24:57.263
Jan 23 13:24:57.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename job 01/23/24 13:24:57.263
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:24:57.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:24:57.274
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 01/23/24 13:24:57.276
STEP: Ensuring job reaches completions 01/23/24 13:24:57.28
STEP: Ensuring pods with index for job exist 01/23/24 13:25:11.283
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 23 13:25:11.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4215" for this suite. 01/23/24 13:25:11.287
------------------------------
• [SLOW TEST] [14.027 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:24:57.263
    Jan 23 13:24:57.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename job 01/23/24 13:24:57.263
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:24:57.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:24:57.274
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 01/23/24 13:24:57.276
    STEP: Ensuring job reaches completions 01/23/24 13:24:57.28
    STEP: Ensuring pods with index for job exist 01/23/24 13:25:11.283
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:25:11.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4215" for this suite. 01/23/24 13:25:11.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:25:11.29
Jan 23 13:25:11.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename namespaces 01/23/24 13:25:11.291
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:11.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:11.301
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 01/23/24 13:25:11.303
STEP: patching the Namespace 01/23/24 13:25:11.31
STEP: get the Namespace and ensuring it has the label 01/23/24 13:25:11.313
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:25:11.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3026" for this suite. 01/23/24 13:25:11.316
STEP: Destroying namespace "nspatchtest-d4676437-3d2d-4b0c-941d-0fcd45e44494-6777" for this suite. 01/23/24 13:25:11.319
------------------------------
• [0.032 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:25:11.29
    Jan 23 13:25:11.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename namespaces 01/23/24 13:25:11.291
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:11.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:11.301
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 01/23/24 13:25:11.303
    STEP: patching the Namespace 01/23/24 13:25:11.31
    STEP: get the Namespace and ensuring it has the label 01/23/24 13:25:11.313
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:25:11.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3026" for this suite. 01/23/24 13:25:11.316
    STEP: Destroying namespace "nspatchtest-d4676437-3d2d-4b0c-941d-0fcd45e44494-6777" for this suite. 01/23/24 13:25:11.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:25:11.323
Jan 23 13:25:11.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename tables 01/23/24 13:25:11.324
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:11.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:11.331
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Jan 23 13:25:11.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-7346" for this suite. 01/23/24 13:25:11.335
------------------------------
• [0.016 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:25:11.323
    Jan 23 13:25:11.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename tables 01/23/24 13:25:11.324
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:11.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:11.331
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:25:11.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-7346" for this suite. 01/23/24 13:25:11.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:25:11.339
Jan 23 13:25:11.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename deployment 01/23/24 13:25:11.34
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:11.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:11.348
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/23/24 13:25:11.364
Jan 23 13:25:11.364: INFO: Creating simple deployment test-deployment-jhpfk
Jan 23 13:25:11.465: INFO: deployment "test-deployment-jhpfk" doesn't have the required revision set
STEP: Getting /status 01/23/24 13:25:13.473
Jan 23 13:25:13.477: INFO: Deployment test-deployment-jhpfk has Conditions: [{Available True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jhpfk-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 01/23/24 13:25:13.477
Jan 23 13:25:13.484: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 25, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 25, 11, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-jhpfk-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/23/24 13:25:13.484
Jan 23 13:25:13.487: INFO: Observed &Deployment event: ADDED
Jan 23 13:25:13.487: INFO: Observed Deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jhpfk-54bc444df"}
Jan 23 13:25:13.487: INFO: Observed &Deployment event: MODIFIED
Jan 23 13:25:13.487: INFO: Observed Deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jhpfk-54bc444df"}
Jan 23 13:25:13.487: INFO: Observed Deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 23 13:25:13.487: INFO: Observed &Deployment event: MODIFIED
Jan 23 13:25:13.487: INFO: Observed Deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 23 13:25:13.487: INFO: Observed Deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jhpfk-54bc444df" is progressing.}
Jan 23 13:25:13.487: INFO: Observed &Deployment event: MODIFIED
Jan 23 13:25:13.487: INFO: Observed Deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 23 13:25:13.487: INFO: Observed Deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jhpfk-54bc444df" has successfully progressed.}
Jan 23 13:25:13.487: INFO: Observed &Deployment event: MODIFIED
Jan 23 13:25:13.487: INFO: Observed Deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 23 13:25:13.487: INFO: Observed Deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jhpfk-54bc444df" has successfully progressed.}
Jan 23 13:25:13.487: INFO: Found Deployment test-deployment-jhpfk in namespace deployment-4500 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 23 13:25:13.487: INFO: Deployment test-deployment-jhpfk has an updated status
STEP: patching the Statefulset Status 01/23/24 13:25:13.487
Jan 23 13:25:13.487: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 23 13:25:13.492: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/23/24 13:25:13.492
Jan 23 13:25:13.494: INFO: Observed &Deployment event: ADDED
Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jhpfk-54bc444df"}
Jan 23 13:25:13.494: INFO: Observed &Deployment event: MODIFIED
Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jhpfk-54bc444df"}
Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 23 13:25:13.494: INFO: Observed &Deployment event: MODIFIED
Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jhpfk-54bc444df" is progressing.}
Jan 23 13:25:13.494: INFO: Observed &Deployment event: MODIFIED
Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jhpfk-54bc444df" has successfully progressed.}
Jan 23 13:25:13.494: INFO: Observed &Deployment event: MODIFIED
Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jhpfk-54bc444df" has successfully progressed.}
Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 23 13:25:13.495: INFO: Observed &Deployment event: MODIFIED
Jan 23 13:25:13.495: INFO: Found deployment test-deployment-jhpfk in namespace deployment-4500 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 23 13:25:13.495: INFO: Deployment test-deployment-jhpfk has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 23 13:25:13.497: INFO: Deployment "test-deployment-jhpfk":
&Deployment{ObjectMeta:{test-deployment-jhpfk  deployment-4500  c8ff1688-45ef-403f-a98e-2549d810b4cd 107392 1 2024-01-23 13:25:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2024-01-23 13:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2024-01-23 13:25:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2024-01-23 13:25:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c21978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-jhpfk-54bc444df",LastUpdateTime:2024-01-23 13:25:13 +0000 UTC,LastTransitionTime:2024-01-23 13:25:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 23 13:25:13.499: INFO: New ReplicaSet "test-deployment-jhpfk-54bc444df" of Deployment "test-deployment-jhpfk":
&ReplicaSet{ObjectMeta:{test-deployment-jhpfk-54bc444df  deployment-4500  92a62c66-2228-4aef-92f1-647f2ed12b2d 107388 1 2024-01-23 13:25:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-jhpfk c8ff1688-45ef-403f-a98e-2549d810b4cd 0xc004acc170 0xc004acc171}] [] [{kube-controller-manager Update apps/v1 2024-01-23 13:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8ff1688-45ef-403f-a98e-2549d810b4cd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 13:25:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004acc218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 23 13:25:13.501: INFO: Pod "test-deployment-jhpfk-54bc444df-44xkl" is available:
&Pod{ObjectMeta:{test-deployment-jhpfk-54bc444df-44xkl test-deployment-jhpfk-54bc444df- deployment-4500  b113eb6e-f67f-40d4-867a-f0dc53d3cd65 107387 0 2024-01-23 13:25:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:1c4197bb9eee8af7d077547595ca5fb2a08974fa6a1e343015acb1a42c34a4ae cni.projectcalico.org/podIP:10.233.87.242/32 cni.projectcalico.org/podIPs:10.233.87.242/32] [{apps/v1 ReplicaSet test-deployment-jhpfk-54bc444df 92a62c66-2228-4aef-92f1-647f2ed12b2d 0xc004acc5e0 0xc004acc5e1}] [] [{calico Update v1 2024-01-23 13:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-23 13:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"92a62c66-2228-4aef-92f1-647f2ed12b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:25:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l9w8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l9w8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:25:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:25:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:25:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:25:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.242,StartTime:2024-01-23 13:25:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:25:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://243c256c3ad6e2f65b9192c5f389780d148e3dc7b438be04e5e949db80b2c1dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.242,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 23 13:25:13.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4500" for this suite. 01/23/24 13:25:13.503
------------------------------
• [2.168 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:25:11.339
    Jan 23 13:25:11.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename deployment 01/23/24 13:25:11.34
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:11.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:11.348
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/23/24 13:25:11.364
    Jan 23 13:25:11.364: INFO: Creating simple deployment test-deployment-jhpfk
    Jan 23 13:25:11.465: INFO: deployment "test-deployment-jhpfk" doesn't have the required revision set
    STEP: Getting /status 01/23/24 13:25:13.473
    Jan 23 13:25:13.477: INFO: Deployment test-deployment-jhpfk has Conditions: [{Available True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jhpfk-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 01/23/24 13:25:13.477
    Jan 23 13:25:13.484: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 25, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 25, 11, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-jhpfk-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/23/24 13:25:13.484
    Jan 23 13:25:13.487: INFO: Observed &Deployment event: ADDED
    Jan 23 13:25:13.487: INFO: Observed Deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jhpfk-54bc444df"}
    Jan 23 13:25:13.487: INFO: Observed &Deployment event: MODIFIED
    Jan 23 13:25:13.487: INFO: Observed Deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jhpfk-54bc444df"}
    Jan 23 13:25:13.487: INFO: Observed Deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 23 13:25:13.487: INFO: Observed &Deployment event: MODIFIED
    Jan 23 13:25:13.487: INFO: Observed Deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 23 13:25:13.487: INFO: Observed Deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jhpfk-54bc444df" is progressing.}
    Jan 23 13:25:13.487: INFO: Observed &Deployment event: MODIFIED
    Jan 23 13:25:13.487: INFO: Observed Deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 23 13:25:13.487: INFO: Observed Deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jhpfk-54bc444df" has successfully progressed.}
    Jan 23 13:25:13.487: INFO: Observed &Deployment event: MODIFIED
    Jan 23 13:25:13.487: INFO: Observed Deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 23 13:25:13.487: INFO: Observed Deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jhpfk-54bc444df" has successfully progressed.}
    Jan 23 13:25:13.487: INFO: Found Deployment test-deployment-jhpfk in namespace deployment-4500 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 23 13:25:13.487: INFO: Deployment test-deployment-jhpfk has an updated status
    STEP: patching the Statefulset Status 01/23/24 13:25:13.487
    Jan 23 13:25:13.487: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 23 13:25:13.492: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/23/24 13:25:13.492
    Jan 23 13:25:13.494: INFO: Observed &Deployment event: ADDED
    Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jhpfk-54bc444df"}
    Jan 23 13:25:13.494: INFO: Observed &Deployment event: MODIFIED
    Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jhpfk-54bc444df"}
    Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 23 13:25:13.494: INFO: Observed &Deployment event: MODIFIED
    Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:11 +0000 UTC 2024-01-23 13:25:11 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jhpfk-54bc444df" is progressing.}
    Jan 23 13:25:13.494: INFO: Observed &Deployment event: MODIFIED
    Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jhpfk-54bc444df" has successfully progressed.}
    Jan 23 13:25:13.494: INFO: Observed &Deployment event: MODIFIED
    Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-23 13:25:13 +0000 UTC 2024-01-23 13:25:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jhpfk-54bc444df" has successfully progressed.}
    Jan 23 13:25:13.494: INFO: Observed deployment test-deployment-jhpfk in namespace deployment-4500 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 23 13:25:13.495: INFO: Observed &Deployment event: MODIFIED
    Jan 23 13:25:13.495: INFO: Found deployment test-deployment-jhpfk in namespace deployment-4500 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan 23 13:25:13.495: INFO: Deployment test-deployment-jhpfk has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 23 13:25:13.497: INFO: Deployment "test-deployment-jhpfk":
    &Deployment{ObjectMeta:{test-deployment-jhpfk  deployment-4500  c8ff1688-45ef-403f-a98e-2549d810b4cd 107392 1 2024-01-23 13:25:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2024-01-23 13:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2024-01-23 13:25:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2024-01-23 13:25:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c21978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-jhpfk-54bc444df",LastUpdateTime:2024-01-23 13:25:13 +0000 UTC,LastTransitionTime:2024-01-23 13:25:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 23 13:25:13.499: INFO: New ReplicaSet "test-deployment-jhpfk-54bc444df" of Deployment "test-deployment-jhpfk":
    &ReplicaSet{ObjectMeta:{test-deployment-jhpfk-54bc444df  deployment-4500  92a62c66-2228-4aef-92f1-647f2ed12b2d 107388 1 2024-01-23 13:25:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-jhpfk c8ff1688-45ef-403f-a98e-2549d810b4cd 0xc004acc170 0xc004acc171}] [] [{kube-controller-manager Update apps/v1 2024-01-23 13:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8ff1688-45ef-403f-a98e-2549d810b4cd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 13:25:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004acc218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 23 13:25:13.501: INFO: Pod "test-deployment-jhpfk-54bc444df-44xkl" is available:
    &Pod{ObjectMeta:{test-deployment-jhpfk-54bc444df-44xkl test-deployment-jhpfk-54bc444df- deployment-4500  b113eb6e-f67f-40d4-867a-f0dc53d3cd65 107387 0 2024-01-23 13:25:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:1c4197bb9eee8af7d077547595ca5fb2a08974fa6a1e343015acb1a42c34a4ae cni.projectcalico.org/podIP:10.233.87.242/32 cni.projectcalico.org/podIPs:10.233.87.242/32] [{apps/v1 ReplicaSet test-deployment-jhpfk-54bc444df 92a62c66-2228-4aef-92f1-647f2ed12b2d 0xc004acc5e0 0xc004acc5e1}] [] [{calico Update v1 2024-01-23 13:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-23 13:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"92a62c66-2228-4aef-92f1-647f2ed12b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:25:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l9w8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l9w8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:25:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:25:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:25:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:25:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.242,StartTime:2024-01-23 13:25:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:25:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://243c256c3ad6e2f65b9192c5f389780d148e3dc7b438be04e5e949db80b2c1dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.242,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:25:13.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4500" for this suite. 01/23/24 13:25:13.503
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:25:13.507
Jan 23 13:25:13.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename secrets 01/23/24 13:25:13.508
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:13.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:13.519
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-9b4065d7-2a53-4ce8-8046-3e42773cde63 01/23/24 13:25:13.521
STEP: Creating a pod to test consume secrets 01/23/24 13:25:13.526
Jan 23 13:25:13.552: INFO: Waiting up to 5m0s for pod "pod-secrets-9630a7b1-5293-4e92-9e4a-ddf11aa2c6eb" in namespace "secrets-6583" to be "Succeeded or Failed"
Jan 23 13:25:13.554: INFO: Pod "pod-secrets-9630a7b1-5293-4e92-9e4a-ddf11aa2c6eb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.852517ms
Jan 23 13:25:15.557: INFO: Pod "pod-secrets-9630a7b1-5293-4e92-9e4a-ddf11aa2c6eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005468512s
Jan 23 13:25:17.560: INFO: Pod "pod-secrets-9630a7b1-5293-4e92-9e4a-ddf11aa2c6eb": Phase="Running", Reason="", readiness=false. Elapsed: 4.00784721s
Jan 23 13:25:19.557: INFO: Pod "pod-secrets-9630a7b1-5293-4e92-9e4a-ddf11aa2c6eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005076639s
STEP: Saw pod success 01/23/24 13:25:19.557
Jan 23 13:25:19.557: INFO: Pod "pod-secrets-9630a7b1-5293-4e92-9e4a-ddf11aa2c6eb" satisfied condition "Succeeded or Failed"
Jan 23 13:25:19.559: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-secrets-9630a7b1-5293-4e92-9e4a-ddf11aa2c6eb container secret-env-test: <nil>
STEP: delete the pod 01/23/24 13:25:19.568
Jan 23 13:25:19.576: INFO: Waiting for pod pod-secrets-9630a7b1-5293-4e92-9e4a-ddf11aa2c6eb to disappear
Jan 23 13:25:19.578: INFO: Pod pod-secrets-9630a7b1-5293-4e92-9e4a-ddf11aa2c6eb no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 23 13:25:19.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6583" for this suite. 01/23/24 13:25:19.58
------------------------------
• [SLOW TEST] [6.076 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:25:13.507
    Jan 23 13:25:13.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename secrets 01/23/24 13:25:13.508
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:13.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:13.519
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-9b4065d7-2a53-4ce8-8046-3e42773cde63 01/23/24 13:25:13.521
    STEP: Creating a pod to test consume secrets 01/23/24 13:25:13.526
    Jan 23 13:25:13.552: INFO: Waiting up to 5m0s for pod "pod-secrets-9630a7b1-5293-4e92-9e4a-ddf11aa2c6eb" in namespace "secrets-6583" to be "Succeeded or Failed"
    Jan 23 13:25:13.554: INFO: Pod "pod-secrets-9630a7b1-5293-4e92-9e4a-ddf11aa2c6eb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.852517ms
    Jan 23 13:25:15.557: INFO: Pod "pod-secrets-9630a7b1-5293-4e92-9e4a-ddf11aa2c6eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005468512s
    Jan 23 13:25:17.560: INFO: Pod "pod-secrets-9630a7b1-5293-4e92-9e4a-ddf11aa2c6eb": Phase="Running", Reason="", readiness=false. Elapsed: 4.00784721s
    Jan 23 13:25:19.557: INFO: Pod "pod-secrets-9630a7b1-5293-4e92-9e4a-ddf11aa2c6eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005076639s
    STEP: Saw pod success 01/23/24 13:25:19.557
    Jan 23 13:25:19.557: INFO: Pod "pod-secrets-9630a7b1-5293-4e92-9e4a-ddf11aa2c6eb" satisfied condition "Succeeded or Failed"
    Jan 23 13:25:19.559: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-secrets-9630a7b1-5293-4e92-9e4a-ddf11aa2c6eb container secret-env-test: <nil>
    STEP: delete the pod 01/23/24 13:25:19.568
    Jan 23 13:25:19.576: INFO: Waiting for pod pod-secrets-9630a7b1-5293-4e92-9e4a-ddf11aa2c6eb to disappear
    Jan 23 13:25:19.578: INFO: Pod pod-secrets-9630a7b1-5293-4e92-9e4a-ddf11aa2c6eb no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:25:19.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6583" for this suite. 01/23/24 13:25:19.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:25:19.584
Jan 23 13:25:19.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename downward-api 01/23/24 13:25:19.584
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:19.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:19.593
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 01/23/24 13:25:19.595
Jan 23 13:25:19.610: INFO: Waiting up to 5m0s for pod "downward-api-c98b0f85-de83-40d5-b1e8-ab6a99b4b125" in namespace "downward-api-5628" to be "Succeeded or Failed"
Jan 23 13:25:19.613: INFO: Pod "downward-api-c98b0f85-de83-40d5-b1e8-ab6a99b4b125": Phase="Pending", Reason="", readiness=false. Elapsed: 2.910919ms
Jan 23 13:25:21.616: INFO: Pod "downward-api-c98b0f85-de83-40d5-b1e8-ab6a99b4b125": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006168671s
Jan 23 13:25:23.616: INFO: Pod "downward-api-c98b0f85-de83-40d5-b1e8-ab6a99b4b125": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006307073s
STEP: Saw pod success 01/23/24 13:25:23.616
Jan 23 13:25:23.616: INFO: Pod "downward-api-c98b0f85-de83-40d5-b1e8-ab6a99b4b125" satisfied condition "Succeeded or Failed"
Jan 23 13:25:23.618: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downward-api-c98b0f85-de83-40d5-b1e8-ab6a99b4b125 container dapi-container: <nil>
STEP: delete the pod 01/23/24 13:25:23.622
Jan 23 13:25:23.630: INFO: Waiting for pod downward-api-c98b0f85-de83-40d5-b1e8-ab6a99b4b125 to disappear
Jan 23 13:25:23.632: INFO: Pod downward-api-c98b0f85-de83-40d5-b1e8-ab6a99b4b125 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 23 13:25:23.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5628" for this suite. 01/23/24 13:25:23.635
------------------------------
• [4.054 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:25:19.584
    Jan 23 13:25:19.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename downward-api 01/23/24 13:25:19.584
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:19.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:19.593
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 01/23/24 13:25:19.595
    Jan 23 13:25:19.610: INFO: Waiting up to 5m0s for pod "downward-api-c98b0f85-de83-40d5-b1e8-ab6a99b4b125" in namespace "downward-api-5628" to be "Succeeded or Failed"
    Jan 23 13:25:19.613: INFO: Pod "downward-api-c98b0f85-de83-40d5-b1e8-ab6a99b4b125": Phase="Pending", Reason="", readiness=false. Elapsed: 2.910919ms
    Jan 23 13:25:21.616: INFO: Pod "downward-api-c98b0f85-de83-40d5-b1e8-ab6a99b4b125": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006168671s
    Jan 23 13:25:23.616: INFO: Pod "downward-api-c98b0f85-de83-40d5-b1e8-ab6a99b4b125": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006307073s
    STEP: Saw pod success 01/23/24 13:25:23.616
    Jan 23 13:25:23.616: INFO: Pod "downward-api-c98b0f85-de83-40d5-b1e8-ab6a99b4b125" satisfied condition "Succeeded or Failed"
    Jan 23 13:25:23.618: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downward-api-c98b0f85-de83-40d5-b1e8-ab6a99b4b125 container dapi-container: <nil>
    STEP: delete the pod 01/23/24 13:25:23.622
    Jan 23 13:25:23.630: INFO: Waiting for pod downward-api-c98b0f85-de83-40d5-b1e8-ab6a99b4b125 to disappear
    Jan 23 13:25:23.632: INFO: Pod downward-api-c98b0f85-de83-40d5-b1e8-ab6a99b4b125 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:25:23.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5628" for this suite. 01/23/24 13:25:23.635
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:25:23.639
Jan 23 13:25:23.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename deployment 01/23/24 13:25:23.64
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:23.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:23.653
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan 23 13:25:23.655: INFO: Creating simple deployment test-new-deployment
Jan 23 13:25:23.664: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 01/23/24 13:25:25.671
STEP: updating a scale subresource 01/23/24 13:25:25.673
STEP: verifying the deployment Spec.Replicas was modified 01/23/24 13:25:25.677
STEP: Patch a scale subresource 01/23/24 13:25:25.679
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 23 13:25:25.687: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-7281  24a879cd-e2fc-4bdf-bb24-f7a4789e0d10 107614 3 2024-01-23 13:25:23 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2024-01-23 13:25:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 13:25:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004dd8cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-23 13:25:25 +0000 UTC,LastTransitionTime:2024-01-23 13:25:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2024-01-23 13:25:25 +0000 UTC,LastTransitionTime:2024-01-23 13:25:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 23 13:25:25.690: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-7281  d50d3fc4-b022-47b9-b1fe-d1a07c94c421 107616 3 2024-01-23 13:25:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 24a879cd-e2fc-4bdf-bb24-f7a4789e0d10 0xc004d3d527 0xc004d3d528}] [] [{kube-controller-manager Update apps/v1 2024-01-23 13:25:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24a879cd-e2fc-4bdf-bb24-f7a4789e0d10\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 13:25:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d3d5b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 23 13:25:25.693: INFO: Pod "test-new-deployment-7f5969cbc7-255cv" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-255cv test-new-deployment-7f5969cbc7- deployment-7281  74d37ac4-b853-43ef-9a4d-c72bcddc5f40 107617 0 2024-01-23 13:25:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 d50d3fc4-b022-47b9-b1fe-d1a07c94c421 0xc004d3d947 0xc004d3d948}] [] [{kube-controller-manager Update v1 2024-01-23 13:25:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d50d3fc4-b022-47b9-b1fe-d1a07c94c421\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qxk7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qxk7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:25:25.693: INFO: Pod "test-new-deployment-7f5969cbc7-qcb7h" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-qcb7h test-new-deployment-7f5969cbc7- deployment-7281  b6fefb8c-a07b-4000-a386-ff919034cb5d 107608 0 2024-01-23 13:25:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:18d8a4716cd2ba2de38dc1d2609663740bc5c398c995db034d894c6ea6889d74 cni.projectcalico.org/podIP:10.233.87.245/32 cni.projectcalico.org/podIPs:10.233.87.245/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 d50d3fc4-b022-47b9-b1fe-d1a07c94c421 0xc004d3dab0 0xc004d3dab1}] [] [{kube-controller-manager Update v1 2024-01-23 13:25:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d50d3fc4-b022-47b9-b1fe-d1a07c94c421\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:25:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:25:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pnrvp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pnrvp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:25:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:25:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.245,StartTime:2024-01-23 13:25:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:25:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://16de2e78960d17c00b00e6fee4189e4860d3f715d4303a1a118565dac99ef4c0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 23 13:25:25.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7281" for this suite. 01/23/24 13:25:25.698
------------------------------
• [2.062 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:25:23.639
    Jan 23 13:25:23.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename deployment 01/23/24 13:25:23.64
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:23.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:23.653
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan 23 13:25:23.655: INFO: Creating simple deployment test-new-deployment
    Jan 23 13:25:23.664: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 01/23/24 13:25:25.671
    STEP: updating a scale subresource 01/23/24 13:25:25.673
    STEP: verifying the deployment Spec.Replicas was modified 01/23/24 13:25:25.677
    STEP: Patch a scale subresource 01/23/24 13:25:25.679
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 23 13:25:25.687: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-7281  24a879cd-e2fc-4bdf-bb24-f7a4789e0d10 107614 3 2024-01-23 13:25:23 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2024-01-23 13:25:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 13:25:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004dd8cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-23 13:25:25 +0000 UTC,LastTransitionTime:2024-01-23 13:25:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2024-01-23 13:25:25 +0000 UTC,LastTransitionTime:2024-01-23 13:25:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 23 13:25:25.690: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-7281  d50d3fc4-b022-47b9-b1fe-d1a07c94c421 107616 3 2024-01-23 13:25:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 24a879cd-e2fc-4bdf-bb24-f7a4789e0d10 0xc004d3d527 0xc004d3d528}] [] [{kube-controller-manager Update apps/v1 2024-01-23 13:25:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24a879cd-e2fc-4bdf-bb24-f7a4789e0d10\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 13:25:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d3d5b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 23 13:25:25.693: INFO: Pod "test-new-deployment-7f5969cbc7-255cv" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-255cv test-new-deployment-7f5969cbc7- deployment-7281  74d37ac4-b853-43ef-9a4d-c72bcddc5f40 107617 0 2024-01-23 13:25:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 d50d3fc4-b022-47b9-b1fe-d1a07c94c421 0xc004d3d947 0xc004d3d948}] [] [{kube-controller-manager Update v1 2024-01-23 13:25:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d50d3fc4-b022-47b9-b1fe-d1a07c94c421\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qxk7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qxk7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:25:25.693: INFO: Pod "test-new-deployment-7f5969cbc7-qcb7h" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-qcb7h test-new-deployment-7f5969cbc7- deployment-7281  b6fefb8c-a07b-4000-a386-ff919034cb5d 107608 0 2024-01-23 13:25:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:18d8a4716cd2ba2de38dc1d2609663740bc5c398c995db034d894c6ea6889d74 cni.projectcalico.org/podIP:10.233.87.245/32 cni.projectcalico.org/podIPs:10.233.87.245/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 d50d3fc4-b022-47b9-b1fe-d1a07c94c421 0xc004d3dab0 0xc004d3dab1}] [] [{kube-controller-manager Update v1 2024-01-23 13:25:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d50d3fc4-b022-47b9-b1fe-d1a07c94c421\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:25:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:25:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pnrvp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pnrvp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:25:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:25:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.245,StartTime:2024-01-23 13:25:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:25:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://16de2e78960d17c00b00e6fee4189e4860d3f715d4303a1a118565dac99ef4c0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:25:25.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7281" for this suite. 01/23/24 13:25:25.698
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:25:25.702
Jan 23 13:25:25.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename downward-api 01/23/24 13:25:25.704
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:25.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:25.716
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 01/23/24 13:25:25.718
Jan 23 13:25:25.741: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7988e3b8-a0f0-45f6-8502-2142fff3ec67" in namespace "downward-api-540" to be "Succeeded or Failed"
Jan 23 13:25:25.743: INFO: Pod "downwardapi-volume-7988e3b8-a0f0-45f6-8502-2142fff3ec67": Phase="Pending", Reason="", readiness=false. Elapsed: 1.902096ms
Jan 23 13:25:27.747: INFO: Pod "downwardapi-volume-7988e3b8-a0f0-45f6-8502-2142fff3ec67": Phase="Running", Reason="", readiness=true. Elapsed: 2.00523756s
Jan 23 13:25:29.747: INFO: Pod "downwardapi-volume-7988e3b8-a0f0-45f6-8502-2142fff3ec67": Phase="Running", Reason="", readiness=false. Elapsed: 4.005473293s
Jan 23 13:25:31.747: INFO: Pod "downwardapi-volume-7988e3b8-a0f0-45f6-8502-2142fff3ec67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005438412s
STEP: Saw pod success 01/23/24 13:25:31.747
Jan 23 13:25:31.747: INFO: Pod "downwardapi-volume-7988e3b8-a0f0-45f6-8502-2142fff3ec67" satisfied condition "Succeeded or Failed"
Jan 23 13:25:31.749: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-7988e3b8-a0f0-45f6-8502-2142fff3ec67 container client-container: <nil>
STEP: delete the pod 01/23/24 13:25:31.752
Jan 23 13:25:31.759: INFO: Waiting for pod downwardapi-volume-7988e3b8-a0f0-45f6-8502-2142fff3ec67 to disappear
Jan 23 13:25:31.761: INFO: Pod downwardapi-volume-7988e3b8-a0f0-45f6-8502-2142fff3ec67 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 23 13:25:31.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-540" for this suite. 01/23/24 13:25:31.763
------------------------------
• [SLOW TEST] [6.064 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:25:25.702
    Jan 23 13:25:25.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename downward-api 01/23/24 13:25:25.704
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:25.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:25.716
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 01/23/24 13:25:25.718
    Jan 23 13:25:25.741: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7988e3b8-a0f0-45f6-8502-2142fff3ec67" in namespace "downward-api-540" to be "Succeeded or Failed"
    Jan 23 13:25:25.743: INFO: Pod "downwardapi-volume-7988e3b8-a0f0-45f6-8502-2142fff3ec67": Phase="Pending", Reason="", readiness=false. Elapsed: 1.902096ms
    Jan 23 13:25:27.747: INFO: Pod "downwardapi-volume-7988e3b8-a0f0-45f6-8502-2142fff3ec67": Phase="Running", Reason="", readiness=true. Elapsed: 2.00523756s
    Jan 23 13:25:29.747: INFO: Pod "downwardapi-volume-7988e3b8-a0f0-45f6-8502-2142fff3ec67": Phase="Running", Reason="", readiness=false. Elapsed: 4.005473293s
    Jan 23 13:25:31.747: INFO: Pod "downwardapi-volume-7988e3b8-a0f0-45f6-8502-2142fff3ec67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005438412s
    STEP: Saw pod success 01/23/24 13:25:31.747
    Jan 23 13:25:31.747: INFO: Pod "downwardapi-volume-7988e3b8-a0f0-45f6-8502-2142fff3ec67" satisfied condition "Succeeded or Failed"
    Jan 23 13:25:31.749: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-7988e3b8-a0f0-45f6-8502-2142fff3ec67 container client-container: <nil>
    STEP: delete the pod 01/23/24 13:25:31.752
    Jan 23 13:25:31.759: INFO: Waiting for pod downwardapi-volume-7988e3b8-a0f0-45f6-8502-2142fff3ec67 to disappear
    Jan 23 13:25:31.761: INFO: Pod downwardapi-volume-7988e3b8-a0f0-45f6-8502-2142fff3ec67 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:25:31.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-540" for this suite. 01/23/24 13:25:31.763
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:25:31.766
Jan 23 13:25:31.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename runtimeclass 01/23/24 13:25:31.767
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:31.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:31.777
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-4129-delete-me 01/23/24 13:25:31.782
STEP: Waiting for the RuntimeClass to disappear 01/23/24 13:25:31.784
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 23 13:25:31.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4129" for this suite. 01/23/24 13:25:31.792
------------------------------
• [0.030 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:25:31.766
    Jan 23 13:25:31.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename runtimeclass 01/23/24 13:25:31.767
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:31.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:31.777
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-4129-delete-me 01/23/24 13:25:31.782
    STEP: Waiting for the RuntimeClass to disappear 01/23/24 13:25:31.784
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:25:31.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4129" for this suite. 01/23/24 13:25:31.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:25:31.797
Jan 23 13:25:31.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename podtemplate 01/23/24 13:25:31.798
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:31.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:31.808
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 23 13:25:31.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-3714" for this suite. 01/23/24 13:25:31.835
------------------------------
• [0.041 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:25:31.797
    Jan 23 13:25:31.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename podtemplate 01/23/24 13:25:31.798
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:31.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:31.808
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:25:31.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-3714" for this suite. 01/23/24 13:25:31.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:25:31.84
Jan 23 13:25:31.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename emptydir 01/23/24 13:25:31.841
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:31.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:31.85
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 01/23/24 13:25:31.852
Jan 23 13:25:31.865: INFO: Waiting up to 5m0s for pod "pod-eb1c6419-7ee2-41ef-9854-dd56741fa262" in namespace "emptydir-3499" to be "Succeeded or Failed"
Jan 23 13:25:31.867: INFO: Pod "pod-eb1c6419-7ee2-41ef-9854-dd56741fa262": Phase="Pending", Reason="", readiness=false. Elapsed: 1.460726ms
Jan 23 13:25:33.870: INFO: Pod "pod-eb1c6419-7ee2-41ef-9854-dd56741fa262": Phase="Running", Reason="", readiness=true. Elapsed: 2.004376944s
Jan 23 13:25:35.870: INFO: Pod "pod-eb1c6419-7ee2-41ef-9854-dd56741fa262": Phase="Running", Reason="", readiness=false. Elapsed: 4.004422274s
Jan 23 13:25:37.870: INFO: Pod "pod-eb1c6419-7ee2-41ef-9854-dd56741fa262": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005086902s
STEP: Saw pod success 01/23/24 13:25:37.87
Jan 23 13:25:37.870: INFO: Pod "pod-eb1c6419-7ee2-41ef-9854-dd56741fa262" satisfied condition "Succeeded or Failed"
Jan 23 13:25:37.873: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-eb1c6419-7ee2-41ef-9854-dd56741fa262 container test-container: <nil>
STEP: delete the pod 01/23/24 13:25:37.878
Jan 23 13:25:37.885: INFO: Waiting for pod pod-eb1c6419-7ee2-41ef-9854-dd56741fa262 to disappear
Jan 23 13:25:37.887: INFO: Pod pod-eb1c6419-7ee2-41ef-9854-dd56741fa262 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 23 13:25:37.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3499" for this suite. 01/23/24 13:25:37.889
------------------------------
• [SLOW TEST] [6.052 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:25:31.84
    Jan 23 13:25:31.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename emptydir 01/23/24 13:25:31.841
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:31.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:31.85
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/23/24 13:25:31.852
    Jan 23 13:25:31.865: INFO: Waiting up to 5m0s for pod "pod-eb1c6419-7ee2-41ef-9854-dd56741fa262" in namespace "emptydir-3499" to be "Succeeded or Failed"
    Jan 23 13:25:31.867: INFO: Pod "pod-eb1c6419-7ee2-41ef-9854-dd56741fa262": Phase="Pending", Reason="", readiness=false. Elapsed: 1.460726ms
    Jan 23 13:25:33.870: INFO: Pod "pod-eb1c6419-7ee2-41ef-9854-dd56741fa262": Phase="Running", Reason="", readiness=true. Elapsed: 2.004376944s
    Jan 23 13:25:35.870: INFO: Pod "pod-eb1c6419-7ee2-41ef-9854-dd56741fa262": Phase="Running", Reason="", readiness=false. Elapsed: 4.004422274s
    Jan 23 13:25:37.870: INFO: Pod "pod-eb1c6419-7ee2-41ef-9854-dd56741fa262": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005086902s
    STEP: Saw pod success 01/23/24 13:25:37.87
    Jan 23 13:25:37.870: INFO: Pod "pod-eb1c6419-7ee2-41ef-9854-dd56741fa262" satisfied condition "Succeeded or Failed"
    Jan 23 13:25:37.873: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-eb1c6419-7ee2-41ef-9854-dd56741fa262 container test-container: <nil>
    STEP: delete the pod 01/23/24 13:25:37.878
    Jan 23 13:25:37.885: INFO: Waiting for pod pod-eb1c6419-7ee2-41ef-9854-dd56741fa262 to disappear
    Jan 23 13:25:37.887: INFO: Pod pod-eb1c6419-7ee2-41ef-9854-dd56741fa262 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:25:37.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3499" for this suite. 01/23/24 13:25:37.889
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:25:37.893
Jan 23 13:25:37.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename watch 01/23/24 13:25:37.893
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:37.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:37.904
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/23/24 13:25:37.906
STEP: creating a watch on configmaps with label B 01/23/24 13:25:37.907
STEP: creating a watch on configmaps with label A or B 01/23/24 13:25:37.908
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/23/24 13:25:37.908
Jan 23 13:25:37.913: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2165  dab36d6a-e93a-4239-b083-ad9b1a7e0f96 107819 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 23 13:25:37.913: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2165  dab36d6a-e93a-4239-b083-ad9b1a7e0f96 107819 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/23/24 13:25:37.913
Jan 23 13:25:37.920: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2165  dab36d6a-e93a-4239-b083-ad9b1a7e0f96 107821 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 23 13:25:37.920: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2165  dab36d6a-e93a-4239-b083-ad9b1a7e0f96 107821 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/23/24 13:25:37.92
Jan 23 13:25:37.931: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2165  dab36d6a-e93a-4239-b083-ad9b1a7e0f96 107823 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 23 13:25:37.931: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2165  dab36d6a-e93a-4239-b083-ad9b1a7e0f96 107823 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/23/24 13:25:37.931
Jan 23 13:25:37.934: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2165  dab36d6a-e93a-4239-b083-ad9b1a7e0f96 107824 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 23 13:25:37.934: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2165  dab36d6a-e93a-4239-b083-ad9b1a7e0f96 107824 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/23/24 13:25:37.934
Jan 23 13:25:37.937: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2165  9eb0d2fa-9011-49db-8fe2-bb816c1e61e8 107825 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 23 13:25:37.938: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2165  9eb0d2fa-9011-49db-8fe2-bb816c1e61e8 107825 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/23/24 13:25:47.938
Jan 23 13:25:47.943: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2165  9eb0d2fa-9011-49db-8fe2-bb816c1e61e8 107882 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 23 13:25:47.943: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2165  9eb0d2fa-9011-49db-8fe2-bb816c1e61e8 107882 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 23 13:25:57.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2165" for this suite. 01/23/24 13:25:57.948
------------------------------
• [SLOW TEST] [20.058 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:25:37.893
    Jan 23 13:25:37.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename watch 01/23/24 13:25:37.893
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:37.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:37.904
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/23/24 13:25:37.906
    STEP: creating a watch on configmaps with label B 01/23/24 13:25:37.907
    STEP: creating a watch on configmaps with label A or B 01/23/24 13:25:37.908
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/23/24 13:25:37.908
    Jan 23 13:25:37.913: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2165  dab36d6a-e93a-4239-b083-ad9b1a7e0f96 107819 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 23 13:25:37.913: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2165  dab36d6a-e93a-4239-b083-ad9b1a7e0f96 107819 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/23/24 13:25:37.913
    Jan 23 13:25:37.920: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2165  dab36d6a-e93a-4239-b083-ad9b1a7e0f96 107821 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 23 13:25:37.920: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2165  dab36d6a-e93a-4239-b083-ad9b1a7e0f96 107821 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/23/24 13:25:37.92
    Jan 23 13:25:37.931: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2165  dab36d6a-e93a-4239-b083-ad9b1a7e0f96 107823 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 23 13:25:37.931: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2165  dab36d6a-e93a-4239-b083-ad9b1a7e0f96 107823 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/23/24 13:25:37.931
    Jan 23 13:25:37.934: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2165  dab36d6a-e93a-4239-b083-ad9b1a7e0f96 107824 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 23 13:25:37.934: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2165  dab36d6a-e93a-4239-b083-ad9b1a7e0f96 107824 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/23/24 13:25:37.934
    Jan 23 13:25:37.937: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2165  9eb0d2fa-9011-49db-8fe2-bb816c1e61e8 107825 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 23 13:25:37.938: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2165  9eb0d2fa-9011-49db-8fe2-bb816c1e61e8 107825 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/23/24 13:25:47.938
    Jan 23 13:25:47.943: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2165  9eb0d2fa-9011-49db-8fe2-bb816c1e61e8 107882 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 23 13:25:47.943: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2165  9eb0d2fa-9011-49db-8fe2-bb816c1e61e8 107882 0 2024-01-23 13:25:37 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-23 13:25:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:25:57.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2165" for this suite. 01/23/24 13:25:57.948
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:25:57.951
Jan 23 13:25:57.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename container-probe 01/23/24 13:25:57.952
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:57.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:57.961
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-fdb4601a-c99f-4b01-bfc4-3c6d7eafe31c in namespace container-probe-3499 01/23/24 13:25:57.963
Jan 23 13:25:57.972: INFO: Waiting up to 5m0s for pod "busybox-fdb4601a-c99f-4b01-bfc4-3c6d7eafe31c" in namespace "container-probe-3499" to be "not pending"
Jan 23 13:25:57.975: INFO: Pod "busybox-fdb4601a-c99f-4b01-bfc4-3c6d7eafe31c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.633839ms
Jan 23 13:25:59.978: INFO: Pod "busybox-fdb4601a-c99f-4b01-bfc4-3c6d7eafe31c": Phase="Running", Reason="", readiness=true. Elapsed: 2.006307067s
Jan 23 13:25:59.979: INFO: Pod "busybox-fdb4601a-c99f-4b01-bfc4-3c6d7eafe31c" satisfied condition "not pending"
Jan 23 13:25:59.979: INFO: Started pod busybox-fdb4601a-c99f-4b01-bfc4-3c6d7eafe31c in namespace container-probe-3499
STEP: checking the pod's current state and verifying that restartCount is present 01/23/24 13:25:59.979
Jan 23 13:25:59.980: INFO: Initial restart count of pod busybox-fdb4601a-c99f-4b01-bfc4-3c6d7eafe31c is 0
Jan 23 13:26:50.068: INFO: Restart count of pod container-probe-3499/busybox-fdb4601a-c99f-4b01-bfc4-3c6d7eafe31c is now 1 (50.087977496s elapsed)
STEP: deleting the pod 01/23/24 13:26:50.068
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 23 13:26:50.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3499" for this suite. 01/23/24 13:26:50.078
------------------------------
• [SLOW TEST] [52.130 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:25:57.951
    Jan 23 13:25:57.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename container-probe 01/23/24 13:25:57.952
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:25:57.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:25:57.961
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-fdb4601a-c99f-4b01-bfc4-3c6d7eafe31c in namespace container-probe-3499 01/23/24 13:25:57.963
    Jan 23 13:25:57.972: INFO: Waiting up to 5m0s for pod "busybox-fdb4601a-c99f-4b01-bfc4-3c6d7eafe31c" in namespace "container-probe-3499" to be "not pending"
    Jan 23 13:25:57.975: INFO: Pod "busybox-fdb4601a-c99f-4b01-bfc4-3c6d7eafe31c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.633839ms
    Jan 23 13:25:59.978: INFO: Pod "busybox-fdb4601a-c99f-4b01-bfc4-3c6d7eafe31c": Phase="Running", Reason="", readiness=true. Elapsed: 2.006307067s
    Jan 23 13:25:59.979: INFO: Pod "busybox-fdb4601a-c99f-4b01-bfc4-3c6d7eafe31c" satisfied condition "not pending"
    Jan 23 13:25:59.979: INFO: Started pod busybox-fdb4601a-c99f-4b01-bfc4-3c6d7eafe31c in namespace container-probe-3499
    STEP: checking the pod's current state and verifying that restartCount is present 01/23/24 13:25:59.979
    Jan 23 13:25:59.980: INFO: Initial restart count of pod busybox-fdb4601a-c99f-4b01-bfc4-3c6d7eafe31c is 0
    Jan 23 13:26:50.068: INFO: Restart count of pod container-probe-3499/busybox-fdb4601a-c99f-4b01-bfc4-3c6d7eafe31c is now 1 (50.087977496s elapsed)
    STEP: deleting the pod 01/23/24 13:26:50.068
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:26:50.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3499" for this suite. 01/23/24 13:26:50.078
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:26:50.083
Jan 23 13:26:50.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename secrets 01/23/24 13:26:50.083
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:26:50.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:26:50.094
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-1504/secret-test-cfd8dbe5-9cfa-44bf-90fc-b5812d8f145b 01/23/24 13:26:50.096
STEP: Creating a pod to test consume secrets 01/23/24 13:26:50.1
Jan 23 13:26:50.117: INFO: Waiting up to 5m0s for pod "pod-configmaps-85e05538-1877-4bca-833d-a158169beb9d" in namespace "secrets-1504" to be "Succeeded or Failed"
Jan 23 13:26:50.118: INFO: Pod "pod-configmaps-85e05538-1877-4bca-833d-a158169beb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.407509ms
Jan 23 13:26:52.121: INFO: Pod "pod-configmaps-85e05538-1877-4bca-833d-a158169beb9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004277563s
Jan 23 13:26:54.121: INFO: Pod "pod-configmaps-85e05538-1877-4bca-833d-a158169beb9d": Phase="Running", Reason="", readiness=false. Elapsed: 4.004217728s
Jan 23 13:26:56.124: INFO: Pod "pod-configmaps-85e05538-1877-4bca-833d-a158169beb9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006607777s
STEP: Saw pod success 01/23/24 13:26:56.124
Jan 23 13:26:56.124: INFO: Pod "pod-configmaps-85e05538-1877-4bca-833d-a158169beb9d" satisfied condition "Succeeded or Failed"
Jan 23 13:26:56.127: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-85e05538-1877-4bca-833d-a158169beb9d container env-test: <nil>
STEP: delete the pod 01/23/24 13:26:56.133
Jan 23 13:26:56.140: INFO: Waiting for pod pod-configmaps-85e05538-1877-4bca-833d-a158169beb9d to disappear
Jan 23 13:26:56.143: INFO: Pod pod-configmaps-85e05538-1877-4bca-833d-a158169beb9d no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 23 13:26:56.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1504" for this suite. 01/23/24 13:26:56.145
------------------------------
• [SLOW TEST] [6.065 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:26:50.083
    Jan 23 13:26:50.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename secrets 01/23/24 13:26:50.083
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:26:50.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:26:50.094
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-1504/secret-test-cfd8dbe5-9cfa-44bf-90fc-b5812d8f145b 01/23/24 13:26:50.096
    STEP: Creating a pod to test consume secrets 01/23/24 13:26:50.1
    Jan 23 13:26:50.117: INFO: Waiting up to 5m0s for pod "pod-configmaps-85e05538-1877-4bca-833d-a158169beb9d" in namespace "secrets-1504" to be "Succeeded or Failed"
    Jan 23 13:26:50.118: INFO: Pod "pod-configmaps-85e05538-1877-4bca-833d-a158169beb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.407509ms
    Jan 23 13:26:52.121: INFO: Pod "pod-configmaps-85e05538-1877-4bca-833d-a158169beb9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004277563s
    Jan 23 13:26:54.121: INFO: Pod "pod-configmaps-85e05538-1877-4bca-833d-a158169beb9d": Phase="Running", Reason="", readiness=false. Elapsed: 4.004217728s
    Jan 23 13:26:56.124: INFO: Pod "pod-configmaps-85e05538-1877-4bca-833d-a158169beb9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006607777s
    STEP: Saw pod success 01/23/24 13:26:56.124
    Jan 23 13:26:56.124: INFO: Pod "pod-configmaps-85e05538-1877-4bca-833d-a158169beb9d" satisfied condition "Succeeded or Failed"
    Jan 23 13:26:56.127: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-85e05538-1877-4bca-833d-a158169beb9d container env-test: <nil>
    STEP: delete the pod 01/23/24 13:26:56.133
    Jan 23 13:26:56.140: INFO: Waiting for pod pod-configmaps-85e05538-1877-4bca-833d-a158169beb9d to disappear
    Jan 23 13:26:56.143: INFO: Pod pod-configmaps-85e05538-1877-4bca-833d-a158169beb9d no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:26:56.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1504" for this suite. 01/23/24 13:26:56.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:26:56.148
Jan 23 13:26:56.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 13:26:56.148
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:26:56.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:26:56.162
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-a1a88030-f0e0-4cfd-9b64-f5afeab02cf2 01/23/24 13:26:56.164
STEP: Creating a pod to test consume secrets 01/23/24 13:26:56.169
Jan 23 13:26:56.185: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-73c3546c-e7b0-4928-92df-d3b8dda1cfd3" in namespace "projected-3670" to be "Succeeded or Failed"
Jan 23 13:26:56.188: INFO: Pod "pod-projected-secrets-73c3546c-e7b0-4928-92df-d3b8dda1cfd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.774524ms
Jan 23 13:26:58.192: INFO: Pod "pod-projected-secrets-73c3546c-e7b0-4928-92df-d3b8dda1cfd3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007075544s
Jan 23 13:27:00.190: INFO: Pod "pod-projected-secrets-73c3546c-e7b0-4928-92df-d3b8dda1cfd3": Phase="Running", Reason="", readiness=false. Elapsed: 4.00499969s
Jan 23 13:27:02.190: INFO: Pod "pod-projected-secrets-73c3546c-e7b0-4928-92df-d3b8dda1cfd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005165574s
STEP: Saw pod success 01/23/24 13:27:02.19
Jan 23 13:27:02.190: INFO: Pod "pod-projected-secrets-73c3546c-e7b0-4928-92df-d3b8dda1cfd3" satisfied condition "Succeeded or Failed"
Jan 23 13:27:02.192: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-secrets-73c3546c-e7b0-4928-92df-d3b8dda1cfd3 container secret-volume-test: <nil>
STEP: delete the pod 01/23/24 13:27:02.196
Jan 23 13:27:02.204: INFO: Waiting for pod pod-projected-secrets-73c3546c-e7b0-4928-92df-d3b8dda1cfd3 to disappear
Jan 23 13:27:02.205: INFO: Pod pod-projected-secrets-73c3546c-e7b0-4928-92df-d3b8dda1cfd3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 23 13:27:02.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3670" for this suite. 01/23/24 13:27:02.208
------------------------------
• [SLOW TEST] [6.066 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:26:56.148
    Jan 23 13:26:56.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 13:26:56.148
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:26:56.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:26:56.162
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-a1a88030-f0e0-4cfd-9b64-f5afeab02cf2 01/23/24 13:26:56.164
    STEP: Creating a pod to test consume secrets 01/23/24 13:26:56.169
    Jan 23 13:26:56.185: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-73c3546c-e7b0-4928-92df-d3b8dda1cfd3" in namespace "projected-3670" to be "Succeeded or Failed"
    Jan 23 13:26:56.188: INFO: Pod "pod-projected-secrets-73c3546c-e7b0-4928-92df-d3b8dda1cfd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.774524ms
    Jan 23 13:26:58.192: INFO: Pod "pod-projected-secrets-73c3546c-e7b0-4928-92df-d3b8dda1cfd3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007075544s
    Jan 23 13:27:00.190: INFO: Pod "pod-projected-secrets-73c3546c-e7b0-4928-92df-d3b8dda1cfd3": Phase="Running", Reason="", readiness=false. Elapsed: 4.00499969s
    Jan 23 13:27:02.190: INFO: Pod "pod-projected-secrets-73c3546c-e7b0-4928-92df-d3b8dda1cfd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005165574s
    STEP: Saw pod success 01/23/24 13:27:02.19
    Jan 23 13:27:02.190: INFO: Pod "pod-projected-secrets-73c3546c-e7b0-4928-92df-d3b8dda1cfd3" satisfied condition "Succeeded or Failed"
    Jan 23 13:27:02.192: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-secrets-73c3546c-e7b0-4928-92df-d3b8dda1cfd3 container secret-volume-test: <nil>
    STEP: delete the pod 01/23/24 13:27:02.196
    Jan 23 13:27:02.204: INFO: Waiting for pod pod-projected-secrets-73c3546c-e7b0-4928-92df-d3b8dda1cfd3 to disappear
    Jan 23 13:27:02.205: INFO: Pod pod-projected-secrets-73c3546c-e7b0-4928-92df-d3b8dda1cfd3 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:27:02.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3670" for this suite. 01/23/24 13:27:02.208
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:27:02.214
Jan 23 13:27:02.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename services 01/23/24 13:27:02.215
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:27:02.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:27:02.237
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-316 01/23/24 13:27:02.245
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/23/24 13:27:02.252
STEP: creating service externalsvc in namespace services-316 01/23/24 13:27:02.252
STEP: creating replication controller externalsvc in namespace services-316 01/23/24 13:27:02.261
I0123 13:27:02.270401      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-316, replica count: 2
I0123 13:27:05.321496      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/23/24 13:27:05.323
Jan 23 13:27:05.334: INFO: Creating new exec pod
Jan 23 13:27:05.344: INFO: Waiting up to 5m0s for pod "execpodrpk2h" in namespace "services-316" to be "running"
Jan 23 13:27:05.346: INFO: Pod "execpodrpk2h": Phase="Pending", Reason="", readiness=false. Elapsed: 1.776749ms
Jan 23 13:27:07.348: INFO: Pod "execpodrpk2h": Phase="Running", Reason="", readiness=true. Elapsed: 2.003563015s
Jan 23 13:27:07.348: INFO: Pod "execpodrpk2h" satisfied condition "running"
Jan 23 13:27:07.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-316 exec execpodrpk2h -- /bin/sh -x -c nslookup nodeport-service.services-316.svc.cluster.local'
Jan 23 13:27:07.592: INFO: stderr: "+ nslookup nodeport-service.services-316.svc.cluster.local\n"
Jan 23 13:27:07.592: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-316.svc.cluster.local\tcanonical name = externalsvc.services-316.svc.cluster.local.\nName:\texternalsvc.services-316.svc.cluster.local\nAddress: 10.233.39.137\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-316, will wait for the garbage collector to delete the pods 01/23/24 13:27:07.593
Jan 23 13:27:07.650: INFO: Deleting ReplicationController externalsvc took: 4.98066ms
Jan 23 13:27:07.750: INFO: Terminating ReplicationController externalsvc pods took: 100.461878ms
Jan 23 13:27:10.461: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 23 13:27:10.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-316" for this suite. 01/23/24 13:27:10.471
------------------------------
• [SLOW TEST] [8.262 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:27:02.214
    Jan 23 13:27:02.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename services 01/23/24 13:27:02.215
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:27:02.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:27:02.237
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-316 01/23/24 13:27:02.245
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/23/24 13:27:02.252
    STEP: creating service externalsvc in namespace services-316 01/23/24 13:27:02.252
    STEP: creating replication controller externalsvc in namespace services-316 01/23/24 13:27:02.261
    I0123 13:27:02.270401      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-316, replica count: 2
    I0123 13:27:05.321496      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/23/24 13:27:05.323
    Jan 23 13:27:05.334: INFO: Creating new exec pod
    Jan 23 13:27:05.344: INFO: Waiting up to 5m0s for pod "execpodrpk2h" in namespace "services-316" to be "running"
    Jan 23 13:27:05.346: INFO: Pod "execpodrpk2h": Phase="Pending", Reason="", readiness=false. Elapsed: 1.776749ms
    Jan 23 13:27:07.348: INFO: Pod "execpodrpk2h": Phase="Running", Reason="", readiness=true. Elapsed: 2.003563015s
    Jan 23 13:27:07.348: INFO: Pod "execpodrpk2h" satisfied condition "running"
    Jan 23 13:27:07.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-316 exec execpodrpk2h -- /bin/sh -x -c nslookup nodeport-service.services-316.svc.cluster.local'
    Jan 23 13:27:07.592: INFO: stderr: "+ nslookup nodeport-service.services-316.svc.cluster.local\n"
    Jan 23 13:27:07.592: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-316.svc.cluster.local\tcanonical name = externalsvc.services-316.svc.cluster.local.\nName:\texternalsvc.services-316.svc.cluster.local\nAddress: 10.233.39.137\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-316, will wait for the garbage collector to delete the pods 01/23/24 13:27:07.593
    Jan 23 13:27:07.650: INFO: Deleting ReplicationController externalsvc took: 4.98066ms
    Jan 23 13:27:07.750: INFO: Terminating ReplicationController externalsvc pods took: 100.461878ms
    Jan 23 13:27:10.461: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:27:10.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-316" for this suite. 01/23/24 13:27:10.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:27:10.476
Jan 23 13:27:10.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 13:27:10.477
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:27:10.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:27:10.488
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-6043a03e-1ca0-4522-8d2b-1003689f7171 01/23/24 13:27:10.49
STEP: Creating a pod to test consume configMaps 01/23/24 13:27:10.495
Jan 23 13:27:10.509: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f1692ff8-215a-445f-94fd-0a056dad1114" in namespace "projected-9519" to be "Succeeded or Failed"
Jan 23 13:27:10.510: INFO: Pod "pod-projected-configmaps-f1692ff8-215a-445f-94fd-0a056dad1114": Phase="Pending", Reason="", readiness=false. Elapsed: 1.483906ms
Jan 23 13:27:12.513: INFO: Pod "pod-projected-configmaps-f1692ff8-215a-445f-94fd-0a056dad1114": Phase="Running", Reason="", readiness=false. Elapsed: 2.004417985s
Jan 23 13:27:14.514: INFO: Pod "pod-projected-configmaps-f1692ff8-215a-445f-94fd-0a056dad1114": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005600373s
STEP: Saw pod success 01/23/24 13:27:14.514
Jan 23 13:27:14.514: INFO: Pod "pod-projected-configmaps-f1692ff8-215a-445f-94fd-0a056dad1114" satisfied condition "Succeeded or Failed"
Jan 23 13:27:14.516: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-configmaps-f1692ff8-215a-445f-94fd-0a056dad1114 container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/23/24 13:27:14.52
Jan 23 13:27:14.524: INFO: Waiting for pod pod-projected-configmaps-f1692ff8-215a-445f-94fd-0a056dad1114 to disappear
Jan 23 13:27:14.525: INFO: Pod pod-projected-configmaps-f1692ff8-215a-445f-94fd-0a056dad1114 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 23 13:27:14.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9519" for this suite. 01/23/24 13:27:14.527
------------------------------
• [4.054 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:27:10.476
    Jan 23 13:27:10.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 13:27:10.477
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:27:10.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:27:10.488
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-6043a03e-1ca0-4522-8d2b-1003689f7171 01/23/24 13:27:10.49
    STEP: Creating a pod to test consume configMaps 01/23/24 13:27:10.495
    Jan 23 13:27:10.509: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f1692ff8-215a-445f-94fd-0a056dad1114" in namespace "projected-9519" to be "Succeeded or Failed"
    Jan 23 13:27:10.510: INFO: Pod "pod-projected-configmaps-f1692ff8-215a-445f-94fd-0a056dad1114": Phase="Pending", Reason="", readiness=false. Elapsed: 1.483906ms
    Jan 23 13:27:12.513: INFO: Pod "pod-projected-configmaps-f1692ff8-215a-445f-94fd-0a056dad1114": Phase="Running", Reason="", readiness=false. Elapsed: 2.004417985s
    Jan 23 13:27:14.514: INFO: Pod "pod-projected-configmaps-f1692ff8-215a-445f-94fd-0a056dad1114": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005600373s
    STEP: Saw pod success 01/23/24 13:27:14.514
    Jan 23 13:27:14.514: INFO: Pod "pod-projected-configmaps-f1692ff8-215a-445f-94fd-0a056dad1114" satisfied condition "Succeeded or Failed"
    Jan 23 13:27:14.516: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-configmaps-f1692ff8-215a-445f-94fd-0a056dad1114 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/23/24 13:27:14.52
    Jan 23 13:27:14.524: INFO: Waiting for pod pod-projected-configmaps-f1692ff8-215a-445f-94fd-0a056dad1114 to disappear
    Jan 23 13:27:14.525: INFO: Pod pod-projected-configmaps-f1692ff8-215a-445f-94fd-0a056dad1114 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:27:14.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9519" for this suite. 01/23/24 13:27:14.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:27:14.531
Jan 23 13:27:14.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename statefulset 01/23/24 13:27:14.532
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:27:14.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:27:14.541
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4431 01/23/24 13:27:14.543
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 01/23/24 13:27:14.548
Jan 23 13:27:14.554: INFO: Found 0 stateful pods, waiting for 3
Jan 23 13:27:24.558: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 13:27:24.558: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 13:27:24.558: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/23/24 13:27:24.562
Jan 23 13:27:24.579: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/23/24 13:27:24.579
STEP: Not applying an update when the partition is greater than the number of replicas 01/23/24 13:27:34.592
STEP: Performing a canary update 01/23/24 13:27:34.593
Jan 23 13:27:34.611: INFO: Updating stateful set ss2
Jan 23 13:27:34.617: INFO: Waiting for Pod statefulset-4431/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 01/23/24 13:27:44.626
Jan 23 13:27:44.657: INFO: Found 1 stateful pods, waiting for 3
Jan 23 13:27:54.661: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 13:27:54.661: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 13:27:54.661: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/23/24 13:27:54.665
Jan 23 13:27:54.681: INFO: Updating stateful set ss2
Jan 23 13:27:54.685: INFO: Waiting for Pod statefulset-4431/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Jan 23 13:28:04.706: INFO: Updating stateful set ss2
Jan 23 13:28:04.710: INFO: Waiting for StatefulSet statefulset-4431/ss2 to complete update
Jan 23 13:28:04.710: INFO: Waiting for Pod statefulset-4431/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 23 13:28:14.716: INFO: Deleting all statefulset in ns statefulset-4431
Jan 23 13:28:14.717: INFO: Scaling statefulset ss2 to 0
Jan 23 13:28:24.733: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 13:28:24.735: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 23 13:28:24.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4431" for this suite. 01/23/24 13:28:24.747
------------------------------
• [SLOW TEST] [70.227 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:27:14.531
    Jan 23 13:27:14.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename statefulset 01/23/24 13:27:14.532
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:27:14.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:27:14.541
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4431 01/23/24 13:27:14.543
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 01/23/24 13:27:14.548
    Jan 23 13:27:14.554: INFO: Found 0 stateful pods, waiting for 3
    Jan 23 13:27:24.558: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 23 13:27:24.558: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 23 13:27:24.558: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/23/24 13:27:24.562
    Jan 23 13:27:24.579: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/23/24 13:27:24.579
    STEP: Not applying an update when the partition is greater than the number of replicas 01/23/24 13:27:34.592
    STEP: Performing a canary update 01/23/24 13:27:34.593
    Jan 23 13:27:34.611: INFO: Updating stateful set ss2
    Jan 23 13:27:34.617: INFO: Waiting for Pod statefulset-4431/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 01/23/24 13:27:44.626
    Jan 23 13:27:44.657: INFO: Found 1 stateful pods, waiting for 3
    Jan 23 13:27:54.661: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 23 13:27:54.661: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 23 13:27:54.661: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/23/24 13:27:54.665
    Jan 23 13:27:54.681: INFO: Updating stateful set ss2
    Jan 23 13:27:54.685: INFO: Waiting for Pod statefulset-4431/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Jan 23 13:28:04.706: INFO: Updating stateful set ss2
    Jan 23 13:28:04.710: INFO: Waiting for StatefulSet statefulset-4431/ss2 to complete update
    Jan 23 13:28:04.710: INFO: Waiting for Pod statefulset-4431/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 23 13:28:14.716: INFO: Deleting all statefulset in ns statefulset-4431
    Jan 23 13:28:14.717: INFO: Scaling statefulset ss2 to 0
    Jan 23 13:28:24.733: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 23 13:28:24.735: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:28:24.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4431" for this suite. 01/23/24 13:28:24.747
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:28:24.759
Jan 23 13:28:24.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 13:28:24.762
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:28:24.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:28:24.774
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 01/23/24 13:28:24.777
Jan 23 13:28:24.794: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0515b0bc-83bb-4f71-a501-00c9074e3b58" in namespace "projected-2566" to be "Succeeded or Failed"
Jan 23 13:28:24.797: INFO: Pod "downwardapi-volume-0515b0bc-83bb-4f71-a501-00c9074e3b58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.552143ms
Jan 23 13:28:26.800: INFO: Pod "downwardapi-volume-0515b0bc-83bb-4f71-a501-00c9074e3b58": Phase="Running", Reason="", readiness=false. Elapsed: 2.0057576s
Jan 23 13:28:28.800: INFO: Pod "downwardapi-volume-0515b0bc-83bb-4f71-a501-00c9074e3b58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005786626s
STEP: Saw pod success 01/23/24 13:28:28.8
Jan 23 13:28:28.800: INFO: Pod "downwardapi-volume-0515b0bc-83bb-4f71-a501-00c9074e3b58" satisfied condition "Succeeded or Failed"
Jan 23 13:28:28.802: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-0515b0bc-83bb-4f71-a501-00c9074e3b58 container client-container: <nil>
STEP: delete the pod 01/23/24 13:28:28.808
Jan 23 13:28:28.814: INFO: Waiting for pod downwardapi-volume-0515b0bc-83bb-4f71-a501-00c9074e3b58 to disappear
Jan 23 13:28:28.816: INFO: Pod downwardapi-volume-0515b0bc-83bb-4f71-a501-00c9074e3b58 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 23 13:28:28.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2566" for this suite. 01/23/24 13:28:28.818
------------------------------
• [4.063 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:28:24.759
    Jan 23 13:28:24.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 13:28:24.762
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:28:24.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:28:24.774
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 01/23/24 13:28:24.777
    Jan 23 13:28:24.794: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0515b0bc-83bb-4f71-a501-00c9074e3b58" in namespace "projected-2566" to be "Succeeded or Failed"
    Jan 23 13:28:24.797: INFO: Pod "downwardapi-volume-0515b0bc-83bb-4f71-a501-00c9074e3b58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.552143ms
    Jan 23 13:28:26.800: INFO: Pod "downwardapi-volume-0515b0bc-83bb-4f71-a501-00c9074e3b58": Phase="Running", Reason="", readiness=false. Elapsed: 2.0057576s
    Jan 23 13:28:28.800: INFO: Pod "downwardapi-volume-0515b0bc-83bb-4f71-a501-00c9074e3b58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005786626s
    STEP: Saw pod success 01/23/24 13:28:28.8
    Jan 23 13:28:28.800: INFO: Pod "downwardapi-volume-0515b0bc-83bb-4f71-a501-00c9074e3b58" satisfied condition "Succeeded or Failed"
    Jan 23 13:28:28.802: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-0515b0bc-83bb-4f71-a501-00c9074e3b58 container client-container: <nil>
    STEP: delete the pod 01/23/24 13:28:28.808
    Jan 23 13:28:28.814: INFO: Waiting for pod downwardapi-volume-0515b0bc-83bb-4f71-a501-00c9074e3b58 to disappear
    Jan 23 13:28:28.816: INFO: Pod downwardapi-volume-0515b0bc-83bb-4f71-a501-00c9074e3b58 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:28:28.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2566" for this suite. 01/23/24 13:28:28.818
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:28:28.822
Jan 23 13:28:28.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename downward-api 01/23/24 13:28:28.823
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:28:28.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:28:28.832
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 01/23/24 13:28:28.834
Jan 23 13:28:28.849: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78db66d3-3487-4572-9810-065fc939def9" in namespace "downward-api-1906" to be "Succeeded or Failed"
Jan 23 13:28:28.851: INFO: Pod "downwardapi-volume-78db66d3-3487-4572-9810-065fc939def9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.960473ms
Jan 23 13:28:30.855: INFO: Pod "downwardapi-volume-78db66d3-3487-4572-9810-065fc939def9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005302116s
Jan 23 13:28:32.856: INFO: Pod "downwardapi-volume-78db66d3-3487-4572-9810-065fc939def9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006117109s
STEP: Saw pod success 01/23/24 13:28:32.856
Jan 23 13:28:32.856: INFO: Pod "downwardapi-volume-78db66d3-3487-4572-9810-065fc939def9" satisfied condition "Succeeded or Failed"
Jan 23 13:28:32.857: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-78db66d3-3487-4572-9810-065fc939def9 container client-container: <nil>
STEP: delete the pod 01/23/24 13:28:32.861
Jan 23 13:28:32.868: INFO: Waiting for pod downwardapi-volume-78db66d3-3487-4572-9810-065fc939def9 to disappear
Jan 23 13:28:32.870: INFO: Pod downwardapi-volume-78db66d3-3487-4572-9810-065fc939def9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 23 13:28:32.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1906" for this suite. 01/23/24 13:28:32.872
------------------------------
• [4.054 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:28:28.822
    Jan 23 13:28:28.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename downward-api 01/23/24 13:28:28.823
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:28:28.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:28:28.832
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 01/23/24 13:28:28.834
    Jan 23 13:28:28.849: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78db66d3-3487-4572-9810-065fc939def9" in namespace "downward-api-1906" to be "Succeeded or Failed"
    Jan 23 13:28:28.851: INFO: Pod "downwardapi-volume-78db66d3-3487-4572-9810-065fc939def9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.960473ms
    Jan 23 13:28:30.855: INFO: Pod "downwardapi-volume-78db66d3-3487-4572-9810-065fc939def9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005302116s
    Jan 23 13:28:32.856: INFO: Pod "downwardapi-volume-78db66d3-3487-4572-9810-065fc939def9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006117109s
    STEP: Saw pod success 01/23/24 13:28:32.856
    Jan 23 13:28:32.856: INFO: Pod "downwardapi-volume-78db66d3-3487-4572-9810-065fc939def9" satisfied condition "Succeeded or Failed"
    Jan 23 13:28:32.857: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-78db66d3-3487-4572-9810-065fc939def9 container client-container: <nil>
    STEP: delete the pod 01/23/24 13:28:32.861
    Jan 23 13:28:32.868: INFO: Waiting for pod downwardapi-volume-78db66d3-3487-4572-9810-065fc939def9 to disappear
    Jan 23 13:28:32.870: INFO: Pod downwardapi-volume-78db66d3-3487-4572-9810-065fc939def9 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:28:32.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1906" for this suite. 01/23/24 13:28:32.872
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:28:32.876
Jan 23 13:28:32.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename crd-publish-openapi 01/23/24 13:28:32.876
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:28:32.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:28:32.885
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/23/24 13:28:32.887
Jan 23 13:28:32.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 13:28:40.362: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:28:54.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6633" for this suite. 01/23/24 13:28:54.608
------------------------------
• [SLOW TEST] [21.735 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:28:32.876
    Jan 23 13:28:32.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename crd-publish-openapi 01/23/24 13:28:32.876
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:28:32.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:28:32.885
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/23/24 13:28:32.887
    Jan 23 13:28:32.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 13:28:40.362: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:28:54.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6633" for this suite. 01/23/24 13:28:54.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:28:54.612
Jan 23 13:28:54.612: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 13:28:54.613
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:28:54.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:28:54.621
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-d7cb84fd-dc85-444c-aa3a-d2b63128a50e 01/23/24 13:28:54.623
STEP: Creating a pod to test consume configMaps 01/23/24 13:28:54.626
Jan 23 13:28:54.639: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3b4457f2-fd93-4e45-ae22-a415c3dc1d3d" in namespace "projected-4876" to be "Succeeded or Failed"
Jan 23 13:28:54.641: INFO: Pod "pod-projected-configmaps-3b4457f2-fd93-4e45-ae22-a415c3dc1d3d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.918967ms
Jan 23 13:28:56.645: INFO: Pod "pod-projected-configmaps-3b4457f2-fd93-4e45-ae22-a415c3dc1d3d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005735837s
Jan 23 13:28:58.644: INFO: Pod "pod-projected-configmaps-3b4457f2-fd93-4e45-ae22-a415c3dc1d3d": Phase="Running", Reason="", readiness=false. Elapsed: 4.004824847s
Jan 23 13:29:00.644: INFO: Pod "pod-projected-configmaps-3b4457f2-fd93-4e45-ae22-a415c3dc1d3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005439706s
STEP: Saw pod success 01/23/24 13:29:00.645
Jan 23 13:29:00.645: INFO: Pod "pod-projected-configmaps-3b4457f2-fd93-4e45-ae22-a415c3dc1d3d" satisfied condition "Succeeded or Failed"
Jan 23 13:29:00.646: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-configmaps-3b4457f2-fd93-4e45-ae22-a415c3dc1d3d container agnhost-container: <nil>
STEP: delete the pod 01/23/24 13:29:00.65
Jan 23 13:29:00.656: INFO: Waiting for pod pod-projected-configmaps-3b4457f2-fd93-4e45-ae22-a415c3dc1d3d to disappear
Jan 23 13:29:00.658: INFO: Pod pod-projected-configmaps-3b4457f2-fd93-4e45-ae22-a415c3dc1d3d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 23 13:29:00.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4876" for this suite. 01/23/24 13:29:00.66
------------------------------
• [SLOW TEST] [6.051 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:28:54.612
    Jan 23 13:28:54.612: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 13:28:54.613
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:28:54.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:28:54.621
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-d7cb84fd-dc85-444c-aa3a-d2b63128a50e 01/23/24 13:28:54.623
    STEP: Creating a pod to test consume configMaps 01/23/24 13:28:54.626
    Jan 23 13:28:54.639: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3b4457f2-fd93-4e45-ae22-a415c3dc1d3d" in namespace "projected-4876" to be "Succeeded or Failed"
    Jan 23 13:28:54.641: INFO: Pod "pod-projected-configmaps-3b4457f2-fd93-4e45-ae22-a415c3dc1d3d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.918967ms
    Jan 23 13:28:56.645: INFO: Pod "pod-projected-configmaps-3b4457f2-fd93-4e45-ae22-a415c3dc1d3d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005735837s
    Jan 23 13:28:58.644: INFO: Pod "pod-projected-configmaps-3b4457f2-fd93-4e45-ae22-a415c3dc1d3d": Phase="Running", Reason="", readiness=false. Elapsed: 4.004824847s
    Jan 23 13:29:00.644: INFO: Pod "pod-projected-configmaps-3b4457f2-fd93-4e45-ae22-a415c3dc1d3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005439706s
    STEP: Saw pod success 01/23/24 13:29:00.645
    Jan 23 13:29:00.645: INFO: Pod "pod-projected-configmaps-3b4457f2-fd93-4e45-ae22-a415c3dc1d3d" satisfied condition "Succeeded or Failed"
    Jan 23 13:29:00.646: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-configmaps-3b4457f2-fd93-4e45-ae22-a415c3dc1d3d container agnhost-container: <nil>
    STEP: delete the pod 01/23/24 13:29:00.65
    Jan 23 13:29:00.656: INFO: Waiting for pod pod-projected-configmaps-3b4457f2-fd93-4e45-ae22-a415c3dc1d3d to disappear
    Jan 23 13:29:00.658: INFO: Pod pod-projected-configmaps-3b4457f2-fd93-4e45-ae22-a415c3dc1d3d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:29:00.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4876" for this suite. 01/23/24 13:29:00.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:29:00.664
Jan 23 13:29:00.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename pod-network-test 01/23/24 13:29:00.664
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:29:00.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:29:00.673
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-3512 01/23/24 13:29:00.675
STEP: creating a selector 01/23/24 13:29:00.675
STEP: Creating the service pods in kubernetes 01/23/24 13:29:00.675
Jan 23 13:29:00.675: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 23 13:29:00.714: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3512" to be "running and ready"
Jan 23 13:29:00.715: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.395201ms
Jan 23 13:29:00.715: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 13:29:02.718: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004224049s
Jan 23 13:29:02.718: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:29:04.719: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005145784s
Jan 23 13:29:04.719: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:29:06.719: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004534237s
Jan 23 13:29:06.719: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:29:08.718: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004342401s
Jan 23 13:29:08.718: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:29:10.718: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004459082s
Jan 23 13:29:10.718: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:29:12.720: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.00600699s
Jan 23 13:29:12.720: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 23 13:29:12.720: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 23 13:29:12.722: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3512" to be "running and ready"
Jan 23 13:29:12.723: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.402998ms
Jan 23 13:29:12.723: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 23 13:29:12.723: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/23/24 13:29:12.724
Jan 23 13:29:12.746: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3512" to be "running"
Jan 23 13:29:12.749: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.486675ms
Jan 23 13:29:14.752: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005943043s
Jan 23 13:29:14.752: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 23 13:29:14.754: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3512" to be "running"
Jan 23 13:29:14.755: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.319758ms
Jan 23 13:29:14.755: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 23 13:29:14.756: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 23 13:29:14.756: INFO: Going to poll 10.233.75.252 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 23 13:29:14.758: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.75.252:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3512 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 13:29:14.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 13:29:14.758: INFO: ExecWithOptions: Clientset creation
Jan 23 13:29:14.758: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3512/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.75.252%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 23 13:29:14.836: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 23 13:29:14.836: INFO: Going to poll 10.233.87.26 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 23 13:29:14.838: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.87.26:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3512 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 13:29:14.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 13:29:14.839: INFO: ExecWithOptions: Clientset creation
Jan 23 13:29:14.839: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3512/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.87.26%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 23 13:29:14.902: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 23 13:29:14.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3512" for this suite. 01/23/24 13:29:14.905
------------------------------
• [SLOW TEST] [14.245 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:29:00.664
    Jan 23 13:29:00.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename pod-network-test 01/23/24 13:29:00.664
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:29:00.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:29:00.673
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-3512 01/23/24 13:29:00.675
    STEP: creating a selector 01/23/24 13:29:00.675
    STEP: Creating the service pods in kubernetes 01/23/24 13:29:00.675
    Jan 23 13:29:00.675: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 23 13:29:00.714: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3512" to be "running and ready"
    Jan 23 13:29:00.715: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.395201ms
    Jan 23 13:29:00.715: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 13:29:02.718: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004224049s
    Jan 23 13:29:02.718: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:29:04.719: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005145784s
    Jan 23 13:29:04.719: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:29:06.719: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004534237s
    Jan 23 13:29:06.719: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:29:08.718: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004342401s
    Jan 23 13:29:08.718: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:29:10.718: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004459082s
    Jan 23 13:29:10.718: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:29:12.720: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.00600699s
    Jan 23 13:29:12.720: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 23 13:29:12.720: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 23 13:29:12.722: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3512" to be "running and ready"
    Jan 23 13:29:12.723: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.402998ms
    Jan 23 13:29:12.723: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 23 13:29:12.723: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/23/24 13:29:12.724
    Jan 23 13:29:12.746: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3512" to be "running"
    Jan 23 13:29:12.749: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.486675ms
    Jan 23 13:29:14.752: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005943043s
    Jan 23 13:29:14.752: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 23 13:29:14.754: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3512" to be "running"
    Jan 23 13:29:14.755: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.319758ms
    Jan 23 13:29:14.755: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 23 13:29:14.756: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 23 13:29:14.756: INFO: Going to poll 10.233.75.252 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 23 13:29:14.758: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.75.252:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3512 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 13:29:14.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 13:29:14.758: INFO: ExecWithOptions: Clientset creation
    Jan 23 13:29:14.758: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3512/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.75.252%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 23 13:29:14.836: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 23 13:29:14.836: INFO: Going to poll 10.233.87.26 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 23 13:29:14.838: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.87.26:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3512 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 13:29:14.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 13:29:14.839: INFO: ExecWithOptions: Clientset creation
    Jan 23 13:29:14.839: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3512/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.87.26%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 23 13:29:14.902: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:29:14.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3512" for this suite. 01/23/24 13:29:14.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:29:14.909
Jan 23 13:29:14.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename gc 01/23/24 13:29:14.91
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:29:14.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:29:14.92
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/23/24 13:29:14.922
STEP: Wait for the Deployment to create new ReplicaSet 01/23/24 13:29:14.925
STEP: delete the deployment 01/23/24 13:29:15.431
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/23/24 13:29:15.442
STEP: Gathering metrics 01/23/24 13:29:15.953
Jan 23 13:29:15.970: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" in namespace "kube-system" to be "running and ready"
Jan 23 13:29:15.972: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local": Phase="Running", Reason="", readiness=true. Elapsed: 1.843794ms
Jan 23 13:29:15.972: INFO: The phase of Pod kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local is Running (Ready = true)
Jan 23 13:29:15.972: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" satisfied condition "running and ready"
Jan 23 13:29:16.016: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 23 13:29:16.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4004" for this suite. 01/23/24 13:29:16.019
------------------------------
• [1.114 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:29:14.909
    Jan 23 13:29:14.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename gc 01/23/24 13:29:14.91
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:29:14.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:29:14.92
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/23/24 13:29:14.922
    STEP: Wait for the Deployment to create new ReplicaSet 01/23/24 13:29:14.925
    STEP: delete the deployment 01/23/24 13:29:15.431
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/23/24 13:29:15.442
    STEP: Gathering metrics 01/23/24 13:29:15.953
    Jan 23 13:29:15.970: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" in namespace "kube-system" to be "running and ready"
    Jan 23 13:29:15.972: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local": Phase="Running", Reason="", readiness=true. Elapsed: 1.843794ms
    Jan 23 13:29:15.972: INFO: The phase of Pod kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local is Running (Ready = true)
    Jan 23 13:29:15.972: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" satisfied condition "running and ready"
    Jan 23 13:29:16.016: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:29:16.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4004" for this suite. 01/23/24 13:29:16.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:29:16.024
Jan 23 13:29:16.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename dns 01/23/24 13:29:16.025
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:29:16.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:29:16.035
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/23/24 13:29:16.037
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6669.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6669.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/23/24 13:29:16.041
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6669.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6669.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/23/24 13:29:16.041
STEP: creating a pod to probe DNS 01/23/24 13:29:16.041
STEP: submitting the pod to kubernetes 01/23/24 13:29:16.041
Jan 23 13:29:16.073: INFO: Waiting up to 15m0s for pod "dns-test-3a6cae29-27e0-4495-b170-3777f107498d" in namespace "dns-6669" to be "running"
Jan 23 13:29:16.075: INFO: Pod "dns-test-3a6cae29-27e0-4495-b170-3777f107498d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.666527ms
Jan 23 13:29:18.078: INFO: Pod "dns-test-3a6cae29-27e0-4495-b170-3777f107498d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004967162s
Jan 23 13:29:18.078: INFO: Pod "dns-test-3a6cae29-27e0-4495-b170-3777f107498d" satisfied condition "running"
STEP: retrieving the pod 01/23/24 13:29:18.078
STEP: looking for the results for each expected name from probers 01/23/24 13:29:18.08
Jan 23 13:29:18.088: INFO: DNS probes using dns-6669/dns-test-3a6cae29-27e0-4495-b170-3777f107498d succeeded

STEP: deleting the pod 01/23/24 13:29:18.088
STEP: deleting the test headless service 01/23/24 13:29:18.096
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 23 13:29:18.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6669" for this suite. 01/23/24 13:29:18.105
------------------------------
• [2.085 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:29:16.024
    Jan 23 13:29:16.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename dns 01/23/24 13:29:16.025
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:29:16.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:29:16.035
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/23/24 13:29:16.037
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6669.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6669.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/23/24 13:29:16.041
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6669.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6669.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/23/24 13:29:16.041
    STEP: creating a pod to probe DNS 01/23/24 13:29:16.041
    STEP: submitting the pod to kubernetes 01/23/24 13:29:16.041
    Jan 23 13:29:16.073: INFO: Waiting up to 15m0s for pod "dns-test-3a6cae29-27e0-4495-b170-3777f107498d" in namespace "dns-6669" to be "running"
    Jan 23 13:29:16.075: INFO: Pod "dns-test-3a6cae29-27e0-4495-b170-3777f107498d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.666527ms
    Jan 23 13:29:18.078: INFO: Pod "dns-test-3a6cae29-27e0-4495-b170-3777f107498d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004967162s
    Jan 23 13:29:18.078: INFO: Pod "dns-test-3a6cae29-27e0-4495-b170-3777f107498d" satisfied condition "running"
    STEP: retrieving the pod 01/23/24 13:29:18.078
    STEP: looking for the results for each expected name from probers 01/23/24 13:29:18.08
    Jan 23 13:29:18.088: INFO: DNS probes using dns-6669/dns-test-3a6cae29-27e0-4495-b170-3777f107498d succeeded

    STEP: deleting the pod 01/23/24 13:29:18.088
    STEP: deleting the test headless service 01/23/24 13:29:18.096
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:29:18.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6669" for this suite. 01/23/24 13:29:18.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:29:18.11
Jan 23 13:29:18.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename emptydir 01/23/24 13:29:18.111
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:29:18.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:29:18.12
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/23/24 13:29:18.122
Jan 23 13:29:18.137: INFO: Waiting up to 5m0s for pod "pod-db30b35c-75d9-479b-bb5e-1f22ef06e41d" in namespace "emptydir-2197" to be "Succeeded or Failed"
Jan 23 13:29:18.140: INFO: Pod "pod-db30b35c-75d9-479b-bb5e-1f22ef06e41d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.786895ms
Jan 23 13:29:20.142: INFO: Pod "pod-db30b35c-75d9-479b-bb5e-1f22ef06e41d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004531317s
Jan 23 13:29:22.143: INFO: Pod "pod-db30b35c-75d9-479b-bb5e-1f22ef06e41d": Phase="Running", Reason="", readiness=false. Elapsed: 4.005478784s
Jan 23 13:29:24.143: INFO: Pod "pod-db30b35c-75d9-479b-bb5e-1f22ef06e41d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005966656s
STEP: Saw pod success 01/23/24 13:29:24.143
Jan 23 13:29:24.144: INFO: Pod "pod-db30b35c-75d9-479b-bb5e-1f22ef06e41d" satisfied condition "Succeeded or Failed"
Jan 23 13:29:24.145: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-db30b35c-75d9-479b-bb5e-1f22ef06e41d container test-container: <nil>
STEP: delete the pod 01/23/24 13:29:24.149
Jan 23 13:29:24.157: INFO: Waiting for pod pod-db30b35c-75d9-479b-bb5e-1f22ef06e41d to disappear
Jan 23 13:29:24.162: INFO: Pod pod-db30b35c-75d9-479b-bb5e-1f22ef06e41d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 23 13:29:24.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2197" for this suite. 01/23/24 13:29:24.165
------------------------------
• [SLOW TEST] [6.059 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:29:18.11
    Jan 23 13:29:18.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename emptydir 01/23/24 13:29:18.111
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:29:18.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:29:18.12
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/23/24 13:29:18.122
    Jan 23 13:29:18.137: INFO: Waiting up to 5m0s for pod "pod-db30b35c-75d9-479b-bb5e-1f22ef06e41d" in namespace "emptydir-2197" to be "Succeeded or Failed"
    Jan 23 13:29:18.140: INFO: Pod "pod-db30b35c-75d9-479b-bb5e-1f22ef06e41d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.786895ms
    Jan 23 13:29:20.142: INFO: Pod "pod-db30b35c-75d9-479b-bb5e-1f22ef06e41d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004531317s
    Jan 23 13:29:22.143: INFO: Pod "pod-db30b35c-75d9-479b-bb5e-1f22ef06e41d": Phase="Running", Reason="", readiness=false. Elapsed: 4.005478784s
    Jan 23 13:29:24.143: INFO: Pod "pod-db30b35c-75d9-479b-bb5e-1f22ef06e41d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005966656s
    STEP: Saw pod success 01/23/24 13:29:24.143
    Jan 23 13:29:24.144: INFO: Pod "pod-db30b35c-75d9-479b-bb5e-1f22ef06e41d" satisfied condition "Succeeded or Failed"
    Jan 23 13:29:24.145: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-db30b35c-75d9-479b-bb5e-1f22ef06e41d container test-container: <nil>
    STEP: delete the pod 01/23/24 13:29:24.149
    Jan 23 13:29:24.157: INFO: Waiting for pod pod-db30b35c-75d9-479b-bb5e-1f22ef06e41d to disappear
    Jan 23 13:29:24.162: INFO: Pod pod-db30b35c-75d9-479b-bb5e-1f22ef06e41d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:29:24.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2197" for this suite. 01/23/24 13:29:24.165
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:29:24.169
Jan 23 13:29:24.169: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename container-probe 01/23/24 13:29:24.17
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:29:24.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:29:24.179
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-b77c7c27-98fc-46c0-821a-6d9957c62868 in namespace container-probe-6517 01/23/24 13:29:24.18
Jan 23 13:29:24.196: INFO: Waiting up to 5m0s for pod "test-webserver-b77c7c27-98fc-46c0-821a-6d9957c62868" in namespace "container-probe-6517" to be "not pending"
Jan 23 13:29:24.198: INFO: Pod "test-webserver-b77c7c27-98fc-46c0-821a-6d9957c62868": Phase="Pending", Reason="", readiness=false. Elapsed: 2.260489ms
Jan 23 13:29:26.202: INFO: Pod "test-webserver-b77c7c27-98fc-46c0-821a-6d9957c62868": Phase="Running", Reason="", readiness=true. Elapsed: 2.005578943s
Jan 23 13:29:26.202: INFO: Pod "test-webserver-b77c7c27-98fc-46c0-821a-6d9957c62868" satisfied condition "not pending"
Jan 23 13:29:26.202: INFO: Started pod test-webserver-b77c7c27-98fc-46c0-821a-6d9957c62868 in namespace container-probe-6517
STEP: checking the pod's current state and verifying that restartCount is present 01/23/24 13:29:26.202
Jan 23 13:29:26.203: INFO: Initial restart count of pod test-webserver-b77c7c27-98fc-46c0-821a-6d9957c62868 is 0
STEP: deleting the pod 01/23/24 13:33:26.604
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 23 13:33:26.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6517" for this suite. 01/23/24 13:33:26.615
------------------------------
• [SLOW TEST] [242.449 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:29:24.169
    Jan 23 13:29:24.169: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename container-probe 01/23/24 13:29:24.17
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:29:24.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:29:24.179
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-b77c7c27-98fc-46c0-821a-6d9957c62868 in namespace container-probe-6517 01/23/24 13:29:24.18
    Jan 23 13:29:24.196: INFO: Waiting up to 5m0s for pod "test-webserver-b77c7c27-98fc-46c0-821a-6d9957c62868" in namespace "container-probe-6517" to be "not pending"
    Jan 23 13:29:24.198: INFO: Pod "test-webserver-b77c7c27-98fc-46c0-821a-6d9957c62868": Phase="Pending", Reason="", readiness=false. Elapsed: 2.260489ms
    Jan 23 13:29:26.202: INFO: Pod "test-webserver-b77c7c27-98fc-46c0-821a-6d9957c62868": Phase="Running", Reason="", readiness=true. Elapsed: 2.005578943s
    Jan 23 13:29:26.202: INFO: Pod "test-webserver-b77c7c27-98fc-46c0-821a-6d9957c62868" satisfied condition "not pending"
    Jan 23 13:29:26.202: INFO: Started pod test-webserver-b77c7c27-98fc-46c0-821a-6d9957c62868 in namespace container-probe-6517
    STEP: checking the pod's current state and verifying that restartCount is present 01/23/24 13:29:26.202
    Jan 23 13:29:26.203: INFO: Initial restart count of pod test-webserver-b77c7c27-98fc-46c0-821a-6d9957c62868 is 0
    STEP: deleting the pod 01/23/24 13:33:26.604
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:33:26.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6517" for this suite. 01/23/24 13:33:26.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:33:26.619
Jan 23 13:33:26.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename services 01/23/24 13:33:26.62
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:33:26.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:33:26.629
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-2281 01/23/24 13:33:26.633
STEP: creating service affinity-clusterip-transition in namespace services-2281 01/23/24 13:33:26.633
STEP: creating replication controller affinity-clusterip-transition in namespace services-2281 01/23/24 13:33:26.643
I0123 13:33:26.651411      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2281, replica count: 3
I0123 13:33:29.702693      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 23 13:33:29.707: INFO: Creating new exec pod
Jan 23 13:33:29.717: INFO: Waiting up to 5m0s for pod "execpod-affinityrrgs9" in namespace "services-2281" to be "running"
Jan 23 13:33:29.719: INFO: Pod "execpod-affinityrrgs9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.658002ms
Jan 23 13:33:31.721: INFO: Pod "execpod-affinityrrgs9": Phase="Running", Reason="", readiness=true. Elapsed: 2.003738068s
Jan 23 13:33:31.721: INFO: Pod "execpod-affinityrrgs9" satisfied condition "running"
Jan 23 13:33:32.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-2281 exec execpod-affinityrrgs9 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jan 23 13:33:32.904: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 23 13:33:32.904: INFO: stdout: ""
Jan 23 13:33:32.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-2281 exec execpod-affinityrrgs9 -- /bin/sh -x -c nc -v -z -w 2 10.233.14.187 80'
Jan 23 13:33:33.077: INFO: stderr: "+ nc -v -z -w 2 10.233.14.187 80\nConnection to 10.233.14.187 80 port [tcp/http] succeeded!\n"
Jan 23 13:33:33.077: INFO: stdout: ""
Jan 23 13:33:33.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-2281 exec execpod-affinityrrgs9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.14.187:80/ ; done'
Jan 23 13:33:33.304: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n"
Jan 23 13:33:33.304: INFO: stdout: "\naffinity-clusterip-transition-87ffp\naffinity-clusterip-transition-qrgt2\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-87ffp\naffinity-clusterip-transition-qrgt2\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-87ffp\naffinity-clusterip-transition-qrgt2\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-87ffp\naffinity-clusterip-transition-qrgt2\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-87ffp\naffinity-clusterip-transition-qrgt2\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-87ffp"
Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-87ffp
Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-qrgt2
Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-87ffp
Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-qrgt2
Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-87ffp
Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-qrgt2
Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-87ffp
Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-qrgt2
Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-87ffp
Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-qrgt2
Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-87ffp
Jan 23 13:33:33.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-2281 exec execpod-affinityrrgs9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.14.187:80/ ; done'
Jan 23 13:33:33.535: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n"
Jan 23 13:33:33.535: INFO: stdout: "\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4"
Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
Jan 23 13:33:33.535: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2281, will wait for the garbage collector to delete the pods 01/23/24 13:33:33.544
Jan 23 13:33:33.603: INFO: Deleting ReplicationController affinity-clusterip-transition took: 4.784903ms
Jan 23 13:33:33.704: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.798426ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 23 13:33:36.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2281" for this suite. 01/23/24 13:33:36.616
------------------------------
• [SLOW TEST] [10.000 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:33:26.619
    Jan 23 13:33:26.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename services 01/23/24 13:33:26.62
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:33:26.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:33:26.629
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-2281 01/23/24 13:33:26.633
    STEP: creating service affinity-clusterip-transition in namespace services-2281 01/23/24 13:33:26.633
    STEP: creating replication controller affinity-clusterip-transition in namespace services-2281 01/23/24 13:33:26.643
    I0123 13:33:26.651411      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2281, replica count: 3
    I0123 13:33:29.702693      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 23 13:33:29.707: INFO: Creating new exec pod
    Jan 23 13:33:29.717: INFO: Waiting up to 5m0s for pod "execpod-affinityrrgs9" in namespace "services-2281" to be "running"
    Jan 23 13:33:29.719: INFO: Pod "execpod-affinityrrgs9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.658002ms
    Jan 23 13:33:31.721: INFO: Pod "execpod-affinityrrgs9": Phase="Running", Reason="", readiness=true. Elapsed: 2.003738068s
    Jan 23 13:33:31.721: INFO: Pod "execpod-affinityrrgs9" satisfied condition "running"
    Jan 23 13:33:32.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-2281 exec execpod-affinityrrgs9 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jan 23 13:33:32.904: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan 23 13:33:32.904: INFO: stdout: ""
    Jan 23 13:33:32.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-2281 exec execpod-affinityrrgs9 -- /bin/sh -x -c nc -v -z -w 2 10.233.14.187 80'
    Jan 23 13:33:33.077: INFO: stderr: "+ nc -v -z -w 2 10.233.14.187 80\nConnection to 10.233.14.187 80 port [tcp/http] succeeded!\n"
    Jan 23 13:33:33.077: INFO: stdout: ""
    Jan 23 13:33:33.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-2281 exec execpod-affinityrrgs9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.14.187:80/ ; done'
    Jan 23 13:33:33.304: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n"
    Jan 23 13:33:33.304: INFO: stdout: "\naffinity-clusterip-transition-87ffp\naffinity-clusterip-transition-qrgt2\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-87ffp\naffinity-clusterip-transition-qrgt2\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-87ffp\naffinity-clusterip-transition-qrgt2\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-87ffp\naffinity-clusterip-transition-qrgt2\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-87ffp\naffinity-clusterip-transition-qrgt2\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-87ffp"
    Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-87ffp
    Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-qrgt2
    Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-87ffp
    Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-qrgt2
    Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-87ffp
    Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-qrgt2
    Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-87ffp
    Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-qrgt2
    Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-87ffp
    Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-qrgt2
    Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.304: INFO: Received response from host: affinity-clusterip-transition-87ffp
    Jan 23 13:33:33.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-2281 exec execpod-affinityrrgs9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.14.187:80/ ; done'
    Jan 23 13:33:33.535: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.187:80/\n"
    Jan 23 13:33:33.535: INFO: stdout: "\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4\naffinity-clusterip-transition-ws5x4"
    Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.535: INFO: Received response from host: affinity-clusterip-transition-ws5x4
    Jan 23 13:33:33.535: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2281, will wait for the garbage collector to delete the pods 01/23/24 13:33:33.544
    Jan 23 13:33:33.603: INFO: Deleting ReplicationController affinity-clusterip-transition took: 4.784903ms
    Jan 23 13:33:33.704: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.798426ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:33:36.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2281" for this suite. 01/23/24 13:33:36.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:33:36.62
Jan 23 13:33:36.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename cronjob 01/23/24 13:33:36.621
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:33:36.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:33:36.632
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/23/24 13:33:36.634
STEP: Ensuring a job is scheduled 01/23/24 13:33:36.638
STEP: Ensuring exactly one is scheduled 01/23/24 13:34:00.641
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/23/24 13:34:00.642
STEP: Ensuring the job is replaced with a new one 01/23/24 13:34:00.644
STEP: Removing cronjob 01/23/24 13:35:00.646
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 23 13:35:00.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9690" for this suite. 01/23/24 13:35:00.652
------------------------------
• [SLOW TEST] [84.036 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:33:36.62
    Jan 23 13:33:36.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename cronjob 01/23/24 13:33:36.621
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:33:36.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:33:36.632
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/23/24 13:33:36.634
    STEP: Ensuring a job is scheduled 01/23/24 13:33:36.638
    STEP: Ensuring exactly one is scheduled 01/23/24 13:34:00.641
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/23/24 13:34:00.642
    STEP: Ensuring the job is replaced with a new one 01/23/24 13:34:00.644
    STEP: Removing cronjob 01/23/24 13:35:00.646
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:35:00.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9690" for this suite. 01/23/24 13:35:00.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:35:00.658
Jan 23 13:35:00.658: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubectl 01/23/24 13:35:00.659
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:35:00.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:35:00.669
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 01/23/24 13:35:00.671
Jan 23 13:35:00.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 create -f -'
Jan 23 13:35:01.779: INFO: stderr: ""
Jan 23 13:35:01.779: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/23/24 13:35:01.779
Jan 23 13:35:01.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 23 13:35:01.928: INFO: stderr: ""
Jan 23 13:35:01.928: INFO: stdout: "update-demo-nautilus-jkh6p update-demo-nautilus-wrqxj "
Jan 23 13:35:01.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-jkh6p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 23 13:35:02.072: INFO: stderr: ""
Jan 23 13:35:02.072: INFO: stdout: ""
Jan 23 13:35:02.072: INFO: update-demo-nautilus-jkh6p is created but not running
Jan 23 13:35:07.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 23 13:35:07.212: INFO: stderr: ""
Jan 23 13:35:07.212: INFO: stdout: "update-demo-nautilus-jkh6p update-demo-nautilus-wrqxj "
Jan 23 13:35:07.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-jkh6p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 23 13:35:07.349: INFO: stderr: ""
Jan 23 13:35:07.349: INFO: stdout: "true"
Jan 23 13:35:07.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-jkh6p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 23 13:35:07.488: INFO: stderr: ""
Jan 23 13:35:07.488: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 23 13:35:07.488: INFO: validating pod update-demo-nautilus-jkh6p
Jan 23 13:35:07.491: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 13:35:07.491: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 13:35:07.491: INFO: update-demo-nautilus-jkh6p is verified up and running
Jan 23 13:35:07.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-wrqxj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 23 13:35:07.630: INFO: stderr: ""
Jan 23 13:35:07.630: INFO: stdout: "true"
Jan 23 13:35:07.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-wrqxj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 23 13:35:07.773: INFO: stderr: ""
Jan 23 13:35:07.773: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 23 13:35:07.773: INFO: validating pod update-demo-nautilus-wrqxj
Jan 23 13:35:07.776: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 13:35:07.776: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 13:35:07.776: INFO: update-demo-nautilus-wrqxj is verified up and running
STEP: scaling down the replication controller 01/23/24 13:35:07.776
Jan 23 13:35:07.778: INFO: scanned /root for discovery docs: <nil>
Jan 23 13:35:07.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 23 13:35:07.962: INFO: stderr: ""
Jan 23 13:35:07.962: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/23/24 13:35:07.962
Jan 23 13:35:07.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 23 13:35:08.107: INFO: stderr: ""
Jan 23 13:35:08.107: INFO: stdout: "update-demo-nautilus-jkh6p update-demo-nautilus-wrqxj "
STEP: Replicas for name=update-demo: expected=1 actual=2 01/23/24 13:35:08.107
Jan 23 13:35:13.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 23 13:35:13.263: INFO: stderr: ""
Jan 23 13:35:13.263: INFO: stdout: "update-demo-nautilus-jkh6p "
Jan 23 13:35:13.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-jkh6p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 23 13:35:13.412: INFO: stderr: ""
Jan 23 13:35:13.412: INFO: stdout: "true"
Jan 23 13:35:13.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-jkh6p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 23 13:35:13.557: INFO: stderr: ""
Jan 23 13:35:13.557: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 23 13:35:13.557: INFO: validating pod update-demo-nautilus-jkh6p
Jan 23 13:35:13.559: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 13:35:13.559: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 13:35:13.559: INFO: update-demo-nautilus-jkh6p is verified up and running
STEP: scaling up the replication controller 01/23/24 13:35:13.559
Jan 23 13:35:13.561: INFO: scanned /root for discovery docs: <nil>
Jan 23 13:35:13.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 23 13:35:13.745: INFO: stderr: ""
Jan 23 13:35:13.745: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/23/24 13:35:13.745
Jan 23 13:35:13.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 23 13:35:13.897: INFO: stderr: ""
Jan 23 13:35:13.897: INFO: stdout: "update-demo-nautilus-b5mrt update-demo-nautilus-jkh6p "
Jan 23 13:35:13.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-b5mrt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 23 13:35:14.039: INFO: stderr: ""
Jan 23 13:35:14.039: INFO: stdout: ""
Jan 23 13:35:14.039: INFO: update-demo-nautilus-b5mrt is created but not running
Jan 23 13:35:19.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 23 13:35:19.203: INFO: stderr: ""
Jan 23 13:35:19.203: INFO: stdout: "update-demo-nautilus-b5mrt update-demo-nautilus-jkh6p "
Jan 23 13:35:19.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-b5mrt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 23 13:35:19.348: INFO: stderr: ""
Jan 23 13:35:19.348: INFO: stdout: "true"
Jan 23 13:35:19.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-b5mrt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 23 13:35:19.496: INFO: stderr: ""
Jan 23 13:35:19.496: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 23 13:35:19.496: INFO: validating pod update-demo-nautilus-b5mrt
Jan 23 13:35:19.503: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 13:35:19.503: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 13:35:19.503: INFO: update-demo-nautilus-b5mrt is verified up and running
Jan 23 13:35:19.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-jkh6p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 23 13:35:19.657: INFO: stderr: ""
Jan 23 13:35:19.657: INFO: stdout: "true"
Jan 23 13:35:19.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-jkh6p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 23 13:35:19.823: INFO: stderr: ""
Jan 23 13:35:19.823: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 23 13:35:19.823: INFO: validating pod update-demo-nautilus-jkh6p
Jan 23 13:35:19.826: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 13:35:19.826: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 13:35:19.826: INFO: update-demo-nautilus-jkh6p is verified up and running
STEP: using delete to clean up resources 01/23/24 13:35:19.826
Jan 23 13:35:19.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 delete --grace-period=0 --force -f -'
Jan 23 13:35:19.943: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 13:35:19.943: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 23 13:35:19.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get rc,svc -l name=update-demo --no-headers'
Jan 23 13:35:20.179: INFO: stderr: "No resources found in kubectl-4765 namespace.\n"
Jan 23 13:35:20.179: INFO: stdout: ""
Jan 23 13:35:20.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 23 13:35:20.327: INFO: stderr: ""
Jan 23 13:35:20.327: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 23 13:35:20.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4765" for this suite. 01/23/24 13:35:20.329
------------------------------
• [SLOW TEST] [19.677 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:35:00.658
    Jan 23 13:35:00.658: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubectl 01/23/24 13:35:00.659
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:35:00.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:35:00.669
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 01/23/24 13:35:00.671
    Jan 23 13:35:00.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 create -f -'
    Jan 23 13:35:01.779: INFO: stderr: ""
    Jan 23 13:35:01.779: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/23/24 13:35:01.779
    Jan 23 13:35:01.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 23 13:35:01.928: INFO: stderr: ""
    Jan 23 13:35:01.928: INFO: stdout: "update-demo-nautilus-jkh6p update-demo-nautilus-wrqxj "
    Jan 23 13:35:01.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-jkh6p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 23 13:35:02.072: INFO: stderr: ""
    Jan 23 13:35:02.072: INFO: stdout: ""
    Jan 23 13:35:02.072: INFO: update-demo-nautilus-jkh6p is created but not running
    Jan 23 13:35:07.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 23 13:35:07.212: INFO: stderr: ""
    Jan 23 13:35:07.212: INFO: stdout: "update-demo-nautilus-jkh6p update-demo-nautilus-wrqxj "
    Jan 23 13:35:07.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-jkh6p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 23 13:35:07.349: INFO: stderr: ""
    Jan 23 13:35:07.349: INFO: stdout: "true"
    Jan 23 13:35:07.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-jkh6p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 23 13:35:07.488: INFO: stderr: ""
    Jan 23 13:35:07.488: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 23 13:35:07.488: INFO: validating pod update-demo-nautilus-jkh6p
    Jan 23 13:35:07.491: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 23 13:35:07.491: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 23 13:35:07.491: INFO: update-demo-nautilus-jkh6p is verified up and running
    Jan 23 13:35:07.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-wrqxj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 23 13:35:07.630: INFO: stderr: ""
    Jan 23 13:35:07.630: INFO: stdout: "true"
    Jan 23 13:35:07.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-wrqxj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 23 13:35:07.773: INFO: stderr: ""
    Jan 23 13:35:07.773: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 23 13:35:07.773: INFO: validating pod update-demo-nautilus-wrqxj
    Jan 23 13:35:07.776: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 23 13:35:07.776: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 23 13:35:07.776: INFO: update-demo-nautilus-wrqxj is verified up and running
    STEP: scaling down the replication controller 01/23/24 13:35:07.776
    Jan 23 13:35:07.778: INFO: scanned /root for discovery docs: <nil>
    Jan 23 13:35:07.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan 23 13:35:07.962: INFO: stderr: ""
    Jan 23 13:35:07.962: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/23/24 13:35:07.962
    Jan 23 13:35:07.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 23 13:35:08.107: INFO: stderr: ""
    Jan 23 13:35:08.107: INFO: stdout: "update-demo-nautilus-jkh6p update-demo-nautilus-wrqxj "
    STEP: Replicas for name=update-demo: expected=1 actual=2 01/23/24 13:35:08.107
    Jan 23 13:35:13.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 23 13:35:13.263: INFO: stderr: ""
    Jan 23 13:35:13.263: INFO: stdout: "update-demo-nautilus-jkh6p "
    Jan 23 13:35:13.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-jkh6p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 23 13:35:13.412: INFO: stderr: ""
    Jan 23 13:35:13.412: INFO: stdout: "true"
    Jan 23 13:35:13.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-jkh6p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 23 13:35:13.557: INFO: stderr: ""
    Jan 23 13:35:13.557: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 23 13:35:13.557: INFO: validating pod update-demo-nautilus-jkh6p
    Jan 23 13:35:13.559: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 23 13:35:13.559: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 23 13:35:13.559: INFO: update-demo-nautilus-jkh6p is verified up and running
    STEP: scaling up the replication controller 01/23/24 13:35:13.559
    Jan 23 13:35:13.561: INFO: scanned /root for discovery docs: <nil>
    Jan 23 13:35:13.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan 23 13:35:13.745: INFO: stderr: ""
    Jan 23 13:35:13.745: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/23/24 13:35:13.745
    Jan 23 13:35:13.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 23 13:35:13.897: INFO: stderr: ""
    Jan 23 13:35:13.897: INFO: stdout: "update-demo-nautilus-b5mrt update-demo-nautilus-jkh6p "
    Jan 23 13:35:13.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-b5mrt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 23 13:35:14.039: INFO: stderr: ""
    Jan 23 13:35:14.039: INFO: stdout: ""
    Jan 23 13:35:14.039: INFO: update-demo-nautilus-b5mrt is created but not running
    Jan 23 13:35:19.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 23 13:35:19.203: INFO: stderr: ""
    Jan 23 13:35:19.203: INFO: stdout: "update-demo-nautilus-b5mrt update-demo-nautilus-jkh6p "
    Jan 23 13:35:19.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-b5mrt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 23 13:35:19.348: INFO: stderr: ""
    Jan 23 13:35:19.348: INFO: stdout: "true"
    Jan 23 13:35:19.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-b5mrt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 23 13:35:19.496: INFO: stderr: ""
    Jan 23 13:35:19.496: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 23 13:35:19.496: INFO: validating pod update-demo-nautilus-b5mrt
    Jan 23 13:35:19.503: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 23 13:35:19.503: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 23 13:35:19.503: INFO: update-demo-nautilus-b5mrt is verified up and running
    Jan 23 13:35:19.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-jkh6p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 23 13:35:19.657: INFO: stderr: ""
    Jan 23 13:35:19.657: INFO: stdout: "true"
    Jan 23 13:35:19.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods update-demo-nautilus-jkh6p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 23 13:35:19.823: INFO: stderr: ""
    Jan 23 13:35:19.823: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 23 13:35:19.823: INFO: validating pod update-demo-nautilus-jkh6p
    Jan 23 13:35:19.826: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 23 13:35:19.826: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 23 13:35:19.826: INFO: update-demo-nautilus-jkh6p is verified up and running
    STEP: using delete to clean up resources 01/23/24 13:35:19.826
    Jan 23 13:35:19.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 delete --grace-period=0 --force -f -'
    Jan 23 13:35:19.943: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 23 13:35:19.943: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 23 13:35:19.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get rc,svc -l name=update-demo --no-headers'
    Jan 23 13:35:20.179: INFO: stderr: "No resources found in kubectl-4765 namespace.\n"
    Jan 23 13:35:20.179: INFO: stdout: ""
    Jan 23 13:35:20.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4765 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 23 13:35:20.327: INFO: stderr: ""
    Jan 23 13:35:20.327: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:35:20.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4765" for this suite. 01/23/24 13:35:20.329
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:35:20.335
Jan 23 13:35:20.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename services 01/23/24 13:35:20.335
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:35:20.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:35:20.346
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-557 01/23/24 13:35:20.348
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/23/24 13:35:20.356
STEP: creating service externalsvc in namespace services-557 01/23/24 13:35:20.356
STEP: creating replication controller externalsvc in namespace services-557 01/23/24 13:35:20.365
I0123 13:35:20.370873      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-557, replica count: 2
I0123 13:35:23.421965      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/23/24 13:35:23.424
Jan 23 13:35:23.431: INFO: Creating new exec pod
Jan 23 13:35:23.442: INFO: Waiting up to 5m0s for pod "execpodq7xjd" in namespace "services-557" to be "running"
Jan 23 13:35:23.444: INFO: Pod "execpodq7xjd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.879414ms
Jan 23 13:35:25.446: INFO: Pod "execpodq7xjd": Phase="Running", Reason="", readiness=true. Elapsed: 2.004428874s
Jan 23 13:35:25.446: INFO: Pod "execpodq7xjd" satisfied condition "running"
Jan 23 13:35:25.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-557 exec execpodq7xjd -- /bin/sh -x -c nslookup clusterip-service.services-557.svc.cluster.local'
Jan 23 13:35:25.641: INFO: stderr: "+ nslookup clusterip-service.services-557.svc.cluster.local\n"
Jan 23 13:35:25.641: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-557.svc.cluster.local\tcanonical name = externalsvc.services-557.svc.cluster.local.\nName:\texternalsvc.services-557.svc.cluster.local\nAddress: 10.233.25.91\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-557, will wait for the garbage collector to delete the pods 01/23/24 13:35:25.641
Jan 23 13:35:25.698: INFO: Deleting ReplicationController externalsvc took: 4.365234ms
Jan 23 13:35:25.798: INFO: Terminating ReplicationController externalsvc pods took: 100.483212ms
Jan 23 13:35:28.614: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 23 13:35:28.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-557" for this suite. 01/23/24 13:35:28.624
------------------------------
• [SLOW TEST] [8.292 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:35:20.335
    Jan 23 13:35:20.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename services 01/23/24 13:35:20.335
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:35:20.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:35:20.346
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-557 01/23/24 13:35:20.348
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/23/24 13:35:20.356
    STEP: creating service externalsvc in namespace services-557 01/23/24 13:35:20.356
    STEP: creating replication controller externalsvc in namespace services-557 01/23/24 13:35:20.365
    I0123 13:35:20.370873      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-557, replica count: 2
    I0123 13:35:23.421965      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/23/24 13:35:23.424
    Jan 23 13:35:23.431: INFO: Creating new exec pod
    Jan 23 13:35:23.442: INFO: Waiting up to 5m0s for pod "execpodq7xjd" in namespace "services-557" to be "running"
    Jan 23 13:35:23.444: INFO: Pod "execpodq7xjd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.879414ms
    Jan 23 13:35:25.446: INFO: Pod "execpodq7xjd": Phase="Running", Reason="", readiness=true. Elapsed: 2.004428874s
    Jan 23 13:35:25.446: INFO: Pod "execpodq7xjd" satisfied condition "running"
    Jan 23 13:35:25.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-557 exec execpodq7xjd -- /bin/sh -x -c nslookup clusterip-service.services-557.svc.cluster.local'
    Jan 23 13:35:25.641: INFO: stderr: "+ nslookup clusterip-service.services-557.svc.cluster.local\n"
    Jan 23 13:35:25.641: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-557.svc.cluster.local\tcanonical name = externalsvc.services-557.svc.cluster.local.\nName:\texternalsvc.services-557.svc.cluster.local\nAddress: 10.233.25.91\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-557, will wait for the garbage collector to delete the pods 01/23/24 13:35:25.641
    Jan 23 13:35:25.698: INFO: Deleting ReplicationController externalsvc took: 4.365234ms
    Jan 23 13:35:25.798: INFO: Terminating ReplicationController externalsvc pods took: 100.483212ms
    Jan 23 13:35:28.614: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:35:28.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-557" for this suite. 01/23/24 13:35:28.624
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:35:28.627
Jan 23 13:35:28.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename webhook 01/23/24 13:35:28.628
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:35:28.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:35:28.641
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/23/24 13:35:28.65
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 13:35:29.006
STEP: Deploying the webhook pod 01/23/24 13:35:29.019
STEP: Wait for the deployment to be ready 01/23/24 13:35:29.029
Jan 23 13:35:29.039: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/23/24 13:35:31.047
STEP: Verifying the service has paired with the endpoint 01/23/24 13:35:31.053
Jan 23 13:35:32.054: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Jan 23 13:35:32.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7280-crds.webhook.example.com via the AdmissionRegistration API 01/23/24 13:35:37.564
STEP: Creating a custom resource that should be mutated by the webhook 01/23/24 13:35:37.577
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:35:40.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2031" for this suite. 01/23/24 13:35:40.152
STEP: Destroying namespace "webhook-2031-markers" for this suite. 01/23/24 13:35:40.155
------------------------------
• [SLOW TEST] [11.534 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:35:28.627
    Jan 23 13:35:28.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename webhook 01/23/24 13:35:28.628
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:35:28.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:35:28.641
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/23/24 13:35:28.65
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 13:35:29.006
    STEP: Deploying the webhook pod 01/23/24 13:35:29.019
    STEP: Wait for the deployment to be ready 01/23/24 13:35:29.029
    Jan 23 13:35:29.039: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/23/24 13:35:31.047
    STEP: Verifying the service has paired with the endpoint 01/23/24 13:35:31.053
    Jan 23 13:35:32.054: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Jan 23 13:35:32.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7280-crds.webhook.example.com via the AdmissionRegistration API 01/23/24 13:35:37.564
    STEP: Creating a custom resource that should be mutated by the webhook 01/23/24 13:35:37.577
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:35:40.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2031" for this suite. 01/23/24 13:35:40.152
    STEP: Destroying namespace "webhook-2031-markers" for this suite. 01/23/24 13:35:40.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:35:40.162
Jan 23 13:35:40.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename gc 01/23/24 13:35:40.163
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:35:40.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:35:40.176
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan 23 13:35:40.222: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ab04b881-0373-40dc-8793-20651f5db97f", Controller:(*bool)(0xc00360d38a), BlockOwnerDeletion:(*bool)(0xc00360d38b)}}
Jan 23 13:35:40.227: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"6ce35e1d-2bd6-43c7-9a26-2f1f60cfa432", Controller:(*bool)(0xc000f8bfca), BlockOwnerDeletion:(*bool)(0xc000f8bfcb)}}
Jan 23 13:35:40.233: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"e7b24f86-c77f-45c9-b893-822fe654f0dd", Controller:(*bool)(0xc00360d61a), BlockOwnerDeletion:(*bool)(0xc00360d61b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 23 13:35:45.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4170" for this suite. 01/23/24 13:35:45.245
------------------------------
• [SLOW TEST] [5.087 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:35:40.162
    Jan 23 13:35:40.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename gc 01/23/24 13:35:40.163
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:35:40.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:35:40.176
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan 23 13:35:40.222: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ab04b881-0373-40dc-8793-20651f5db97f", Controller:(*bool)(0xc00360d38a), BlockOwnerDeletion:(*bool)(0xc00360d38b)}}
    Jan 23 13:35:40.227: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"6ce35e1d-2bd6-43c7-9a26-2f1f60cfa432", Controller:(*bool)(0xc000f8bfca), BlockOwnerDeletion:(*bool)(0xc000f8bfcb)}}
    Jan 23 13:35:40.233: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"e7b24f86-c77f-45c9-b893-822fe654f0dd", Controller:(*bool)(0xc00360d61a), BlockOwnerDeletion:(*bool)(0xc00360d61b)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:35:45.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4170" for this suite. 01/23/24 13:35:45.245
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:35:45.249
Jan 23 13:35:45.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename pods 01/23/24 13:35:45.25
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:35:45.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:35:45.26
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 01/23/24 13:35:45.264
Jan 23 13:35:45.275: INFO: created test-pod-1
Jan 23 13:35:45.286: INFO: created test-pod-2
Jan 23 13:35:45.299: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/23/24 13:35:45.299
Jan 23 13:35:45.299: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-471' to be running and ready
Jan 23 13:35:45.304: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 23 13:35:45.304: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 23 13:35:45.304: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 23 13:35:45.304: INFO: 0 / 3 pods in namespace 'pods-471' are running and ready (0 seconds elapsed)
Jan 23 13:35:45.304: INFO: expected 0 pod replicas in namespace 'pods-471', 0 are Running and Ready.
Jan 23 13:35:45.304: INFO: POD         NODE                                          PHASE    GRACE  CONDITIONS
Jan 23 13:35:45.304: INFO: test-pod-1  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC  }]
Jan 23 13:35:45.304: INFO: test-pod-2  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC  }]
Jan 23 13:35:45.304: INFO: test-pod-3  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC  }]
Jan 23 13:35:45.304: INFO: 
Jan 23 13:35:47.310: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 23 13:35:47.310: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 23 13:35:47.310: INFO: 1 / 3 pods in namespace 'pods-471' are running and ready (2 seconds elapsed)
Jan 23 13:35:47.310: INFO: expected 0 pod replicas in namespace 'pods-471', 0 are Running and Ready.
Jan 23 13:35:47.310: INFO: POD         NODE                                          PHASE    GRACE  CONDITIONS
Jan 23 13:35:47.310: INFO: test-pod-2  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC  }]
Jan 23 13:35:47.310: INFO: test-pod-3  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC  }]
Jan 23 13:35:47.310: INFO: 
Jan 23 13:35:49.310: INFO: 3 / 3 pods in namespace 'pods-471' are running and ready (4 seconds elapsed)
Jan 23 13:35:49.310: INFO: expected 0 pod replicas in namespace 'pods-471', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/23/24 13:35:49.321
Jan 23 13:35:49.323: INFO: Pod quantity 3 is different from expected quantity 0
Jan 23 13:35:50.325: INFO: Pod quantity 3 is different from expected quantity 0
Jan 23 13:35:51.326: INFO: Pod quantity 3 is different from expected quantity 0
Jan 23 13:35:52.326: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 23 13:35:53.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-471" for this suite. 01/23/24 13:35:53.329
------------------------------
• [SLOW TEST] [8.083 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:35:45.249
    Jan 23 13:35:45.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename pods 01/23/24 13:35:45.25
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:35:45.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:35:45.26
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 01/23/24 13:35:45.264
    Jan 23 13:35:45.275: INFO: created test-pod-1
    Jan 23 13:35:45.286: INFO: created test-pod-2
    Jan 23 13:35:45.299: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/23/24 13:35:45.299
    Jan 23 13:35:45.299: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-471' to be running and ready
    Jan 23 13:35:45.304: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 23 13:35:45.304: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 23 13:35:45.304: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 23 13:35:45.304: INFO: 0 / 3 pods in namespace 'pods-471' are running and ready (0 seconds elapsed)
    Jan 23 13:35:45.304: INFO: expected 0 pod replicas in namespace 'pods-471', 0 are Running and Ready.
    Jan 23 13:35:45.304: INFO: POD         NODE                                          PHASE    GRACE  CONDITIONS
    Jan 23 13:35:45.304: INFO: test-pod-1  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC  }]
    Jan 23 13:35:45.304: INFO: test-pod-2  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC  }]
    Jan 23 13:35:45.304: INFO: test-pod-3  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC  }]
    Jan 23 13:35:45.304: INFO: 
    Jan 23 13:35:47.310: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 23 13:35:47.310: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 23 13:35:47.310: INFO: 1 / 3 pods in namespace 'pods-471' are running and ready (2 seconds elapsed)
    Jan 23 13:35:47.310: INFO: expected 0 pod replicas in namespace 'pods-471', 0 are Running and Ready.
    Jan 23 13:35:47.310: INFO: POD         NODE                                          PHASE    GRACE  CONDITIONS
    Jan 23 13:35:47.310: INFO: test-pod-2  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC  }]
    Jan 23 13:35:47.310: INFO: test-pod-3  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:35:45 +0000 UTC  }]
    Jan 23 13:35:47.310: INFO: 
    Jan 23 13:35:49.310: INFO: 3 / 3 pods in namespace 'pods-471' are running and ready (4 seconds elapsed)
    Jan 23 13:35:49.310: INFO: expected 0 pod replicas in namespace 'pods-471', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/23/24 13:35:49.321
    Jan 23 13:35:49.323: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 23 13:35:50.325: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 23 13:35:51.326: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 23 13:35:52.326: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:35:53.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-471" for this suite. 01/23/24 13:35:53.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:35:53.333
Jan 23 13:35:53.333: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 13:35:53.333
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:35:53.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:35:53.343
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 01/23/24 13:35:53.345
Jan 23 13:35:53.357: INFO: Waiting up to 5m0s for pod "annotationupdate994a043e-a765-4c2c-bb81-2ac8e8ff680f" in namespace "projected-8576" to be "running and ready"
Jan 23 13:35:53.362: INFO: Pod "annotationupdate994a043e-a765-4c2c-bb81-2ac8e8ff680f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.253581ms
Jan 23 13:35:53.362: INFO: The phase of Pod annotationupdate994a043e-a765-4c2c-bb81-2ac8e8ff680f is Pending, waiting for it to be Running (with Ready = true)
Jan 23 13:35:55.367: INFO: Pod "annotationupdate994a043e-a765-4c2c-bb81-2ac8e8ff680f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009696053s
Jan 23 13:35:55.367: INFO: The phase of Pod annotationupdate994a043e-a765-4c2c-bb81-2ac8e8ff680f is Running (Ready = true)
Jan 23 13:35:55.367: INFO: Pod "annotationupdate994a043e-a765-4c2c-bb81-2ac8e8ff680f" satisfied condition "running and ready"
Jan 23 13:35:55.889: INFO: Successfully updated pod "annotationupdate994a043e-a765-4c2c-bb81-2ac8e8ff680f"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 23 13:35:59.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8576" for this suite. 01/23/24 13:35:59.908
------------------------------
• [SLOW TEST] [6.579 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:35:53.333
    Jan 23 13:35:53.333: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 13:35:53.333
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:35:53.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:35:53.343
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 01/23/24 13:35:53.345
    Jan 23 13:35:53.357: INFO: Waiting up to 5m0s for pod "annotationupdate994a043e-a765-4c2c-bb81-2ac8e8ff680f" in namespace "projected-8576" to be "running and ready"
    Jan 23 13:35:53.362: INFO: Pod "annotationupdate994a043e-a765-4c2c-bb81-2ac8e8ff680f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.253581ms
    Jan 23 13:35:53.362: INFO: The phase of Pod annotationupdate994a043e-a765-4c2c-bb81-2ac8e8ff680f is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 13:35:55.367: INFO: Pod "annotationupdate994a043e-a765-4c2c-bb81-2ac8e8ff680f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009696053s
    Jan 23 13:35:55.367: INFO: The phase of Pod annotationupdate994a043e-a765-4c2c-bb81-2ac8e8ff680f is Running (Ready = true)
    Jan 23 13:35:55.367: INFO: Pod "annotationupdate994a043e-a765-4c2c-bb81-2ac8e8ff680f" satisfied condition "running and ready"
    Jan 23 13:35:55.889: INFO: Successfully updated pod "annotationupdate994a043e-a765-4c2c-bb81-2ac8e8ff680f"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:35:59.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8576" for this suite. 01/23/24 13:35:59.908
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:35:59.912
Jan 23 13:35:59.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename downward-api 01/23/24 13:35:59.913
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:35:59.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:35:59.923
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 01/23/24 13:35:59.927
Jan 23 13:35:59.940: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fbc4e6ec-8e39-4eba-8e92-93ffb79e8132" in namespace "downward-api-4375" to be "Succeeded or Failed"
Jan 23 13:35:59.942: INFO: Pod "downwardapi-volume-fbc4e6ec-8e39-4eba-8e92-93ffb79e8132": Phase="Pending", Reason="", readiness=false. Elapsed: 1.750065ms
Jan 23 13:36:01.946: INFO: Pod "downwardapi-volume-fbc4e6ec-8e39-4eba-8e92-93ffb79e8132": Phase="Running", Reason="", readiness=true. Elapsed: 2.00564s
Jan 23 13:36:03.945: INFO: Pod "downwardapi-volume-fbc4e6ec-8e39-4eba-8e92-93ffb79e8132": Phase="Running", Reason="", readiness=false. Elapsed: 4.005062168s
Jan 23 13:36:05.945: INFO: Pod "downwardapi-volume-fbc4e6ec-8e39-4eba-8e92-93ffb79e8132": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005182695s
STEP: Saw pod success 01/23/24 13:36:05.945
Jan 23 13:36:05.946: INFO: Pod "downwardapi-volume-fbc4e6ec-8e39-4eba-8e92-93ffb79e8132" satisfied condition "Succeeded or Failed"
Jan 23 13:36:05.947: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-fbc4e6ec-8e39-4eba-8e92-93ffb79e8132 container client-container: <nil>
STEP: delete the pod 01/23/24 13:36:05.951
Jan 23 13:36:05.957: INFO: Waiting for pod downwardapi-volume-fbc4e6ec-8e39-4eba-8e92-93ffb79e8132 to disappear
Jan 23 13:36:05.958: INFO: Pod downwardapi-volume-fbc4e6ec-8e39-4eba-8e92-93ffb79e8132 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 23 13:36:05.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4375" for this suite. 01/23/24 13:36:05.961
------------------------------
• [SLOW TEST] [6.052 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:35:59.912
    Jan 23 13:35:59.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename downward-api 01/23/24 13:35:59.913
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:35:59.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:35:59.923
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 01/23/24 13:35:59.927
    Jan 23 13:35:59.940: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fbc4e6ec-8e39-4eba-8e92-93ffb79e8132" in namespace "downward-api-4375" to be "Succeeded or Failed"
    Jan 23 13:35:59.942: INFO: Pod "downwardapi-volume-fbc4e6ec-8e39-4eba-8e92-93ffb79e8132": Phase="Pending", Reason="", readiness=false. Elapsed: 1.750065ms
    Jan 23 13:36:01.946: INFO: Pod "downwardapi-volume-fbc4e6ec-8e39-4eba-8e92-93ffb79e8132": Phase="Running", Reason="", readiness=true. Elapsed: 2.00564s
    Jan 23 13:36:03.945: INFO: Pod "downwardapi-volume-fbc4e6ec-8e39-4eba-8e92-93ffb79e8132": Phase="Running", Reason="", readiness=false. Elapsed: 4.005062168s
    Jan 23 13:36:05.945: INFO: Pod "downwardapi-volume-fbc4e6ec-8e39-4eba-8e92-93ffb79e8132": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005182695s
    STEP: Saw pod success 01/23/24 13:36:05.945
    Jan 23 13:36:05.946: INFO: Pod "downwardapi-volume-fbc4e6ec-8e39-4eba-8e92-93ffb79e8132" satisfied condition "Succeeded or Failed"
    Jan 23 13:36:05.947: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-fbc4e6ec-8e39-4eba-8e92-93ffb79e8132 container client-container: <nil>
    STEP: delete the pod 01/23/24 13:36:05.951
    Jan 23 13:36:05.957: INFO: Waiting for pod downwardapi-volume-fbc4e6ec-8e39-4eba-8e92-93ffb79e8132 to disappear
    Jan 23 13:36:05.958: INFO: Pod downwardapi-volume-fbc4e6ec-8e39-4eba-8e92-93ffb79e8132 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:36:05.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4375" for this suite. 01/23/24 13:36:05.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:36:05.965
Jan 23 13:36:05.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename emptydir 01/23/24 13:36:05.965
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:36:05.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:36:05.976
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/23/24 13:36:05.978
Jan 23 13:36:05.989: INFO: Waiting up to 5m0s for pod "pod-ce4c50bf-d4a7-4e5b-831d-abf5f843fa37" in namespace "emptydir-9681" to be "Succeeded or Failed"
Jan 23 13:36:05.991: INFO: Pod "pod-ce4c50bf-d4a7-4e5b-831d-abf5f843fa37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.471692ms
Jan 23 13:36:07.995: INFO: Pod "pod-ce4c50bf-d4a7-4e5b-831d-abf5f843fa37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005519222s
Jan 23 13:36:09.994: INFO: Pod "pod-ce4c50bf-d4a7-4e5b-831d-abf5f843fa37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005270056s
STEP: Saw pod success 01/23/24 13:36:09.994
Jan 23 13:36:09.994: INFO: Pod "pod-ce4c50bf-d4a7-4e5b-831d-abf5f843fa37" satisfied condition "Succeeded or Failed"
Jan 23 13:36:09.996: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-ce4c50bf-d4a7-4e5b-831d-abf5f843fa37 container test-container: <nil>
STEP: delete the pod 01/23/24 13:36:09.999
Jan 23 13:36:10.007: INFO: Waiting for pod pod-ce4c50bf-d4a7-4e5b-831d-abf5f843fa37 to disappear
Jan 23 13:36:10.009: INFO: Pod pod-ce4c50bf-d4a7-4e5b-831d-abf5f843fa37 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 23 13:36:10.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9681" for this suite. 01/23/24 13:36:10.011
------------------------------
• [4.049 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:36:05.965
    Jan 23 13:36:05.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename emptydir 01/23/24 13:36:05.965
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:36:05.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:36:05.976
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/23/24 13:36:05.978
    Jan 23 13:36:05.989: INFO: Waiting up to 5m0s for pod "pod-ce4c50bf-d4a7-4e5b-831d-abf5f843fa37" in namespace "emptydir-9681" to be "Succeeded or Failed"
    Jan 23 13:36:05.991: INFO: Pod "pod-ce4c50bf-d4a7-4e5b-831d-abf5f843fa37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.471692ms
    Jan 23 13:36:07.995: INFO: Pod "pod-ce4c50bf-d4a7-4e5b-831d-abf5f843fa37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005519222s
    Jan 23 13:36:09.994: INFO: Pod "pod-ce4c50bf-d4a7-4e5b-831d-abf5f843fa37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005270056s
    STEP: Saw pod success 01/23/24 13:36:09.994
    Jan 23 13:36:09.994: INFO: Pod "pod-ce4c50bf-d4a7-4e5b-831d-abf5f843fa37" satisfied condition "Succeeded or Failed"
    Jan 23 13:36:09.996: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-ce4c50bf-d4a7-4e5b-831d-abf5f843fa37 container test-container: <nil>
    STEP: delete the pod 01/23/24 13:36:09.999
    Jan 23 13:36:10.007: INFO: Waiting for pod pod-ce4c50bf-d4a7-4e5b-831d-abf5f843fa37 to disappear
    Jan 23 13:36:10.009: INFO: Pod pod-ce4c50bf-d4a7-4e5b-831d-abf5f843fa37 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:36:10.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9681" for this suite. 01/23/24 13:36:10.011
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:36:10.014
Jan 23 13:36:10.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename replicaset 01/23/24 13:36:10.015
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:36:10.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:36:10.025
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/23/24 13:36:10.03
STEP: Verify that the required pods have come up. 01/23/24 13:36:10.034
Jan 23 13:36:10.036: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 23 13:36:15.042: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/23/24 13:36:15.042
STEP: Getting /status 01/23/24 13:36:15.042
Jan 23 13:36:15.044: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/23/24 13:36:15.044
Jan 23 13:36:15.049: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/23/24 13:36:15.049
Jan 23 13:36:15.051: INFO: Observed &ReplicaSet event: ADDED
Jan 23 13:36:15.051: INFO: Observed &ReplicaSet event: MODIFIED
Jan 23 13:36:15.051: INFO: Observed &ReplicaSet event: MODIFIED
Jan 23 13:36:15.051: INFO: Observed &ReplicaSet event: MODIFIED
Jan 23 13:36:15.051: INFO: Found replicaset test-rs in namespace replicaset-408 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 23 13:36:15.051: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/23/24 13:36:15.051
Jan 23 13:36:15.051: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 23 13:36:15.057: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/23/24 13:36:15.057
Jan 23 13:36:15.058: INFO: Observed &ReplicaSet event: ADDED
Jan 23 13:36:15.058: INFO: Observed &ReplicaSet event: MODIFIED
Jan 23 13:36:15.058: INFO: Observed &ReplicaSet event: MODIFIED
Jan 23 13:36:15.058: INFO: Observed &ReplicaSet event: MODIFIED
Jan 23 13:36:15.059: INFO: Observed replicaset test-rs in namespace replicaset-408 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 23 13:36:15.059: INFO: Observed &ReplicaSet event: MODIFIED
Jan 23 13:36:15.059: INFO: Found replicaset test-rs in namespace replicaset-408 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 23 13:36:15.059: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 23 13:36:15.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-408" for this suite. 01/23/24 13:36:15.061
------------------------------
• [SLOW TEST] [5.049 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:36:10.014
    Jan 23 13:36:10.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename replicaset 01/23/24 13:36:10.015
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:36:10.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:36:10.025
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/23/24 13:36:10.03
    STEP: Verify that the required pods have come up. 01/23/24 13:36:10.034
    Jan 23 13:36:10.036: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 23 13:36:15.042: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/23/24 13:36:15.042
    STEP: Getting /status 01/23/24 13:36:15.042
    Jan 23 13:36:15.044: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/23/24 13:36:15.044
    Jan 23 13:36:15.049: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/23/24 13:36:15.049
    Jan 23 13:36:15.051: INFO: Observed &ReplicaSet event: ADDED
    Jan 23 13:36:15.051: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 23 13:36:15.051: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 23 13:36:15.051: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 23 13:36:15.051: INFO: Found replicaset test-rs in namespace replicaset-408 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 23 13:36:15.051: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/23/24 13:36:15.051
    Jan 23 13:36:15.051: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 23 13:36:15.057: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/23/24 13:36:15.057
    Jan 23 13:36:15.058: INFO: Observed &ReplicaSet event: ADDED
    Jan 23 13:36:15.058: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 23 13:36:15.058: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 23 13:36:15.058: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 23 13:36:15.059: INFO: Observed replicaset test-rs in namespace replicaset-408 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 23 13:36:15.059: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 23 13:36:15.059: INFO: Found replicaset test-rs in namespace replicaset-408 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan 23 13:36:15.059: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:36:15.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-408" for this suite. 01/23/24 13:36:15.061
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:36:15.065
Jan 23 13:36:15.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 13:36:15.066
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:36:15.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:36:15.074
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-dca41801-88c6-4ef6-8b23-e506e2c75f00 01/23/24 13:36:15.076
STEP: Creating a pod to test consume configMaps 01/23/24 13:36:15.08
Jan 23 13:36:15.093: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5b5846a5-52fb-490f-8def-85e26b5721e9" in namespace "projected-4748" to be "Succeeded or Failed"
Jan 23 13:36:15.095: INFO: Pod "pod-projected-configmaps-5b5846a5-52fb-490f-8def-85e26b5721e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.37236ms
Jan 23 13:36:17.098: INFO: Pod "pod-projected-configmaps-5b5846a5-52fb-490f-8def-85e26b5721e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.005423768s
Jan 23 13:36:19.100: INFO: Pod "pod-projected-configmaps-5b5846a5-52fb-490f-8def-85e26b5721e9": Phase="Running", Reason="", readiness=false. Elapsed: 4.007003416s
Jan 23 13:36:21.099: INFO: Pod "pod-projected-configmaps-5b5846a5-52fb-490f-8def-85e26b5721e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005542019s
STEP: Saw pod success 01/23/24 13:36:21.099
Jan 23 13:36:21.099: INFO: Pod "pod-projected-configmaps-5b5846a5-52fb-490f-8def-85e26b5721e9" satisfied condition "Succeeded or Failed"
Jan 23 13:36:21.100: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-configmaps-5b5846a5-52fb-490f-8def-85e26b5721e9 container agnhost-container: <nil>
STEP: delete the pod 01/23/24 13:36:21.104
Jan 23 13:36:21.111: INFO: Waiting for pod pod-projected-configmaps-5b5846a5-52fb-490f-8def-85e26b5721e9 to disappear
Jan 23 13:36:21.113: INFO: Pod pod-projected-configmaps-5b5846a5-52fb-490f-8def-85e26b5721e9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 23 13:36:21.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4748" for this suite. 01/23/24 13:36:21.116
------------------------------
• [SLOW TEST] [6.054 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:36:15.065
    Jan 23 13:36:15.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 13:36:15.066
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:36:15.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:36:15.074
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-dca41801-88c6-4ef6-8b23-e506e2c75f00 01/23/24 13:36:15.076
    STEP: Creating a pod to test consume configMaps 01/23/24 13:36:15.08
    Jan 23 13:36:15.093: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5b5846a5-52fb-490f-8def-85e26b5721e9" in namespace "projected-4748" to be "Succeeded or Failed"
    Jan 23 13:36:15.095: INFO: Pod "pod-projected-configmaps-5b5846a5-52fb-490f-8def-85e26b5721e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.37236ms
    Jan 23 13:36:17.098: INFO: Pod "pod-projected-configmaps-5b5846a5-52fb-490f-8def-85e26b5721e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.005423768s
    Jan 23 13:36:19.100: INFO: Pod "pod-projected-configmaps-5b5846a5-52fb-490f-8def-85e26b5721e9": Phase="Running", Reason="", readiness=false. Elapsed: 4.007003416s
    Jan 23 13:36:21.099: INFO: Pod "pod-projected-configmaps-5b5846a5-52fb-490f-8def-85e26b5721e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005542019s
    STEP: Saw pod success 01/23/24 13:36:21.099
    Jan 23 13:36:21.099: INFO: Pod "pod-projected-configmaps-5b5846a5-52fb-490f-8def-85e26b5721e9" satisfied condition "Succeeded or Failed"
    Jan 23 13:36:21.100: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-configmaps-5b5846a5-52fb-490f-8def-85e26b5721e9 container agnhost-container: <nil>
    STEP: delete the pod 01/23/24 13:36:21.104
    Jan 23 13:36:21.111: INFO: Waiting for pod pod-projected-configmaps-5b5846a5-52fb-490f-8def-85e26b5721e9 to disappear
    Jan 23 13:36:21.113: INFO: Pod pod-projected-configmaps-5b5846a5-52fb-490f-8def-85e26b5721e9 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:36:21.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4748" for this suite. 01/23/24 13:36:21.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:36:21.12
Jan 23 13:36:21.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename dns 01/23/24 13:36:21.121
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:36:21.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:36:21.131
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/23/24 13:36:21.133
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-349.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-349.svc.cluster.local; sleep 1; done
 01/23/24 13:36:21.135
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-349.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-349.svc.cluster.local; sleep 1; done
 01/23/24 13:36:21.135
STEP: creating a pod to probe DNS 01/23/24 13:36:21.135
STEP: submitting the pod to kubernetes 01/23/24 13:36:21.135
Jan 23 13:36:21.151: INFO: Waiting up to 15m0s for pod "dns-test-5ad879c6-ba11-4bce-8abf-72e94a8cf01d" in namespace "dns-349" to be "running"
Jan 23 13:36:21.153: INFO: Pod "dns-test-5ad879c6-ba11-4bce-8abf-72e94a8cf01d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.803372ms
Jan 23 13:36:23.157: INFO: Pod "dns-test-5ad879c6-ba11-4bce-8abf-72e94a8cf01d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005466605s
Jan 23 13:36:23.157: INFO: Pod "dns-test-5ad879c6-ba11-4bce-8abf-72e94a8cf01d" satisfied condition "running"
STEP: retrieving the pod 01/23/24 13:36:23.157
STEP: looking for the results for each expected name from probers 01/23/24 13:36:23.159
Jan 23 13:36:23.164: INFO: DNS probes using dns-test-5ad879c6-ba11-4bce-8abf-72e94a8cf01d succeeded

STEP: deleting the pod 01/23/24 13:36:23.164
STEP: changing the externalName to bar.example.com 01/23/24 13:36:23.172
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-349.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-349.svc.cluster.local; sleep 1; done
 01/23/24 13:36:23.176
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-349.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-349.svc.cluster.local; sleep 1; done
 01/23/24 13:36:23.177
STEP: creating a second pod to probe DNS 01/23/24 13:36:23.177
STEP: submitting the pod to kubernetes 01/23/24 13:36:23.177
Jan 23 13:36:23.192: INFO: Waiting up to 15m0s for pod "dns-test-ab3639df-5c10-485a-90f0-adfeb71d399b" in namespace "dns-349" to be "running"
Jan 23 13:36:23.194: INFO: Pod "dns-test-ab3639df-5c10-485a-90f0-adfeb71d399b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.815615ms
Jan 23 13:36:25.197: INFO: Pod "dns-test-ab3639df-5c10-485a-90f0-adfeb71d399b": Phase="Running", Reason="", readiness=true. Elapsed: 2.004641289s
Jan 23 13:36:25.197: INFO: Pod "dns-test-ab3639df-5c10-485a-90f0-adfeb71d399b" satisfied condition "running"
STEP: retrieving the pod 01/23/24 13:36:25.197
STEP: looking for the results for each expected name from probers 01/23/24 13:36:25.199
Jan 23 13:36:25.213: INFO: Unable to read wheezy_udp@dns-test-service-3.dns-349.svc.cluster.local from pod dns-349/dns-test-ab3639df-5c10-485a-90f0-adfeb71d399b: Get "https://10.233.0.1:443/api/v1/namespaces/dns-349/pods/dns-test-ab3639df-5c10-485a-90f0-adfeb71d399b/proxy/results/wheezy_udp@dns-test-service-3.dns-349.svc.cluster.local": stream error: stream ID 1163; INTERNAL_ERROR; received from peer
Jan 23 13:36:25.216: INFO: File jessie_udp@dns-test-service-3.dns-349.svc.cluster.local from pod  dns-349/dns-test-ab3639df-5c10-485a-90f0-adfeb71d399b contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 23 13:36:25.216: INFO: Lookups using dns-349/dns-test-ab3639df-5c10-485a-90f0-adfeb71d399b failed for: [wheezy_udp@dns-test-service-3.dns-349.svc.cluster.local jessie_udp@dns-test-service-3.dns-349.svc.cluster.local]

Jan 23 13:36:30.222: INFO: DNS probes using dns-test-ab3639df-5c10-485a-90f0-adfeb71d399b succeeded

STEP: deleting the pod 01/23/24 13:36:30.222
STEP: changing the service to type=ClusterIP 01/23/24 13:36:30.23
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-349.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-349.svc.cluster.local; sleep 1; done
 01/23/24 13:36:30.239
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-349.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-349.svc.cluster.local; sleep 1; done
 01/23/24 13:36:30.239
STEP: creating a third pod to probe DNS 01/23/24 13:36:30.239
STEP: submitting the pod to kubernetes 01/23/24 13:36:30.24
Jan 23 13:36:30.262: INFO: Waiting up to 15m0s for pod "dns-test-219adbf2-8b14-4d9a-acc0-cf876a76d209" in namespace "dns-349" to be "running"
Jan 23 13:36:30.263: INFO: Pod "dns-test-219adbf2-8b14-4d9a-acc0-cf876a76d209": Phase="Pending", Reason="", readiness=false. Elapsed: 1.704106ms
Jan 23 13:36:32.266: INFO: Pod "dns-test-219adbf2-8b14-4d9a-acc0-cf876a76d209": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004717038s
Jan 23 13:36:34.268: INFO: Pod "dns-test-219adbf2-8b14-4d9a-acc0-cf876a76d209": Phase="Running", Reason="", readiness=true. Elapsed: 4.006301103s
Jan 23 13:36:34.268: INFO: Pod "dns-test-219adbf2-8b14-4d9a-acc0-cf876a76d209" satisfied condition "running"
STEP: retrieving the pod 01/23/24 13:36:34.268
STEP: looking for the results for each expected name from probers 01/23/24 13:36:34.27
Jan 23 13:36:34.276: INFO: DNS probes using dns-test-219adbf2-8b14-4d9a-acc0-cf876a76d209 succeeded

STEP: deleting the pod 01/23/24 13:36:34.276
STEP: deleting the test externalName service 01/23/24 13:36:34.285
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 23 13:36:34.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-349" for this suite. 01/23/24 13:36:34.295
------------------------------
• [SLOW TEST] [13.178 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:36:21.12
    Jan 23 13:36:21.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename dns 01/23/24 13:36:21.121
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:36:21.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:36:21.131
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/23/24 13:36:21.133
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-349.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-349.svc.cluster.local; sleep 1; done
     01/23/24 13:36:21.135
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-349.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-349.svc.cluster.local; sleep 1; done
     01/23/24 13:36:21.135
    STEP: creating a pod to probe DNS 01/23/24 13:36:21.135
    STEP: submitting the pod to kubernetes 01/23/24 13:36:21.135
    Jan 23 13:36:21.151: INFO: Waiting up to 15m0s for pod "dns-test-5ad879c6-ba11-4bce-8abf-72e94a8cf01d" in namespace "dns-349" to be "running"
    Jan 23 13:36:21.153: INFO: Pod "dns-test-5ad879c6-ba11-4bce-8abf-72e94a8cf01d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.803372ms
    Jan 23 13:36:23.157: INFO: Pod "dns-test-5ad879c6-ba11-4bce-8abf-72e94a8cf01d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005466605s
    Jan 23 13:36:23.157: INFO: Pod "dns-test-5ad879c6-ba11-4bce-8abf-72e94a8cf01d" satisfied condition "running"
    STEP: retrieving the pod 01/23/24 13:36:23.157
    STEP: looking for the results for each expected name from probers 01/23/24 13:36:23.159
    Jan 23 13:36:23.164: INFO: DNS probes using dns-test-5ad879c6-ba11-4bce-8abf-72e94a8cf01d succeeded

    STEP: deleting the pod 01/23/24 13:36:23.164
    STEP: changing the externalName to bar.example.com 01/23/24 13:36:23.172
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-349.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-349.svc.cluster.local; sleep 1; done
     01/23/24 13:36:23.176
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-349.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-349.svc.cluster.local; sleep 1; done
     01/23/24 13:36:23.177
    STEP: creating a second pod to probe DNS 01/23/24 13:36:23.177
    STEP: submitting the pod to kubernetes 01/23/24 13:36:23.177
    Jan 23 13:36:23.192: INFO: Waiting up to 15m0s for pod "dns-test-ab3639df-5c10-485a-90f0-adfeb71d399b" in namespace "dns-349" to be "running"
    Jan 23 13:36:23.194: INFO: Pod "dns-test-ab3639df-5c10-485a-90f0-adfeb71d399b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.815615ms
    Jan 23 13:36:25.197: INFO: Pod "dns-test-ab3639df-5c10-485a-90f0-adfeb71d399b": Phase="Running", Reason="", readiness=true. Elapsed: 2.004641289s
    Jan 23 13:36:25.197: INFO: Pod "dns-test-ab3639df-5c10-485a-90f0-adfeb71d399b" satisfied condition "running"
    STEP: retrieving the pod 01/23/24 13:36:25.197
    STEP: looking for the results for each expected name from probers 01/23/24 13:36:25.199
    Jan 23 13:36:25.213: INFO: Unable to read wheezy_udp@dns-test-service-3.dns-349.svc.cluster.local from pod dns-349/dns-test-ab3639df-5c10-485a-90f0-adfeb71d399b: Get "https://10.233.0.1:443/api/v1/namespaces/dns-349/pods/dns-test-ab3639df-5c10-485a-90f0-adfeb71d399b/proxy/results/wheezy_udp@dns-test-service-3.dns-349.svc.cluster.local": stream error: stream ID 1163; INTERNAL_ERROR; received from peer
    Jan 23 13:36:25.216: INFO: File jessie_udp@dns-test-service-3.dns-349.svc.cluster.local from pod  dns-349/dns-test-ab3639df-5c10-485a-90f0-adfeb71d399b contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 23 13:36:25.216: INFO: Lookups using dns-349/dns-test-ab3639df-5c10-485a-90f0-adfeb71d399b failed for: [wheezy_udp@dns-test-service-3.dns-349.svc.cluster.local jessie_udp@dns-test-service-3.dns-349.svc.cluster.local]

    Jan 23 13:36:30.222: INFO: DNS probes using dns-test-ab3639df-5c10-485a-90f0-adfeb71d399b succeeded

    STEP: deleting the pod 01/23/24 13:36:30.222
    STEP: changing the service to type=ClusterIP 01/23/24 13:36:30.23
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-349.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-349.svc.cluster.local; sleep 1; done
     01/23/24 13:36:30.239
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-349.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-349.svc.cluster.local; sleep 1; done
     01/23/24 13:36:30.239
    STEP: creating a third pod to probe DNS 01/23/24 13:36:30.239
    STEP: submitting the pod to kubernetes 01/23/24 13:36:30.24
    Jan 23 13:36:30.262: INFO: Waiting up to 15m0s for pod "dns-test-219adbf2-8b14-4d9a-acc0-cf876a76d209" in namespace "dns-349" to be "running"
    Jan 23 13:36:30.263: INFO: Pod "dns-test-219adbf2-8b14-4d9a-acc0-cf876a76d209": Phase="Pending", Reason="", readiness=false. Elapsed: 1.704106ms
    Jan 23 13:36:32.266: INFO: Pod "dns-test-219adbf2-8b14-4d9a-acc0-cf876a76d209": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004717038s
    Jan 23 13:36:34.268: INFO: Pod "dns-test-219adbf2-8b14-4d9a-acc0-cf876a76d209": Phase="Running", Reason="", readiness=true. Elapsed: 4.006301103s
    Jan 23 13:36:34.268: INFO: Pod "dns-test-219adbf2-8b14-4d9a-acc0-cf876a76d209" satisfied condition "running"
    STEP: retrieving the pod 01/23/24 13:36:34.268
    STEP: looking for the results for each expected name from probers 01/23/24 13:36:34.27
    Jan 23 13:36:34.276: INFO: DNS probes using dns-test-219adbf2-8b14-4d9a-acc0-cf876a76d209 succeeded

    STEP: deleting the pod 01/23/24 13:36:34.276
    STEP: deleting the test externalName service 01/23/24 13:36:34.285
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:36:34.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-349" for this suite. 01/23/24 13:36:34.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:36:34.299
Jan 23 13:36:34.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename crd-publish-openapi 01/23/24 13:36:34.299
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:36:34.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:36:34.312
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 01/23/24 13:36:34.313
Jan 23 13:36:34.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: mark a version not serverd 01/23/24 13:36:44.988
STEP: check the unserved version gets removed 01/23/24 13:36:44.999
STEP: check the other version is not changed 01/23/24 13:36:47.946
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:36:52.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3530" for this suite. 01/23/24 13:36:52.719
------------------------------
• [SLOW TEST] [18.424 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:36:34.299
    Jan 23 13:36:34.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename crd-publish-openapi 01/23/24 13:36:34.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:36:34.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:36:34.312
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 01/23/24 13:36:34.313
    Jan 23 13:36:34.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: mark a version not serverd 01/23/24 13:36:44.988
    STEP: check the unserved version gets removed 01/23/24 13:36:44.999
    STEP: check the other version is not changed 01/23/24 13:36:47.946
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:36:52.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3530" for this suite. 01/23/24 13:36:52.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:36:52.724
Jan 23 13:36:52.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename webhook 01/23/24 13:36:52.724
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:36:52.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:36:52.733
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/23/24 13:36:52.743
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 13:36:53.046
STEP: Deploying the webhook pod 01/23/24 13:36:53.05
STEP: Wait for the deployment to be ready 01/23/24 13:36:53.059
Jan 23 13:36:53.064: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/23/24 13:36:55.07
STEP: Verifying the service has paired with the endpoint 01/23/24 13:36:55.075
Jan 23 13:36:56.075: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 01/23/24 13:36:56.107
STEP: Creating a configMap that does not comply to the validation webhook rules 01/23/24 13:36:56.132
STEP: Deleting the collection of validation webhooks 01/23/24 13:36:56.16
STEP: Creating a configMap that does not comply to the validation webhook rules 01/23/24 13:36:56.191
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:36:56.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4919" for this suite. 01/23/24 13:36:56.237
STEP: Destroying namespace "webhook-4919-markers" for this suite. 01/23/24 13:36:56.241
------------------------------
• [3.522 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:36:52.724
    Jan 23 13:36:52.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename webhook 01/23/24 13:36:52.724
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:36:52.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:36:52.733
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/23/24 13:36:52.743
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 13:36:53.046
    STEP: Deploying the webhook pod 01/23/24 13:36:53.05
    STEP: Wait for the deployment to be ready 01/23/24 13:36:53.059
    Jan 23 13:36:53.064: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/23/24 13:36:55.07
    STEP: Verifying the service has paired with the endpoint 01/23/24 13:36:55.075
    Jan 23 13:36:56.075: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 01/23/24 13:36:56.107
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/23/24 13:36:56.132
    STEP: Deleting the collection of validation webhooks 01/23/24 13:36:56.16
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/23/24 13:36:56.191
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:36:56.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4919" for this suite. 01/23/24 13:36:56.237
    STEP: Destroying namespace "webhook-4919-markers" for this suite. 01/23/24 13:36:56.241
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:36:56.246
Jan 23 13:36:56.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename container-probe 01/23/24 13:36:56.247
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:36:56.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:36:56.26
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-b89e67e4-f770-423c-8806-d358e0b8d1bd in namespace container-probe-3125 01/23/24 13:36:56.265
Jan 23 13:36:56.289: INFO: Waiting up to 5m0s for pod "busybox-b89e67e4-f770-423c-8806-d358e0b8d1bd" in namespace "container-probe-3125" to be "not pending"
Jan 23 13:36:56.296: INFO: Pod "busybox-b89e67e4-f770-423c-8806-d358e0b8d1bd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.421683ms
Jan 23 13:36:58.299: INFO: Pod "busybox-b89e67e4-f770-423c-8806-d358e0b8d1bd": Phase="Running", Reason="", readiness=true. Elapsed: 2.009273579s
Jan 23 13:36:58.299: INFO: Pod "busybox-b89e67e4-f770-423c-8806-d358e0b8d1bd" satisfied condition "not pending"
Jan 23 13:36:58.299: INFO: Started pod busybox-b89e67e4-f770-423c-8806-d358e0b8d1bd in namespace container-probe-3125
STEP: checking the pod's current state and verifying that restartCount is present 01/23/24 13:36:58.299
Jan 23 13:36:58.300: INFO: Initial restart count of pod busybox-b89e67e4-f770-423c-8806-d358e0b8d1bd is 0
STEP: deleting the pod 01/23/24 13:40:58.732
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 23 13:40:58.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3125" for this suite. 01/23/24 13:40:58.743
------------------------------
• [SLOW TEST] [242.500 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:36:56.246
    Jan 23 13:36:56.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename container-probe 01/23/24 13:36:56.247
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:36:56.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:36:56.26
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-b89e67e4-f770-423c-8806-d358e0b8d1bd in namespace container-probe-3125 01/23/24 13:36:56.265
    Jan 23 13:36:56.289: INFO: Waiting up to 5m0s for pod "busybox-b89e67e4-f770-423c-8806-d358e0b8d1bd" in namespace "container-probe-3125" to be "not pending"
    Jan 23 13:36:56.296: INFO: Pod "busybox-b89e67e4-f770-423c-8806-d358e0b8d1bd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.421683ms
    Jan 23 13:36:58.299: INFO: Pod "busybox-b89e67e4-f770-423c-8806-d358e0b8d1bd": Phase="Running", Reason="", readiness=true. Elapsed: 2.009273579s
    Jan 23 13:36:58.299: INFO: Pod "busybox-b89e67e4-f770-423c-8806-d358e0b8d1bd" satisfied condition "not pending"
    Jan 23 13:36:58.299: INFO: Started pod busybox-b89e67e4-f770-423c-8806-d358e0b8d1bd in namespace container-probe-3125
    STEP: checking the pod's current state and verifying that restartCount is present 01/23/24 13:36:58.299
    Jan 23 13:36:58.300: INFO: Initial restart count of pod busybox-b89e67e4-f770-423c-8806-d358e0b8d1bd is 0
    STEP: deleting the pod 01/23/24 13:40:58.732
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:40:58.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3125" for this suite. 01/23/24 13:40:58.743
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:40:58.746
Jan 23 13:40:58.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename emptydir 01/23/24 13:40:58.747
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:40:58.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:40:58.756
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/23/24 13:40:58.759
Jan 23 13:40:58.777: INFO: Waiting up to 5m0s for pod "pod-50984c64-13f5-497e-b786-98204b5f3f34" in namespace "emptydir-6977" to be "Succeeded or Failed"
Jan 23 13:40:58.779: INFO: Pod "pod-50984c64-13f5-497e-b786-98204b5f3f34": Phase="Pending", Reason="", readiness=false. Elapsed: 1.730713ms
Jan 23 13:41:00.781: INFO: Pod "pod-50984c64-13f5-497e-b786-98204b5f3f34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004474657s
Jan 23 13:41:02.784: INFO: Pod "pod-50984c64-13f5-497e-b786-98204b5f3f34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006678733s
STEP: Saw pod success 01/23/24 13:41:02.784
Jan 23 13:41:02.784: INFO: Pod "pod-50984c64-13f5-497e-b786-98204b5f3f34" satisfied condition "Succeeded or Failed"
Jan 23 13:41:02.785: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-50984c64-13f5-497e-b786-98204b5f3f34 container test-container: <nil>
STEP: delete the pod 01/23/24 13:41:02.797
Jan 23 13:41:02.804: INFO: Waiting for pod pod-50984c64-13f5-497e-b786-98204b5f3f34 to disappear
Jan 23 13:41:02.805: INFO: Pod pod-50984c64-13f5-497e-b786-98204b5f3f34 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 23 13:41:02.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6977" for this suite. 01/23/24 13:41:02.808
------------------------------
• [4.065 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:40:58.746
    Jan 23 13:40:58.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename emptydir 01/23/24 13:40:58.747
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:40:58.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:40:58.756
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/23/24 13:40:58.759
    Jan 23 13:40:58.777: INFO: Waiting up to 5m0s for pod "pod-50984c64-13f5-497e-b786-98204b5f3f34" in namespace "emptydir-6977" to be "Succeeded or Failed"
    Jan 23 13:40:58.779: INFO: Pod "pod-50984c64-13f5-497e-b786-98204b5f3f34": Phase="Pending", Reason="", readiness=false. Elapsed: 1.730713ms
    Jan 23 13:41:00.781: INFO: Pod "pod-50984c64-13f5-497e-b786-98204b5f3f34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004474657s
    Jan 23 13:41:02.784: INFO: Pod "pod-50984c64-13f5-497e-b786-98204b5f3f34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006678733s
    STEP: Saw pod success 01/23/24 13:41:02.784
    Jan 23 13:41:02.784: INFO: Pod "pod-50984c64-13f5-497e-b786-98204b5f3f34" satisfied condition "Succeeded or Failed"
    Jan 23 13:41:02.785: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-50984c64-13f5-497e-b786-98204b5f3f34 container test-container: <nil>
    STEP: delete the pod 01/23/24 13:41:02.797
    Jan 23 13:41:02.804: INFO: Waiting for pod pod-50984c64-13f5-497e-b786-98204b5f3f34 to disappear
    Jan 23 13:41:02.805: INFO: Pod pod-50984c64-13f5-497e-b786-98204b5f3f34 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:41:02.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6977" for this suite. 01/23/24 13:41:02.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:41:02.812
Jan 23 13:41:02.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename services 01/23/24 13:41:02.813
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:41:02.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:41:02.821
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 01/23/24 13:41:02.825
STEP: waiting for available Endpoint 01/23/24 13:41:02.828
STEP: listing all Endpoints 01/23/24 13:41:02.829
STEP: updating the Endpoint 01/23/24 13:41:02.831
STEP: fetching the Endpoint 01/23/24 13:41:02.834
STEP: patching the Endpoint 01/23/24 13:41:02.835
STEP: fetching the Endpoint 01/23/24 13:41:02.84
STEP: deleting the Endpoint by Collection 01/23/24 13:41:02.842
STEP: waiting for Endpoint deletion 01/23/24 13:41:02.847
STEP: fetching the Endpoint 01/23/24 13:41:02.848
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 23 13:41:02.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7937" for this suite. 01/23/24 13:41:02.852
------------------------------
• [0.043 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:41:02.812
    Jan 23 13:41:02.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename services 01/23/24 13:41:02.813
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:41:02.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:41:02.821
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 01/23/24 13:41:02.825
    STEP: waiting for available Endpoint 01/23/24 13:41:02.828
    STEP: listing all Endpoints 01/23/24 13:41:02.829
    STEP: updating the Endpoint 01/23/24 13:41:02.831
    STEP: fetching the Endpoint 01/23/24 13:41:02.834
    STEP: patching the Endpoint 01/23/24 13:41:02.835
    STEP: fetching the Endpoint 01/23/24 13:41:02.84
    STEP: deleting the Endpoint by Collection 01/23/24 13:41:02.842
    STEP: waiting for Endpoint deletion 01/23/24 13:41:02.847
    STEP: fetching the Endpoint 01/23/24 13:41:02.848
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:41:02.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7937" for this suite. 01/23/24 13:41:02.852
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:41:02.856
Jan 23 13:41:02.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename custom-resource-definition 01/23/24 13:41:02.857
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:41:02.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:41:02.864
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan 23 13:41:02.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:42:04.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1686" for this suite. 01/23/24 13:42:04.452
------------------------------
• [SLOW TEST] [61.599 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:41:02.856
    Jan 23 13:41:02.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename custom-resource-definition 01/23/24 13:41:02.857
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:41:02.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:41:02.864
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan 23 13:41:02.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:42:04.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1686" for this suite. 01/23/24 13:42:04.452
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:42:04.455
Jan 23 13:42:04.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename pod-network-test 01/23/24 13:42:04.456
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:42:04.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:42:04.465
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-2370 01/23/24 13:42:04.466
STEP: creating a selector 01/23/24 13:42:04.466
STEP: Creating the service pods in kubernetes 01/23/24 13:42:04.466
Jan 23 13:42:04.466: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 23 13:42:04.498: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2370" to be "running and ready"
Jan 23 13:42:04.504: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.963755ms
Jan 23 13:42:04.504: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 13:42:06.508: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009345928s
Jan 23 13:42:06.508: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:42:08.508: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009840347s
Jan 23 13:42:08.508: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:42:10.509: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010366853s
Jan 23 13:42:10.509: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:42:12.508: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009251216s
Jan 23 13:42:12.508: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:42:14.508: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00981722s
Jan 23 13:42:14.508: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:42:16.507: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008836439s
Jan 23 13:42:16.507: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:42:18.509: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010429029s
Jan 23 13:42:18.509: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:42:20.508: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009535066s
Jan 23 13:42:20.508: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:42:22.509: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010307354s
Jan 23 13:42:22.509: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:42:24.515: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.01621056s
Jan 23 13:42:24.515: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:42:26.507: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008510592s
Jan 23 13:42:26.507: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 23 13:42:26.507: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 23 13:42:26.509: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2370" to be "running and ready"
Jan 23 13:42:26.510: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.541975ms
Jan 23 13:42:26.510: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 23 13:42:26.510: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/23/24 13:42:26.512
Jan 23 13:42:26.521: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2370" to be "running"
Jan 23 13:42:26.523: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.521939ms
Jan 23 13:42:28.526: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004179139s
Jan 23 13:42:28.526: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 23 13:42:28.527: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 23 13:42:28.527: INFO: Breadth first check of 10.233.75.11 on host 172.31.11.40...
Jan 23 13:42:28.529: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.87.55:9080/dial?request=hostname&protocol=udp&host=10.233.75.11&port=8081&tries=1'] Namespace:pod-network-test-2370 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 13:42:28.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 13:42:28.529: INFO: ExecWithOptions: Clientset creation
Jan 23 13:42:28.529: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2370/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.87.55%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.75.11%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 23 13:42:28.587: INFO: Waiting for responses: map[]
Jan 23 13:42:28.587: INFO: reached 10.233.75.11 after 0/1 tries
Jan 23 13:42:28.587: INFO: Breadth first check of 10.233.87.40 on host 172.31.11.67...
Jan 23 13:42:28.589: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.87.55:9080/dial?request=hostname&protocol=udp&host=10.233.87.40&port=8081&tries=1'] Namespace:pod-network-test-2370 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 13:42:28.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 13:42:28.589: INFO: ExecWithOptions: Clientset creation
Jan 23 13:42:28.589: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2370/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.87.55%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.87.40%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 23 13:42:28.648: INFO: Waiting for responses: map[]
Jan 23 13:42:28.648: INFO: reached 10.233.87.40 after 0/1 tries
Jan 23 13:42:28.648: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 23 13:42:28.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2370" for this suite. 01/23/24 13:42:28.651
------------------------------
• [SLOW TEST] [24.200 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:42:04.455
    Jan 23 13:42:04.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename pod-network-test 01/23/24 13:42:04.456
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:42:04.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:42:04.465
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-2370 01/23/24 13:42:04.466
    STEP: creating a selector 01/23/24 13:42:04.466
    STEP: Creating the service pods in kubernetes 01/23/24 13:42:04.466
    Jan 23 13:42:04.466: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 23 13:42:04.498: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2370" to be "running and ready"
    Jan 23 13:42:04.504: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.963755ms
    Jan 23 13:42:04.504: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 13:42:06.508: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009345928s
    Jan 23 13:42:06.508: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:42:08.508: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009840347s
    Jan 23 13:42:08.508: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:42:10.509: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010366853s
    Jan 23 13:42:10.509: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:42:12.508: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009251216s
    Jan 23 13:42:12.508: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:42:14.508: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00981722s
    Jan 23 13:42:14.508: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:42:16.507: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008836439s
    Jan 23 13:42:16.507: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:42:18.509: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010429029s
    Jan 23 13:42:18.509: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:42:20.508: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009535066s
    Jan 23 13:42:20.508: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:42:22.509: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010307354s
    Jan 23 13:42:22.509: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:42:24.515: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.01621056s
    Jan 23 13:42:24.515: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:42:26.507: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008510592s
    Jan 23 13:42:26.507: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 23 13:42:26.507: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 23 13:42:26.509: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2370" to be "running and ready"
    Jan 23 13:42:26.510: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.541975ms
    Jan 23 13:42:26.510: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 23 13:42:26.510: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/23/24 13:42:26.512
    Jan 23 13:42:26.521: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2370" to be "running"
    Jan 23 13:42:26.523: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.521939ms
    Jan 23 13:42:28.526: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004179139s
    Jan 23 13:42:28.526: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 23 13:42:28.527: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 23 13:42:28.527: INFO: Breadth first check of 10.233.75.11 on host 172.31.11.40...
    Jan 23 13:42:28.529: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.87.55:9080/dial?request=hostname&protocol=udp&host=10.233.75.11&port=8081&tries=1'] Namespace:pod-network-test-2370 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 13:42:28.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 13:42:28.529: INFO: ExecWithOptions: Clientset creation
    Jan 23 13:42:28.529: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2370/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.87.55%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.75.11%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 23 13:42:28.587: INFO: Waiting for responses: map[]
    Jan 23 13:42:28.587: INFO: reached 10.233.75.11 after 0/1 tries
    Jan 23 13:42:28.587: INFO: Breadth first check of 10.233.87.40 on host 172.31.11.67...
    Jan 23 13:42:28.589: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.87.55:9080/dial?request=hostname&protocol=udp&host=10.233.87.40&port=8081&tries=1'] Namespace:pod-network-test-2370 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 13:42:28.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 13:42:28.589: INFO: ExecWithOptions: Clientset creation
    Jan 23 13:42:28.589: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2370/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.87.55%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.87.40%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 23 13:42:28.648: INFO: Waiting for responses: map[]
    Jan 23 13:42:28.648: INFO: reached 10.233.87.40 after 0/1 tries
    Jan 23 13:42:28.648: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:42:28.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2370" for this suite. 01/23/24 13:42:28.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:42:28.656
Jan 23 13:42:28.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename configmap 01/23/24 13:42:28.657
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:42:28.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:42:28.666
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-6850/configmap-test-08088c82-31a5-4a05-97f5-bd98b9ab8b75 01/23/24 13:42:28.668
STEP: Creating a pod to test consume configMaps 01/23/24 13:42:28.671
Jan 23 13:42:28.687: INFO: Waiting up to 5m0s for pod "pod-configmaps-45d45a96-120d-4e7c-b073-02e1f42e4153" in namespace "configmap-6850" to be "Succeeded or Failed"
Jan 23 13:42:28.689: INFO: Pod "pod-configmaps-45d45a96-120d-4e7c-b073-02e1f42e4153": Phase="Pending", Reason="", readiness=false. Elapsed: 1.800548ms
Jan 23 13:42:30.692: INFO: Pod "pod-configmaps-45d45a96-120d-4e7c-b073-02e1f42e4153": Phase="Running", Reason="", readiness=true. Elapsed: 2.004904674s
Jan 23 13:42:32.694: INFO: Pod "pod-configmaps-45d45a96-120d-4e7c-b073-02e1f42e4153": Phase="Running", Reason="", readiness=false. Elapsed: 4.006479958s
Jan 23 13:42:34.693: INFO: Pod "pod-configmaps-45d45a96-120d-4e7c-b073-02e1f42e4153": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00577372s
STEP: Saw pod success 01/23/24 13:42:34.693
Jan 23 13:42:34.693: INFO: Pod "pod-configmaps-45d45a96-120d-4e7c-b073-02e1f42e4153" satisfied condition "Succeeded or Failed"
Jan 23 13:42:34.695: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-45d45a96-120d-4e7c-b073-02e1f42e4153 container env-test: <nil>
STEP: delete the pod 01/23/24 13:42:34.705
Jan 23 13:42:34.713: INFO: Waiting for pod pod-configmaps-45d45a96-120d-4e7c-b073-02e1f42e4153 to disappear
Jan 23 13:42:34.714: INFO: Pod pod-configmaps-45d45a96-120d-4e7c-b073-02e1f42e4153 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 23 13:42:34.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6850" for this suite. 01/23/24 13:42:34.717
------------------------------
• [SLOW TEST] [6.064 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:42:28.656
    Jan 23 13:42:28.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename configmap 01/23/24 13:42:28.657
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:42:28.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:42:28.666
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-6850/configmap-test-08088c82-31a5-4a05-97f5-bd98b9ab8b75 01/23/24 13:42:28.668
    STEP: Creating a pod to test consume configMaps 01/23/24 13:42:28.671
    Jan 23 13:42:28.687: INFO: Waiting up to 5m0s for pod "pod-configmaps-45d45a96-120d-4e7c-b073-02e1f42e4153" in namespace "configmap-6850" to be "Succeeded or Failed"
    Jan 23 13:42:28.689: INFO: Pod "pod-configmaps-45d45a96-120d-4e7c-b073-02e1f42e4153": Phase="Pending", Reason="", readiness=false. Elapsed: 1.800548ms
    Jan 23 13:42:30.692: INFO: Pod "pod-configmaps-45d45a96-120d-4e7c-b073-02e1f42e4153": Phase="Running", Reason="", readiness=true. Elapsed: 2.004904674s
    Jan 23 13:42:32.694: INFO: Pod "pod-configmaps-45d45a96-120d-4e7c-b073-02e1f42e4153": Phase="Running", Reason="", readiness=false. Elapsed: 4.006479958s
    Jan 23 13:42:34.693: INFO: Pod "pod-configmaps-45d45a96-120d-4e7c-b073-02e1f42e4153": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00577372s
    STEP: Saw pod success 01/23/24 13:42:34.693
    Jan 23 13:42:34.693: INFO: Pod "pod-configmaps-45d45a96-120d-4e7c-b073-02e1f42e4153" satisfied condition "Succeeded or Failed"
    Jan 23 13:42:34.695: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-45d45a96-120d-4e7c-b073-02e1f42e4153 container env-test: <nil>
    STEP: delete the pod 01/23/24 13:42:34.705
    Jan 23 13:42:34.713: INFO: Waiting for pod pod-configmaps-45d45a96-120d-4e7c-b073-02e1f42e4153 to disappear
    Jan 23 13:42:34.714: INFO: Pod pod-configmaps-45d45a96-120d-4e7c-b073-02e1f42e4153 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:42:34.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6850" for this suite. 01/23/24 13:42:34.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:42:34.723
Jan 23 13:42:34.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename svcaccounts 01/23/24 13:42:34.724
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:42:34.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:42:34.735
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-5tmjv"  01/23/24 13:42:34.736
Jan 23 13:42:34.739: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-5tmjv"  01/23/24 13:42:34.739
Jan 23 13:42:34.744: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 23 13:42:34.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2675" for this suite. 01/23/24 13:42:34.746
------------------------------
• [0.027 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:42:34.723
    Jan 23 13:42:34.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename svcaccounts 01/23/24 13:42:34.724
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:42:34.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:42:34.735
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-5tmjv"  01/23/24 13:42:34.736
    Jan 23 13:42:34.739: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-5tmjv"  01/23/24 13:42:34.739
    Jan 23 13:42:34.744: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:42:34.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2675" for this suite. 01/23/24 13:42:34.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:42:34.75
Jan 23 13:42:34.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename downward-api 01/23/24 13:42:34.751
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:42:34.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:42:34.76
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 01/23/24 13:42:34.762
Jan 23 13:42:34.773: INFO: Waiting up to 5m0s for pod "downward-api-86c0463f-3d8c-436f-a3ab-f4f042d886b9" in namespace "downward-api-7400" to be "Succeeded or Failed"
Jan 23 13:42:34.775: INFO: Pod "downward-api-86c0463f-3d8c-436f-a3ab-f4f042d886b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.856723ms
Jan 23 13:42:36.778: INFO: Pod "downward-api-86c0463f-3d8c-436f-a3ab-f4f042d886b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.005065621s
Jan 23 13:42:38.778: INFO: Pod "downward-api-86c0463f-3d8c-436f-a3ab-f4f042d886b9": Phase="Running", Reason="", readiness=false. Elapsed: 4.005103834s
Jan 23 13:42:40.777: INFO: Pod "downward-api-86c0463f-3d8c-436f-a3ab-f4f042d886b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004105341s
STEP: Saw pod success 01/23/24 13:42:40.777
Jan 23 13:42:40.777: INFO: Pod "downward-api-86c0463f-3d8c-436f-a3ab-f4f042d886b9" satisfied condition "Succeeded or Failed"
Jan 23 13:42:40.778: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downward-api-86c0463f-3d8c-436f-a3ab-f4f042d886b9 container dapi-container: <nil>
STEP: delete the pod 01/23/24 13:42:40.783
Jan 23 13:42:40.789: INFO: Waiting for pod downward-api-86c0463f-3d8c-436f-a3ab-f4f042d886b9 to disappear
Jan 23 13:42:40.790: INFO: Pod downward-api-86c0463f-3d8c-436f-a3ab-f4f042d886b9 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 23 13:42:40.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7400" for this suite. 01/23/24 13:42:40.792
------------------------------
• [SLOW TEST] [6.045 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:42:34.75
    Jan 23 13:42:34.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename downward-api 01/23/24 13:42:34.751
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:42:34.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:42:34.76
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 01/23/24 13:42:34.762
    Jan 23 13:42:34.773: INFO: Waiting up to 5m0s for pod "downward-api-86c0463f-3d8c-436f-a3ab-f4f042d886b9" in namespace "downward-api-7400" to be "Succeeded or Failed"
    Jan 23 13:42:34.775: INFO: Pod "downward-api-86c0463f-3d8c-436f-a3ab-f4f042d886b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.856723ms
    Jan 23 13:42:36.778: INFO: Pod "downward-api-86c0463f-3d8c-436f-a3ab-f4f042d886b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.005065621s
    Jan 23 13:42:38.778: INFO: Pod "downward-api-86c0463f-3d8c-436f-a3ab-f4f042d886b9": Phase="Running", Reason="", readiness=false. Elapsed: 4.005103834s
    Jan 23 13:42:40.777: INFO: Pod "downward-api-86c0463f-3d8c-436f-a3ab-f4f042d886b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004105341s
    STEP: Saw pod success 01/23/24 13:42:40.777
    Jan 23 13:42:40.777: INFO: Pod "downward-api-86c0463f-3d8c-436f-a3ab-f4f042d886b9" satisfied condition "Succeeded or Failed"
    Jan 23 13:42:40.778: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downward-api-86c0463f-3d8c-436f-a3ab-f4f042d886b9 container dapi-container: <nil>
    STEP: delete the pod 01/23/24 13:42:40.783
    Jan 23 13:42:40.789: INFO: Waiting for pod downward-api-86c0463f-3d8c-436f-a3ab-f4f042d886b9 to disappear
    Jan 23 13:42:40.790: INFO: Pod downward-api-86c0463f-3d8c-436f-a3ab-f4f042d886b9 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:42:40.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7400" for this suite. 01/23/24 13:42:40.792
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:42:40.795
Jan 23 13:42:40.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename container-lifecycle-hook 01/23/24 13:42:40.796
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:42:40.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:42:40.807
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/23/24 13:42:40.812
Jan 23 13:42:40.827: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6674" to be "running and ready"
Jan 23 13:42:40.828: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.892256ms
Jan 23 13:42:40.828: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 23 13:42:42.831: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004873344s
Jan 23 13:42:42.831: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 23 13:42:42.831: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 01/23/24 13:42:42.833
Jan 23 13:42:42.845: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6674" to be "running and ready"
Jan 23 13:42:42.847: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.525852ms
Jan 23 13:42:42.847: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 23 13:42:44.850: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00476326s
Jan 23 13:42:44.850: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan 23 13:42:44.850: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/23/24 13:42:44.852
Jan 23 13:42:44.855: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 13:42:44.857: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 13:42:46.857: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 13:42:46.860: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 13:42:48.858: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 13:42:48.860: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/23/24 13:42:48.86
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 23 13:42:48.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6674" for this suite. 01/23/24 13:42:48.875
------------------------------
• [SLOW TEST] [8.083 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:42:40.795
    Jan 23 13:42:40.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/23/24 13:42:40.796
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:42:40.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:42:40.807
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/23/24 13:42:40.812
    Jan 23 13:42:40.827: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6674" to be "running and ready"
    Jan 23 13:42:40.828: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.892256ms
    Jan 23 13:42:40.828: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 13:42:42.831: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004873344s
    Jan 23 13:42:42.831: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 23 13:42:42.831: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 01/23/24 13:42:42.833
    Jan 23 13:42:42.845: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6674" to be "running and ready"
    Jan 23 13:42:42.847: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.525852ms
    Jan 23 13:42:42.847: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 13:42:44.850: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00476326s
    Jan 23 13:42:44.850: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan 23 13:42:44.850: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/23/24 13:42:44.852
    Jan 23 13:42:44.855: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 23 13:42:44.857: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 23 13:42:46.857: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 23 13:42:46.860: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 23 13:42:48.858: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 23 13:42:48.860: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/23/24 13:42:48.86
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:42:48.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6674" for this suite. 01/23/24 13:42:48.875
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:42:48.878
Jan 23 13:42:48.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename crd-publish-openapi 01/23/24 13:42:48.879
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:42:48.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:42:48.887
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/23/24 13:42:48.889
Jan 23 13:42:48.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/23/24 13:43:03.425
Jan 23 13:43:03.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 13:43:10.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:43:25.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-880" for this suite. 01/23/24 13:43:25.593
------------------------------
• [SLOW TEST] [36.718 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:42:48.878
    Jan 23 13:42:48.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename crd-publish-openapi 01/23/24 13:42:48.879
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:42:48.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:42:48.887
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/23/24 13:42:48.889
    Jan 23 13:42:48.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/23/24 13:43:03.425
    Jan 23 13:43:03.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 13:43:10.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:43:25.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-880" for this suite. 01/23/24 13:43:25.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:43:25.598
Jan 23 13:43:25.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename var-expansion 01/23/24 13:43:25.599
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:43:25.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:43:25.608
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 01/23/24 13:43:25.61
Jan 23 13:43:25.623: INFO: Waiting up to 2m0s for pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae" in namespace "var-expansion-1343" to be "running"
Jan 23 13:43:25.624: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1.689235ms
Jan 23 13:43:27.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004209298s
Jan 23 13:43:29.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004739926s
Jan 23 13:43:31.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005097707s
Jan 23 13:43:33.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004936045s
Jan 23 13:43:35.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005581451s
Jan 23 13:43:37.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005116411s
Jan 23 13:43:39.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005718887s
Jan 23 13:43:41.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004417721s
Jan 23 13:43:43.629: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005888838s
Jan 23 13:43:45.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004855745s
Jan 23 13:43:47.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004970881s
Jan 23 13:43:49.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 24.005428651s
Jan 23 13:43:51.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004839743s
Jan 23 13:43:53.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004534022s
Jan 23 13:43:55.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005401078s
Jan 23 13:43:57.629: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 32.006267002s
Jan 23 13:43:59.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 34.0053868s
Jan 23 13:44:01.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 36.004442292s
Jan 23 13:44:03.629: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006050734s
Jan 23 13:44:05.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004556378s
Jan 23 13:44:07.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 42.00448743s
Jan 23 13:44:09.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005587441s
Jan 23 13:44:11.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004697706s
Jan 23 13:44:13.629: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006144007s
Jan 23 13:44:15.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 50.004311632s
Jan 23 13:44:17.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005357681s
Jan 23 13:44:19.629: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005868188s
Jan 23 13:44:21.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005188702s
Jan 23 13:44:23.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 58.005606117s
Jan 23 13:44:25.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004886469s
Jan 23 13:44:27.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005349222s
Jan 23 13:44:29.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.00489594s
Jan 23 13:44:31.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004430519s
Jan 23 13:44:33.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.004844451s
Jan 23 13:44:35.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004675435s
Jan 23 13:44:37.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004910124s
Jan 23 13:44:39.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004181992s
Jan 23 13:44:41.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.005384484s
Jan 23 13:44:43.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005697417s
Jan 23 13:44:45.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004504497s
Jan 23 13:44:47.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.005644439s
Jan 23 13:44:49.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.004003301s
Jan 23 13:44:51.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.004675835s
Jan 23 13:44:53.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.005101944s
Jan 23 13:44:55.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004799256s
Jan 23 13:44:57.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004352977s
Jan 23 13:44:59.626: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.003694167s
Jan 23 13:45:01.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.004443899s
Jan 23 13:45:03.629: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006578661s
Jan 23 13:45:05.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004566207s
Jan 23 13:45:07.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005630035s
Jan 23 13:45:09.629: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.005870878s
Jan 23 13:45:11.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.004661517s
Jan 23 13:45:13.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005386399s
Jan 23 13:45:15.629: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.005769107s
Jan 23 13:45:17.629: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006088469s
Jan 23 13:45:19.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.004431528s
Jan 23 13:45:21.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004917183s
Jan 23 13:45:23.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.00534948s
Jan 23 13:45:25.637: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.014696008s
Jan 23 13:45:25.643: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.019785855s
STEP: updating the pod 01/23/24 13:45:25.643
Jan 23 13:45:26.151: INFO: Successfully updated pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae"
STEP: waiting for pod running 01/23/24 13:45:26.151
Jan 23 13:45:26.151: INFO: Waiting up to 2m0s for pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae" in namespace "var-expansion-1343" to be "running"
Jan 23 13:45:26.154: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.203349ms
Jan 23 13:45:28.158: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Running", Reason="", readiness=true. Elapsed: 2.006189793s
Jan 23 13:45:28.158: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae" satisfied condition "running"
STEP: deleting the pod gracefully 01/23/24 13:45:28.158
Jan 23 13:45:28.158: INFO: Deleting pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae" in namespace "var-expansion-1343"
Jan 23 13:45:28.162: INFO: Wait up to 5m0s for pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 23 13:46:00.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1343" for this suite. 01/23/24 13:46:00.173
------------------------------
• [SLOW TEST] [154.578 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:43:25.598
    Jan 23 13:43:25.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename var-expansion 01/23/24 13:43:25.599
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:43:25.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:43:25.608
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 01/23/24 13:43:25.61
    Jan 23 13:43:25.623: INFO: Waiting up to 2m0s for pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae" in namespace "var-expansion-1343" to be "running"
    Jan 23 13:43:25.624: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1.689235ms
    Jan 23 13:43:27.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004209298s
    Jan 23 13:43:29.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004739926s
    Jan 23 13:43:31.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005097707s
    Jan 23 13:43:33.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004936045s
    Jan 23 13:43:35.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005581451s
    Jan 23 13:43:37.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005116411s
    Jan 23 13:43:39.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005718887s
    Jan 23 13:43:41.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004417721s
    Jan 23 13:43:43.629: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005888838s
    Jan 23 13:43:45.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004855745s
    Jan 23 13:43:47.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004970881s
    Jan 23 13:43:49.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 24.005428651s
    Jan 23 13:43:51.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004839743s
    Jan 23 13:43:53.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004534022s
    Jan 23 13:43:55.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005401078s
    Jan 23 13:43:57.629: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 32.006267002s
    Jan 23 13:43:59.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 34.0053868s
    Jan 23 13:44:01.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 36.004442292s
    Jan 23 13:44:03.629: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006050734s
    Jan 23 13:44:05.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004556378s
    Jan 23 13:44:07.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 42.00448743s
    Jan 23 13:44:09.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005587441s
    Jan 23 13:44:11.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004697706s
    Jan 23 13:44:13.629: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006144007s
    Jan 23 13:44:15.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 50.004311632s
    Jan 23 13:44:17.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005357681s
    Jan 23 13:44:19.629: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005868188s
    Jan 23 13:44:21.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005188702s
    Jan 23 13:44:23.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 58.005606117s
    Jan 23 13:44:25.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004886469s
    Jan 23 13:44:27.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005349222s
    Jan 23 13:44:29.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.00489594s
    Jan 23 13:44:31.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004430519s
    Jan 23 13:44:33.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.004844451s
    Jan 23 13:44:35.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004675435s
    Jan 23 13:44:37.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004910124s
    Jan 23 13:44:39.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004181992s
    Jan 23 13:44:41.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.005384484s
    Jan 23 13:44:43.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005697417s
    Jan 23 13:44:45.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004504497s
    Jan 23 13:44:47.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.005644439s
    Jan 23 13:44:49.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.004003301s
    Jan 23 13:44:51.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.004675835s
    Jan 23 13:44:53.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.005101944s
    Jan 23 13:44:55.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004799256s
    Jan 23 13:44:57.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004352977s
    Jan 23 13:44:59.626: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.003694167s
    Jan 23 13:45:01.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.004443899s
    Jan 23 13:45:03.629: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006578661s
    Jan 23 13:45:05.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004566207s
    Jan 23 13:45:07.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005630035s
    Jan 23 13:45:09.629: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.005870878s
    Jan 23 13:45:11.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.004661517s
    Jan 23 13:45:13.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005386399s
    Jan 23 13:45:15.629: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.005769107s
    Jan 23 13:45:17.629: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006088469s
    Jan 23 13:45:19.627: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.004431528s
    Jan 23 13:45:21.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004917183s
    Jan 23 13:45:23.628: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.00534948s
    Jan 23 13:45:25.637: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.014696008s
    Jan 23 13:45:25.643: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.019785855s
    STEP: updating the pod 01/23/24 13:45:25.643
    Jan 23 13:45:26.151: INFO: Successfully updated pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae"
    STEP: waiting for pod running 01/23/24 13:45:26.151
    Jan 23 13:45:26.151: INFO: Waiting up to 2m0s for pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae" in namespace "var-expansion-1343" to be "running"
    Jan 23 13:45:26.154: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.203349ms
    Jan 23 13:45:28.158: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae": Phase="Running", Reason="", readiness=true. Elapsed: 2.006189793s
    Jan 23 13:45:28.158: INFO: Pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae" satisfied condition "running"
    STEP: deleting the pod gracefully 01/23/24 13:45:28.158
    Jan 23 13:45:28.158: INFO: Deleting pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae" in namespace "var-expansion-1343"
    Jan 23 13:45:28.162: INFO: Wait up to 5m0s for pod "var-expansion-195b5382-aaf2-4521-b9be-e9a0ed2130ae" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:46:00.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1343" for this suite. 01/23/24 13:46:00.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:46:00.177
Jan 23 13:46:00.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename webhook 01/23/24 13:46:00.178
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:00.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:00.187
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/23/24 13:46:00.197
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 13:46:00.604
STEP: Deploying the webhook pod 01/23/24 13:46:00.608
STEP: Wait for the deployment to be ready 01/23/24 13:46:00.616
Jan 23 13:46:00.619: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/23/24 13:46:02.626
STEP: Verifying the service has paired with the endpoint 01/23/24 13:46:02.631
Jan 23 13:46:03.632: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 01/23/24 13:46:03.634
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/23/24 13:46:03.635
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/23/24 13:46:03.635
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/23/24 13:46:03.636
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/23/24 13:46:03.636
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/23/24 13:46:03.636
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/23/24 13:46:03.637
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:46:03.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3624" for this suite. 01/23/24 13:46:03.665
STEP: Destroying namespace "webhook-3624-markers" for this suite. 01/23/24 13:46:03.668
------------------------------
• [3.496 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:46:00.177
    Jan 23 13:46:00.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename webhook 01/23/24 13:46:00.178
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:00.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:00.187
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/23/24 13:46:00.197
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 13:46:00.604
    STEP: Deploying the webhook pod 01/23/24 13:46:00.608
    STEP: Wait for the deployment to be ready 01/23/24 13:46:00.616
    Jan 23 13:46:00.619: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/23/24 13:46:02.626
    STEP: Verifying the service has paired with the endpoint 01/23/24 13:46:02.631
    Jan 23 13:46:03.632: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 01/23/24 13:46:03.634
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/23/24 13:46:03.635
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/23/24 13:46:03.635
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/23/24 13:46:03.636
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/23/24 13:46:03.636
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/23/24 13:46:03.636
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/23/24 13:46:03.637
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:46:03.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3624" for this suite. 01/23/24 13:46:03.665
    STEP: Destroying namespace "webhook-3624-markers" for this suite. 01/23/24 13:46:03.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:46:03.674
Jan 23 13:46:03.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename emptydir 01/23/24 13:46:03.675
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:03.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:03.693
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/23/24 13:46:03.695
Jan 23 13:46:03.713: INFO: Waiting up to 5m0s for pod "pod-60c12599-a550-4330-b520-ce3b4c6ee28b" in namespace "emptydir-7222" to be "Succeeded or Failed"
Jan 23 13:46:03.715: INFO: Pod "pod-60c12599-a550-4330-b520-ce3b4c6ee28b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.927237ms
Jan 23 13:46:05.718: INFO: Pod "pod-60c12599-a550-4330-b520-ce3b4c6ee28b": Phase="Running", Reason="", readiness=true. Elapsed: 2.005075224s
Jan 23 13:46:07.717: INFO: Pod "pod-60c12599-a550-4330-b520-ce3b4c6ee28b": Phase="Running", Reason="", readiness=false. Elapsed: 4.004392028s
Jan 23 13:46:09.718: INFO: Pod "pod-60c12599-a550-4330-b520-ce3b4c6ee28b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005230684s
STEP: Saw pod success 01/23/24 13:46:09.718
Jan 23 13:46:09.718: INFO: Pod "pod-60c12599-a550-4330-b520-ce3b4c6ee28b" satisfied condition "Succeeded or Failed"
Jan 23 13:46:09.719: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-60c12599-a550-4330-b520-ce3b4c6ee28b container test-container: <nil>
STEP: delete the pod 01/23/24 13:46:09.731
Jan 23 13:46:09.737: INFO: Waiting for pod pod-60c12599-a550-4330-b520-ce3b4c6ee28b to disappear
Jan 23 13:46:09.738: INFO: Pod pod-60c12599-a550-4330-b520-ce3b4c6ee28b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 23 13:46:09.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7222" for this suite. 01/23/24 13:46:09.741
------------------------------
• [SLOW TEST] [6.070 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:46:03.674
    Jan 23 13:46:03.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename emptydir 01/23/24 13:46:03.675
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:03.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:03.693
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/23/24 13:46:03.695
    Jan 23 13:46:03.713: INFO: Waiting up to 5m0s for pod "pod-60c12599-a550-4330-b520-ce3b4c6ee28b" in namespace "emptydir-7222" to be "Succeeded or Failed"
    Jan 23 13:46:03.715: INFO: Pod "pod-60c12599-a550-4330-b520-ce3b4c6ee28b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.927237ms
    Jan 23 13:46:05.718: INFO: Pod "pod-60c12599-a550-4330-b520-ce3b4c6ee28b": Phase="Running", Reason="", readiness=true. Elapsed: 2.005075224s
    Jan 23 13:46:07.717: INFO: Pod "pod-60c12599-a550-4330-b520-ce3b4c6ee28b": Phase="Running", Reason="", readiness=false. Elapsed: 4.004392028s
    Jan 23 13:46:09.718: INFO: Pod "pod-60c12599-a550-4330-b520-ce3b4c6ee28b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005230684s
    STEP: Saw pod success 01/23/24 13:46:09.718
    Jan 23 13:46:09.718: INFO: Pod "pod-60c12599-a550-4330-b520-ce3b4c6ee28b" satisfied condition "Succeeded or Failed"
    Jan 23 13:46:09.719: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-60c12599-a550-4330-b520-ce3b4c6ee28b container test-container: <nil>
    STEP: delete the pod 01/23/24 13:46:09.731
    Jan 23 13:46:09.737: INFO: Waiting for pod pod-60c12599-a550-4330-b520-ce3b4c6ee28b to disappear
    Jan 23 13:46:09.738: INFO: Pod pod-60c12599-a550-4330-b520-ce3b4c6ee28b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:46:09.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7222" for this suite. 01/23/24 13:46:09.741
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:46:09.744
Jan 23 13:46:09.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubectl 01/23/24 13:46:09.745
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:09.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:09.754
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 01/23/24 13:46:09.756
Jan 23 13:46:09.756: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-6341 proxy --unix-socket=/tmp/kubectl-proxy-unix3731674774/test'
STEP: retrieving proxy /api/ output 01/23/24 13:46:09.8
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 23 13:46:09.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6341" for this suite. 01/23/24 13:46:09.803
------------------------------
• [0.062 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:46:09.744
    Jan 23 13:46:09.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubectl 01/23/24 13:46:09.745
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:09.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:09.754
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 01/23/24 13:46:09.756
    Jan 23 13:46:09.756: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-6341 proxy --unix-socket=/tmp/kubectl-proxy-unix3731674774/test'
    STEP: retrieving proxy /api/ output 01/23/24 13:46:09.8
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:46:09.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6341" for this suite. 01/23/24 13:46:09.803
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:46:09.807
Jan 23 13:46:09.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename configmap 01/23/24 13:46:09.808
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:09.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:09.817
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-b3cee17f-f827-4807-868c-25bd988913fb 01/23/24 13:46:09.819
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 23 13:46:09.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4357" for this suite. 01/23/24 13:46:09.824
------------------------------
• [0.020 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:46:09.807
    Jan 23 13:46:09.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename configmap 01/23/24 13:46:09.808
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:09.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:09.817
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-b3cee17f-f827-4807-868c-25bd988913fb 01/23/24 13:46:09.819
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:46:09.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4357" for this suite. 01/23/24 13:46:09.824
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:46:09.827
Jan 23 13:46:09.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename downward-api 01/23/24 13:46:09.828
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:09.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:09.837
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 01/23/24 13:46:09.839
Jan 23 13:46:09.853: INFO: Waiting up to 5m0s for pod "downward-api-e3faea58-090c-4a1f-ac71-7736de5640bd" in namespace "downward-api-7316" to be "Succeeded or Failed"
Jan 23 13:46:09.855: INFO: Pod "downward-api-e3faea58-090c-4a1f-ac71-7736de5640bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.80531ms
Jan 23 13:46:11.857: INFO: Pod "downward-api-e3faea58-090c-4a1f-ac71-7736de5640bd": Phase="Running", Reason="", readiness=true. Elapsed: 2.003943505s
Jan 23 13:46:13.858: INFO: Pod "downward-api-e3faea58-090c-4a1f-ac71-7736de5640bd": Phase="Running", Reason="", readiness=false. Elapsed: 4.004688387s
Jan 23 13:46:15.859: INFO: Pod "downward-api-e3faea58-090c-4a1f-ac71-7736de5640bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0052148s
STEP: Saw pod success 01/23/24 13:46:15.859
Jan 23 13:46:15.859: INFO: Pod "downward-api-e3faea58-090c-4a1f-ac71-7736de5640bd" satisfied condition "Succeeded or Failed"
Jan 23 13:46:15.860: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downward-api-e3faea58-090c-4a1f-ac71-7736de5640bd container dapi-container: <nil>
STEP: delete the pod 01/23/24 13:46:15.865
Jan 23 13:46:15.872: INFO: Waiting for pod downward-api-e3faea58-090c-4a1f-ac71-7736de5640bd to disappear
Jan 23 13:46:15.873: INFO: Pod downward-api-e3faea58-090c-4a1f-ac71-7736de5640bd no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 23 13:46:15.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7316" for this suite. 01/23/24 13:46:15.876
------------------------------
• [SLOW TEST] [6.051 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:46:09.827
    Jan 23 13:46:09.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename downward-api 01/23/24 13:46:09.828
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:09.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:09.837
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 01/23/24 13:46:09.839
    Jan 23 13:46:09.853: INFO: Waiting up to 5m0s for pod "downward-api-e3faea58-090c-4a1f-ac71-7736de5640bd" in namespace "downward-api-7316" to be "Succeeded or Failed"
    Jan 23 13:46:09.855: INFO: Pod "downward-api-e3faea58-090c-4a1f-ac71-7736de5640bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.80531ms
    Jan 23 13:46:11.857: INFO: Pod "downward-api-e3faea58-090c-4a1f-ac71-7736de5640bd": Phase="Running", Reason="", readiness=true. Elapsed: 2.003943505s
    Jan 23 13:46:13.858: INFO: Pod "downward-api-e3faea58-090c-4a1f-ac71-7736de5640bd": Phase="Running", Reason="", readiness=false. Elapsed: 4.004688387s
    Jan 23 13:46:15.859: INFO: Pod "downward-api-e3faea58-090c-4a1f-ac71-7736de5640bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0052148s
    STEP: Saw pod success 01/23/24 13:46:15.859
    Jan 23 13:46:15.859: INFO: Pod "downward-api-e3faea58-090c-4a1f-ac71-7736de5640bd" satisfied condition "Succeeded or Failed"
    Jan 23 13:46:15.860: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downward-api-e3faea58-090c-4a1f-ac71-7736de5640bd container dapi-container: <nil>
    STEP: delete the pod 01/23/24 13:46:15.865
    Jan 23 13:46:15.872: INFO: Waiting for pod downward-api-e3faea58-090c-4a1f-ac71-7736de5640bd to disappear
    Jan 23 13:46:15.873: INFO: Pod downward-api-e3faea58-090c-4a1f-ac71-7736de5640bd no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:46:15.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7316" for this suite. 01/23/24 13:46:15.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:46:15.879
Jan 23 13:46:15.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename configmap 01/23/24 13:46:15.88
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:15.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:15.889
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 01/23/24 13:46:15.891
STEP: fetching the ConfigMap 01/23/24 13:46:15.895
STEP: patching the ConfigMap 01/23/24 13:46:15.897
STEP: listing all ConfigMaps in all namespaces with a label selector 01/23/24 13:46:15.901
STEP: deleting the ConfigMap by collection with a label selector 01/23/24 13:46:15.932
STEP: listing all ConfigMaps in test namespace 01/23/24 13:46:15.936
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 23 13:46:15.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6912" for this suite. 01/23/24 13:46:15.94
------------------------------
• [0.064 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:46:15.879
    Jan 23 13:46:15.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename configmap 01/23/24 13:46:15.88
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:15.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:15.889
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 01/23/24 13:46:15.891
    STEP: fetching the ConfigMap 01/23/24 13:46:15.895
    STEP: patching the ConfigMap 01/23/24 13:46:15.897
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/23/24 13:46:15.901
    STEP: deleting the ConfigMap by collection with a label selector 01/23/24 13:46:15.932
    STEP: listing all ConfigMaps in test namespace 01/23/24 13:46:15.936
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:46:15.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6912" for this suite. 01/23/24 13:46:15.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:46:15.943
Jan 23 13:46:15.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename endpointslice 01/23/24 13:46:15.944
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:15.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:15.952
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 23 13:46:17.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3004" for this suite. 01/23/24 13:46:17.979
------------------------------
• [2.039 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:46:15.943
    Jan 23 13:46:15.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename endpointslice 01/23/24 13:46:15.944
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:15.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:15.952
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:46:17.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3004" for this suite. 01/23/24 13:46:17.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:46:17.983
Jan 23 13:46:17.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename secrets 01/23/24 13:46:17.984
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:17.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:17.995
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 23 13:46:18.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2372" for this suite. 01/23/24 13:46:18.03
------------------------------
• [0.051 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:46:17.983
    Jan 23 13:46:17.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename secrets 01/23/24 13:46:17.984
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:17.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:17.995
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:46:18.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2372" for this suite. 01/23/24 13:46:18.03
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:46:18.035
Jan 23 13:46:18.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename proxy 01/23/24 13:46:18.036
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:18.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:18.049
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan 23 13:46:18.052: INFO: Creating pod...
Jan 23 13:46:18.064: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-596" to be "running"
Jan 23 13:46:18.066: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.385034ms
Jan 23 13:46:20.070: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005180661s
Jan 23 13:46:20.070: INFO: Pod "agnhost" satisfied condition "running"
Jan 23 13:46:20.070: INFO: Creating service...
Jan 23 13:46:20.076: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/pods/agnhost/proxy?method=DELETE
Jan 23 13:46:20.079: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 23 13:46:20.079: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/pods/agnhost/proxy?method=OPTIONS
Jan 23 13:46:20.081: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 23 13:46:20.081: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/pods/agnhost/proxy?method=PATCH
Jan 23 13:46:20.083: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 23 13:46:20.083: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/pods/agnhost/proxy?method=POST
Jan 23 13:46:20.084: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 23 13:46:20.084: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/pods/agnhost/proxy?method=PUT
Jan 23 13:46:20.086: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 23 13:46:20.086: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 23 13:46:20.088: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 23 13:46:20.088: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 23 13:46:20.090: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 23 13:46:20.090: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 23 13:46:20.092: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 23 13:46:20.092: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/services/e2e-proxy-test-service/proxy?method=POST
Jan 23 13:46:20.094: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 23 13:46:20.094: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/services/e2e-proxy-test-service/proxy?method=PUT
Jan 23 13:46:20.096: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 23 13:46:20.096: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/pods/agnhost/proxy?method=GET
Jan 23 13:46:20.097: INFO: http.Client request:GET StatusCode:301
Jan 23 13:46:20.097: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/services/e2e-proxy-test-service/proxy?method=GET
Jan 23 13:46:20.099: INFO: http.Client request:GET StatusCode:301
Jan 23 13:46:20.099: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/pods/agnhost/proxy?method=HEAD
Jan 23 13:46:20.100: INFO: http.Client request:HEAD StatusCode:301
Jan 23 13:46:20.100: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 23 13:46:20.102: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 23 13:46:20.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-596" for this suite. 01/23/24 13:46:20.104
------------------------------
• [2.072 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:46:18.035
    Jan 23 13:46:18.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename proxy 01/23/24 13:46:18.036
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:18.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:18.049
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan 23 13:46:18.052: INFO: Creating pod...
    Jan 23 13:46:18.064: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-596" to be "running"
    Jan 23 13:46:18.066: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.385034ms
    Jan 23 13:46:20.070: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005180661s
    Jan 23 13:46:20.070: INFO: Pod "agnhost" satisfied condition "running"
    Jan 23 13:46:20.070: INFO: Creating service...
    Jan 23 13:46:20.076: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/pods/agnhost/proxy?method=DELETE
    Jan 23 13:46:20.079: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 23 13:46:20.079: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/pods/agnhost/proxy?method=OPTIONS
    Jan 23 13:46:20.081: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 23 13:46:20.081: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/pods/agnhost/proxy?method=PATCH
    Jan 23 13:46:20.083: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 23 13:46:20.083: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/pods/agnhost/proxy?method=POST
    Jan 23 13:46:20.084: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 23 13:46:20.084: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/pods/agnhost/proxy?method=PUT
    Jan 23 13:46:20.086: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 23 13:46:20.086: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan 23 13:46:20.088: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 23 13:46:20.088: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan 23 13:46:20.090: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 23 13:46:20.090: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan 23 13:46:20.092: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 23 13:46:20.092: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/services/e2e-proxy-test-service/proxy?method=POST
    Jan 23 13:46:20.094: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 23 13:46:20.094: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/services/e2e-proxy-test-service/proxy?method=PUT
    Jan 23 13:46:20.096: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 23 13:46:20.096: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/pods/agnhost/proxy?method=GET
    Jan 23 13:46:20.097: INFO: http.Client request:GET StatusCode:301
    Jan 23 13:46:20.097: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/services/e2e-proxy-test-service/proxy?method=GET
    Jan 23 13:46:20.099: INFO: http.Client request:GET StatusCode:301
    Jan 23 13:46:20.099: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/pods/agnhost/proxy?method=HEAD
    Jan 23 13:46:20.100: INFO: http.Client request:HEAD StatusCode:301
    Jan 23 13:46:20.100: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-596/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan 23 13:46:20.102: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:46:20.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-596" for this suite. 01/23/24 13:46:20.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:46:20.108
Jan 23 13:46:20.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 13:46:20.109
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:20.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:20.12
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-e43d1232-37b3-4d24-b4fa-ba0690e82d50 01/23/24 13:46:20.121
STEP: Creating a pod to test consume configMaps 01/23/24 13:46:20.126
Jan 23 13:46:20.141: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bfe9354a-0a73-4047-a192-e99c093266b9" in namespace "projected-5676" to be "Succeeded or Failed"
Jan 23 13:46:20.143: INFO: Pod "pod-projected-configmaps-bfe9354a-0a73-4047-a192-e99c093266b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.636629ms
Jan 23 13:46:22.147: INFO: Pod "pod-projected-configmaps-bfe9354a-0a73-4047-a192-e99c093266b9": Phase="Running", Reason="", readiness=false. Elapsed: 2.005474007s
Jan 23 13:46:24.146: INFO: Pod "pod-projected-configmaps-bfe9354a-0a73-4047-a192-e99c093266b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004952125s
STEP: Saw pod success 01/23/24 13:46:24.146
Jan 23 13:46:24.146: INFO: Pod "pod-projected-configmaps-bfe9354a-0a73-4047-a192-e99c093266b9" satisfied condition "Succeeded or Failed"
Jan 23 13:46:24.147: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-configmaps-bfe9354a-0a73-4047-a192-e99c093266b9 container agnhost-container: <nil>
STEP: delete the pod 01/23/24 13:46:24.15
Jan 23 13:46:24.156: INFO: Waiting for pod pod-projected-configmaps-bfe9354a-0a73-4047-a192-e99c093266b9 to disappear
Jan 23 13:46:24.157: INFO: Pod pod-projected-configmaps-bfe9354a-0a73-4047-a192-e99c093266b9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 23 13:46:24.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5676" for this suite. 01/23/24 13:46:24.158
------------------------------
• [4.054 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:46:20.108
    Jan 23 13:46:20.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 13:46:20.109
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:20.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:20.12
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-e43d1232-37b3-4d24-b4fa-ba0690e82d50 01/23/24 13:46:20.121
    STEP: Creating a pod to test consume configMaps 01/23/24 13:46:20.126
    Jan 23 13:46:20.141: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bfe9354a-0a73-4047-a192-e99c093266b9" in namespace "projected-5676" to be "Succeeded or Failed"
    Jan 23 13:46:20.143: INFO: Pod "pod-projected-configmaps-bfe9354a-0a73-4047-a192-e99c093266b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.636629ms
    Jan 23 13:46:22.147: INFO: Pod "pod-projected-configmaps-bfe9354a-0a73-4047-a192-e99c093266b9": Phase="Running", Reason="", readiness=false. Elapsed: 2.005474007s
    Jan 23 13:46:24.146: INFO: Pod "pod-projected-configmaps-bfe9354a-0a73-4047-a192-e99c093266b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004952125s
    STEP: Saw pod success 01/23/24 13:46:24.146
    Jan 23 13:46:24.146: INFO: Pod "pod-projected-configmaps-bfe9354a-0a73-4047-a192-e99c093266b9" satisfied condition "Succeeded or Failed"
    Jan 23 13:46:24.147: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-configmaps-bfe9354a-0a73-4047-a192-e99c093266b9 container agnhost-container: <nil>
    STEP: delete the pod 01/23/24 13:46:24.15
    Jan 23 13:46:24.156: INFO: Waiting for pod pod-projected-configmaps-bfe9354a-0a73-4047-a192-e99c093266b9 to disappear
    Jan 23 13:46:24.157: INFO: Pod pod-projected-configmaps-bfe9354a-0a73-4047-a192-e99c093266b9 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:46:24.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5676" for this suite. 01/23/24 13:46:24.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:46:24.163
Jan 23 13:46:24.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename disruption 01/23/24 13:46:24.163
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:24.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:24.174
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 01/23/24 13:46:24.179
STEP: Updating PodDisruptionBudget status 01/23/24 13:46:26.183
STEP: Waiting for all pods to be running 01/23/24 13:46:26.208
Jan 23 13:46:26.215: INFO: running pods: 0 < 1
STEP: locating a running pod 01/23/24 13:46:28.219
STEP: Waiting for the pdb to be processed 01/23/24 13:46:28.226
STEP: Patching PodDisruptionBudget status 01/23/24 13:46:28.23
STEP: Waiting for the pdb to be processed 01/23/24 13:46:28.236
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 23 13:46:28.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4668" for this suite. 01/23/24 13:46:28.24
------------------------------
• [4.081 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:46:24.163
    Jan 23 13:46:24.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename disruption 01/23/24 13:46:24.163
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:24.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:24.174
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 01/23/24 13:46:24.179
    STEP: Updating PodDisruptionBudget status 01/23/24 13:46:26.183
    STEP: Waiting for all pods to be running 01/23/24 13:46:26.208
    Jan 23 13:46:26.215: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/23/24 13:46:28.219
    STEP: Waiting for the pdb to be processed 01/23/24 13:46:28.226
    STEP: Patching PodDisruptionBudget status 01/23/24 13:46:28.23
    STEP: Waiting for the pdb to be processed 01/23/24 13:46:28.236
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:46:28.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4668" for this suite. 01/23/24 13:46:28.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:46:28.244
Jan 23 13:46:28.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename sched-preemption 01/23/24 13:46:28.244
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:28.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:28.254
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jan 23 13:46:28.270: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 23 13:47:28.310: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 01/23/24 13:47:28.312
Jan 23 13:47:28.336: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 23 13:47:28.348: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 23 13:47:28.370: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 23 13:47:28.381: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/23/24 13:47:28.381
Jan 23 13:47:28.381: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7441" to be "running"
Jan 23 13:47:28.383: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.898374ms
Jan 23 13:47:30.387: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.005636538s
Jan 23 13:47:30.387: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 23 13:47:30.387: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7441" to be "running"
Jan 23 13:47:30.389: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.592711ms
Jan 23 13:47:30.389: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 23 13:47:30.389: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7441" to be "running"
Jan 23 13:47:30.390: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.315764ms
Jan 23 13:47:30.390: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 23 13:47:30.390: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7441" to be "running"
Jan 23 13:47:30.391: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.303574ms
Jan 23 13:47:30.391: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/23/24 13:47:30.391
Jan 23 13:47:30.403: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan 23 13:47:30.404: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.733515ms
Jan 23 13:47:32.408: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005342323s
Jan 23 13:47:34.408: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005640695s
Jan 23 13:47:36.407: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.004632342s
Jan 23 13:47:36.407: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:47:36.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7441" for this suite. 01/23/24 13:47:36.444
------------------------------
• [SLOW TEST] [68.203 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:46:28.244
    Jan 23 13:46:28.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename sched-preemption 01/23/24 13:46:28.244
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:46:28.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:46:28.254
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jan 23 13:46:28.270: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 23 13:47:28.310: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 01/23/24 13:47:28.312
    Jan 23 13:47:28.336: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 23 13:47:28.348: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 23 13:47:28.370: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 23 13:47:28.381: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/23/24 13:47:28.381
    Jan 23 13:47:28.381: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7441" to be "running"
    Jan 23 13:47:28.383: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.898374ms
    Jan 23 13:47:30.387: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.005636538s
    Jan 23 13:47:30.387: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 23 13:47:30.387: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7441" to be "running"
    Jan 23 13:47:30.389: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.592711ms
    Jan 23 13:47:30.389: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 23 13:47:30.389: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7441" to be "running"
    Jan 23 13:47:30.390: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.315764ms
    Jan 23 13:47:30.390: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 23 13:47:30.390: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7441" to be "running"
    Jan 23 13:47:30.391: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.303574ms
    Jan 23 13:47:30.391: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/23/24 13:47:30.391
    Jan 23 13:47:30.403: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan 23 13:47:30.404: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.733515ms
    Jan 23 13:47:32.408: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005342323s
    Jan 23 13:47:34.408: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005640695s
    Jan 23 13:47:36.407: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.004632342s
    Jan 23 13:47:36.407: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:47:36.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7441" for this suite. 01/23/24 13:47:36.444
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:47:36.447
Jan 23 13:47:36.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename secrets 01/23/24 13:47:36.448
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:47:36.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:47:36.457
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-5c63561a-f2b2-4f43-ab7c-3400ef107c57 01/23/24 13:47:36.461
STEP: Creating secret with name s-test-opt-upd-e584ab5b-366e-4502-9d02-fb0a3989bb1c 01/23/24 13:47:36.465
STEP: Creating the pod 01/23/24 13:47:36.478
Jan 23 13:47:36.496: INFO: Waiting up to 5m0s for pod "pod-secrets-46ab92c8-73e4-42e1-962f-b49ba1df0b30" in namespace "secrets-7851" to be "running and ready"
Jan 23 13:47:36.498: INFO: Pod "pod-secrets-46ab92c8-73e4-42e1-962f-b49ba1df0b30": Phase="Pending", Reason="", readiness=false. Elapsed: 1.373762ms
Jan 23 13:47:36.498: INFO: The phase of Pod pod-secrets-46ab92c8-73e4-42e1-962f-b49ba1df0b30 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 13:47:38.501: INFO: Pod "pod-secrets-46ab92c8-73e4-42e1-962f-b49ba1df0b30": Phase="Running", Reason="", readiness=true. Elapsed: 2.005155512s
Jan 23 13:47:38.501: INFO: The phase of Pod pod-secrets-46ab92c8-73e4-42e1-962f-b49ba1df0b30 is Running (Ready = true)
Jan 23 13:47:38.501: INFO: Pod "pod-secrets-46ab92c8-73e4-42e1-962f-b49ba1df0b30" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-5c63561a-f2b2-4f43-ab7c-3400ef107c57 01/23/24 13:47:38.514
STEP: Updating secret s-test-opt-upd-e584ab5b-366e-4502-9d02-fb0a3989bb1c 01/23/24 13:47:38.518
STEP: Creating secret with name s-test-opt-create-faca40b9-2a46-4019-bd18-39f0f889be61 01/23/24 13:47:38.523
STEP: waiting to observe update in volume 01/23/24 13:47:38.527
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 23 13:47:40.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7851" for this suite. 01/23/24 13:47:40.543
------------------------------
• [4.101 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:47:36.447
    Jan 23 13:47:36.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename secrets 01/23/24 13:47:36.448
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:47:36.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:47:36.457
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-5c63561a-f2b2-4f43-ab7c-3400ef107c57 01/23/24 13:47:36.461
    STEP: Creating secret with name s-test-opt-upd-e584ab5b-366e-4502-9d02-fb0a3989bb1c 01/23/24 13:47:36.465
    STEP: Creating the pod 01/23/24 13:47:36.478
    Jan 23 13:47:36.496: INFO: Waiting up to 5m0s for pod "pod-secrets-46ab92c8-73e4-42e1-962f-b49ba1df0b30" in namespace "secrets-7851" to be "running and ready"
    Jan 23 13:47:36.498: INFO: Pod "pod-secrets-46ab92c8-73e4-42e1-962f-b49ba1df0b30": Phase="Pending", Reason="", readiness=false. Elapsed: 1.373762ms
    Jan 23 13:47:36.498: INFO: The phase of Pod pod-secrets-46ab92c8-73e4-42e1-962f-b49ba1df0b30 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 13:47:38.501: INFO: Pod "pod-secrets-46ab92c8-73e4-42e1-962f-b49ba1df0b30": Phase="Running", Reason="", readiness=true. Elapsed: 2.005155512s
    Jan 23 13:47:38.501: INFO: The phase of Pod pod-secrets-46ab92c8-73e4-42e1-962f-b49ba1df0b30 is Running (Ready = true)
    Jan 23 13:47:38.501: INFO: Pod "pod-secrets-46ab92c8-73e4-42e1-962f-b49ba1df0b30" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-5c63561a-f2b2-4f43-ab7c-3400ef107c57 01/23/24 13:47:38.514
    STEP: Updating secret s-test-opt-upd-e584ab5b-366e-4502-9d02-fb0a3989bb1c 01/23/24 13:47:38.518
    STEP: Creating secret with name s-test-opt-create-faca40b9-2a46-4019-bd18-39f0f889be61 01/23/24 13:47:38.523
    STEP: waiting to observe update in volume 01/23/24 13:47:38.527
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:47:40.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7851" for this suite. 01/23/24 13:47:40.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:47:40.548
Jan 23 13:47:40.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename events 01/23/24 13:47:40.551
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:47:40.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:47:40.561
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/23/24 13:47:40.563
STEP: listing events in all namespaces 01/23/24 13:47:40.568
STEP: listing events in test namespace 01/23/24 13:47:40.583
STEP: listing events with field selection filtering on source 01/23/24 13:47:40.585
STEP: listing events with field selection filtering on reportingController 01/23/24 13:47:40.586
STEP: getting the test event 01/23/24 13:47:40.588
STEP: patching the test event 01/23/24 13:47:40.589
STEP: getting the test event 01/23/24 13:47:40.597
STEP: updating the test event 01/23/24 13:47:40.599
STEP: getting the test event 01/23/24 13:47:40.602
STEP: deleting the test event 01/23/24 13:47:40.603
STEP: listing events in all namespaces 01/23/24 13:47:40.606
STEP: listing events in test namespace 01/23/24 13:47:40.62
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan 23 13:47:40.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1381" for this suite. 01/23/24 13:47:40.623
------------------------------
• [0.078 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:47:40.548
    Jan 23 13:47:40.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename events 01/23/24 13:47:40.551
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:47:40.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:47:40.561
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/23/24 13:47:40.563
    STEP: listing events in all namespaces 01/23/24 13:47:40.568
    STEP: listing events in test namespace 01/23/24 13:47:40.583
    STEP: listing events with field selection filtering on source 01/23/24 13:47:40.585
    STEP: listing events with field selection filtering on reportingController 01/23/24 13:47:40.586
    STEP: getting the test event 01/23/24 13:47:40.588
    STEP: patching the test event 01/23/24 13:47:40.589
    STEP: getting the test event 01/23/24 13:47:40.597
    STEP: updating the test event 01/23/24 13:47:40.599
    STEP: getting the test event 01/23/24 13:47:40.602
    STEP: deleting the test event 01/23/24 13:47:40.603
    STEP: listing events in all namespaces 01/23/24 13:47:40.606
    STEP: listing events in test namespace 01/23/24 13:47:40.62
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:47:40.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1381" for this suite. 01/23/24 13:47:40.623
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:47:40.627
Jan 23 13:47:40.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename resourcequota 01/23/24 13:47:40.628
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:47:40.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:47:40.638
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 01/23/24 13:47:40.64
STEP: Creating a ResourceQuota 01/23/24 13:47:45.642
STEP: Ensuring resource quota status is calculated 01/23/24 13:47:45.647
STEP: Creating a Pod that fits quota 01/23/24 13:47:47.65
STEP: Ensuring ResourceQuota status captures the pod usage 01/23/24 13:47:47.666
STEP: Not allowing a pod to be created that exceeds remaining quota 01/23/24 13:47:49.669
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/23/24 13:47:49.678
STEP: Ensuring a pod cannot update its resource requirements 01/23/24 13:47:49.686
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/23/24 13:47:49.689
STEP: Deleting the pod 01/23/24 13:47:51.693
STEP: Ensuring resource quota status released the pod usage 01/23/24 13:47:51.701
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 23 13:47:53.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5426" for this suite. 01/23/24 13:47:53.707
------------------------------
• [SLOW TEST] [13.084 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:47:40.627
    Jan 23 13:47:40.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename resourcequota 01/23/24 13:47:40.628
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:47:40.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:47:40.638
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 01/23/24 13:47:40.64
    STEP: Creating a ResourceQuota 01/23/24 13:47:45.642
    STEP: Ensuring resource quota status is calculated 01/23/24 13:47:45.647
    STEP: Creating a Pod that fits quota 01/23/24 13:47:47.65
    STEP: Ensuring ResourceQuota status captures the pod usage 01/23/24 13:47:47.666
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/23/24 13:47:49.669
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/23/24 13:47:49.678
    STEP: Ensuring a pod cannot update its resource requirements 01/23/24 13:47:49.686
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/23/24 13:47:49.689
    STEP: Deleting the pod 01/23/24 13:47:51.693
    STEP: Ensuring resource quota status released the pod usage 01/23/24 13:47:51.701
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:47:53.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5426" for this suite. 01/23/24 13:47:53.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:47:53.711
Jan 23 13:47:53.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename init-container 01/23/24 13:47:53.712
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:47:53.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:47:53.725
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 01/23/24 13:47:53.726
Jan 23 13:47:53.726: INFO: PodSpec: initContainers in spec.initContainers
Jan 23 13:48:40.261: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-8765e3a4-272c-4b62-bd9c-c75cc5f055ca", GenerateName:"", Namespace:"init-container-859", SelfLink:"", UID:"5a6b9bd4-cd17-4b98-9b71-b3ff12f1bbac", ResourceVersion:"117897", Generation:0, CreationTimestamp:time.Date(2024, time.January, 23, 13, 47, 53, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"726864362"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"8b7d8ab83cb8faad2d651467125a3218b84f8aee23ab7d75fbaf44c501822ff3", "cni.projectcalico.org/podIP":"10.233.87.58/32", "cni.projectcalico.org/podIPs":"10.233.87.58/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.January, 23, 13, 47, 53, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0000130c8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.January, 23, 13, 47, 54, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000013158), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.January, 23, 13, 48, 40, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000013488), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-f6gcr", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004ee60e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-f6gcr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-f6gcr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-f6gcr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00356fbb0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00064c4d0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00356fc30)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00356fc50)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"nova-user-critical", Priority:(*int32)(0xc00356fc58), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00356fc5c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001190ee0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 23, 13, 47, 53, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 23, 13, 47, 53, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 23, 13, 47, 53, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 23, 13, 47, 53, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.11.67", PodIP:"10.233.87.58", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.87.58"}}, StartTime:time.Date(2024, time.January, 23, 13, 47, 53, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00064c620)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00064c690)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://31b6446774e56637c05728b07cad2abeb0d0e9dfd2e87872fb9683bc0720d753", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004ee6160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004ee6140), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00356fcdf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:48:40.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-859" for this suite. 01/23/24 13:48:40.265
------------------------------
• [SLOW TEST] [46.556 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:47:53.711
    Jan 23 13:47:53.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename init-container 01/23/24 13:47:53.712
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:47:53.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:47:53.725
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 01/23/24 13:47:53.726
    Jan 23 13:47:53.726: INFO: PodSpec: initContainers in spec.initContainers
    Jan 23 13:48:40.261: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-8765e3a4-272c-4b62-bd9c-c75cc5f055ca", GenerateName:"", Namespace:"init-container-859", SelfLink:"", UID:"5a6b9bd4-cd17-4b98-9b71-b3ff12f1bbac", ResourceVersion:"117897", Generation:0, CreationTimestamp:time.Date(2024, time.January, 23, 13, 47, 53, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"726864362"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"8b7d8ab83cb8faad2d651467125a3218b84f8aee23ab7d75fbaf44c501822ff3", "cni.projectcalico.org/podIP":"10.233.87.58/32", "cni.projectcalico.org/podIPs":"10.233.87.58/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.January, 23, 13, 47, 53, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0000130c8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.January, 23, 13, 47, 54, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000013158), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.January, 23, 13, 48, 40, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000013488), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-f6gcr", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004ee60e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-f6gcr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-f6gcr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-f6gcr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00356fbb0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00064c4d0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00356fc30)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00356fc50)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"nova-user-critical", Priority:(*int32)(0xc00356fc58), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00356fc5c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001190ee0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 23, 13, 47, 53, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 23, 13, 47, 53, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 23, 13, 47, 53, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 23, 13, 47, 53, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.11.67", PodIP:"10.233.87.58", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.87.58"}}, StartTime:time.Date(2024, time.January, 23, 13, 47, 53, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00064c620)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00064c690)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://31b6446774e56637c05728b07cad2abeb0d0e9dfd2e87872fb9683bc0720d753", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004ee6160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004ee6140), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00356fcdf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:48:40.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-859" for this suite. 01/23/24 13:48:40.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:48:40.268
Jan 23 13:48:40.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename crd-publish-openapi 01/23/24 13:48:40.268
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:48:40.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:48:40.276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Jan 23 13:48:40.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/23/24 13:48:47.354
Jan 23 13:48:47.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5047 --namespace=crd-publish-openapi-5047 create -f -'
Jan 23 13:48:48.563: INFO: stderr: ""
Jan 23 13:48:48.563: INFO: stdout: "e2e-test-crd-publish-openapi-7088-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 23 13:48:48.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5047 --namespace=crd-publish-openapi-5047 delete e2e-test-crd-publish-openapi-7088-crds test-cr'
Jan 23 13:48:48.719: INFO: stderr: ""
Jan 23 13:48:48.719: INFO: stdout: "e2e-test-crd-publish-openapi-7088-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 23 13:48:48.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5047 --namespace=crd-publish-openapi-5047 apply -f -'
Jan 23 13:48:49.076: INFO: stderr: ""
Jan 23 13:48:49.076: INFO: stdout: "e2e-test-crd-publish-openapi-7088-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 23 13:48:49.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5047 --namespace=crd-publish-openapi-5047 delete e2e-test-crd-publish-openapi-7088-crds test-cr'
Jan 23 13:48:49.219: INFO: stderr: ""
Jan 23 13:48:49.219: INFO: stdout: "e2e-test-crd-publish-openapi-7088-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/23/24 13:48:49.219
Jan 23 13:48:49.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5047 explain e2e-test-crd-publish-openapi-7088-crds'
Jan 23 13:48:49.604: INFO: stderr: ""
Jan 23 13:48:49.604: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7088-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:48:52.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5047" for this suite. 01/23/24 13:48:52.091
------------------------------
• [SLOW TEST] [11.826 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:48:40.268
    Jan 23 13:48:40.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename crd-publish-openapi 01/23/24 13:48:40.268
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:48:40.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:48:40.276
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Jan 23 13:48:40.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/23/24 13:48:47.354
    Jan 23 13:48:47.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5047 --namespace=crd-publish-openapi-5047 create -f -'
    Jan 23 13:48:48.563: INFO: stderr: ""
    Jan 23 13:48:48.563: INFO: stdout: "e2e-test-crd-publish-openapi-7088-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 23 13:48:48.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5047 --namespace=crd-publish-openapi-5047 delete e2e-test-crd-publish-openapi-7088-crds test-cr'
    Jan 23 13:48:48.719: INFO: stderr: ""
    Jan 23 13:48:48.719: INFO: stdout: "e2e-test-crd-publish-openapi-7088-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan 23 13:48:48.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5047 --namespace=crd-publish-openapi-5047 apply -f -'
    Jan 23 13:48:49.076: INFO: stderr: ""
    Jan 23 13:48:49.076: INFO: stdout: "e2e-test-crd-publish-openapi-7088-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 23 13:48:49.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5047 --namespace=crd-publish-openapi-5047 delete e2e-test-crd-publish-openapi-7088-crds test-cr'
    Jan 23 13:48:49.219: INFO: stderr: ""
    Jan 23 13:48:49.219: INFO: stdout: "e2e-test-crd-publish-openapi-7088-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/23/24 13:48:49.219
    Jan 23 13:48:49.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5047 explain e2e-test-crd-publish-openapi-7088-crds'
    Jan 23 13:48:49.604: INFO: stderr: ""
    Jan 23 13:48:49.604: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7088-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:48:52.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5047" for this suite. 01/23/24 13:48:52.091
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:48:52.095
Jan 23 13:48:52.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename containers 01/23/24 13:48:52.095
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:48:52.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:48:52.105
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 01/23/24 13:48:52.106
Jan 23 13:48:52.117: INFO: Waiting up to 5m0s for pod "client-containers-2bac264d-02bb-4e80-a623-75e1ade1b67f" in namespace "containers-226" to be "Succeeded or Failed"
Jan 23 13:48:52.119: INFO: Pod "client-containers-2bac264d-02bb-4e80-a623-75e1ade1b67f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.490255ms
Jan 23 13:48:54.122: INFO: Pod "client-containers-2bac264d-02bb-4e80-a623-75e1ade1b67f": Phase="Running", Reason="", readiness=true. Elapsed: 2.004692143s
Jan 23 13:48:56.122: INFO: Pod "client-containers-2bac264d-02bb-4e80-a623-75e1ade1b67f": Phase="Running", Reason="", readiness=false. Elapsed: 4.004366s
Jan 23 13:48:58.123: INFO: Pod "client-containers-2bac264d-02bb-4e80-a623-75e1ade1b67f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005862208s
STEP: Saw pod success 01/23/24 13:48:58.123
Jan 23 13:48:58.123: INFO: Pod "client-containers-2bac264d-02bb-4e80-a623-75e1ade1b67f" satisfied condition "Succeeded or Failed"
Jan 23 13:48:58.125: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod client-containers-2bac264d-02bb-4e80-a623-75e1ade1b67f container agnhost-container: <nil>
STEP: delete the pod 01/23/24 13:48:58.128
Jan 23 13:48:58.134: INFO: Waiting for pod client-containers-2bac264d-02bb-4e80-a623-75e1ade1b67f to disappear
Jan 23 13:48:58.136: INFO: Pod client-containers-2bac264d-02bb-4e80-a623-75e1ade1b67f no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 23 13:48:58.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-226" for this suite. 01/23/24 13:48:58.138
------------------------------
• [SLOW TEST] [6.047 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:48:52.095
    Jan 23 13:48:52.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename containers 01/23/24 13:48:52.095
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:48:52.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:48:52.105
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 01/23/24 13:48:52.106
    Jan 23 13:48:52.117: INFO: Waiting up to 5m0s for pod "client-containers-2bac264d-02bb-4e80-a623-75e1ade1b67f" in namespace "containers-226" to be "Succeeded or Failed"
    Jan 23 13:48:52.119: INFO: Pod "client-containers-2bac264d-02bb-4e80-a623-75e1ade1b67f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.490255ms
    Jan 23 13:48:54.122: INFO: Pod "client-containers-2bac264d-02bb-4e80-a623-75e1ade1b67f": Phase="Running", Reason="", readiness=true. Elapsed: 2.004692143s
    Jan 23 13:48:56.122: INFO: Pod "client-containers-2bac264d-02bb-4e80-a623-75e1ade1b67f": Phase="Running", Reason="", readiness=false. Elapsed: 4.004366s
    Jan 23 13:48:58.123: INFO: Pod "client-containers-2bac264d-02bb-4e80-a623-75e1ade1b67f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005862208s
    STEP: Saw pod success 01/23/24 13:48:58.123
    Jan 23 13:48:58.123: INFO: Pod "client-containers-2bac264d-02bb-4e80-a623-75e1ade1b67f" satisfied condition "Succeeded or Failed"
    Jan 23 13:48:58.125: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod client-containers-2bac264d-02bb-4e80-a623-75e1ade1b67f container agnhost-container: <nil>
    STEP: delete the pod 01/23/24 13:48:58.128
    Jan 23 13:48:58.134: INFO: Waiting for pod client-containers-2bac264d-02bb-4e80-a623-75e1ade1b67f to disappear
    Jan 23 13:48:58.136: INFO: Pod client-containers-2bac264d-02bb-4e80-a623-75e1ade1b67f no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:48:58.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-226" for this suite. 01/23/24 13:48:58.138
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:48:58.142
Jan 23 13:48:58.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename webhook 01/23/24 13:48:58.143
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:48:58.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:48:58.153
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/23/24 13:48:58.162
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 13:48:58.492
STEP: Deploying the webhook pod 01/23/24 13:48:58.498
STEP: Wait for the deployment to be ready 01/23/24 13:48:58.507
Jan 23 13:48:58.514: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/23/24 13:49:00.522
STEP: Verifying the service has paired with the endpoint 01/23/24 13:49:00.527
Jan 23 13:49:01.528: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/23/24 13:49:01.531
STEP: create a configmap that should be updated by the webhook 01/23/24 13:49:01.543
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:49:01.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7069" for this suite. 01/23/24 13:49:01.586
STEP: Destroying namespace "webhook-7069-markers" for this suite. 01/23/24 13:49:01.591
------------------------------
• [3.451 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:48:58.142
    Jan 23 13:48:58.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename webhook 01/23/24 13:48:58.143
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:48:58.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:48:58.153
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/23/24 13:48:58.162
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 13:48:58.492
    STEP: Deploying the webhook pod 01/23/24 13:48:58.498
    STEP: Wait for the deployment to be ready 01/23/24 13:48:58.507
    Jan 23 13:48:58.514: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/23/24 13:49:00.522
    STEP: Verifying the service has paired with the endpoint 01/23/24 13:49:00.527
    Jan 23 13:49:01.528: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/23/24 13:49:01.531
    STEP: create a configmap that should be updated by the webhook 01/23/24 13:49:01.543
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:49:01.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7069" for this suite. 01/23/24 13:49:01.586
    STEP: Destroying namespace "webhook-7069-markers" for this suite. 01/23/24 13:49:01.591
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:49:01.594
Jan 23 13:49:01.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubectl 01/23/24 13:49:01.595
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:01.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:01.607
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/23/24 13:49:01.612
Jan 23 13:49:01.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4068 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Jan 23 13:49:01.718: INFO: stderr: ""
Jan 23 13:49:01.718: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/23/24 13:49:01.718
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Jan 23 13:49:01.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4068 delete pods e2e-test-httpd-pod'
Jan 23 13:49:06.334: INFO: stderr: ""
Jan 23 13:49:06.334: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 23 13:49:06.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4068" for this suite. 01/23/24 13:49:06.337
------------------------------
• [4.746 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:49:01.594
    Jan 23 13:49:01.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubectl 01/23/24 13:49:01.595
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:01.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:01.607
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/23/24 13:49:01.612
    Jan 23 13:49:01.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4068 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Jan 23 13:49:01.718: INFO: stderr: ""
    Jan 23 13:49:01.718: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/23/24 13:49:01.718
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Jan 23 13:49:01.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4068 delete pods e2e-test-httpd-pod'
    Jan 23 13:49:06.334: INFO: stderr: ""
    Jan 23 13:49:06.334: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:49:06.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4068" for this suite. 01/23/24 13:49:06.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:49:06.341
Jan 23 13:49:06.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename csiinlinevolumes 01/23/24 13:49:06.341
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:06.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:06.353
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 01/23/24 13:49:06.355
STEP: getting 01/23/24 13:49:06.383
STEP: listing in namespace 01/23/24 13:49:06.386
STEP: patching 01/23/24 13:49:06.388
STEP: deleting 01/23/24 13:49:06.392
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan 23 13:49:06.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-822" for this suite. 01/23/24 13:49:06.401
------------------------------
• [0.063 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:49:06.341
    Jan 23 13:49:06.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename csiinlinevolumes 01/23/24 13:49:06.341
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:06.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:06.353
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 01/23/24 13:49:06.355
    STEP: getting 01/23/24 13:49:06.383
    STEP: listing in namespace 01/23/24 13:49:06.386
    STEP: patching 01/23/24 13:49:06.388
    STEP: deleting 01/23/24 13:49:06.392
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:49:06.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-822" for this suite. 01/23/24 13:49:06.401
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:49:06.404
Jan 23 13:49:06.404: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename services 01/23/24 13:49:06.405
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:06.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:06.415
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 01/23/24 13:49:06.419
STEP: watching for the Service to be added 01/23/24 13:49:06.424
Jan 23 13:49:06.427: INFO: Found Service test-service-6wffr in namespace services-4186 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 23 13:49:06.427: INFO: Service test-service-6wffr created
STEP: Getting /status 01/23/24 13:49:06.427
Jan 23 13:49:06.429: INFO: Service test-service-6wffr has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/23/24 13:49:06.429
STEP: watching for the Service to be patched 01/23/24 13:49:06.434
Jan 23 13:49:06.435: INFO: observed Service test-service-6wffr in namespace services-4186 with annotations: map[] & LoadBalancer: {[]}
Jan 23 13:49:06.435: INFO: Found Service test-service-6wffr in namespace services-4186 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 23 13:49:06.435: INFO: Service test-service-6wffr has service status patched
STEP: updating the ServiceStatus 01/23/24 13:49:06.435
Jan 23 13:49:06.440: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/23/24 13:49:06.44
Jan 23 13:49:06.441: INFO: Observed Service test-service-6wffr in namespace services-4186 with annotations: map[] & Conditions: {[]}
Jan 23 13:49:06.441: INFO: Observed event: &Service{ObjectMeta:{test-service-6wffr  services-4186  07e94d73-b71e-4363-ac83-84f672f3e2c5 118208 0 2024-01-23 13:49:06 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2024-01-23 13:49:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2024-01-23 13:49:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.27.189,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.27.189],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 23 13:49:06.441: INFO: Found Service test-service-6wffr in namespace services-4186 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 23 13:49:06.441: INFO: Service test-service-6wffr has service status updated
STEP: patching the service 01/23/24 13:49:06.441
STEP: watching for the Service to be patched 01/23/24 13:49:06.451
Jan 23 13:49:06.452: INFO: observed Service test-service-6wffr in namespace services-4186 with labels: map[test-service-static:true]
Jan 23 13:49:06.452: INFO: observed Service test-service-6wffr in namespace services-4186 with labels: map[test-service-static:true]
Jan 23 13:49:06.452: INFO: observed Service test-service-6wffr in namespace services-4186 with labels: map[test-service-static:true]
Jan 23 13:49:06.452: INFO: Found Service test-service-6wffr in namespace services-4186 with labels: map[test-service:patched test-service-static:true]
Jan 23 13:49:06.452: INFO: Service test-service-6wffr patched
STEP: deleting the service 01/23/24 13:49:06.452
STEP: watching for the Service to be deleted 01/23/24 13:49:06.459
Jan 23 13:49:06.460: INFO: Observed event: ADDED
Jan 23 13:49:06.460: INFO: Observed event: MODIFIED
Jan 23 13:49:06.460: INFO: Observed event: MODIFIED
Jan 23 13:49:06.460: INFO: Observed event: MODIFIED
Jan 23 13:49:06.460: INFO: Found Service test-service-6wffr in namespace services-4186 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 23 13:49:06.460: INFO: Service test-service-6wffr deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 23 13:49:06.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4186" for this suite. 01/23/24 13:49:06.463
------------------------------
• [0.067 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:49:06.404
    Jan 23 13:49:06.404: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename services 01/23/24 13:49:06.405
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:06.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:06.415
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 01/23/24 13:49:06.419
    STEP: watching for the Service to be added 01/23/24 13:49:06.424
    Jan 23 13:49:06.427: INFO: Found Service test-service-6wffr in namespace services-4186 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan 23 13:49:06.427: INFO: Service test-service-6wffr created
    STEP: Getting /status 01/23/24 13:49:06.427
    Jan 23 13:49:06.429: INFO: Service test-service-6wffr has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/23/24 13:49:06.429
    STEP: watching for the Service to be patched 01/23/24 13:49:06.434
    Jan 23 13:49:06.435: INFO: observed Service test-service-6wffr in namespace services-4186 with annotations: map[] & LoadBalancer: {[]}
    Jan 23 13:49:06.435: INFO: Found Service test-service-6wffr in namespace services-4186 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan 23 13:49:06.435: INFO: Service test-service-6wffr has service status patched
    STEP: updating the ServiceStatus 01/23/24 13:49:06.435
    Jan 23 13:49:06.440: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/23/24 13:49:06.44
    Jan 23 13:49:06.441: INFO: Observed Service test-service-6wffr in namespace services-4186 with annotations: map[] & Conditions: {[]}
    Jan 23 13:49:06.441: INFO: Observed event: &Service{ObjectMeta:{test-service-6wffr  services-4186  07e94d73-b71e-4363-ac83-84f672f3e2c5 118208 0 2024-01-23 13:49:06 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2024-01-23 13:49:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2024-01-23 13:49:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.27.189,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.27.189],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan 23 13:49:06.441: INFO: Found Service test-service-6wffr in namespace services-4186 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 23 13:49:06.441: INFO: Service test-service-6wffr has service status updated
    STEP: patching the service 01/23/24 13:49:06.441
    STEP: watching for the Service to be patched 01/23/24 13:49:06.451
    Jan 23 13:49:06.452: INFO: observed Service test-service-6wffr in namespace services-4186 with labels: map[test-service-static:true]
    Jan 23 13:49:06.452: INFO: observed Service test-service-6wffr in namespace services-4186 with labels: map[test-service-static:true]
    Jan 23 13:49:06.452: INFO: observed Service test-service-6wffr in namespace services-4186 with labels: map[test-service-static:true]
    Jan 23 13:49:06.452: INFO: Found Service test-service-6wffr in namespace services-4186 with labels: map[test-service:patched test-service-static:true]
    Jan 23 13:49:06.452: INFO: Service test-service-6wffr patched
    STEP: deleting the service 01/23/24 13:49:06.452
    STEP: watching for the Service to be deleted 01/23/24 13:49:06.459
    Jan 23 13:49:06.460: INFO: Observed event: ADDED
    Jan 23 13:49:06.460: INFO: Observed event: MODIFIED
    Jan 23 13:49:06.460: INFO: Observed event: MODIFIED
    Jan 23 13:49:06.460: INFO: Observed event: MODIFIED
    Jan 23 13:49:06.460: INFO: Found Service test-service-6wffr in namespace services-4186 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan 23 13:49:06.460: INFO: Service test-service-6wffr deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:49:06.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4186" for this suite. 01/23/24 13:49:06.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:49:06.471
Jan 23 13:49:06.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename gc 01/23/24 13:49:06.475
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:06.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:06.486
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/23/24 13:49:06.489
STEP: delete the rc 01/23/24 13:49:11.537
STEP: wait for all pods to be garbage collected 01/23/24 13:49:11.563
STEP: Gathering metrics 01/23/24 13:49:16.567
Jan 23 13:49:16.585: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" in namespace "kube-system" to be "running and ready"
Jan 23 13:49:16.586: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local": Phase="Running", Reason="", readiness=true. Elapsed: 1.755879ms
Jan 23 13:49:16.586: INFO: The phase of Pod kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local is Running (Ready = true)
Jan 23 13:49:16.586: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" satisfied condition "running and ready"
Jan 23 13:49:16.632: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 23 13:49:16.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7007" for this suite. 01/23/24 13:49:16.635
------------------------------
• [SLOW TEST] [10.167 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:49:06.471
    Jan 23 13:49:06.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename gc 01/23/24 13:49:06.475
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:06.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:06.486
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/23/24 13:49:06.489
    STEP: delete the rc 01/23/24 13:49:11.537
    STEP: wait for all pods to be garbage collected 01/23/24 13:49:11.563
    STEP: Gathering metrics 01/23/24 13:49:16.567
    Jan 23 13:49:16.585: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" in namespace "kube-system" to be "running and ready"
    Jan 23 13:49:16.586: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local": Phase="Running", Reason="", readiness=true. Elapsed: 1.755879ms
    Jan 23 13:49:16.586: INFO: The phase of Pod kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local is Running (Ready = true)
    Jan 23 13:49:16.586: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" satisfied condition "running and ready"
    Jan 23 13:49:16.632: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:49:16.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7007" for this suite. 01/23/24 13:49:16.635
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:49:16.638
Jan 23 13:49:16.638: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename sched-pred 01/23/24 13:49:16.639
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:16.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:16.648
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 23 13:49:16.650: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 23 13:49:16.655: INFO: Waiting for terminating namespaces to be deleted...
Jan 23 13:49:16.657: INFO: 
Logging pods the apiserver thinks is on node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local before test
Jan 23 13:49:16.668: INFO: calico-kube-controllers-7d4c856855-qrf8w from kube-system started at 2024-01-23 09:02:23 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container calico-kube-controllers ready: true, restart count 4
Jan 23 13:49:16.668: INFO: calico-node-hx9hg from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container calico-node ready: true, restart count 1
Jan 23 13:49:16.668: INFO: coredns-8446d7bc66-zglp7 from kube-system started at 2024-01-23 09:02:28 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container coredns ready: true, restart count 1
Jan 23 13:49:16.668: INFO: kube-proxy-5p4pt from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container kube-proxy ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nginx-proxy-node-worker-hohyvwot.nova-ht9xu6tk2ptb.local from kube-system started at 2024-01-23 09:02:32 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container nginx-proxy ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-dns-667b6f9dd9-f4wkr from kube-system started at 2024-01-23 09:02:53 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container nova-dns ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-dns-667b6f9dd9-fc8vq from kube-system started at 2024-01-23 09:02:53 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container nova-dns ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-reflector-7bc9b5d4dd-vgml8 from nova-automation started at 2024-01-23 09:06:13 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container reflector ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-release-git-main-0 from nova-automation started at 2024-01-23 09:08:44 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container gitea ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-reloader-744fcf7b8f-ztn6c from nova-automation started at 2024-01-23 09:06:13 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container nova-reloader ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-cert-manager-74fb9fd7f9-q6q9f from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container cert-manager-controller ready: true, restart count 2
Jan 23 13:49:16.668: INFO: nova-cert-manager-cainjector-74465474c6-pjhbz from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container cert-manager-cainjector ready: true, restart count 4
Jan 23 13:49:16.668: INFO: nova-cert-manager-webhook-74b6ccdf8-f5bt2 from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container cert-manager-webhook ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-console-675844f8c4-p92th from nova-console started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container nova-console ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-local-path-provisioner-59754bbcb5-nm6ps from nova-csi-drivers started at 2024-01-23 09:06:07 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container local-path-provisioner ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-oauth-csi-provider-msq2s from nova-csi-drivers started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container vault-csi-provider ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-secrets-store-csi-driver-s4f8p from nova-csi-drivers started at 2024-01-23 09:03:57 +0000 UTC (3 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container liveness-probe ready: true, restart count 1
Jan 23 13:49:16.668: INFO: 	Container node-driver-registrar ready: true, restart count 1
Jan 23 13:49:16.668: INFO: 	Container secrets-store ready: true, restart count 1
Jan 23 13:49:16.668: INFO: secrets-store-csi-driver-upgrade-crds-jvnt9 from nova-csi-drivers started at 2024-01-23 09:03:57 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container crds-upgrade ready: false, restart count 0
Jan 23 13:49:16.668: INFO: nova-descheduler-575f46487d-wcml5 from nova-descheduler started at 2024-01-23 09:06:07 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container descheduler ready: true, restart count 4
Jan 23 13:49:16.668: INFO: helm-controller-6ffdd7974c-ndrg6 from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container manager ready: true, restart count 5
Jan 23 13:49:16.668: INFO: image-automation-controller-79bb688dbd-sq8gm from nova-gitops started at 2024-01-23 09:09:27 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container manager ready: true, restart count 4
Jan 23 13:49:16.668: INFO: image-reflector-controller-6b744758c7-nwppg from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container manager ready: true, restart count 4
Jan 23 13:49:16.668: INFO: kustomize-controller-5d5bb4d48-99q8z from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container manager ready: true, restart count 4
Jan 23 13:49:16.668: INFO: notification-controller-5974fbb84-8wcdc from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container manager ready: true, restart count 4
Jan 23 13:49:16.668: INFO: source-controller-7f8d6bc9d7-dbb88 from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container manager ready: true, restart count 4
Jan 23 13:49:16.668: INFO: nova-ingress-internal-controller-g8rl8 from nova-ingress-internal started at 2024-01-23 09:08:09 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container controller ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-logging-operator-7587849584-mq59r from nova-logging-operator started at 2024-01-23 09:06:18 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container logging-operator ready: true, restart count 5
Jan 23 13:49:16.668: INFO: alertmanager-main-0 from nova-monitoring started at 2024-01-23 09:09:10 +0000 UTC (3 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container alertmanager ready: true, restart count 1
Jan 23 13:49:16.668: INFO: 	Container config-reloader ready: true, restart count 1
Jan 23 13:49:16.668: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 23 13:49:16.668: INFO: monitoring-plugin-6dcd875fb-6sn96 from nova-monitoring started at 2024-01-23 09:08:38 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container monitoring-plugin ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-cadvisor-jvkj7 from nova-monitoring started at 2024-01-23 09:09:34 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container nova-cadvisor ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-grafana-5fcb766f99-xlrkr from nova-monitoring started at 2024-01-23 09:09:09 +0000 UTC (3 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container grafana ready: true, restart count 1
Jan 23 13:49:16.668: INFO: 	Container nova-release-grafana-sc-dashboard ready: true, restart count 1
Jan 23 13:49:16.668: INFO: 	Container nova-release-grafana-sc-datasources ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-kube-state-metrics-6c99956449-vwb5x from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container nova-release-kube-state-metrics ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-metrics-server-6fc9cb6c86-6dfwd from nova-monitoring started at 2024-01-23 09:06:11 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container metrics-server ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-prometheus-adapter-55b7c8779-b9ffb from nova-monitoring started at 2024-01-23 09:09:27 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container prometheus-adapter ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-prometheus-main-operator-fcb966c79-2wnc9 from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container main ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-prometheus-node-exporter-cc7w5 from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container node-exporter ready: true, restart count 1
Jan 23 13:49:16.668: INFO: prometheus-main-0 from nova-monitoring started at 2024-01-23 09:09:42 +0000 UTC (5 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container config-reloader ready: true, restart count 1
Jan 23 13:49:16.668: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 23 13:49:16.668: INFO: 	Container prometheus ready: true, restart count 1
Jan 23 13:49:16.668: INFO: 	Container thanos-sidecar ready: true, restart count 1
Jan 23 13:49:16.668: INFO: 	Container vault-agent-auth ready: true, restart count 1
Jan 23 13:49:16.668: INFO: nova-oauth-secrets-webhook-gmmxd from nova-secrets-webhook started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container vault-secrets-webhook ready: true, restart count 1
Jan 23 13:49:16.668: INFO: sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-4htxf from sonobuoy started at 2024-01-23 13:21:23 +0000 UTC (2 container statuses recorded)
Jan 23 13:49:16.668: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 13:49:16.668: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 23 13:49:16.668: INFO: 
Logging pods the apiserver thinks is on node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local before test
Jan 23 13:49:16.676: INFO: pod-csi-inline-volumes from csiinlinevolumes-822 started at 2024-01-23 13:49:06 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.676: INFO: 	Container pod-csi-inline-volumes ready: false, restart count 0
Jan 23 13:49:16.676: INFO: calico-node-44dpv from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.676: INFO: 	Container calico-node ready: true, restart count 1
Jan 23 13:49:16.676: INFO: coredns-8446d7bc66-rpjs8 from kube-system started at 2024-01-23 09:02:28 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.676: INFO: 	Container coredns ready: true, restart count 1
Jan 23 13:49:16.676: INFO: kube-proxy-dvptq from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.676: INFO: 	Container kube-proxy ready: true, restart count 1
Jan 23 13:49:16.676: INFO: nginx-proxy-node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local from kube-system started at 2024-01-23 09:02:45 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.676: INFO: 	Container nginx-proxy ready: true, restart count 1
Jan 23 13:49:16.676: INFO: nova-oauth-csi-provider-pj45z from nova-csi-drivers started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.676: INFO: 	Container vault-csi-provider ready: true, restart count 1
Jan 23 13:49:16.676: INFO: nova-secrets-store-csi-driver-ft87l from nova-csi-drivers started at 2024-01-23 09:03:58 +0000 UTC (3 container statuses recorded)
Jan 23 13:49:16.676: INFO: 	Container liveness-probe ready: true, restart count 1
Jan 23 13:49:16.676: INFO: 	Container node-driver-registrar ready: true, restart count 1
Jan 23 13:49:16.676: INFO: 	Container secrets-store ready: true, restart count 1
Jan 23 13:49:16.676: INFO: nova-ingress-public-controller-b6dmc from nova-ingress-public started at 2024-01-23 13:24:57 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.676: INFO: 	Container controller ready: true, restart count 0
Jan 23 13:49:16.676: INFO: nova-cadvisor-tq2rp from nova-monitoring started at 2024-01-23 09:09:34 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.676: INFO: 	Container nova-cadvisor ready: true, restart count 1
Jan 23 13:49:16.676: INFO: nova-prometheus-node-exporter-lbfhw from nova-monitoring started at 2024-01-23 13:24:57 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.676: INFO: 	Container node-exporter ready: true, restart count 0
Jan 23 13:49:16.676: INFO: nova-oauth-secrets-webhook-hhwh4 from nova-secrets-webhook started at 2024-01-23 13:24:57 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.676: INFO: 	Container vault-secrets-webhook ready: true, restart count 0
Jan 23 13:49:16.676: INFO: sonobuoy from sonobuoy started at 2024-01-23 13:21:17 +0000 UTC (1 container statuses recorded)
Jan 23 13:49:16.676: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 23 13:49:16.676: INFO: sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-7csgd from sonobuoy started at 2024-01-23 13:21:23 +0000 UTC (2 container statuses recorded)
Jan 23 13:49:16.676: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 13:49:16.676: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/23/24 13:49:16.676
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17acfe1c6164cb2e], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 01/23/24 13:49:16.716
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:49:17.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7255" for this suite. 01/23/24 13:49:17.718
------------------------------
• [1.083 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:49:16.638
    Jan 23 13:49:16.638: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename sched-pred 01/23/24 13:49:16.639
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:16.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:16.648
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 23 13:49:16.650: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 23 13:49:16.655: INFO: Waiting for terminating namespaces to be deleted...
    Jan 23 13:49:16.657: INFO: 
    Logging pods the apiserver thinks is on node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local before test
    Jan 23 13:49:16.668: INFO: calico-kube-controllers-7d4c856855-qrf8w from kube-system started at 2024-01-23 09:02:23 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container calico-kube-controllers ready: true, restart count 4
    Jan 23 13:49:16.668: INFO: calico-node-hx9hg from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container calico-node ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: coredns-8446d7bc66-zglp7 from kube-system started at 2024-01-23 09:02:28 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container coredns ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: kube-proxy-5p4pt from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container kube-proxy ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nginx-proxy-node-worker-hohyvwot.nova-ht9xu6tk2ptb.local from kube-system started at 2024-01-23 09:02:32 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container nginx-proxy ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-dns-667b6f9dd9-f4wkr from kube-system started at 2024-01-23 09:02:53 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container nova-dns ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-dns-667b6f9dd9-fc8vq from kube-system started at 2024-01-23 09:02:53 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container nova-dns ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-reflector-7bc9b5d4dd-vgml8 from nova-automation started at 2024-01-23 09:06:13 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container reflector ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-release-git-main-0 from nova-automation started at 2024-01-23 09:08:44 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container gitea ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-reloader-744fcf7b8f-ztn6c from nova-automation started at 2024-01-23 09:06:13 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container nova-reloader ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-cert-manager-74fb9fd7f9-q6q9f from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container cert-manager-controller ready: true, restart count 2
    Jan 23 13:49:16.668: INFO: nova-cert-manager-cainjector-74465474c6-pjhbz from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container cert-manager-cainjector ready: true, restart count 4
    Jan 23 13:49:16.668: INFO: nova-cert-manager-webhook-74b6ccdf8-f5bt2 from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container cert-manager-webhook ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-console-675844f8c4-p92th from nova-console started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container nova-console ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-local-path-provisioner-59754bbcb5-nm6ps from nova-csi-drivers started at 2024-01-23 09:06:07 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container local-path-provisioner ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-oauth-csi-provider-msq2s from nova-csi-drivers started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container vault-csi-provider ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-secrets-store-csi-driver-s4f8p from nova-csi-drivers started at 2024-01-23 09:03:57 +0000 UTC (3 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container liveness-probe ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: 	Container node-driver-registrar ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: 	Container secrets-store ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: secrets-store-csi-driver-upgrade-crds-jvnt9 from nova-csi-drivers started at 2024-01-23 09:03:57 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container crds-upgrade ready: false, restart count 0
    Jan 23 13:49:16.668: INFO: nova-descheduler-575f46487d-wcml5 from nova-descheduler started at 2024-01-23 09:06:07 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container descheduler ready: true, restart count 4
    Jan 23 13:49:16.668: INFO: helm-controller-6ffdd7974c-ndrg6 from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container manager ready: true, restart count 5
    Jan 23 13:49:16.668: INFO: image-automation-controller-79bb688dbd-sq8gm from nova-gitops started at 2024-01-23 09:09:27 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container manager ready: true, restart count 4
    Jan 23 13:49:16.668: INFO: image-reflector-controller-6b744758c7-nwppg from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container manager ready: true, restart count 4
    Jan 23 13:49:16.668: INFO: kustomize-controller-5d5bb4d48-99q8z from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container manager ready: true, restart count 4
    Jan 23 13:49:16.668: INFO: notification-controller-5974fbb84-8wcdc from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container manager ready: true, restart count 4
    Jan 23 13:49:16.668: INFO: source-controller-7f8d6bc9d7-dbb88 from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container manager ready: true, restart count 4
    Jan 23 13:49:16.668: INFO: nova-ingress-internal-controller-g8rl8 from nova-ingress-internal started at 2024-01-23 09:08:09 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container controller ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-logging-operator-7587849584-mq59r from nova-logging-operator started at 2024-01-23 09:06:18 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container logging-operator ready: true, restart count 5
    Jan 23 13:49:16.668: INFO: alertmanager-main-0 from nova-monitoring started at 2024-01-23 09:09:10 +0000 UTC (3 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: 	Container config-reloader ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: monitoring-plugin-6dcd875fb-6sn96 from nova-monitoring started at 2024-01-23 09:08:38 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container monitoring-plugin ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-cadvisor-jvkj7 from nova-monitoring started at 2024-01-23 09:09:34 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container nova-cadvisor ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-grafana-5fcb766f99-xlrkr from nova-monitoring started at 2024-01-23 09:09:09 +0000 UTC (3 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container grafana ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: 	Container nova-release-grafana-sc-dashboard ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: 	Container nova-release-grafana-sc-datasources ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-kube-state-metrics-6c99956449-vwb5x from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container nova-release-kube-state-metrics ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-metrics-server-6fc9cb6c86-6dfwd from nova-monitoring started at 2024-01-23 09:06:11 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container metrics-server ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-prometheus-adapter-55b7c8779-b9ffb from nova-monitoring started at 2024-01-23 09:09:27 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container prometheus-adapter ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-prometheus-main-operator-fcb966c79-2wnc9 from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container main ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-prometheus-node-exporter-cc7w5 from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: prometheus-main-0 from nova-monitoring started at 2024-01-23 09:09:42 +0000 UTC (5 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container config-reloader ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: 	Container prometheus ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: 	Container thanos-sidecar ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: 	Container vault-agent-auth ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: nova-oauth-secrets-webhook-gmmxd from nova-secrets-webhook started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container vault-secrets-webhook ready: true, restart count 1
    Jan 23 13:49:16.668: INFO: sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-4htxf from sonobuoy started at 2024-01-23 13:21:23 +0000 UTC (2 container statuses recorded)
    Jan 23 13:49:16.668: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 23 13:49:16.668: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 23 13:49:16.668: INFO: 
    Logging pods the apiserver thinks is on node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local before test
    Jan 23 13:49:16.676: INFO: pod-csi-inline-volumes from csiinlinevolumes-822 started at 2024-01-23 13:49:06 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.676: INFO: 	Container pod-csi-inline-volumes ready: false, restart count 0
    Jan 23 13:49:16.676: INFO: calico-node-44dpv from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.676: INFO: 	Container calico-node ready: true, restart count 1
    Jan 23 13:49:16.676: INFO: coredns-8446d7bc66-rpjs8 from kube-system started at 2024-01-23 09:02:28 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.676: INFO: 	Container coredns ready: true, restart count 1
    Jan 23 13:49:16.676: INFO: kube-proxy-dvptq from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.676: INFO: 	Container kube-proxy ready: true, restart count 1
    Jan 23 13:49:16.676: INFO: nginx-proxy-node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local from kube-system started at 2024-01-23 09:02:45 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.676: INFO: 	Container nginx-proxy ready: true, restart count 1
    Jan 23 13:49:16.676: INFO: nova-oauth-csi-provider-pj45z from nova-csi-drivers started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.676: INFO: 	Container vault-csi-provider ready: true, restart count 1
    Jan 23 13:49:16.676: INFO: nova-secrets-store-csi-driver-ft87l from nova-csi-drivers started at 2024-01-23 09:03:58 +0000 UTC (3 container statuses recorded)
    Jan 23 13:49:16.676: INFO: 	Container liveness-probe ready: true, restart count 1
    Jan 23 13:49:16.676: INFO: 	Container node-driver-registrar ready: true, restart count 1
    Jan 23 13:49:16.676: INFO: 	Container secrets-store ready: true, restart count 1
    Jan 23 13:49:16.676: INFO: nova-ingress-public-controller-b6dmc from nova-ingress-public started at 2024-01-23 13:24:57 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.676: INFO: 	Container controller ready: true, restart count 0
    Jan 23 13:49:16.676: INFO: nova-cadvisor-tq2rp from nova-monitoring started at 2024-01-23 09:09:34 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.676: INFO: 	Container nova-cadvisor ready: true, restart count 1
    Jan 23 13:49:16.676: INFO: nova-prometheus-node-exporter-lbfhw from nova-monitoring started at 2024-01-23 13:24:57 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.676: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 23 13:49:16.676: INFO: nova-oauth-secrets-webhook-hhwh4 from nova-secrets-webhook started at 2024-01-23 13:24:57 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.676: INFO: 	Container vault-secrets-webhook ready: true, restart count 0
    Jan 23 13:49:16.676: INFO: sonobuoy from sonobuoy started at 2024-01-23 13:21:17 +0000 UTC (1 container statuses recorded)
    Jan 23 13:49:16.676: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 23 13:49:16.676: INFO: sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-7csgd from sonobuoy started at 2024-01-23 13:21:23 +0000 UTC (2 container statuses recorded)
    Jan 23 13:49:16.676: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 23 13:49:16.676: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/23/24 13:49:16.676
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17acfe1c6164cb2e], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 01/23/24 13:49:16.716
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:49:17.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7255" for this suite. 01/23/24 13:49:17.718
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:49:17.721
Jan 23 13:49:17.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename secrets 01/23/24 13:49:17.722
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:17.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:17.731
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-eae39e30-68cd-4e57-8637-0ae38c010e54 01/23/24 13:49:17.732
STEP: Creating a pod to test consume secrets 01/23/24 13:49:17.737
Jan 23 13:49:17.748: INFO: Waiting up to 5m0s for pod "pod-secrets-b7330fba-2219-4c1c-be3a-7d3b5520efab" in namespace "secrets-2269" to be "Succeeded or Failed"
Jan 23 13:49:17.749: INFO: Pod "pod-secrets-b7330fba-2219-4c1c-be3a-7d3b5520efab": Phase="Pending", Reason="", readiness=false. Elapsed: 1.249368ms
Jan 23 13:49:19.751: INFO: Pod "pod-secrets-b7330fba-2219-4c1c-be3a-7d3b5520efab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003412386s
Jan 23 13:49:21.752: INFO: Pod "pod-secrets-b7330fba-2219-4c1c-be3a-7d3b5520efab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004129454s
STEP: Saw pod success 01/23/24 13:49:21.752
Jan 23 13:49:21.752: INFO: Pod "pod-secrets-b7330fba-2219-4c1c-be3a-7d3b5520efab" satisfied condition "Succeeded or Failed"
Jan 23 13:49:21.753: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-secrets-b7330fba-2219-4c1c-be3a-7d3b5520efab container secret-volume-test: <nil>
STEP: delete the pod 01/23/24 13:49:21.759
Jan 23 13:49:21.764: INFO: Waiting for pod pod-secrets-b7330fba-2219-4c1c-be3a-7d3b5520efab to disappear
Jan 23 13:49:21.766: INFO: Pod pod-secrets-b7330fba-2219-4c1c-be3a-7d3b5520efab no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 23 13:49:21.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2269" for this suite. 01/23/24 13:49:21.768
------------------------------
• [4.049 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:49:17.721
    Jan 23 13:49:17.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename secrets 01/23/24 13:49:17.722
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:17.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:17.731
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-eae39e30-68cd-4e57-8637-0ae38c010e54 01/23/24 13:49:17.732
    STEP: Creating a pod to test consume secrets 01/23/24 13:49:17.737
    Jan 23 13:49:17.748: INFO: Waiting up to 5m0s for pod "pod-secrets-b7330fba-2219-4c1c-be3a-7d3b5520efab" in namespace "secrets-2269" to be "Succeeded or Failed"
    Jan 23 13:49:17.749: INFO: Pod "pod-secrets-b7330fba-2219-4c1c-be3a-7d3b5520efab": Phase="Pending", Reason="", readiness=false. Elapsed: 1.249368ms
    Jan 23 13:49:19.751: INFO: Pod "pod-secrets-b7330fba-2219-4c1c-be3a-7d3b5520efab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003412386s
    Jan 23 13:49:21.752: INFO: Pod "pod-secrets-b7330fba-2219-4c1c-be3a-7d3b5520efab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004129454s
    STEP: Saw pod success 01/23/24 13:49:21.752
    Jan 23 13:49:21.752: INFO: Pod "pod-secrets-b7330fba-2219-4c1c-be3a-7d3b5520efab" satisfied condition "Succeeded or Failed"
    Jan 23 13:49:21.753: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-secrets-b7330fba-2219-4c1c-be3a-7d3b5520efab container secret-volume-test: <nil>
    STEP: delete the pod 01/23/24 13:49:21.759
    Jan 23 13:49:21.764: INFO: Waiting for pod pod-secrets-b7330fba-2219-4c1c-be3a-7d3b5520efab to disappear
    Jan 23 13:49:21.766: INFO: Pod pod-secrets-b7330fba-2219-4c1c-be3a-7d3b5520efab no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:49:21.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2269" for this suite. 01/23/24 13:49:21.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:49:21.771
Jan 23 13:49:21.771: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename container-runtime 01/23/24 13:49:21.772
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:21.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:21.779
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 01/23/24 13:49:21.785
STEP: wait for the container to reach Succeeded 01/23/24 13:49:21.795
STEP: get the container status 01/23/24 13:49:25.809
STEP: the container should be terminated 01/23/24 13:49:25.81
STEP: the termination message should be set 01/23/24 13:49:25.81
Jan 23 13:49:25.810: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/23/24 13:49:25.81
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 23 13:49:25.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1495" for this suite. 01/23/24 13:49:25.82
------------------------------
• [4.052 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:49:21.771
    Jan 23 13:49:21.771: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename container-runtime 01/23/24 13:49:21.772
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:21.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:21.779
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 01/23/24 13:49:21.785
    STEP: wait for the container to reach Succeeded 01/23/24 13:49:21.795
    STEP: get the container status 01/23/24 13:49:25.809
    STEP: the container should be terminated 01/23/24 13:49:25.81
    STEP: the termination message should be set 01/23/24 13:49:25.81
    Jan 23 13:49:25.810: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/23/24 13:49:25.81
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:49:25.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1495" for this suite. 01/23/24 13:49:25.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:49:25.823
Jan 23 13:49:25.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename certificates 01/23/24 13:49:25.824
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:25.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:25.831
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/23/24 13:49:26.552
STEP: getting /apis/certificates.k8s.io 01/23/24 13:49:26.554
STEP: getting /apis/certificates.k8s.io/v1 01/23/24 13:49:26.555
STEP: creating 01/23/24 13:49:26.556
STEP: getting 01/23/24 13:49:26.565
STEP: listing 01/23/24 13:49:26.567
STEP: watching 01/23/24 13:49:26.568
Jan 23 13:49:26.568: INFO: starting watch
STEP: patching 01/23/24 13:49:26.569
STEP: updating 01/23/24 13:49:26.572
Jan 23 13:49:26.575: INFO: waiting for watch events with expected annotations
Jan 23 13:49:26.575: INFO: saw patched and updated annotations
STEP: getting /approval 01/23/24 13:49:26.575
STEP: patching /approval 01/23/24 13:49:26.577
STEP: updating /approval 01/23/24 13:49:26.58
STEP: getting /status 01/23/24 13:49:26.583
STEP: patching /status 01/23/24 13:49:26.584
STEP: updating /status 01/23/24 13:49:26.588
STEP: deleting 01/23/24 13:49:26.592
STEP: deleting a collection 01/23/24 13:49:26.597
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:49:26.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-9609" for this suite. 01/23/24 13:49:26.604
------------------------------
• [0.785 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:49:25.823
    Jan 23 13:49:25.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename certificates 01/23/24 13:49:25.824
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:25.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:25.831
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/23/24 13:49:26.552
    STEP: getting /apis/certificates.k8s.io 01/23/24 13:49:26.554
    STEP: getting /apis/certificates.k8s.io/v1 01/23/24 13:49:26.555
    STEP: creating 01/23/24 13:49:26.556
    STEP: getting 01/23/24 13:49:26.565
    STEP: listing 01/23/24 13:49:26.567
    STEP: watching 01/23/24 13:49:26.568
    Jan 23 13:49:26.568: INFO: starting watch
    STEP: patching 01/23/24 13:49:26.569
    STEP: updating 01/23/24 13:49:26.572
    Jan 23 13:49:26.575: INFO: waiting for watch events with expected annotations
    Jan 23 13:49:26.575: INFO: saw patched and updated annotations
    STEP: getting /approval 01/23/24 13:49:26.575
    STEP: patching /approval 01/23/24 13:49:26.577
    STEP: updating /approval 01/23/24 13:49:26.58
    STEP: getting /status 01/23/24 13:49:26.583
    STEP: patching /status 01/23/24 13:49:26.584
    STEP: updating /status 01/23/24 13:49:26.588
    STEP: deleting 01/23/24 13:49:26.592
    STEP: deleting a collection 01/23/24 13:49:26.597
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:49:26.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-9609" for this suite. 01/23/24 13:49:26.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:49:26.611
Jan 23 13:49:26.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 13:49:26.611
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:26.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:26.62
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-4fc930f3-fad5-4c08-af69-56f34fe5ad6a 01/23/24 13:49:26.622
STEP: Creating a pod to test consume configMaps 01/23/24 13:49:26.625
Jan 23 13:49:26.636: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1caee42b-ae68-44e1-bdef-c6e506f77391" in namespace "projected-3594" to be "Succeeded or Failed"
Jan 23 13:49:26.637: INFO: Pod "pod-projected-configmaps-1caee42b-ae68-44e1-bdef-c6e506f77391": Phase="Pending", Reason="", readiness=false. Elapsed: 1.176757ms
Jan 23 13:49:28.640: INFO: Pod "pod-projected-configmaps-1caee42b-ae68-44e1-bdef-c6e506f77391": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003776076s
Jan 23 13:49:30.641: INFO: Pod "pod-projected-configmaps-1caee42b-ae68-44e1-bdef-c6e506f77391": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004324794s
STEP: Saw pod success 01/23/24 13:49:30.641
Jan 23 13:49:30.641: INFO: Pod "pod-projected-configmaps-1caee42b-ae68-44e1-bdef-c6e506f77391" satisfied condition "Succeeded or Failed"
Jan 23 13:49:30.642: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-configmaps-1caee42b-ae68-44e1-bdef-c6e506f77391 container agnhost-container: <nil>
STEP: delete the pod 01/23/24 13:49:30.646
Jan 23 13:49:30.652: INFO: Waiting for pod pod-projected-configmaps-1caee42b-ae68-44e1-bdef-c6e506f77391 to disappear
Jan 23 13:49:30.653: INFO: Pod pod-projected-configmaps-1caee42b-ae68-44e1-bdef-c6e506f77391 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 23 13:49:30.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3594" for this suite. 01/23/24 13:49:30.655
------------------------------
• [4.048 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:49:26.611
    Jan 23 13:49:26.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 13:49:26.611
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:26.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:26.62
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-4fc930f3-fad5-4c08-af69-56f34fe5ad6a 01/23/24 13:49:26.622
    STEP: Creating a pod to test consume configMaps 01/23/24 13:49:26.625
    Jan 23 13:49:26.636: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1caee42b-ae68-44e1-bdef-c6e506f77391" in namespace "projected-3594" to be "Succeeded or Failed"
    Jan 23 13:49:26.637: INFO: Pod "pod-projected-configmaps-1caee42b-ae68-44e1-bdef-c6e506f77391": Phase="Pending", Reason="", readiness=false. Elapsed: 1.176757ms
    Jan 23 13:49:28.640: INFO: Pod "pod-projected-configmaps-1caee42b-ae68-44e1-bdef-c6e506f77391": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003776076s
    Jan 23 13:49:30.641: INFO: Pod "pod-projected-configmaps-1caee42b-ae68-44e1-bdef-c6e506f77391": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004324794s
    STEP: Saw pod success 01/23/24 13:49:30.641
    Jan 23 13:49:30.641: INFO: Pod "pod-projected-configmaps-1caee42b-ae68-44e1-bdef-c6e506f77391" satisfied condition "Succeeded or Failed"
    Jan 23 13:49:30.642: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-configmaps-1caee42b-ae68-44e1-bdef-c6e506f77391 container agnhost-container: <nil>
    STEP: delete the pod 01/23/24 13:49:30.646
    Jan 23 13:49:30.652: INFO: Waiting for pod pod-projected-configmaps-1caee42b-ae68-44e1-bdef-c6e506f77391 to disappear
    Jan 23 13:49:30.653: INFO: Pod pod-projected-configmaps-1caee42b-ae68-44e1-bdef-c6e506f77391 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:49:30.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3594" for this suite. 01/23/24 13:49:30.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:49:30.66
Jan 23 13:49:30.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename svcaccounts 01/23/24 13:49:30.66
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:30.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:30.669
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Jan 23 13:49:30.688: INFO: created pod
Jan 23 13:49:30.688: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8673" to be "Succeeded or Failed"
Jan 23 13:49:30.690: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.325593ms
Jan 23 13:49:32.694: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006197429s
Jan 23 13:49:34.693: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004884739s
STEP: Saw pod success 01/23/24 13:49:34.693
Jan 23 13:49:34.693: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 23 13:50:04.694: INFO: polling logs
Jan 23 13:50:04.699: INFO: Pod logs: 
I0123 13:49:31.397752       1 log.go:198] OK: Got token
I0123 13:49:31.397791       1 log.go:198] validating with in-cluster discovery
I0123 13:49:31.398066       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0123 13:49:31.398087       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8673:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1706018370, NotBefore:1706017770, IssuedAt:1706017770, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8673", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c201c750-2bc3-4434-b8b8-28b05df9cccd"}}}
I0123 13:49:31.405675       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0123 13:49:31.410210       1 log.go:198] OK: Validated signature on JWT
I0123 13:49:31.410261       1 log.go:198] OK: Got valid claims from token!
I0123 13:49:31.410276       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8673:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1706018370, NotBefore:1706017770, IssuedAt:1706017770, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8673", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c201c750-2bc3-4434-b8b8-28b05df9cccd"}}}

Jan 23 13:50:04.699: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 23 13:50:04.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8673" for this suite. 01/23/24 13:50:04.704
------------------------------
• [SLOW TEST] [34.048 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:49:30.66
    Jan 23 13:49:30.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename svcaccounts 01/23/24 13:49:30.66
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:49:30.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:49:30.669
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Jan 23 13:49:30.688: INFO: created pod
    Jan 23 13:49:30.688: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8673" to be "Succeeded or Failed"
    Jan 23 13:49:30.690: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.325593ms
    Jan 23 13:49:32.694: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006197429s
    Jan 23 13:49:34.693: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004884739s
    STEP: Saw pod success 01/23/24 13:49:34.693
    Jan 23 13:49:34.693: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan 23 13:50:04.694: INFO: polling logs
    Jan 23 13:50:04.699: INFO: Pod logs: 
    I0123 13:49:31.397752       1 log.go:198] OK: Got token
    I0123 13:49:31.397791       1 log.go:198] validating with in-cluster discovery
    I0123 13:49:31.398066       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0123 13:49:31.398087       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8673:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1706018370, NotBefore:1706017770, IssuedAt:1706017770, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8673", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c201c750-2bc3-4434-b8b8-28b05df9cccd"}}}
    I0123 13:49:31.405675       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0123 13:49:31.410210       1 log.go:198] OK: Validated signature on JWT
    I0123 13:49:31.410261       1 log.go:198] OK: Got valid claims from token!
    I0123 13:49:31.410276       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8673:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1706018370, NotBefore:1706017770, IssuedAt:1706017770, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8673", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c201c750-2bc3-4434-b8b8-28b05df9cccd"}}}

    Jan 23 13:50:04.699: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:50:04.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8673" for this suite. 01/23/24 13:50:04.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:50:04.709
Jan 23 13:50:04.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename sched-preemption 01/23/24 13:50:04.71
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:50:04.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:50:04.719
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jan 23 13:50:04.729: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 23 13:51:04.767: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 01/23/24 13:51:04.769
Jan 23 13:51:04.794: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 23 13:51:04.805: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 23 13:51:04.820: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 23 13:51:04.831: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/23/24 13:51:04.831
Jan 23 13:51:04.831: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7860" to be "running"
Jan 23 13:51:04.833: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.453924ms
Jan 23 13:51:06.836: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.004906511s
Jan 23 13:51:06.836: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 23 13:51:06.836: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7860" to be "running"
Jan 23 13:51:06.838: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.379224ms
Jan 23 13:51:06.838: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 23 13:51:06.838: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7860" to be "running"
Jan 23 13:51:06.839: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.19409ms
Jan 23 13:51:06.839: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 23 13:51:06.839: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7860" to be "running"
Jan 23 13:51:06.840: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.272194ms
Jan 23 13:51:06.840: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/23/24 13:51:06.84
Jan 23 13:51:06.850: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7860" to be "running"
Jan 23 13:51:06.851: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.318681ms
Jan 23 13:51:08.854: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004109586s
Jan 23 13:51:10.854: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004730556s
Jan 23 13:51:12.855: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.005252909s
Jan 23 13:51:12.855: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:51:12.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7860" for this suite. 01/23/24 13:51:12.887
------------------------------
• [SLOW TEST] [68.183 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:50:04.709
    Jan 23 13:50:04.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename sched-preemption 01/23/24 13:50:04.71
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:50:04.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:50:04.719
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jan 23 13:50:04.729: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 23 13:51:04.767: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 01/23/24 13:51:04.769
    Jan 23 13:51:04.794: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 23 13:51:04.805: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 23 13:51:04.820: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 23 13:51:04.831: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/23/24 13:51:04.831
    Jan 23 13:51:04.831: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7860" to be "running"
    Jan 23 13:51:04.833: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.453924ms
    Jan 23 13:51:06.836: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.004906511s
    Jan 23 13:51:06.836: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 23 13:51:06.836: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7860" to be "running"
    Jan 23 13:51:06.838: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.379224ms
    Jan 23 13:51:06.838: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 23 13:51:06.838: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7860" to be "running"
    Jan 23 13:51:06.839: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.19409ms
    Jan 23 13:51:06.839: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 23 13:51:06.839: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7860" to be "running"
    Jan 23 13:51:06.840: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.272194ms
    Jan 23 13:51:06.840: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/23/24 13:51:06.84
    Jan 23 13:51:06.850: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7860" to be "running"
    Jan 23 13:51:06.851: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.318681ms
    Jan 23 13:51:08.854: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004109586s
    Jan 23 13:51:10.854: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004730556s
    Jan 23 13:51:12.855: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.005252909s
    Jan 23 13:51:12.855: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:51:12.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7860" for this suite. 01/23/24 13:51:12.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:51:12.893
Jan 23 13:51:12.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 13:51:12.893
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:51:12.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:51:12.902
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 01/23/24 13:51:12.904
Jan 23 13:51:12.919: INFO: Waiting up to 5m0s for pod "labelsupdate69abe9a0-860a-4287-bb02-6b6aa1f89d29" in namespace "projected-7088" to be "running and ready"
Jan 23 13:51:12.920: INFO: Pod "labelsupdate69abe9a0-860a-4287-bb02-6b6aa1f89d29": Phase="Pending", Reason="", readiness=false. Elapsed: 1.499142ms
Jan 23 13:51:12.920: INFO: The phase of Pod labelsupdate69abe9a0-860a-4287-bb02-6b6aa1f89d29 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 13:51:14.923: INFO: Pod "labelsupdate69abe9a0-860a-4287-bb02-6b6aa1f89d29": Phase="Running", Reason="", readiness=true. Elapsed: 2.004256951s
Jan 23 13:51:14.923: INFO: The phase of Pod labelsupdate69abe9a0-860a-4287-bb02-6b6aa1f89d29 is Running (Ready = true)
Jan 23 13:51:14.923: INFO: Pod "labelsupdate69abe9a0-860a-4287-bb02-6b6aa1f89d29" satisfied condition "running and ready"
Jan 23 13:51:15.439: INFO: Successfully updated pod "labelsupdate69abe9a0-860a-4287-bb02-6b6aa1f89d29"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 23 13:51:19.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7088" for this suite. 01/23/24 13:51:19.455
------------------------------
• [SLOW TEST] [6.565 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:51:12.893
    Jan 23 13:51:12.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 13:51:12.893
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:51:12.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:51:12.902
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 01/23/24 13:51:12.904
    Jan 23 13:51:12.919: INFO: Waiting up to 5m0s for pod "labelsupdate69abe9a0-860a-4287-bb02-6b6aa1f89d29" in namespace "projected-7088" to be "running and ready"
    Jan 23 13:51:12.920: INFO: Pod "labelsupdate69abe9a0-860a-4287-bb02-6b6aa1f89d29": Phase="Pending", Reason="", readiness=false. Elapsed: 1.499142ms
    Jan 23 13:51:12.920: INFO: The phase of Pod labelsupdate69abe9a0-860a-4287-bb02-6b6aa1f89d29 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 13:51:14.923: INFO: Pod "labelsupdate69abe9a0-860a-4287-bb02-6b6aa1f89d29": Phase="Running", Reason="", readiness=true. Elapsed: 2.004256951s
    Jan 23 13:51:14.923: INFO: The phase of Pod labelsupdate69abe9a0-860a-4287-bb02-6b6aa1f89d29 is Running (Ready = true)
    Jan 23 13:51:14.923: INFO: Pod "labelsupdate69abe9a0-860a-4287-bb02-6b6aa1f89d29" satisfied condition "running and ready"
    Jan 23 13:51:15.439: INFO: Successfully updated pod "labelsupdate69abe9a0-860a-4287-bb02-6b6aa1f89d29"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:51:19.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7088" for this suite. 01/23/24 13:51:19.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:51:19.459
Jan 23 13:51:19.459: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 13:51:19.46
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:51:19.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:51:19.469
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-e2387afa-86a2-4e47-838d-b7b94d05b263 01/23/24 13:51:19.471
STEP: Creating a pod to test consume configMaps 01/23/24 13:51:19.475
Jan 23 13:51:19.486: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1afc4eca-bc2f-48e1-8d40-41f27099699e" in namespace "projected-1168" to be "Succeeded or Failed"
Jan 23 13:51:19.487: INFO: Pod "pod-projected-configmaps-1afc4eca-bc2f-48e1-8d40-41f27099699e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.075481ms
Jan 23 13:51:21.491: INFO: Pod "pod-projected-configmaps-1afc4eca-bc2f-48e1-8d40-41f27099699e": Phase="Running", Reason="", readiness=true. Elapsed: 2.004958758s
Jan 23 13:51:23.490: INFO: Pod "pod-projected-configmaps-1afc4eca-bc2f-48e1-8d40-41f27099699e": Phase="Running", Reason="", readiness=false. Elapsed: 4.003979522s
Jan 23 13:51:25.490: INFO: Pod "pod-projected-configmaps-1afc4eca-bc2f-48e1-8d40-41f27099699e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.003920538s
STEP: Saw pod success 01/23/24 13:51:25.49
Jan 23 13:51:25.490: INFO: Pod "pod-projected-configmaps-1afc4eca-bc2f-48e1-8d40-41f27099699e" satisfied condition "Succeeded or Failed"
Jan 23 13:51:25.493: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-configmaps-1afc4eca-bc2f-48e1-8d40-41f27099699e container agnhost-container: <nil>
STEP: delete the pod 01/23/24 13:51:25.496
Jan 23 13:51:25.504: INFO: Waiting for pod pod-projected-configmaps-1afc4eca-bc2f-48e1-8d40-41f27099699e to disappear
Jan 23 13:51:25.506: INFO: Pod pod-projected-configmaps-1afc4eca-bc2f-48e1-8d40-41f27099699e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 23 13:51:25.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1168" for this suite. 01/23/24 13:51:25.508
------------------------------
• [SLOW TEST] [6.052 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:51:19.459
    Jan 23 13:51:19.459: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 13:51:19.46
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:51:19.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:51:19.469
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-e2387afa-86a2-4e47-838d-b7b94d05b263 01/23/24 13:51:19.471
    STEP: Creating a pod to test consume configMaps 01/23/24 13:51:19.475
    Jan 23 13:51:19.486: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1afc4eca-bc2f-48e1-8d40-41f27099699e" in namespace "projected-1168" to be "Succeeded or Failed"
    Jan 23 13:51:19.487: INFO: Pod "pod-projected-configmaps-1afc4eca-bc2f-48e1-8d40-41f27099699e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.075481ms
    Jan 23 13:51:21.491: INFO: Pod "pod-projected-configmaps-1afc4eca-bc2f-48e1-8d40-41f27099699e": Phase="Running", Reason="", readiness=true. Elapsed: 2.004958758s
    Jan 23 13:51:23.490: INFO: Pod "pod-projected-configmaps-1afc4eca-bc2f-48e1-8d40-41f27099699e": Phase="Running", Reason="", readiness=false. Elapsed: 4.003979522s
    Jan 23 13:51:25.490: INFO: Pod "pod-projected-configmaps-1afc4eca-bc2f-48e1-8d40-41f27099699e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.003920538s
    STEP: Saw pod success 01/23/24 13:51:25.49
    Jan 23 13:51:25.490: INFO: Pod "pod-projected-configmaps-1afc4eca-bc2f-48e1-8d40-41f27099699e" satisfied condition "Succeeded or Failed"
    Jan 23 13:51:25.493: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-configmaps-1afc4eca-bc2f-48e1-8d40-41f27099699e container agnhost-container: <nil>
    STEP: delete the pod 01/23/24 13:51:25.496
    Jan 23 13:51:25.504: INFO: Waiting for pod pod-projected-configmaps-1afc4eca-bc2f-48e1-8d40-41f27099699e to disappear
    Jan 23 13:51:25.506: INFO: Pod pod-projected-configmaps-1afc4eca-bc2f-48e1-8d40-41f27099699e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:51:25.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1168" for this suite. 01/23/24 13:51:25.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:51:25.513
Jan 23 13:51:25.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename services 01/23/24 13:51:25.513
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:51:25.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:51:25.522
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-8245 01/23/24 13:51:25.523
STEP: creating service affinity-nodeport-transition in namespace services-8245 01/23/24 13:51:25.524
STEP: creating replication controller affinity-nodeport-transition in namespace services-8245 01/23/24 13:51:25.531
I0123 13:51:25.538026      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8245, replica count: 3
I0123 13:51:28.588875      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 23 13:51:28.594: INFO: Creating new exec pod
Jan 23 13:51:28.603: INFO: Waiting up to 5m0s for pod "execpod-affinityphn6n" in namespace "services-8245" to be "running"
Jan 23 13:51:28.605: INFO: Pod "execpod-affinityphn6n": Phase="Pending", Reason="", readiness=false. Elapsed: 1.482667ms
Jan 23 13:51:30.607: INFO: Pod "execpod-affinityphn6n": Phase="Running", Reason="", readiness=true. Elapsed: 2.003220806s
Jan 23 13:51:30.607: INFO: Pod "execpod-affinityphn6n" satisfied condition "running"
Jan 23 13:51:31.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-8245 exec execpod-affinityphn6n -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jan 23 13:51:31.785: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 23 13:51:31.785: INFO: stdout: ""
Jan 23 13:51:31.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-8245 exec execpod-affinityphn6n -- /bin/sh -x -c nc -v -z -w 2 10.233.3.153 80'
Jan 23 13:51:31.963: INFO: stderr: "+ nc -v -z -w 2 10.233.3.153 80\nConnection to 10.233.3.153 80 port [tcp/http] succeeded!\n"
Jan 23 13:51:31.963: INFO: stdout: ""
Jan 23 13:51:31.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-8245 exec execpod-affinityphn6n -- /bin/sh -x -c nc -v -z -w 2 172.31.11.40 30413'
Jan 23 13:51:32.138: INFO: stderr: "+ nc -v -z -w 2 172.31.11.40 30413\nConnection to 172.31.11.40 30413 port [tcp/*] succeeded!\n"
Jan 23 13:51:32.138: INFO: stdout: ""
Jan 23 13:51:32.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-8245 exec execpod-affinityphn6n -- /bin/sh -x -c nc -v -z -w 2 172.31.11.67 30413'
Jan 23 13:51:32.313: INFO: stderr: "+ nc -v -z -w 2 172.31.11.67 30413\nConnection to 172.31.11.67 30413 port [tcp/*] succeeded!\n"
Jan 23 13:51:32.313: INFO: stdout: ""
Jan 23 13:51:32.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-8245 exec execpod-affinityphn6n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.40:30413/ ; done'
Jan 23 13:51:32.551: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n"
Jan 23 13:51:32.551: INFO: stdout: "\naffinity-nodeport-transition-nsxdx\naffinity-nodeport-transition-6nkht\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-nsxdx\naffinity-nodeport-transition-6nkht\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-nsxdx\naffinity-nodeport-transition-6nkht\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-nsxdx\naffinity-nodeport-transition-6nkht\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-nsxdx\naffinity-nodeport-transition-6nkht\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-nsxdx"
Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-nsxdx
Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-6nkht
Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-nsxdx
Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-6nkht
Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-nsxdx
Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-6nkht
Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-nsxdx
Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-6nkht
Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-nsxdx
Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-6nkht
Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-nsxdx
Jan 23 13:51:32.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-8245 exec execpod-affinityphn6n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.40:30413/ ; done'
Jan 23 13:51:32.866: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n"
Jan 23 13:51:32.866: INFO: stdout: "\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7"
Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
Jan 23 13:51:32.866: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8245, will wait for the garbage collector to delete the pods 01/23/24 13:51:32.875
Jan 23 13:51:32.932: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.06603ms
Jan 23 13:51:33.033: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.096526ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 23 13:51:37.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8245" for this suite. 01/23/24 13:51:37.047
------------------------------
• [SLOW TEST] [11.537 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:51:25.513
    Jan 23 13:51:25.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename services 01/23/24 13:51:25.513
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:51:25.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:51:25.522
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-8245 01/23/24 13:51:25.523
    STEP: creating service affinity-nodeport-transition in namespace services-8245 01/23/24 13:51:25.524
    STEP: creating replication controller affinity-nodeport-transition in namespace services-8245 01/23/24 13:51:25.531
    I0123 13:51:25.538026      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8245, replica count: 3
    I0123 13:51:28.588875      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 23 13:51:28.594: INFO: Creating new exec pod
    Jan 23 13:51:28.603: INFO: Waiting up to 5m0s for pod "execpod-affinityphn6n" in namespace "services-8245" to be "running"
    Jan 23 13:51:28.605: INFO: Pod "execpod-affinityphn6n": Phase="Pending", Reason="", readiness=false. Elapsed: 1.482667ms
    Jan 23 13:51:30.607: INFO: Pod "execpod-affinityphn6n": Phase="Running", Reason="", readiness=true. Elapsed: 2.003220806s
    Jan 23 13:51:30.607: INFO: Pod "execpod-affinityphn6n" satisfied condition "running"
    Jan 23 13:51:31.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-8245 exec execpod-affinityphn6n -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jan 23 13:51:31.785: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan 23 13:51:31.785: INFO: stdout: ""
    Jan 23 13:51:31.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-8245 exec execpod-affinityphn6n -- /bin/sh -x -c nc -v -z -w 2 10.233.3.153 80'
    Jan 23 13:51:31.963: INFO: stderr: "+ nc -v -z -w 2 10.233.3.153 80\nConnection to 10.233.3.153 80 port [tcp/http] succeeded!\n"
    Jan 23 13:51:31.963: INFO: stdout: ""
    Jan 23 13:51:31.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-8245 exec execpod-affinityphn6n -- /bin/sh -x -c nc -v -z -w 2 172.31.11.40 30413'
    Jan 23 13:51:32.138: INFO: stderr: "+ nc -v -z -w 2 172.31.11.40 30413\nConnection to 172.31.11.40 30413 port [tcp/*] succeeded!\n"
    Jan 23 13:51:32.138: INFO: stdout: ""
    Jan 23 13:51:32.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-8245 exec execpod-affinityphn6n -- /bin/sh -x -c nc -v -z -w 2 172.31.11.67 30413'
    Jan 23 13:51:32.313: INFO: stderr: "+ nc -v -z -w 2 172.31.11.67 30413\nConnection to 172.31.11.67 30413 port [tcp/*] succeeded!\n"
    Jan 23 13:51:32.313: INFO: stdout: ""
    Jan 23 13:51:32.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-8245 exec execpod-affinityphn6n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.40:30413/ ; done'
    Jan 23 13:51:32.551: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n"
    Jan 23 13:51:32.551: INFO: stdout: "\naffinity-nodeport-transition-nsxdx\naffinity-nodeport-transition-6nkht\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-nsxdx\naffinity-nodeport-transition-6nkht\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-nsxdx\naffinity-nodeport-transition-6nkht\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-nsxdx\naffinity-nodeport-transition-6nkht\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-nsxdx\naffinity-nodeport-transition-6nkht\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-nsxdx"
    Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-nsxdx
    Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-6nkht
    Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-nsxdx
    Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-6nkht
    Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-nsxdx
    Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-6nkht
    Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-nsxdx
    Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-6nkht
    Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-nsxdx
    Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-6nkht
    Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.551: INFO: Received response from host: affinity-nodeport-transition-nsxdx
    Jan 23 13:51:32.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-8245 exec execpod-affinityphn6n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.40:30413/ ; done'
    Jan 23 13:51:32.866: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:30413/\n"
    Jan 23 13:51:32.866: INFO: stdout: "\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7\naffinity-nodeport-transition-7zwp7"
    Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.866: INFO: Received response from host: affinity-nodeport-transition-7zwp7
    Jan 23 13:51:32.866: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8245, will wait for the garbage collector to delete the pods 01/23/24 13:51:32.875
    Jan 23 13:51:32.932: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.06603ms
    Jan 23 13:51:33.033: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.096526ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:51:37.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8245" for this suite. 01/23/24 13:51:37.047
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:51:37.05
Jan 23 13:51:37.050: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename security-context-test 01/23/24 13:51:37.051
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:51:37.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:51:37.061
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Jan 23 13:51:37.073: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-59d9f5dc-f747-43ac-8a88-2f98093d7d26" in namespace "security-context-test-5141" to be "Succeeded or Failed"
Jan 23 13:51:37.079: INFO: Pod "busybox-readonly-false-59d9f5dc-f747-43ac-8a88-2f98093d7d26": Phase="Pending", Reason="", readiness=false. Elapsed: 6.471339ms
Jan 23 13:51:39.083: INFO: Pod "busybox-readonly-false-59d9f5dc-f747-43ac-8a88-2f98093d7d26": Phase="Running", Reason="", readiness=true. Elapsed: 2.010183278s
Jan 23 13:51:41.082: INFO: Pod "busybox-readonly-false-59d9f5dc-f747-43ac-8a88-2f98093d7d26": Phase="Running", Reason="", readiness=false. Elapsed: 4.009774094s
Jan 23 13:51:43.083: INFO: Pod "busybox-readonly-false-59d9f5dc-f747-43ac-8a88-2f98093d7d26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010371534s
Jan 23 13:51:43.083: INFO: Pod "busybox-readonly-false-59d9f5dc-f747-43ac-8a88-2f98093d7d26" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 23 13:51:43.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-5141" for this suite. 01/23/24 13:51:43.086
------------------------------
• [SLOW TEST] [6.040 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:51:37.05
    Jan 23 13:51:37.050: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename security-context-test 01/23/24 13:51:37.051
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:51:37.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:51:37.061
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Jan 23 13:51:37.073: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-59d9f5dc-f747-43ac-8a88-2f98093d7d26" in namespace "security-context-test-5141" to be "Succeeded or Failed"
    Jan 23 13:51:37.079: INFO: Pod "busybox-readonly-false-59d9f5dc-f747-43ac-8a88-2f98093d7d26": Phase="Pending", Reason="", readiness=false. Elapsed: 6.471339ms
    Jan 23 13:51:39.083: INFO: Pod "busybox-readonly-false-59d9f5dc-f747-43ac-8a88-2f98093d7d26": Phase="Running", Reason="", readiness=true. Elapsed: 2.010183278s
    Jan 23 13:51:41.082: INFO: Pod "busybox-readonly-false-59d9f5dc-f747-43ac-8a88-2f98093d7d26": Phase="Running", Reason="", readiness=false. Elapsed: 4.009774094s
    Jan 23 13:51:43.083: INFO: Pod "busybox-readonly-false-59d9f5dc-f747-43ac-8a88-2f98093d7d26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010371534s
    Jan 23 13:51:43.083: INFO: Pod "busybox-readonly-false-59d9f5dc-f747-43ac-8a88-2f98093d7d26" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:51:43.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-5141" for this suite. 01/23/24 13:51:43.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:51:43.09
Jan 23 13:51:43.090: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename resourcequota 01/23/24 13:51:43.091
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:51:43.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:51:43.099
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-zbgd4" 01/23/24 13:51:43.103
Jan 23 13:51:43.106: INFO: Resource quota "e2e-rq-status-zbgd4" reports spec: hard cpu limit of 500m
Jan 23 13:51:43.106: INFO: Resource quota "e2e-rq-status-zbgd4" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-zbgd4" /status 01/23/24 13:51:43.106
STEP: Confirm /status for "e2e-rq-status-zbgd4" resourceQuota via watch 01/23/24 13:51:43.11
Jan 23 13:51:43.111: INFO: observed resourceQuota "e2e-rq-status-zbgd4" in namespace "resourcequota-5998" with hard status: v1.ResourceList(nil)
Jan 23 13:51:43.111: INFO: Found resourceQuota "e2e-rq-status-zbgd4" in namespace "resourcequota-5998" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan 23 13:51:43.111: INFO: ResourceQuota "e2e-rq-status-zbgd4" /status was updated
STEP: Patching hard spec values for cpu & memory 01/23/24 13:51:43.113
Jan 23 13:51:43.117: INFO: Resource quota "e2e-rq-status-zbgd4" reports spec: hard cpu limit of 1
Jan 23 13:51:43.117: INFO: Resource quota "e2e-rq-status-zbgd4" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-zbgd4" /status 01/23/24 13:51:43.117
STEP: Confirm /status for "e2e-rq-status-zbgd4" resourceQuota via watch 01/23/24 13:51:43.12
Jan 23 13:51:43.121: INFO: observed resourceQuota "e2e-rq-status-zbgd4" in namespace "resourcequota-5998" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan 23 13:51:43.121: INFO: Found resourceQuota "e2e-rq-status-zbgd4" in namespace "resourcequota-5998" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Jan 23 13:51:43.121: INFO: ResourceQuota "e2e-rq-status-zbgd4" /status was patched
STEP: Get "e2e-rq-status-zbgd4" /status 01/23/24 13:51:43.121
Jan 23 13:51:43.124: INFO: Resourcequota "e2e-rq-status-zbgd4" reports status: hard cpu of 1
Jan 23 13:51:43.124: INFO: Resourcequota "e2e-rq-status-zbgd4" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-zbgd4" /status before checking Spec is unchanged 01/23/24 13:51:43.125
Jan 23 13:51:43.129: INFO: Resourcequota "e2e-rq-status-zbgd4" reports status: hard cpu of 2
Jan 23 13:51:43.129: INFO: Resourcequota "e2e-rq-status-zbgd4" reports status: hard memory of 2Gi
Jan 23 13:51:43.130: INFO: Found resourceQuota "e2e-rq-status-zbgd4" in namespace "resourcequota-5998" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Jan 23 13:51:48.134: INFO: ResourceQuota "e2e-rq-status-zbgd4" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 23 13:51:48.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5998" for this suite. 01/23/24 13:51:48.136
------------------------------
• [SLOW TEST] [5.049 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:51:43.09
    Jan 23 13:51:43.090: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename resourcequota 01/23/24 13:51:43.091
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:51:43.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:51:43.099
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-zbgd4" 01/23/24 13:51:43.103
    Jan 23 13:51:43.106: INFO: Resource quota "e2e-rq-status-zbgd4" reports spec: hard cpu limit of 500m
    Jan 23 13:51:43.106: INFO: Resource quota "e2e-rq-status-zbgd4" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-zbgd4" /status 01/23/24 13:51:43.106
    STEP: Confirm /status for "e2e-rq-status-zbgd4" resourceQuota via watch 01/23/24 13:51:43.11
    Jan 23 13:51:43.111: INFO: observed resourceQuota "e2e-rq-status-zbgd4" in namespace "resourcequota-5998" with hard status: v1.ResourceList(nil)
    Jan 23 13:51:43.111: INFO: Found resourceQuota "e2e-rq-status-zbgd4" in namespace "resourcequota-5998" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan 23 13:51:43.111: INFO: ResourceQuota "e2e-rq-status-zbgd4" /status was updated
    STEP: Patching hard spec values for cpu & memory 01/23/24 13:51:43.113
    Jan 23 13:51:43.117: INFO: Resource quota "e2e-rq-status-zbgd4" reports spec: hard cpu limit of 1
    Jan 23 13:51:43.117: INFO: Resource quota "e2e-rq-status-zbgd4" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-zbgd4" /status 01/23/24 13:51:43.117
    STEP: Confirm /status for "e2e-rq-status-zbgd4" resourceQuota via watch 01/23/24 13:51:43.12
    Jan 23 13:51:43.121: INFO: observed resourceQuota "e2e-rq-status-zbgd4" in namespace "resourcequota-5998" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan 23 13:51:43.121: INFO: Found resourceQuota "e2e-rq-status-zbgd4" in namespace "resourcequota-5998" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Jan 23 13:51:43.121: INFO: ResourceQuota "e2e-rq-status-zbgd4" /status was patched
    STEP: Get "e2e-rq-status-zbgd4" /status 01/23/24 13:51:43.121
    Jan 23 13:51:43.124: INFO: Resourcequota "e2e-rq-status-zbgd4" reports status: hard cpu of 1
    Jan 23 13:51:43.124: INFO: Resourcequota "e2e-rq-status-zbgd4" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-zbgd4" /status before checking Spec is unchanged 01/23/24 13:51:43.125
    Jan 23 13:51:43.129: INFO: Resourcequota "e2e-rq-status-zbgd4" reports status: hard cpu of 2
    Jan 23 13:51:43.129: INFO: Resourcequota "e2e-rq-status-zbgd4" reports status: hard memory of 2Gi
    Jan 23 13:51:43.130: INFO: Found resourceQuota "e2e-rq-status-zbgd4" in namespace "resourcequota-5998" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Jan 23 13:51:48.134: INFO: ResourceQuota "e2e-rq-status-zbgd4" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:51:48.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5998" for this suite. 01/23/24 13:51:48.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:51:48.139
Jan 23 13:51:48.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename aggregator 01/23/24 13:51:48.14
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:51:48.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:51:48.149
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan 23 13:51:48.151: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/23/24 13:51:48.152
Jan 23 13:51:48.409: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 23 13:51:50.434: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 13:51:52.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 13:51:54.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 13:51:56.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 13:51:58.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 13:52:00.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 13:52:02.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 13:52:04.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 13:52:06.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 13:52:08.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 13:52:10.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 13:52:12.757: INFO: Waited 315.825275ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/23/24 13:52:13.473
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/23/24 13:52:13.516
STEP: List APIServices 01/23/24 13:52:13.567
Jan 23 13:52:13.619: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Jan 23 13:52:14.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-7004" for this suite. 01/23/24 13:52:14.466
------------------------------
• [SLOW TEST] [26.378 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:51:48.139
    Jan 23 13:51:48.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename aggregator 01/23/24 13:51:48.14
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:51:48.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:51:48.149
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan 23 13:51:48.151: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/23/24 13:51:48.152
    Jan 23 13:51:48.409: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jan 23 13:51:50.434: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 13:51:52.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 13:51:54.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 13:51:56.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 13:51:58.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 13:52:00.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 13:52:02.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 13:52:04.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 13:52:06.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 13:52:08.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 13:52:10.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 13, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 13:52:12.757: INFO: Waited 315.825275ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/23/24 13:52:13.473
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/23/24 13:52:13.516
    STEP: List APIServices 01/23/24 13:52:13.567
    Jan 23 13:52:13.619: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:52:14.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-7004" for this suite. 01/23/24 13:52:14.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:52:14.518
Jan 23 13:52:14.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename container-probe 01/23/24 13:52:14.518
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:52:14.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:52:14.527
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-5d50ce96-f803-4362-9fbc-44d9b5c8eb3f in namespace container-probe-8557 01/23/24 13:52:14.529
Jan 23 13:52:14.539: INFO: Waiting up to 5m0s for pod "liveness-5d50ce96-f803-4362-9fbc-44d9b5c8eb3f" in namespace "container-probe-8557" to be "not pending"
Jan 23 13:52:14.540: INFO: Pod "liveness-5d50ce96-f803-4362-9fbc-44d9b5c8eb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.183009ms
Jan 23 13:52:16.545: INFO: Pod "liveness-5d50ce96-f803-4362-9fbc-44d9b5c8eb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006100529s
Jan 23 13:52:18.544: INFO: Pod "liveness-5d50ce96-f803-4362-9fbc-44d9b5c8eb3f": Phase="Running", Reason="", readiness=true. Elapsed: 4.004924947s
Jan 23 13:52:18.544: INFO: Pod "liveness-5d50ce96-f803-4362-9fbc-44d9b5c8eb3f" satisfied condition "not pending"
Jan 23 13:52:18.544: INFO: Started pod liveness-5d50ce96-f803-4362-9fbc-44d9b5c8eb3f in namespace container-probe-8557
STEP: checking the pod's current state and verifying that restartCount is present 01/23/24 13:52:18.544
Jan 23 13:52:18.546: INFO: Initial restart count of pod liveness-5d50ce96-f803-4362-9fbc-44d9b5c8eb3f is 0
Jan 23 13:52:38.579: INFO: Restart count of pod container-probe-8557/liveness-5d50ce96-f803-4362-9fbc-44d9b5c8eb3f is now 1 (20.032617829s elapsed)
STEP: deleting the pod 01/23/24 13:52:38.579
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 23 13:52:38.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8557" for this suite. 01/23/24 13:52:38.589
------------------------------
• [SLOW TEST] [24.075 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:52:14.518
    Jan 23 13:52:14.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename container-probe 01/23/24 13:52:14.518
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:52:14.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:52:14.527
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-5d50ce96-f803-4362-9fbc-44d9b5c8eb3f in namespace container-probe-8557 01/23/24 13:52:14.529
    Jan 23 13:52:14.539: INFO: Waiting up to 5m0s for pod "liveness-5d50ce96-f803-4362-9fbc-44d9b5c8eb3f" in namespace "container-probe-8557" to be "not pending"
    Jan 23 13:52:14.540: INFO: Pod "liveness-5d50ce96-f803-4362-9fbc-44d9b5c8eb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.183009ms
    Jan 23 13:52:16.545: INFO: Pod "liveness-5d50ce96-f803-4362-9fbc-44d9b5c8eb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006100529s
    Jan 23 13:52:18.544: INFO: Pod "liveness-5d50ce96-f803-4362-9fbc-44d9b5c8eb3f": Phase="Running", Reason="", readiness=true. Elapsed: 4.004924947s
    Jan 23 13:52:18.544: INFO: Pod "liveness-5d50ce96-f803-4362-9fbc-44d9b5c8eb3f" satisfied condition "not pending"
    Jan 23 13:52:18.544: INFO: Started pod liveness-5d50ce96-f803-4362-9fbc-44d9b5c8eb3f in namespace container-probe-8557
    STEP: checking the pod's current state and verifying that restartCount is present 01/23/24 13:52:18.544
    Jan 23 13:52:18.546: INFO: Initial restart count of pod liveness-5d50ce96-f803-4362-9fbc-44d9b5c8eb3f is 0
    Jan 23 13:52:38.579: INFO: Restart count of pod container-probe-8557/liveness-5d50ce96-f803-4362-9fbc-44d9b5c8eb3f is now 1 (20.032617829s elapsed)
    STEP: deleting the pod 01/23/24 13:52:38.579
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:52:38.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8557" for this suite. 01/23/24 13:52:38.589
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:52:38.594
Jan 23 13:52:38.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename crd-webhook 01/23/24 13:52:38.595
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:52:38.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:52:38.604
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/23/24 13:52:38.606
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/23/24 13:52:39.04
STEP: Deploying the custom resource conversion webhook pod 01/23/24 13:52:39.044
STEP: Wait for the deployment to be ready 01/23/24 13:52:39.054
Jan 23 13:52:39.057: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/23/24 13:52:41.063
STEP: Verifying the service has paired with the endpoint 01/23/24 13:52:41.068
Jan 23 13:52:42.068: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan 23 13:52:42.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Creating a v1 custom resource 01/23/24 13:52:49.633
STEP: v2 custom resource should be converted 01/23/24 13:52:49.636
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:52:50.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-7462" for this suite. 01/23/24 13:52:50.177
------------------------------
• [SLOW TEST] [11.591 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:52:38.594
    Jan 23 13:52:38.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename crd-webhook 01/23/24 13:52:38.595
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:52:38.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:52:38.604
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/23/24 13:52:38.606
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/23/24 13:52:39.04
    STEP: Deploying the custom resource conversion webhook pod 01/23/24 13:52:39.044
    STEP: Wait for the deployment to be ready 01/23/24 13:52:39.054
    Jan 23 13:52:39.057: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/23/24 13:52:41.063
    STEP: Verifying the service has paired with the endpoint 01/23/24 13:52:41.068
    Jan 23 13:52:42.068: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan 23 13:52:42.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Creating a v1 custom resource 01/23/24 13:52:49.633
    STEP: v2 custom resource should be converted 01/23/24 13:52:49.636
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:52:50.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-7462" for this suite. 01/23/24 13:52:50.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:52:50.186
Jan 23 13:52:50.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename resourcequota 01/23/24 13:52:50.187
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:52:50.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:52:50.2
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 01/23/24 13:52:50.202
STEP: Getting a ResourceQuota 01/23/24 13:52:50.207
STEP: Listing all ResourceQuotas with LabelSelector 01/23/24 13:52:50.209
STEP: Patching the ResourceQuota 01/23/24 13:52:50.21
STEP: Deleting a Collection of ResourceQuotas 01/23/24 13:52:50.215
STEP: Verifying the deleted ResourceQuota 01/23/24 13:52:50.22
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 23 13:52:50.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7112" for this suite. 01/23/24 13:52:50.223
------------------------------
• [0.040 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:52:50.186
    Jan 23 13:52:50.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename resourcequota 01/23/24 13:52:50.187
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:52:50.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:52:50.2
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 01/23/24 13:52:50.202
    STEP: Getting a ResourceQuota 01/23/24 13:52:50.207
    STEP: Listing all ResourceQuotas with LabelSelector 01/23/24 13:52:50.209
    STEP: Patching the ResourceQuota 01/23/24 13:52:50.21
    STEP: Deleting a Collection of ResourceQuotas 01/23/24 13:52:50.215
    STEP: Verifying the deleted ResourceQuota 01/23/24 13:52:50.22
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:52:50.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7112" for this suite. 01/23/24 13:52:50.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:52:50.227
Jan 23 13:52:50.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename csiinlinevolumes 01/23/24 13:52:50.228
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:52:50.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:52:50.237
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 01/23/24 13:52:50.24
STEP: getting 01/23/24 13:52:50.249
STEP: listing 01/23/24 13:52:50.254
STEP: deleting 01/23/24 13:52:50.257
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan 23 13:52:50.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-3510" for this suite. 01/23/24 13:52:50.268
------------------------------
• [0.044 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:52:50.227
    Jan 23 13:52:50.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename csiinlinevolumes 01/23/24 13:52:50.228
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:52:50.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:52:50.237
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 01/23/24 13:52:50.24
    STEP: getting 01/23/24 13:52:50.249
    STEP: listing 01/23/24 13:52:50.254
    STEP: deleting 01/23/24 13:52:50.257
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:52:50.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-3510" for this suite. 01/23/24 13:52:50.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:52:50.271
Jan 23 13:52:50.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename custom-resource-definition 01/23/24 13:52:50.272
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:52:50.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:52:50.283
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan 23 13:52:50.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:52:55.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7565" for this suite. 01/23/24 13:52:55.811
------------------------------
• [SLOW TEST] [5.543 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:52:50.271
    Jan 23 13:52:50.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename custom-resource-definition 01/23/24 13:52:50.272
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:52:50.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:52:50.283
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan 23 13:52:50.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:52:55.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7565" for this suite. 01/23/24 13:52:55.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:52:55.815
Jan 23 13:52:55.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename webhook 01/23/24 13:52:55.816
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:52:55.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:52:55.827
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/23/24 13:52:55.835
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 13:52:56.292
STEP: Deploying the webhook pod 01/23/24 13:52:56.296
STEP: Wait for the deployment to be ready 01/23/24 13:52:56.304
Jan 23 13:52:56.308: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/23/24 13:52:58.313
STEP: Verifying the service has paired with the endpoint 01/23/24 13:52:58.317
Jan 23 13:52:59.318: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 01/23/24 13:52:59.32
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/23/24 13:52:59.332
STEP: Creating a configMap that should not be mutated 01/23/24 13:52:59.336
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/23/24 13:52:59.342
STEP: Creating a configMap that should be mutated 01/23/24 13:52:59.349
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:52:59.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5002" for this suite. 01/23/24 13:52:59.393
STEP: Destroying namespace "webhook-5002-markers" for this suite. 01/23/24 13:52:59.398
------------------------------
• [3.595 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:52:55.815
    Jan 23 13:52:55.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename webhook 01/23/24 13:52:55.816
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:52:55.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:52:55.827
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/23/24 13:52:55.835
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 13:52:56.292
    STEP: Deploying the webhook pod 01/23/24 13:52:56.296
    STEP: Wait for the deployment to be ready 01/23/24 13:52:56.304
    Jan 23 13:52:56.308: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/23/24 13:52:58.313
    STEP: Verifying the service has paired with the endpoint 01/23/24 13:52:58.317
    Jan 23 13:52:59.318: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 01/23/24 13:52:59.32
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/23/24 13:52:59.332
    STEP: Creating a configMap that should not be mutated 01/23/24 13:52:59.336
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/23/24 13:52:59.342
    STEP: Creating a configMap that should be mutated 01/23/24 13:52:59.349
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:52:59.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5002" for this suite. 01/23/24 13:52:59.393
    STEP: Destroying namespace "webhook-5002-markers" for this suite. 01/23/24 13:52:59.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:52:59.411
Jan 23 13:52:59.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename var-expansion 01/23/24 13:52:59.412
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:52:59.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:52:59.424
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Jan 23 13:52:59.441: INFO: Waiting up to 2m0s for pod "var-expansion-8e1db5fe-515c-4b09-815c-8d79a4059b4c" in namespace "var-expansion-7013" to be "container 0 failed with reason CreateContainerConfigError"
Jan 23 13:52:59.443: INFO: Pod "var-expansion-8e1db5fe-515c-4b09-815c-8d79a4059b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.480982ms
Jan 23 13:53:01.445: INFO: Pod "var-expansion-8e1db5fe-515c-4b09-815c-8d79a4059b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003953844s
Jan 23 13:53:01.445: INFO: Pod "var-expansion-8e1db5fe-515c-4b09-815c-8d79a4059b4c" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 23 13:53:01.445: INFO: Deleting pod "var-expansion-8e1db5fe-515c-4b09-815c-8d79a4059b4c" in namespace "var-expansion-7013"
Jan 23 13:53:01.449: INFO: Wait up to 5m0s for pod "var-expansion-8e1db5fe-515c-4b09-815c-8d79a4059b4c" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 23 13:53:05.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7013" for this suite. 01/23/24 13:53:05.456
------------------------------
• [SLOW TEST] [6.050 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:52:59.411
    Jan 23 13:52:59.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename var-expansion 01/23/24 13:52:59.412
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:52:59.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:52:59.424
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Jan 23 13:52:59.441: INFO: Waiting up to 2m0s for pod "var-expansion-8e1db5fe-515c-4b09-815c-8d79a4059b4c" in namespace "var-expansion-7013" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 23 13:52:59.443: INFO: Pod "var-expansion-8e1db5fe-515c-4b09-815c-8d79a4059b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.480982ms
    Jan 23 13:53:01.445: INFO: Pod "var-expansion-8e1db5fe-515c-4b09-815c-8d79a4059b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003953844s
    Jan 23 13:53:01.445: INFO: Pod "var-expansion-8e1db5fe-515c-4b09-815c-8d79a4059b4c" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 23 13:53:01.445: INFO: Deleting pod "var-expansion-8e1db5fe-515c-4b09-815c-8d79a4059b4c" in namespace "var-expansion-7013"
    Jan 23 13:53:01.449: INFO: Wait up to 5m0s for pod "var-expansion-8e1db5fe-515c-4b09-815c-8d79a4059b4c" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:53:05.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7013" for this suite. 01/23/24 13:53:05.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:53:05.461
Jan 23 13:53:05.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename sched-preemption 01/23/24 13:53:05.462
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:53:05.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:53:05.472
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jan 23 13:53:05.484: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 23 13:54:05.525: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:54:05.528
Jan 23 13:54:05.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename sched-preemption-path 01/23/24 13:54:05.528
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:54:05.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:54:05.54
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 01/23/24 13:54:05.541
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/23/24 13:54:05.541
Jan 23 13:54:05.552: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1289" to be "running"
Jan 23 13:54:05.554: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.671144ms
Jan 23 13:54:07.557: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005305948s
Jan 23 13:54:07.557: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/23/24 13:54:07.559
Jan 23 13:54:07.567: INFO: found a healthy node: node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Jan 23 13:54:15.609: INFO: pods created so far: [1 1 1]
Jan 23 13:54:15.609: INFO: length of pods created so far: 3
Jan 23 13:54:19.625: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Jan 23 13:54:26.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:54:26.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-1289" for this suite. 01/23/24 13:54:26.67
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5575" for this suite. 01/23/24 13:54:26.674
------------------------------
• [SLOW TEST] [81.217 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:53:05.461
    Jan 23 13:53:05.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename sched-preemption 01/23/24 13:53:05.462
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:53:05.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:53:05.472
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jan 23 13:53:05.484: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 23 13:54:05.525: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:54:05.528
    Jan 23 13:54:05.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename sched-preemption-path 01/23/24 13:54:05.528
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:54:05.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:54:05.54
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 01/23/24 13:54:05.541
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/23/24 13:54:05.541
    Jan 23 13:54:05.552: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1289" to be "running"
    Jan 23 13:54:05.554: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.671144ms
    Jan 23 13:54:07.557: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005305948s
    Jan 23 13:54:07.557: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/23/24 13:54:07.559
    Jan 23 13:54:07.567: INFO: found a healthy node: node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Jan 23 13:54:15.609: INFO: pods created so far: [1 1 1]
    Jan 23 13:54:15.609: INFO: length of pods created so far: 3
    Jan 23 13:54:19.625: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:54:26.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:54:26.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-1289" for this suite. 01/23/24 13:54:26.67
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5575" for this suite. 01/23/24 13:54:26.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:54:26.679
Jan 23 13:54:26.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename downward-api 01/23/24 13:54:26.679
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:54:26.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:54:26.688
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 01/23/24 13:54:26.69
Jan 23 13:54:26.704: INFO: Waiting up to 5m0s for pod "downwardapi-volume-630676e2-fd2b-4d64-80ae-379d2948bc64" in namespace "downward-api-8970" to be "Succeeded or Failed"
Jan 23 13:54:26.708: INFO: Pod "downwardapi-volume-630676e2-fd2b-4d64-80ae-379d2948bc64": Phase="Pending", Reason="", readiness=false. Elapsed: 3.386038ms
Jan 23 13:54:28.710: INFO: Pod "downwardapi-volume-630676e2-fd2b-4d64-80ae-379d2948bc64": Phase="Running", Reason="", readiness=true. Elapsed: 2.005885833s
Jan 23 13:54:30.711: INFO: Pod "downwardapi-volume-630676e2-fd2b-4d64-80ae-379d2948bc64": Phase="Running", Reason="", readiness=false. Elapsed: 4.006545768s
Jan 23 13:54:32.711: INFO: Pod "downwardapi-volume-630676e2-fd2b-4d64-80ae-379d2948bc64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006793114s
STEP: Saw pod success 01/23/24 13:54:32.711
Jan 23 13:54:32.711: INFO: Pod "downwardapi-volume-630676e2-fd2b-4d64-80ae-379d2948bc64" satisfied condition "Succeeded or Failed"
Jan 23 13:54:32.713: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-630676e2-fd2b-4d64-80ae-379d2948bc64 container client-container: <nil>
STEP: delete the pod 01/23/24 13:54:32.722
Jan 23 13:54:32.732: INFO: Waiting for pod downwardapi-volume-630676e2-fd2b-4d64-80ae-379d2948bc64 to disappear
Jan 23 13:54:32.733: INFO: Pod downwardapi-volume-630676e2-fd2b-4d64-80ae-379d2948bc64 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 23 13:54:32.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8970" for this suite. 01/23/24 13:54:32.735
------------------------------
• [SLOW TEST] [6.060 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:54:26.679
    Jan 23 13:54:26.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename downward-api 01/23/24 13:54:26.679
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:54:26.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:54:26.688
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 01/23/24 13:54:26.69
    Jan 23 13:54:26.704: INFO: Waiting up to 5m0s for pod "downwardapi-volume-630676e2-fd2b-4d64-80ae-379d2948bc64" in namespace "downward-api-8970" to be "Succeeded or Failed"
    Jan 23 13:54:26.708: INFO: Pod "downwardapi-volume-630676e2-fd2b-4d64-80ae-379d2948bc64": Phase="Pending", Reason="", readiness=false. Elapsed: 3.386038ms
    Jan 23 13:54:28.710: INFO: Pod "downwardapi-volume-630676e2-fd2b-4d64-80ae-379d2948bc64": Phase="Running", Reason="", readiness=true. Elapsed: 2.005885833s
    Jan 23 13:54:30.711: INFO: Pod "downwardapi-volume-630676e2-fd2b-4d64-80ae-379d2948bc64": Phase="Running", Reason="", readiness=false. Elapsed: 4.006545768s
    Jan 23 13:54:32.711: INFO: Pod "downwardapi-volume-630676e2-fd2b-4d64-80ae-379d2948bc64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006793114s
    STEP: Saw pod success 01/23/24 13:54:32.711
    Jan 23 13:54:32.711: INFO: Pod "downwardapi-volume-630676e2-fd2b-4d64-80ae-379d2948bc64" satisfied condition "Succeeded or Failed"
    Jan 23 13:54:32.713: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-630676e2-fd2b-4d64-80ae-379d2948bc64 container client-container: <nil>
    STEP: delete the pod 01/23/24 13:54:32.722
    Jan 23 13:54:32.732: INFO: Waiting for pod downwardapi-volume-630676e2-fd2b-4d64-80ae-379d2948bc64 to disappear
    Jan 23 13:54:32.733: INFO: Pod downwardapi-volume-630676e2-fd2b-4d64-80ae-379d2948bc64 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:54:32.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8970" for this suite. 01/23/24 13:54:32.735
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:54:32.739
Jan 23 13:54:32.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename disruption 01/23/24 13:54:32.74
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:54:32.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:54:32.749
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:54:32.751
Jan 23 13:54:32.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename disruption-2 01/23/24 13:54:32.752
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:54:32.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:54:32.761
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 01/23/24 13:54:32.765
STEP: Waiting for the pdb to be processed 01/23/24 13:54:34.771
STEP: Waiting for the pdb to be processed 01/23/24 13:54:36.779
STEP: listing a collection of PDBs across all namespaces 01/23/24 13:54:38.784
STEP: listing a collection of PDBs in namespace disruption-3367 01/23/24 13:54:38.786
STEP: deleting a collection of PDBs 01/23/24 13:54:38.788
STEP: Waiting for the PDB collection to be deleted 01/23/24 13:54:38.794
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Jan 23 13:54:38.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 23 13:54:38.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-6863" for this suite. 01/23/24 13:54:38.8
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-3367" for this suite. 01/23/24 13:54:38.803
------------------------------
• [SLOW TEST] [6.068 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:54:32.739
    Jan 23 13:54:32.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename disruption 01/23/24 13:54:32.74
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:54:32.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:54:32.749
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:54:32.751
    Jan 23 13:54:32.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename disruption-2 01/23/24 13:54:32.752
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:54:32.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:54:32.761
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 01/23/24 13:54:32.765
    STEP: Waiting for the pdb to be processed 01/23/24 13:54:34.771
    STEP: Waiting for the pdb to be processed 01/23/24 13:54:36.779
    STEP: listing a collection of PDBs across all namespaces 01/23/24 13:54:38.784
    STEP: listing a collection of PDBs in namespace disruption-3367 01/23/24 13:54:38.786
    STEP: deleting a collection of PDBs 01/23/24 13:54:38.788
    STEP: Waiting for the PDB collection to be deleted 01/23/24 13:54:38.794
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:54:38.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:54:38.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-6863" for this suite. 01/23/24 13:54:38.8
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-3367" for this suite. 01/23/24 13:54:38.803
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:54:38.808
Jan 23 13:54:38.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename sysctl 01/23/24 13:54:38.809
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:54:38.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:54:38.823
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/23/24 13:54:38.826
STEP: Watching for error events or started pod 01/23/24 13:54:38.839
STEP: Waiting for pod completion 01/23/24 13:54:40.844
Jan 23 13:54:40.844: INFO: Waiting up to 3m0s for pod "sysctl-64ecb0b2-330f-4bc9-ab46-2694078c3cf8" in namespace "sysctl-3911" to be "completed"
Jan 23 13:54:40.845: INFO: Pod "sysctl-64ecb0b2-330f-4bc9-ab46-2694078c3cf8": Phase="Running", Reason="", readiness=true. Elapsed: 1.649227ms
Jan 23 13:54:42.849: INFO: Pod "sysctl-64ecb0b2-330f-4bc9-ab46-2694078c3cf8": Phase="Running", Reason="", readiness=false. Elapsed: 2.004956124s
Jan 23 13:54:44.849: INFO: Pod "sysctl-64ecb0b2-330f-4bc9-ab46-2694078c3cf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005130786s
Jan 23 13:54:44.849: INFO: Pod "sysctl-64ecb0b2-330f-4bc9-ab46-2694078c3cf8" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/23/24 13:54:44.851
STEP: Getting logs from the pod 01/23/24 13:54:44.851
STEP: Checking that the sysctl is actually updated 01/23/24 13:54:44.855
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:54:44.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-3911" for this suite. 01/23/24 13:54:44.857
------------------------------
• [SLOW TEST] [6.052 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:54:38.808
    Jan 23 13:54:38.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename sysctl 01/23/24 13:54:38.809
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:54:38.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:54:38.823
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/23/24 13:54:38.826
    STEP: Watching for error events or started pod 01/23/24 13:54:38.839
    STEP: Waiting for pod completion 01/23/24 13:54:40.844
    Jan 23 13:54:40.844: INFO: Waiting up to 3m0s for pod "sysctl-64ecb0b2-330f-4bc9-ab46-2694078c3cf8" in namespace "sysctl-3911" to be "completed"
    Jan 23 13:54:40.845: INFO: Pod "sysctl-64ecb0b2-330f-4bc9-ab46-2694078c3cf8": Phase="Running", Reason="", readiness=true. Elapsed: 1.649227ms
    Jan 23 13:54:42.849: INFO: Pod "sysctl-64ecb0b2-330f-4bc9-ab46-2694078c3cf8": Phase="Running", Reason="", readiness=false. Elapsed: 2.004956124s
    Jan 23 13:54:44.849: INFO: Pod "sysctl-64ecb0b2-330f-4bc9-ab46-2694078c3cf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005130786s
    Jan 23 13:54:44.849: INFO: Pod "sysctl-64ecb0b2-330f-4bc9-ab46-2694078c3cf8" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/23/24 13:54:44.851
    STEP: Getting logs from the pod 01/23/24 13:54:44.851
    STEP: Checking that the sysctl is actually updated 01/23/24 13:54:44.855
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:54:44.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-3911" for this suite. 01/23/24 13:54:44.857
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:54:44.861
Jan 23 13:54:44.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename containers 01/23/24 13:54:44.862
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:54:44.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:54:44.871
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 01/23/24 13:54:44.873
Jan 23 13:54:44.885: INFO: Waiting up to 5m0s for pod "client-containers-a7c533ea-46d2-4b4f-8a31-c31a82839230" in namespace "containers-2492" to be "Succeeded or Failed"
Jan 23 13:54:44.887: INFO: Pod "client-containers-a7c533ea-46d2-4b4f-8a31-c31a82839230": Phase="Pending", Reason="", readiness=false. Elapsed: 2.229017ms
Jan 23 13:54:46.891: INFO: Pod "client-containers-a7c533ea-46d2-4b4f-8a31-c31a82839230": Phase="Running", Reason="", readiness=true. Elapsed: 2.006072273s
Jan 23 13:54:48.891: INFO: Pod "client-containers-a7c533ea-46d2-4b4f-8a31-c31a82839230": Phase="Running", Reason="", readiness=false. Elapsed: 4.00610824s
Jan 23 13:54:50.891: INFO: Pod "client-containers-a7c533ea-46d2-4b4f-8a31-c31a82839230": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006138765s
STEP: Saw pod success 01/23/24 13:54:50.891
Jan 23 13:54:50.891: INFO: Pod "client-containers-a7c533ea-46d2-4b4f-8a31-c31a82839230" satisfied condition "Succeeded or Failed"
Jan 23 13:54:50.893: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod client-containers-a7c533ea-46d2-4b4f-8a31-c31a82839230 container agnhost-container: <nil>
STEP: delete the pod 01/23/24 13:54:50.896
Jan 23 13:54:50.903: INFO: Waiting for pod client-containers-a7c533ea-46d2-4b4f-8a31-c31a82839230 to disappear
Jan 23 13:54:50.905: INFO: Pod client-containers-a7c533ea-46d2-4b4f-8a31-c31a82839230 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 23 13:54:50.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2492" for this suite. 01/23/24 13:54:50.907
------------------------------
• [SLOW TEST] [6.049 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:54:44.861
    Jan 23 13:54:44.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename containers 01/23/24 13:54:44.862
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:54:44.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:54:44.871
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 01/23/24 13:54:44.873
    Jan 23 13:54:44.885: INFO: Waiting up to 5m0s for pod "client-containers-a7c533ea-46d2-4b4f-8a31-c31a82839230" in namespace "containers-2492" to be "Succeeded or Failed"
    Jan 23 13:54:44.887: INFO: Pod "client-containers-a7c533ea-46d2-4b4f-8a31-c31a82839230": Phase="Pending", Reason="", readiness=false. Elapsed: 2.229017ms
    Jan 23 13:54:46.891: INFO: Pod "client-containers-a7c533ea-46d2-4b4f-8a31-c31a82839230": Phase="Running", Reason="", readiness=true. Elapsed: 2.006072273s
    Jan 23 13:54:48.891: INFO: Pod "client-containers-a7c533ea-46d2-4b4f-8a31-c31a82839230": Phase="Running", Reason="", readiness=false. Elapsed: 4.00610824s
    Jan 23 13:54:50.891: INFO: Pod "client-containers-a7c533ea-46d2-4b4f-8a31-c31a82839230": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006138765s
    STEP: Saw pod success 01/23/24 13:54:50.891
    Jan 23 13:54:50.891: INFO: Pod "client-containers-a7c533ea-46d2-4b4f-8a31-c31a82839230" satisfied condition "Succeeded or Failed"
    Jan 23 13:54:50.893: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod client-containers-a7c533ea-46d2-4b4f-8a31-c31a82839230 container agnhost-container: <nil>
    STEP: delete the pod 01/23/24 13:54:50.896
    Jan 23 13:54:50.903: INFO: Waiting for pod client-containers-a7c533ea-46d2-4b4f-8a31-c31a82839230 to disappear
    Jan 23 13:54:50.905: INFO: Pod client-containers-a7c533ea-46d2-4b4f-8a31-c31a82839230 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:54:50.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2492" for this suite. 01/23/24 13:54:50.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:54:50.91
Jan 23 13:54:50.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename emptydir 01/23/24 13:54:50.911
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:54:50.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:54:50.919
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 01/23/24 13:54:50.921
Jan 23 13:54:50.936: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-d409dcf9-f991-4422-81c6-ad1cbe60d702" in namespace "emptydir-4616" to be "running"
Jan 23 13:54:50.937: INFO: Pod "pod-sharedvolume-d409dcf9-f991-4422-81c6-ad1cbe60d702": Phase="Pending", Reason="", readiness=false. Elapsed: 1.409041ms
Jan 23 13:54:52.941: INFO: Pod "pod-sharedvolume-d409dcf9-f991-4422-81c6-ad1cbe60d702": Phase="Running", Reason="", readiness=true. Elapsed: 2.005199376s
Jan 23 13:54:52.941: INFO: Pod "pod-sharedvolume-d409dcf9-f991-4422-81c6-ad1cbe60d702" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/23/24 13:54:52.941
Jan 23 13:54:52.941: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4616 PodName:pod-sharedvolume-d409dcf9-f991-4422-81c6-ad1cbe60d702 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 13:54:52.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 13:54:52.942: INFO: ExecWithOptions: Clientset creation
Jan 23 13:54:52.942: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-4616/pods/pod-sharedvolume-d409dcf9-f991-4422-81c6-ad1cbe60d702/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 23 13:54:53.001: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 23 13:54:53.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4616" for this suite. 01/23/24 13:54:53.005
------------------------------
• [2.099 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:54:50.91
    Jan 23 13:54:50.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename emptydir 01/23/24 13:54:50.911
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:54:50.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:54:50.919
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 01/23/24 13:54:50.921
    Jan 23 13:54:50.936: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-d409dcf9-f991-4422-81c6-ad1cbe60d702" in namespace "emptydir-4616" to be "running"
    Jan 23 13:54:50.937: INFO: Pod "pod-sharedvolume-d409dcf9-f991-4422-81c6-ad1cbe60d702": Phase="Pending", Reason="", readiness=false. Elapsed: 1.409041ms
    Jan 23 13:54:52.941: INFO: Pod "pod-sharedvolume-d409dcf9-f991-4422-81c6-ad1cbe60d702": Phase="Running", Reason="", readiness=true. Elapsed: 2.005199376s
    Jan 23 13:54:52.941: INFO: Pod "pod-sharedvolume-d409dcf9-f991-4422-81c6-ad1cbe60d702" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/23/24 13:54:52.941
    Jan 23 13:54:52.941: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4616 PodName:pod-sharedvolume-d409dcf9-f991-4422-81c6-ad1cbe60d702 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 13:54:52.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 13:54:52.942: INFO: ExecWithOptions: Clientset creation
    Jan 23 13:54:52.942: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-4616/pods/pod-sharedvolume-d409dcf9-f991-4422-81c6-ad1cbe60d702/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan 23 13:54:53.001: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:54:53.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4616" for this suite. 01/23/24 13:54:53.005
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:54:53.009
Jan 23 13:54:53.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename resourcequota 01/23/24 13:54:53.009
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:54:53.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:54:53.021
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 01/23/24 13:54:53.023
STEP: Creating a ResourceQuota 01/23/24 13:54:58.027
STEP: Ensuring resource quota status is calculated 01/23/24 13:54:58.03
STEP: Creating a ReplicationController 01/23/24 13:55:00.033
STEP: Ensuring resource quota status captures replication controller creation 01/23/24 13:55:00.041
STEP: Deleting a ReplicationController 01/23/24 13:55:02.045
STEP: Ensuring resource quota status released usage 01/23/24 13:55:02.048
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 23 13:55:04.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6068" for this suite. 01/23/24 13:55:04.054
------------------------------
• [SLOW TEST] [11.049 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:54:53.009
    Jan 23 13:54:53.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename resourcequota 01/23/24 13:54:53.009
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:54:53.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:54:53.021
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 01/23/24 13:54:53.023
    STEP: Creating a ResourceQuota 01/23/24 13:54:58.027
    STEP: Ensuring resource quota status is calculated 01/23/24 13:54:58.03
    STEP: Creating a ReplicationController 01/23/24 13:55:00.033
    STEP: Ensuring resource quota status captures replication controller creation 01/23/24 13:55:00.041
    STEP: Deleting a ReplicationController 01/23/24 13:55:02.045
    STEP: Ensuring resource quota status released usage 01/23/24 13:55:02.048
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:55:04.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6068" for this suite. 01/23/24 13:55:04.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:55:04.058
Jan 23 13:55:04.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename configmap 01/23/24 13:55:04.059
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:55:04.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:55:04.069
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-0ad1a7ea-5535-4a61-a232-aa43706f0864 01/23/24 13:55:04.07
STEP: Creating a pod to test consume configMaps 01/23/24 13:55:04.074
Jan 23 13:55:04.085: INFO: Waiting up to 5m0s for pod "pod-configmaps-7caa70e2-4b44-4603-9e35-d16f2d24529a" in namespace "configmap-9745" to be "Succeeded or Failed"
Jan 23 13:55:04.087: INFO: Pod "pod-configmaps-7caa70e2-4b44-4603-9e35-d16f2d24529a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.716178ms
Jan 23 13:55:06.090: INFO: Pod "pod-configmaps-7caa70e2-4b44-4603-9e35-d16f2d24529a": Phase="Running", Reason="", readiness=true. Elapsed: 2.004700026s
Jan 23 13:55:08.091: INFO: Pod "pod-configmaps-7caa70e2-4b44-4603-9e35-d16f2d24529a": Phase="Running", Reason="", readiness=false. Elapsed: 4.005762827s
Jan 23 13:55:10.093: INFO: Pod "pod-configmaps-7caa70e2-4b44-4603-9e35-d16f2d24529a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007942301s
STEP: Saw pod success 01/23/24 13:55:10.093
Jan 23 13:55:10.093: INFO: Pod "pod-configmaps-7caa70e2-4b44-4603-9e35-d16f2d24529a" satisfied condition "Succeeded or Failed"
Jan 23 13:55:10.095: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-7caa70e2-4b44-4603-9e35-d16f2d24529a container agnhost-container: <nil>
STEP: delete the pod 01/23/24 13:55:10.098
Jan 23 13:55:10.105: INFO: Waiting for pod pod-configmaps-7caa70e2-4b44-4603-9e35-d16f2d24529a to disappear
Jan 23 13:55:10.106: INFO: Pod pod-configmaps-7caa70e2-4b44-4603-9e35-d16f2d24529a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 23 13:55:10.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9745" for this suite. 01/23/24 13:55:10.109
------------------------------
• [SLOW TEST] [6.054 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:55:04.058
    Jan 23 13:55:04.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename configmap 01/23/24 13:55:04.059
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:55:04.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:55:04.069
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-0ad1a7ea-5535-4a61-a232-aa43706f0864 01/23/24 13:55:04.07
    STEP: Creating a pod to test consume configMaps 01/23/24 13:55:04.074
    Jan 23 13:55:04.085: INFO: Waiting up to 5m0s for pod "pod-configmaps-7caa70e2-4b44-4603-9e35-d16f2d24529a" in namespace "configmap-9745" to be "Succeeded or Failed"
    Jan 23 13:55:04.087: INFO: Pod "pod-configmaps-7caa70e2-4b44-4603-9e35-d16f2d24529a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.716178ms
    Jan 23 13:55:06.090: INFO: Pod "pod-configmaps-7caa70e2-4b44-4603-9e35-d16f2d24529a": Phase="Running", Reason="", readiness=true. Elapsed: 2.004700026s
    Jan 23 13:55:08.091: INFO: Pod "pod-configmaps-7caa70e2-4b44-4603-9e35-d16f2d24529a": Phase="Running", Reason="", readiness=false. Elapsed: 4.005762827s
    Jan 23 13:55:10.093: INFO: Pod "pod-configmaps-7caa70e2-4b44-4603-9e35-d16f2d24529a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007942301s
    STEP: Saw pod success 01/23/24 13:55:10.093
    Jan 23 13:55:10.093: INFO: Pod "pod-configmaps-7caa70e2-4b44-4603-9e35-d16f2d24529a" satisfied condition "Succeeded or Failed"
    Jan 23 13:55:10.095: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-7caa70e2-4b44-4603-9e35-d16f2d24529a container agnhost-container: <nil>
    STEP: delete the pod 01/23/24 13:55:10.098
    Jan 23 13:55:10.105: INFO: Waiting for pod pod-configmaps-7caa70e2-4b44-4603-9e35-d16f2d24529a to disappear
    Jan 23 13:55:10.106: INFO: Pod pod-configmaps-7caa70e2-4b44-4603-9e35-d16f2d24529a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:55:10.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9745" for this suite. 01/23/24 13:55:10.109
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:55:10.112
Jan 23 13:55:10.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename endpointslice 01/23/24 13:55:10.113
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:55:10.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:55:10.124
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 01/23/24 13:55:10.126
STEP: getting /apis/discovery.k8s.io 01/23/24 13:55:10.127
STEP: getting /apis/discovery.k8s.iov1 01/23/24 13:55:10.128
STEP: creating 01/23/24 13:55:10.129
STEP: getting 01/23/24 13:55:10.137
STEP: listing 01/23/24 13:55:10.138
STEP: watching 01/23/24 13:55:10.14
Jan 23 13:55:10.140: INFO: starting watch
STEP: cluster-wide listing 01/23/24 13:55:10.14
STEP: cluster-wide watching 01/23/24 13:55:10.143
Jan 23 13:55:10.143: INFO: starting watch
STEP: patching 01/23/24 13:55:10.143
STEP: updating 01/23/24 13:55:10.147
Jan 23 13:55:10.151: INFO: waiting for watch events with expected annotations
Jan 23 13:55:10.151: INFO: saw patched and updated annotations
STEP: deleting 01/23/24 13:55:10.151
STEP: deleting a collection 01/23/24 13:55:10.157
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 23 13:55:10.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9239" for this suite. 01/23/24 13:55:10.167
------------------------------
• [0.060 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:55:10.112
    Jan 23 13:55:10.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename endpointslice 01/23/24 13:55:10.113
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:55:10.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:55:10.124
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 01/23/24 13:55:10.126
    STEP: getting /apis/discovery.k8s.io 01/23/24 13:55:10.127
    STEP: getting /apis/discovery.k8s.iov1 01/23/24 13:55:10.128
    STEP: creating 01/23/24 13:55:10.129
    STEP: getting 01/23/24 13:55:10.137
    STEP: listing 01/23/24 13:55:10.138
    STEP: watching 01/23/24 13:55:10.14
    Jan 23 13:55:10.140: INFO: starting watch
    STEP: cluster-wide listing 01/23/24 13:55:10.14
    STEP: cluster-wide watching 01/23/24 13:55:10.143
    Jan 23 13:55:10.143: INFO: starting watch
    STEP: patching 01/23/24 13:55:10.143
    STEP: updating 01/23/24 13:55:10.147
    Jan 23 13:55:10.151: INFO: waiting for watch events with expected annotations
    Jan 23 13:55:10.151: INFO: saw patched and updated annotations
    STEP: deleting 01/23/24 13:55:10.151
    STEP: deleting a collection 01/23/24 13:55:10.157
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:55:10.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9239" for this suite. 01/23/24 13:55:10.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:55:10.172
Jan 23 13:55:10.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename containers 01/23/24 13:55:10.173
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:55:10.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:55:10.181
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 01/23/24 13:55:10.183
Jan 23 13:55:10.195: INFO: Waiting up to 5m0s for pod "client-containers-44315058-5256-4aad-a308-404a63f235dd" in namespace "containers-6306" to be "Succeeded or Failed"
Jan 23 13:55:10.197: INFO: Pod "client-containers-44315058-5256-4aad-a308-404a63f235dd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.498924ms
Jan 23 13:55:12.199: INFO: Pod "client-containers-44315058-5256-4aad-a308-404a63f235dd": Phase="Running", Reason="", readiness=false. Elapsed: 2.004016178s
Jan 23 13:55:14.212: INFO: Pod "client-containers-44315058-5256-4aad-a308-404a63f235dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016736676s
STEP: Saw pod success 01/23/24 13:55:14.212
Jan 23 13:55:14.218: INFO: Pod "client-containers-44315058-5256-4aad-a308-404a63f235dd" satisfied condition "Succeeded or Failed"
Jan 23 13:55:14.243: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod client-containers-44315058-5256-4aad-a308-404a63f235dd container agnhost-container: <nil>
STEP: delete the pod 01/23/24 13:55:14.277
Jan 23 13:55:14.315: INFO: Waiting for pod client-containers-44315058-5256-4aad-a308-404a63f235dd to disappear
Jan 23 13:55:14.317: INFO: Pod client-containers-44315058-5256-4aad-a308-404a63f235dd no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 23 13:55:14.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6306" for this suite. 01/23/24 13:55:14.319
------------------------------
• [4.151 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:55:10.172
    Jan 23 13:55:10.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename containers 01/23/24 13:55:10.173
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:55:10.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:55:10.181
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 01/23/24 13:55:10.183
    Jan 23 13:55:10.195: INFO: Waiting up to 5m0s for pod "client-containers-44315058-5256-4aad-a308-404a63f235dd" in namespace "containers-6306" to be "Succeeded or Failed"
    Jan 23 13:55:10.197: INFO: Pod "client-containers-44315058-5256-4aad-a308-404a63f235dd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.498924ms
    Jan 23 13:55:12.199: INFO: Pod "client-containers-44315058-5256-4aad-a308-404a63f235dd": Phase="Running", Reason="", readiness=false. Elapsed: 2.004016178s
    Jan 23 13:55:14.212: INFO: Pod "client-containers-44315058-5256-4aad-a308-404a63f235dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016736676s
    STEP: Saw pod success 01/23/24 13:55:14.212
    Jan 23 13:55:14.218: INFO: Pod "client-containers-44315058-5256-4aad-a308-404a63f235dd" satisfied condition "Succeeded or Failed"
    Jan 23 13:55:14.243: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod client-containers-44315058-5256-4aad-a308-404a63f235dd container agnhost-container: <nil>
    STEP: delete the pod 01/23/24 13:55:14.277
    Jan 23 13:55:14.315: INFO: Waiting for pod client-containers-44315058-5256-4aad-a308-404a63f235dd to disappear
    Jan 23 13:55:14.317: INFO: Pod client-containers-44315058-5256-4aad-a308-404a63f235dd no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:55:14.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6306" for this suite. 01/23/24 13:55:14.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:55:14.324
Jan 23 13:55:14.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename configmap 01/23/24 13:55:14.325
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:55:14.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:55:14.335
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-64e671b6-9a1e-4215-86f4-2bac26c82dac 01/23/24 13:55:14.336
STEP: Creating a pod to test consume configMaps 01/23/24 13:55:14.34
Jan 23 13:55:14.352: INFO: Waiting up to 5m0s for pod "pod-configmaps-b970c64e-45f0-42d4-9aaa-d42752dd7a16" in namespace "configmap-6763" to be "Succeeded or Failed"
Jan 23 13:55:14.353: INFO: Pod "pod-configmaps-b970c64e-45f0-42d4-9aaa-d42752dd7a16": Phase="Pending", Reason="", readiness=false. Elapsed: 1.463514ms
Jan 23 13:55:16.356: INFO: Pod "pod-configmaps-b970c64e-45f0-42d4-9aaa-d42752dd7a16": Phase="Running", Reason="", readiness=false. Elapsed: 2.004451973s
Jan 23 13:55:18.358: INFO: Pod "pod-configmaps-b970c64e-45f0-42d4-9aaa-d42752dd7a16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005932342s
STEP: Saw pod success 01/23/24 13:55:18.358
Jan 23 13:55:18.358: INFO: Pod "pod-configmaps-b970c64e-45f0-42d4-9aaa-d42752dd7a16" satisfied condition "Succeeded or Failed"
Jan 23 13:55:18.360: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-b970c64e-45f0-42d4-9aaa-d42752dd7a16 container agnhost-container: <nil>
STEP: delete the pod 01/23/24 13:55:18.363
Jan 23 13:55:18.370: INFO: Waiting for pod pod-configmaps-b970c64e-45f0-42d4-9aaa-d42752dd7a16 to disappear
Jan 23 13:55:18.371: INFO: Pod pod-configmaps-b970c64e-45f0-42d4-9aaa-d42752dd7a16 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 23 13:55:18.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6763" for this suite. 01/23/24 13:55:18.374
------------------------------
• [4.057 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:55:14.324
    Jan 23 13:55:14.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename configmap 01/23/24 13:55:14.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:55:14.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:55:14.335
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-64e671b6-9a1e-4215-86f4-2bac26c82dac 01/23/24 13:55:14.336
    STEP: Creating a pod to test consume configMaps 01/23/24 13:55:14.34
    Jan 23 13:55:14.352: INFO: Waiting up to 5m0s for pod "pod-configmaps-b970c64e-45f0-42d4-9aaa-d42752dd7a16" in namespace "configmap-6763" to be "Succeeded or Failed"
    Jan 23 13:55:14.353: INFO: Pod "pod-configmaps-b970c64e-45f0-42d4-9aaa-d42752dd7a16": Phase="Pending", Reason="", readiness=false. Elapsed: 1.463514ms
    Jan 23 13:55:16.356: INFO: Pod "pod-configmaps-b970c64e-45f0-42d4-9aaa-d42752dd7a16": Phase="Running", Reason="", readiness=false. Elapsed: 2.004451973s
    Jan 23 13:55:18.358: INFO: Pod "pod-configmaps-b970c64e-45f0-42d4-9aaa-d42752dd7a16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005932342s
    STEP: Saw pod success 01/23/24 13:55:18.358
    Jan 23 13:55:18.358: INFO: Pod "pod-configmaps-b970c64e-45f0-42d4-9aaa-d42752dd7a16" satisfied condition "Succeeded or Failed"
    Jan 23 13:55:18.360: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-b970c64e-45f0-42d4-9aaa-d42752dd7a16 container agnhost-container: <nil>
    STEP: delete the pod 01/23/24 13:55:18.363
    Jan 23 13:55:18.370: INFO: Waiting for pod pod-configmaps-b970c64e-45f0-42d4-9aaa-d42752dd7a16 to disappear
    Jan 23 13:55:18.371: INFO: Pod pod-configmaps-b970c64e-45f0-42d4-9aaa-d42752dd7a16 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:55:18.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6763" for this suite. 01/23/24 13:55:18.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:55:18.383
Jan 23 13:55:18.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename runtimeclass 01/23/24 13:55:18.383
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:55:18.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:55:18.391
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 23 13:55:18.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5481" for this suite. 01/23/24 13:55:18.398
------------------------------
• [0.019 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:55:18.383
    Jan 23 13:55:18.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename runtimeclass 01/23/24 13:55:18.383
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:55:18.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:55:18.391
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:55:18.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5481" for this suite. 01/23/24 13:55:18.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:55:18.405
Jan 23 13:55:18.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename pod-network-test 01/23/24 13:55:18.406
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:55:18.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:55:18.416
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-5438 01/23/24 13:55:18.418
STEP: creating a selector 01/23/24 13:55:18.418
STEP: Creating the service pods in kubernetes 01/23/24 13:55:18.418
Jan 23 13:55:18.418: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 23 13:55:18.453: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5438" to be "running and ready"
Jan 23 13:55:18.454: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.355046ms
Jan 23 13:55:18.454: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 13:55:20.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004671315s
Jan 23 13:55:20.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:55:22.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.00458871s
Jan 23 13:55:22.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:55:24.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.00418917s
Jan 23 13:55:24.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:55:26.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012565603s
Jan 23 13:55:26.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:55:28.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004825616s
Jan 23 13:55:28.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:55:30.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.004088253s
Jan 23 13:55:30.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:55:32.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.004302717s
Jan 23 13:55:32.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:55:34.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.005249172s
Jan 23 13:55:34.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:55:36.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.004599315s
Jan 23 13:55:36.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:55:38.461: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.008583278s
Jan 23 13:55:38.461: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 13:55:40.456: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.003436333s
Jan 23 13:55:40.456: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 23 13:55:40.456: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 23 13:55:40.458: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5438" to be "running and ready"
Jan 23 13:55:40.459: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.431636ms
Jan 23 13:55:40.459: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 23 13:55:40.459: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/23/24 13:55:40.461
Jan 23 13:55:40.481: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5438" to be "running"
Jan 23 13:55:40.482: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.402571ms
Jan 23 13:55:42.485: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004252966s
Jan 23 13:55:42.485: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 23 13:55:42.486: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5438" to be "running"
Jan 23 13:55:42.488: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.312456ms
Jan 23 13:55:42.488: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 23 13:55:42.489: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 23 13:55:42.489: INFO: Going to poll 10.233.75.73 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 23 13:55:42.490: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.75.73 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5438 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 13:55:42.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 13:55:42.491: INFO: ExecWithOptions: Clientset creation
Jan 23 13:55:42.491: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5438/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.75.73+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 23 13:55:43.549: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 23 13:55:43.549: INFO: Going to poll 10.233.87.89 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 23 13:55:43.552: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.87.89 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5438 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 13:55:43.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 13:55:43.552: INFO: ExecWithOptions: Clientset creation
Jan 23 13:55:43.552: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5438/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.87.89+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 23 13:55:44.608: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 23 13:55:44.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-5438" for this suite. 01/23/24 13:55:44.611
------------------------------
• [SLOW TEST] [26.209 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:55:18.405
    Jan 23 13:55:18.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename pod-network-test 01/23/24 13:55:18.406
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:55:18.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:55:18.416
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-5438 01/23/24 13:55:18.418
    STEP: creating a selector 01/23/24 13:55:18.418
    STEP: Creating the service pods in kubernetes 01/23/24 13:55:18.418
    Jan 23 13:55:18.418: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 23 13:55:18.453: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5438" to be "running and ready"
    Jan 23 13:55:18.454: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.355046ms
    Jan 23 13:55:18.454: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 13:55:20.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004671315s
    Jan 23 13:55:20.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:55:22.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.00458871s
    Jan 23 13:55:22.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:55:24.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.00418917s
    Jan 23 13:55:24.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:55:26.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012565603s
    Jan 23 13:55:26.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:55:28.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004825616s
    Jan 23 13:55:28.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:55:30.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.004088253s
    Jan 23 13:55:30.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:55:32.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.004302717s
    Jan 23 13:55:32.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:55:34.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.005249172s
    Jan 23 13:55:34.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:55:36.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.004599315s
    Jan 23 13:55:36.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:55:38.461: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.008583278s
    Jan 23 13:55:38.461: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 13:55:40.456: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.003436333s
    Jan 23 13:55:40.456: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 23 13:55:40.456: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 23 13:55:40.458: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5438" to be "running and ready"
    Jan 23 13:55:40.459: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.431636ms
    Jan 23 13:55:40.459: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 23 13:55:40.459: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/23/24 13:55:40.461
    Jan 23 13:55:40.481: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5438" to be "running"
    Jan 23 13:55:40.482: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.402571ms
    Jan 23 13:55:42.485: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004252966s
    Jan 23 13:55:42.485: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 23 13:55:42.486: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5438" to be "running"
    Jan 23 13:55:42.488: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.312456ms
    Jan 23 13:55:42.488: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 23 13:55:42.489: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 23 13:55:42.489: INFO: Going to poll 10.233.75.73 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 23 13:55:42.490: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.75.73 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5438 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 13:55:42.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 13:55:42.491: INFO: ExecWithOptions: Clientset creation
    Jan 23 13:55:42.491: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5438/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.75.73+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 23 13:55:43.549: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 23 13:55:43.549: INFO: Going to poll 10.233.87.89 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 23 13:55:43.552: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.87.89 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5438 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 13:55:43.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 13:55:43.552: INFO: ExecWithOptions: Clientset creation
    Jan 23 13:55:43.552: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5438/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.87.89+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 23 13:55:44.608: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:55:44.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-5438" for this suite. 01/23/24 13:55:44.611
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:55:44.614
Jan 23 13:55:44.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename emptydir 01/23/24 13:55:44.615
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:55:44.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:55:44.625
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 01/23/24 13:55:44.626
Jan 23 13:55:44.641: INFO: Waiting up to 5m0s for pod "pod-bc9ef05d-01b1-450c-9035-eacbf58f5782" in namespace "emptydir-228" to be "Succeeded or Failed"
Jan 23 13:55:44.643: INFO: Pod "pod-bc9ef05d-01b1-450c-9035-eacbf58f5782": Phase="Pending", Reason="", readiness=false. Elapsed: 1.423165ms
Jan 23 13:55:46.646: INFO: Pod "pod-bc9ef05d-01b1-450c-9035-eacbf58f5782": Phase="Running", Reason="", readiness=true. Elapsed: 2.005002451s
Jan 23 13:55:48.646: INFO: Pod "pod-bc9ef05d-01b1-450c-9035-eacbf58f5782": Phase="Running", Reason="", readiness=false. Elapsed: 4.004978969s
Jan 23 13:55:50.645: INFO: Pod "pod-bc9ef05d-01b1-450c-9035-eacbf58f5782": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004039455s
STEP: Saw pod success 01/23/24 13:55:50.645
Jan 23 13:55:50.646: INFO: Pod "pod-bc9ef05d-01b1-450c-9035-eacbf58f5782" satisfied condition "Succeeded or Failed"
Jan 23 13:55:50.647: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-bc9ef05d-01b1-450c-9035-eacbf58f5782 container test-container: <nil>
STEP: delete the pod 01/23/24 13:55:50.651
Jan 23 13:55:50.659: INFO: Waiting for pod pod-bc9ef05d-01b1-450c-9035-eacbf58f5782 to disappear
Jan 23 13:55:50.661: INFO: Pod pod-bc9ef05d-01b1-450c-9035-eacbf58f5782 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 23 13:55:50.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-228" for this suite. 01/23/24 13:55:50.663
------------------------------
• [SLOW TEST] [6.052 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:55:44.614
    Jan 23 13:55:44.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename emptydir 01/23/24 13:55:44.615
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:55:44.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:55:44.625
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/23/24 13:55:44.626
    Jan 23 13:55:44.641: INFO: Waiting up to 5m0s for pod "pod-bc9ef05d-01b1-450c-9035-eacbf58f5782" in namespace "emptydir-228" to be "Succeeded or Failed"
    Jan 23 13:55:44.643: INFO: Pod "pod-bc9ef05d-01b1-450c-9035-eacbf58f5782": Phase="Pending", Reason="", readiness=false. Elapsed: 1.423165ms
    Jan 23 13:55:46.646: INFO: Pod "pod-bc9ef05d-01b1-450c-9035-eacbf58f5782": Phase="Running", Reason="", readiness=true. Elapsed: 2.005002451s
    Jan 23 13:55:48.646: INFO: Pod "pod-bc9ef05d-01b1-450c-9035-eacbf58f5782": Phase="Running", Reason="", readiness=false. Elapsed: 4.004978969s
    Jan 23 13:55:50.645: INFO: Pod "pod-bc9ef05d-01b1-450c-9035-eacbf58f5782": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004039455s
    STEP: Saw pod success 01/23/24 13:55:50.645
    Jan 23 13:55:50.646: INFO: Pod "pod-bc9ef05d-01b1-450c-9035-eacbf58f5782" satisfied condition "Succeeded or Failed"
    Jan 23 13:55:50.647: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-bc9ef05d-01b1-450c-9035-eacbf58f5782 container test-container: <nil>
    STEP: delete the pod 01/23/24 13:55:50.651
    Jan 23 13:55:50.659: INFO: Waiting for pod pod-bc9ef05d-01b1-450c-9035-eacbf58f5782 to disappear
    Jan 23 13:55:50.661: INFO: Pod pod-bc9ef05d-01b1-450c-9035-eacbf58f5782 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:55:50.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-228" for this suite. 01/23/24 13:55:50.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:55:50.667
Jan 23 13:55:50.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename resourcequota 01/23/24 13:55:50.667
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:55:50.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:55:50.677
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 01/23/24 13:55:50.679
STEP: Counting existing ResourceQuota 01/23/24 13:55:56.681
STEP: Creating a ResourceQuota 01/23/24 13:56:01.683
STEP: Ensuring resource quota status is calculated 01/23/24 13:56:01.688
STEP: Creating a Secret 01/23/24 13:56:03.691
STEP: Ensuring resource quota status captures secret creation 01/23/24 13:56:03.701
STEP: Deleting a secret 01/23/24 13:56:05.704
STEP: Ensuring resource quota status released usage 01/23/24 13:56:05.707
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 23 13:56:07.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8471" for this suite. 01/23/24 13:56:07.713
------------------------------
• [SLOW TEST] [17.051 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:55:50.667
    Jan 23 13:55:50.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename resourcequota 01/23/24 13:55:50.667
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:55:50.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:55:50.677
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 01/23/24 13:55:50.679
    STEP: Counting existing ResourceQuota 01/23/24 13:55:56.681
    STEP: Creating a ResourceQuota 01/23/24 13:56:01.683
    STEP: Ensuring resource quota status is calculated 01/23/24 13:56:01.688
    STEP: Creating a Secret 01/23/24 13:56:03.691
    STEP: Ensuring resource quota status captures secret creation 01/23/24 13:56:03.701
    STEP: Deleting a secret 01/23/24 13:56:05.704
    STEP: Ensuring resource quota status released usage 01/23/24 13:56:05.707
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:56:07.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8471" for this suite. 01/23/24 13:56:07.713
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:56:07.718
Jan 23 13:56:07.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename replicaset 01/23/24 13:56:07.719
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:07.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:07.728
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/23/24 13:56:07.73
STEP: Verify that the required pods have come up 01/23/24 13:56:07.734
Jan 23 13:56:07.736: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 23 13:56:12.739: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/23/24 13:56:12.739
Jan 23 13:56:12.741: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/23/24 13:56:12.741
STEP: DeleteCollection of the ReplicaSets 01/23/24 13:56:12.744
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/23/24 13:56:12.748
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 23 13:56:12.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-522" for this suite. 01/23/24 13:56:12.763
------------------------------
• [SLOW TEST] [5.051 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:56:07.718
    Jan 23 13:56:07.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename replicaset 01/23/24 13:56:07.719
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:07.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:07.728
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/23/24 13:56:07.73
    STEP: Verify that the required pods have come up 01/23/24 13:56:07.734
    Jan 23 13:56:07.736: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan 23 13:56:12.739: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/23/24 13:56:12.739
    Jan 23 13:56:12.741: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/23/24 13:56:12.741
    STEP: DeleteCollection of the ReplicaSets 01/23/24 13:56:12.744
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/23/24 13:56:12.748
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:56:12.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-522" for this suite. 01/23/24 13:56:12.763
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:56:12.769
Jan 23 13:56:12.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename svcaccounts 01/23/24 13:56:12.77
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:12.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:12.795
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Jan 23 13:56:12.817: INFO: created pod pod-service-account-defaultsa
Jan 23 13:56:12.817: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 23 13:56:12.828: INFO: created pod pod-service-account-mountsa
Jan 23 13:56:12.828: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 23 13:56:12.835: INFO: created pod pod-service-account-nomountsa
Jan 23 13:56:12.835: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 23 13:56:12.845: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 23 13:56:12.845: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 23 13:56:12.856: INFO: created pod pod-service-account-mountsa-mountspec
Jan 23 13:56:12.856: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 23 13:56:12.871: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 23 13:56:12.871: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 23 13:56:12.883: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 23 13:56:12.883: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 23 13:56:12.896: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 23 13:56:12.896: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 23 13:56:12.903: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 23 13:56:12.903: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 23 13:56:12.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1167" for this suite. 01/23/24 13:56:12.962
------------------------------
• [0.206 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:56:12.769
    Jan 23 13:56:12.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename svcaccounts 01/23/24 13:56:12.77
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:12.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:12.795
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Jan 23 13:56:12.817: INFO: created pod pod-service-account-defaultsa
    Jan 23 13:56:12.817: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan 23 13:56:12.828: INFO: created pod pod-service-account-mountsa
    Jan 23 13:56:12.828: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan 23 13:56:12.835: INFO: created pod pod-service-account-nomountsa
    Jan 23 13:56:12.835: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan 23 13:56:12.845: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan 23 13:56:12.845: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan 23 13:56:12.856: INFO: created pod pod-service-account-mountsa-mountspec
    Jan 23 13:56:12.856: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan 23 13:56:12.871: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan 23 13:56:12.871: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan 23 13:56:12.883: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan 23 13:56:12.883: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan 23 13:56:12.896: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan 23 13:56:12.896: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan 23 13:56:12.903: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan 23 13:56:12.903: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:56:12.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1167" for this suite. 01/23/24 13:56:12.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:56:12.976
Jan 23 13:56:12.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename services 01/23/24 13:56:12.977
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:12.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:12.986
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 23 13:56:12.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2842" for this suite. 01/23/24 13:56:13.001
------------------------------
• [0.029 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:56:12.976
    Jan 23 13:56:12.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename services 01/23/24 13:56:12.977
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:12.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:12.986
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:56:12.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2842" for this suite. 01/23/24 13:56:13.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:56:13.007
Jan 23 13:56:13.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename security-context-test 01/23/24 13:56:13.008
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:13.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:13.018
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Jan 23 13:56:13.030: INFO: Waiting up to 5m0s for pod "busybox-user-65534-2fea1520-9e1f-4470-8d13-0b2cd67dbcd6" in namespace "security-context-test-5344" to be "Succeeded or Failed"
Jan 23 13:56:13.031: INFO: Pod "busybox-user-65534-2fea1520-9e1f-4470-8d13-0b2cd67dbcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.415876ms
Jan 23 13:56:15.034: INFO: Pod "busybox-user-65534-2fea1520-9e1f-4470-8d13-0b2cd67dbcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003889408s
Jan 23 13:56:17.035: INFO: Pod "busybox-user-65534-2fea1520-9e1f-4470-8d13-0b2cd67dbcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005263971s
Jan 23 13:56:19.034: INFO: Pod "busybox-user-65534-2fea1520-9e1f-4470-8d13-0b2cd67dbcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004480298s
Jan 23 13:56:21.034: INFO: Pod "busybox-user-65534-2fea1520-9e1f-4470-8d13-0b2cd67dbcd6": Phase="Running", Reason="", readiness=true. Elapsed: 8.004143224s
Jan 23 13:56:23.035: INFO: Pod "busybox-user-65534-2fea1520-9e1f-4470-8d13-0b2cd67dbcd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.005101417s
Jan 23 13:56:23.035: INFO: Pod "busybox-user-65534-2fea1520-9e1f-4470-8d13-0b2cd67dbcd6" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 23 13:56:23.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-5344" for this suite. 01/23/24 13:56:23.037
------------------------------
• [SLOW TEST] [10.033 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:56:13.007
    Jan 23 13:56:13.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename security-context-test 01/23/24 13:56:13.008
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:13.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:13.018
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Jan 23 13:56:13.030: INFO: Waiting up to 5m0s for pod "busybox-user-65534-2fea1520-9e1f-4470-8d13-0b2cd67dbcd6" in namespace "security-context-test-5344" to be "Succeeded or Failed"
    Jan 23 13:56:13.031: INFO: Pod "busybox-user-65534-2fea1520-9e1f-4470-8d13-0b2cd67dbcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.415876ms
    Jan 23 13:56:15.034: INFO: Pod "busybox-user-65534-2fea1520-9e1f-4470-8d13-0b2cd67dbcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003889408s
    Jan 23 13:56:17.035: INFO: Pod "busybox-user-65534-2fea1520-9e1f-4470-8d13-0b2cd67dbcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005263971s
    Jan 23 13:56:19.034: INFO: Pod "busybox-user-65534-2fea1520-9e1f-4470-8d13-0b2cd67dbcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004480298s
    Jan 23 13:56:21.034: INFO: Pod "busybox-user-65534-2fea1520-9e1f-4470-8d13-0b2cd67dbcd6": Phase="Running", Reason="", readiness=true. Elapsed: 8.004143224s
    Jan 23 13:56:23.035: INFO: Pod "busybox-user-65534-2fea1520-9e1f-4470-8d13-0b2cd67dbcd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.005101417s
    Jan 23 13:56:23.035: INFO: Pod "busybox-user-65534-2fea1520-9e1f-4470-8d13-0b2cd67dbcd6" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:56:23.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-5344" for this suite. 01/23/24 13:56:23.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:56:23.041
Jan 23 13:56:23.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename pods 01/23/24 13:56:23.042
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:23.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:23.051
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 01/23/24 13:56:23.059
STEP: watching for Pod to be ready 01/23/24 13:56:23.071
Jan 23 13:56:23.072: INFO: observed Pod pod-test in namespace pods-3783 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 23 13:56:23.074: INFO: observed Pod pod-test in namespace pods-3783 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:23 +0000 UTC  }]
Jan 23 13:56:24.474: INFO: observed Pod pod-test in namespace pods-3783 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:23 +0000 UTC  }]
Jan 23 13:56:26.811: INFO: observed Pod pod-test in namespace pods-3783 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:23 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:23 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:23 +0000 UTC  }]
Jan 23 13:56:27.210: INFO: Found Pod pod-test in namespace pods-3783 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:23 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:23 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/23/24 13:56:27.212
STEP: getting the Pod and ensuring that it's patched 01/23/24 13:56:27.219
STEP: replacing the Pod's status Ready condition to False 01/23/24 13:56:27.221
STEP: check the Pod again to ensure its Ready conditions are False 01/23/24 13:56:27.228
STEP: deleting the Pod via a Collection with a LabelSelector 01/23/24 13:56:27.228
STEP: watching for the Pod to be deleted 01/23/24 13:56:27.234
Jan 23 13:56:27.237: INFO: observed event type MODIFIED
Jan 23 13:56:29.398: INFO: observed event type MODIFIED
Jan 23 13:56:29.706: INFO: observed event type MODIFIED
Jan 23 13:56:30.385: INFO: observed event type MODIFIED
Jan 23 13:56:30.388: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 23 13:56:30.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3783" for this suite. 01/23/24 13:56:30.395
------------------------------
• [SLOW TEST] [7.357 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:56:23.041
    Jan 23 13:56:23.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename pods 01/23/24 13:56:23.042
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:23.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:23.051
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 01/23/24 13:56:23.059
    STEP: watching for Pod to be ready 01/23/24 13:56:23.071
    Jan 23 13:56:23.072: INFO: observed Pod pod-test in namespace pods-3783 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan 23 13:56:23.074: INFO: observed Pod pod-test in namespace pods-3783 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:23 +0000 UTC  }]
    Jan 23 13:56:24.474: INFO: observed Pod pod-test in namespace pods-3783 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:23 +0000 UTC  }]
    Jan 23 13:56:26.811: INFO: observed Pod pod-test in namespace pods-3783 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:23 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:23 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:23 +0000 UTC  }]
    Jan 23 13:56:27.210: INFO: Found Pod pod-test in namespace pods-3783 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:23 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 13:56:23 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/23/24 13:56:27.212
    STEP: getting the Pod and ensuring that it's patched 01/23/24 13:56:27.219
    STEP: replacing the Pod's status Ready condition to False 01/23/24 13:56:27.221
    STEP: check the Pod again to ensure its Ready conditions are False 01/23/24 13:56:27.228
    STEP: deleting the Pod via a Collection with a LabelSelector 01/23/24 13:56:27.228
    STEP: watching for the Pod to be deleted 01/23/24 13:56:27.234
    Jan 23 13:56:27.237: INFO: observed event type MODIFIED
    Jan 23 13:56:29.398: INFO: observed event type MODIFIED
    Jan 23 13:56:29.706: INFO: observed event type MODIFIED
    Jan 23 13:56:30.385: INFO: observed event type MODIFIED
    Jan 23 13:56:30.388: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:56:30.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3783" for this suite. 01/23/24 13:56:30.395
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:56:30.398
Jan 23 13:56:30.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename namespaces 01/23/24 13:56:30.399
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:30.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:30.409
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-5684" 01/23/24 13:56:30.411
Jan 23 13:56:30.415: INFO: Namespace "namespaces-5684" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"13937306-003f-4b12-8d30-d166651e4e5b", "kubernetes.io/metadata.name":"namespaces-5684", "namespaces-5684":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:56:30.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5684" for this suite. 01/23/24 13:56:30.418
------------------------------
• [0.024 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:56:30.398
    Jan 23 13:56:30.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename namespaces 01/23/24 13:56:30.399
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:30.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:30.409
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-5684" 01/23/24 13:56:30.411
    Jan 23 13:56:30.415: INFO: Namespace "namespaces-5684" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"13937306-003f-4b12-8d30-d166651e4e5b", "kubernetes.io/metadata.name":"namespaces-5684", "namespaces-5684":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:56:30.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5684" for this suite. 01/23/24 13:56:30.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:56:30.423
Jan 23 13:56:30.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename server-version 01/23/24 13:56:30.424
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:30.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:30.432
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/23/24 13:56:30.434
STEP: Confirm major version 01/23/24 13:56:30.435
Jan 23 13:56:30.435: INFO: Major version: 1
STEP: Confirm minor version 01/23/24 13:56:30.435
Jan 23 13:56:30.435: INFO: cleanMinorVersion: 26
Jan 23 13:56:30.435: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Jan 23 13:56:30.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-9330" for this suite. 01/23/24 13:56:30.438
------------------------------
• [0.017 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:56:30.423
    Jan 23 13:56:30.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename server-version 01/23/24 13:56:30.424
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:30.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:30.432
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/23/24 13:56:30.434
    STEP: Confirm major version 01/23/24 13:56:30.435
    Jan 23 13:56:30.435: INFO: Major version: 1
    STEP: Confirm minor version 01/23/24 13:56:30.435
    Jan 23 13:56:30.435: INFO: cleanMinorVersion: 26
    Jan 23 13:56:30.435: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:56:30.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-9330" for this suite. 01/23/24 13:56:30.438
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:56:30.441
Jan 23 13:56:30.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename downward-api 01/23/24 13:56:30.442
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:30.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:30.45
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 01/23/24 13:56:30.452
Jan 23 13:56:30.465: INFO: Waiting up to 5m0s for pod "downwardapi-volume-116da647-d13d-4ae5-9055-da64352d0d97" in namespace "downward-api-7822" to be "Succeeded or Failed"
Jan 23 13:56:30.467: INFO: Pod "downwardapi-volume-116da647-d13d-4ae5-9055-da64352d0d97": Phase="Pending", Reason="", readiness=false. Elapsed: 1.618255ms
Jan 23 13:56:32.470: INFO: Pod "downwardapi-volume-116da647-d13d-4ae5-9055-da64352d0d97": Phase="Running", Reason="", readiness=false. Elapsed: 2.004474911s
Jan 23 13:56:34.471: INFO: Pod "downwardapi-volume-116da647-d13d-4ae5-9055-da64352d0d97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005519234s
STEP: Saw pod success 01/23/24 13:56:34.471
Jan 23 13:56:34.471: INFO: Pod "downwardapi-volume-116da647-d13d-4ae5-9055-da64352d0d97" satisfied condition "Succeeded or Failed"
Jan 23 13:56:34.473: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-116da647-d13d-4ae5-9055-da64352d0d97 container client-container: <nil>
STEP: delete the pod 01/23/24 13:56:34.478
Jan 23 13:56:34.487: INFO: Waiting for pod downwardapi-volume-116da647-d13d-4ae5-9055-da64352d0d97 to disappear
Jan 23 13:56:34.489: INFO: Pod downwardapi-volume-116da647-d13d-4ae5-9055-da64352d0d97 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 23 13:56:34.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7822" for this suite. 01/23/24 13:56:34.491
------------------------------
• [4.052 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:56:30.441
    Jan 23 13:56:30.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename downward-api 01/23/24 13:56:30.442
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:30.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:30.45
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 01/23/24 13:56:30.452
    Jan 23 13:56:30.465: INFO: Waiting up to 5m0s for pod "downwardapi-volume-116da647-d13d-4ae5-9055-da64352d0d97" in namespace "downward-api-7822" to be "Succeeded or Failed"
    Jan 23 13:56:30.467: INFO: Pod "downwardapi-volume-116da647-d13d-4ae5-9055-da64352d0d97": Phase="Pending", Reason="", readiness=false. Elapsed: 1.618255ms
    Jan 23 13:56:32.470: INFO: Pod "downwardapi-volume-116da647-d13d-4ae5-9055-da64352d0d97": Phase="Running", Reason="", readiness=false. Elapsed: 2.004474911s
    Jan 23 13:56:34.471: INFO: Pod "downwardapi-volume-116da647-d13d-4ae5-9055-da64352d0d97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005519234s
    STEP: Saw pod success 01/23/24 13:56:34.471
    Jan 23 13:56:34.471: INFO: Pod "downwardapi-volume-116da647-d13d-4ae5-9055-da64352d0d97" satisfied condition "Succeeded or Failed"
    Jan 23 13:56:34.473: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-116da647-d13d-4ae5-9055-da64352d0d97 container client-container: <nil>
    STEP: delete the pod 01/23/24 13:56:34.478
    Jan 23 13:56:34.487: INFO: Waiting for pod downwardapi-volume-116da647-d13d-4ae5-9055-da64352d0d97 to disappear
    Jan 23 13:56:34.489: INFO: Pod downwardapi-volume-116da647-d13d-4ae5-9055-da64352d0d97 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:56:34.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7822" for this suite. 01/23/24 13:56:34.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:56:34.493
Jan 23 13:56:34.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename ingress 01/23/24 13:56:34.495
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:34.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:34.511
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/23/24 13:56:34.512
STEP: getting /apis/networking.k8s.io 01/23/24 13:56:34.514
STEP: getting /apis/networking.k8s.iov1 01/23/24 13:56:34.515
STEP: creating 01/23/24 13:56:34.516
STEP: getting 01/23/24 13:56:34.713
STEP: listing 01/23/24 13:56:34.715
STEP: watching 01/23/24 13:56:34.716
Jan 23 13:56:34.716: INFO: starting watch
STEP: cluster-wide listing 01/23/24 13:56:34.717
STEP: cluster-wide watching 01/23/24 13:56:34.719
Jan 23 13:56:34.719: INFO: starting watch
STEP: patching 01/23/24 13:56:34.719
STEP: updating 01/23/24 13:56:34.763
Jan 23 13:56:34.804: INFO: waiting for watch events with expected annotations
Jan 23 13:56:34.804: INFO: saw patched and updated annotations
STEP: patching /status 01/23/24 13:56:34.804
STEP: updating /status 01/23/24 13:56:34.808
STEP: get /status 01/23/24 13:56:34.812
STEP: deleting 01/23/24 13:56:34.814
STEP: deleting a collection 01/23/24 13:56:34.82
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Jan 23 13:56:34.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-750" for this suite. 01/23/24 13:56:34.827
------------------------------
• [0.338 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:56:34.493
    Jan 23 13:56:34.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename ingress 01/23/24 13:56:34.495
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:34.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:34.511
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/23/24 13:56:34.512
    STEP: getting /apis/networking.k8s.io 01/23/24 13:56:34.514
    STEP: getting /apis/networking.k8s.iov1 01/23/24 13:56:34.515
    STEP: creating 01/23/24 13:56:34.516
    STEP: getting 01/23/24 13:56:34.713
    STEP: listing 01/23/24 13:56:34.715
    STEP: watching 01/23/24 13:56:34.716
    Jan 23 13:56:34.716: INFO: starting watch
    STEP: cluster-wide listing 01/23/24 13:56:34.717
    STEP: cluster-wide watching 01/23/24 13:56:34.719
    Jan 23 13:56:34.719: INFO: starting watch
    STEP: patching 01/23/24 13:56:34.719
    STEP: updating 01/23/24 13:56:34.763
    Jan 23 13:56:34.804: INFO: waiting for watch events with expected annotations
    Jan 23 13:56:34.804: INFO: saw patched and updated annotations
    STEP: patching /status 01/23/24 13:56:34.804
    STEP: updating /status 01/23/24 13:56:34.808
    STEP: get /status 01/23/24 13:56:34.812
    STEP: deleting 01/23/24 13:56:34.814
    STEP: deleting a collection 01/23/24 13:56:34.82
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:56:34.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-750" for this suite. 01/23/24 13:56:34.827
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:56:34.831
Jan 23 13:56:34.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename downward-api 01/23/24 13:56:34.832
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:34.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:34.843
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 01/23/24 13:56:34.844
Jan 23 13:56:34.859: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e83f596-6860-4ea7-852b-34a7f88fc430" in namespace "downward-api-5018" to be "Succeeded or Failed"
Jan 23 13:56:34.861: INFO: Pod "downwardapi-volume-1e83f596-6860-4ea7-852b-34a7f88fc430": Phase="Pending", Reason="", readiness=false. Elapsed: 1.293621ms
Jan 23 13:56:36.864: INFO: Pod "downwardapi-volume-1e83f596-6860-4ea7-852b-34a7f88fc430": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004213284s
Jan 23 13:56:38.867: INFO: Pod "downwardapi-volume-1e83f596-6860-4ea7-852b-34a7f88fc430": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007513784s
STEP: Saw pod success 01/23/24 13:56:38.867
Jan 23 13:56:38.867: INFO: Pod "downwardapi-volume-1e83f596-6860-4ea7-852b-34a7f88fc430" satisfied condition "Succeeded or Failed"
Jan 23 13:56:38.869: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-1e83f596-6860-4ea7-852b-34a7f88fc430 container client-container: <nil>
STEP: delete the pod 01/23/24 13:56:38.875
Jan 23 13:56:38.888: INFO: Waiting for pod downwardapi-volume-1e83f596-6860-4ea7-852b-34a7f88fc430 to disappear
Jan 23 13:56:38.890: INFO: Pod downwardapi-volume-1e83f596-6860-4ea7-852b-34a7f88fc430 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 23 13:56:38.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5018" for this suite. 01/23/24 13:56:38.892
------------------------------
• [4.066 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:56:34.831
    Jan 23 13:56:34.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename downward-api 01/23/24 13:56:34.832
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:34.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:34.843
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 01/23/24 13:56:34.844
    Jan 23 13:56:34.859: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e83f596-6860-4ea7-852b-34a7f88fc430" in namespace "downward-api-5018" to be "Succeeded or Failed"
    Jan 23 13:56:34.861: INFO: Pod "downwardapi-volume-1e83f596-6860-4ea7-852b-34a7f88fc430": Phase="Pending", Reason="", readiness=false. Elapsed: 1.293621ms
    Jan 23 13:56:36.864: INFO: Pod "downwardapi-volume-1e83f596-6860-4ea7-852b-34a7f88fc430": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004213284s
    Jan 23 13:56:38.867: INFO: Pod "downwardapi-volume-1e83f596-6860-4ea7-852b-34a7f88fc430": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007513784s
    STEP: Saw pod success 01/23/24 13:56:38.867
    Jan 23 13:56:38.867: INFO: Pod "downwardapi-volume-1e83f596-6860-4ea7-852b-34a7f88fc430" satisfied condition "Succeeded or Failed"
    Jan 23 13:56:38.869: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-1e83f596-6860-4ea7-852b-34a7f88fc430 container client-container: <nil>
    STEP: delete the pod 01/23/24 13:56:38.875
    Jan 23 13:56:38.888: INFO: Waiting for pod downwardapi-volume-1e83f596-6860-4ea7-852b-34a7f88fc430 to disappear
    Jan 23 13:56:38.890: INFO: Pod downwardapi-volume-1e83f596-6860-4ea7-852b-34a7f88fc430 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:56:38.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5018" for this suite. 01/23/24 13:56:38.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:56:38.899
Jan 23 13:56:38.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename controllerrevisions 01/23/24 13:56:38.899
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:38.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:38.91
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-t4jz9-daemon-set" 01/23/24 13:56:38.927
STEP: Check that daemon pods launch on every node of the cluster. 01/23/24 13:56:38.934
Jan 23 13:56:38.939: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 13:56:38.945: INFO: Number of nodes with available pods controlled by daemonset e2e-t4jz9-daemon-set: 0
Jan 23 13:56:38.945: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 13:56:39.947: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 13:56:39.949: INFO: Number of nodes with available pods controlled by daemonset e2e-t4jz9-daemon-set: 0
Jan 23 13:56:39.949: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 13:56:40.948: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 13:56:40.950: INFO: Number of nodes with available pods controlled by daemonset e2e-t4jz9-daemon-set: 2
Jan 23 13:56:40.950: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-t4jz9-daemon-set
STEP: Confirm DaemonSet "e2e-t4jz9-daemon-set" successfully created with "daemonset-name=e2e-t4jz9-daemon-set" label 01/23/24 13:56:40.951
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-t4jz9-daemon-set" 01/23/24 13:56:40.955
Jan 23 13:56:40.957: INFO: Located ControllerRevision: "e2e-t4jz9-daemon-set-74b5cf44"
STEP: Patching ControllerRevision "e2e-t4jz9-daemon-set-74b5cf44" 01/23/24 13:56:40.958
Jan 23 13:56:40.962: INFO: e2e-t4jz9-daemon-set-74b5cf44 has been patched
STEP: Create a new ControllerRevision 01/23/24 13:56:40.962
Jan 23 13:56:40.965: INFO: Created ControllerRevision: e2e-t4jz9-daemon-set-7d44bd6cf8
STEP: Confirm that there are two ControllerRevisions 01/23/24 13:56:40.965
Jan 23 13:56:40.965: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 23 13:56:40.967: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-t4jz9-daemon-set-74b5cf44" 01/23/24 13:56:40.967
STEP: Confirm that there is only one ControllerRevision 01/23/24 13:56:40.969
Jan 23 13:56:40.969: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 23 13:56:40.970: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-t4jz9-daemon-set-7d44bd6cf8" 01/23/24 13:56:40.973
Jan 23 13:56:40.977: INFO: e2e-t4jz9-daemon-set-7d44bd6cf8 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/23/24 13:56:40.977
W0123 13:56:40.985887      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/23/24 13:56:40.985
Jan 23 13:56:40.986: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 23 13:56:41.988: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 23 13:56:41.991: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-t4jz9-daemon-set-7d44bd6cf8=updated" 01/23/24 13:56:41.991
STEP: Confirm that there is only one ControllerRevision 01/23/24 13:56:41.995
Jan 23 13:56:41.995: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 23 13:56:41.996: INFO: Found 1 ControllerRevisions
Jan 23 13:56:41.997: INFO: ControllerRevision "e2e-t4jz9-daemon-set-f94f66567" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-t4jz9-daemon-set" 01/23/24 13:56:41.999
STEP: deleting DaemonSet.extensions e2e-t4jz9-daemon-set in namespace controllerrevisions-590, will wait for the garbage collector to delete the pods 01/23/24 13:56:41.999
Jan 23 13:56:42.056: INFO: Deleting DaemonSet.extensions e2e-t4jz9-daemon-set took: 4.89747ms
Jan 23 13:56:42.157: INFO: Terminating DaemonSet.extensions e2e-t4jz9-daemon-set pods took: 100.556116ms
Jan 23 13:56:44.860: INFO: Number of nodes with available pods controlled by daemonset e2e-t4jz9-daemon-set: 0
Jan 23 13:56:44.860: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-t4jz9-daemon-set
Jan 23 13:56:44.862: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"122729"},"items":null}

Jan 23 13:56:44.863: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"122729"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:56:44.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-590" for this suite. 01/23/24 13:56:44.87
------------------------------
• [SLOW TEST] [5.974 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:56:38.899
    Jan 23 13:56:38.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename controllerrevisions 01/23/24 13:56:38.899
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:38.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:38.91
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-t4jz9-daemon-set" 01/23/24 13:56:38.927
    STEP: Check that daemon pods launch on every node of the cluster. 01/23/24 13:56:38.934
    Jan 23 13:56:38.939: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 13:56:38.945: INFO: Number of nodes with available pods controlled by daemonset e2e-t4jz9-daemon-set: 0
    Jan 23 13:56:38.945: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 13:56:39.947: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 13:56:39.949: INFO: Number of nodes with available pods controlled by daemonset e2e-t4jz9-daemon-set: 0
    Jan 23 13:56:39.949: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 13:56:40.948: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 13:56:40.950: INFO: Number of nodes with available pods controlled by daemonset e2e-t4jz9-daemon-set: 2
    Jan 23 13:56:40.950: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-t4jz9-daemon-set
    STEP: Confirm DaemonSet "e2e-t4jz9-daemon-set" successfully created with "daemonset-name=e2e-t4jz9-daemon-set" label 01/23/24 13:56:40.951
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-t4jz9-daemon-set" 01/23/24 13:56:40.955
    Jan 23 13:56:40.957: INFO: Located ControllerRevision: "e2e-t4jz9-daemon-set-74b5cf44"
    STEP: Patching ControllerRevision "e2e-t4jz9-daemon-set-74b5cf44" 01/23/24 13:56:40.958
    Jan 23 13:56:40.962: INFO: e2e-t4jz9-daemon-set-74b5cf44 has been patched
    STEP: Create a new ControllerRevision 01/23/24 13:56:40.962
    Jan 23 13:56:40.965: INFO: Created ControllerRevision: e2e-t4jz9-daemon-set-7d44bd6cf8
    STEP: Confirm that there are two ControllerRevisions 01/23/24 13:56:40.965
    Jan 23 13:56:40.965: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 23 13:56:40.967: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-t4jz9-daemon-set-74b5cf44" 01/23/24 13:56:40.967
    STEP: Confirm that there is only one ControllerRevision 01/23/24 13:56:40.969
    Jan 23 13:56:40.969: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 23 13:56:40.970: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-t4jz9-daemon-set-7d44bd6cf8" 01/23/24 13:56:40.973
    Jan 23 13:56:40.977: INFO: e2e-t4jz9-daemon-set-7d44bd6cf8 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/23/24 13:56:40.977
    W0123 13:56:40.985887      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/23/24 13:56:40.985
    Jan 23 13:56:40.986: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 23 13:56:41.988: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 23 13:56:41.991: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-t4jz9-daemon-set-7d44bd6cf8=updated" 01/23/24 13:56:41.991
    STEP: Confirm that there is only one ControllerRevision 01/23/24 13:56:41.995
    Jan 23 13:56:41.995: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 23 13:56:41.996: INFO: Found 1 ControllerRevisions
    Jan 23 13:56:41.997: INFO: ControllerRevision "e2e-t4jz9-daemon-set-f94f66567" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-t4jz9-daemon-set" 01/23/24 13:56:41.999
    STEP: deleting DaemonSet.extensions e2e-t4jz9-daemon-set in namespace controllerrevisions-590, will wait for the garbage collector to delete the pods 01/23/24 13:56:41.999
    Jan 23 13:56:42.056: INFO: Deleting DaemonSet.extensions e2e-t4jz9-daemon-set took: 4.89747ms
    Jan 23 13:56:42.157: INFO: Terminating DaemonSet.extensions e2e-t4jz9-daemon-set pods took: 100.556116ms
    Jan 23 13:56:44.860: INFO: Number of nodes with available pods controlled by daemonset e2e-t4jz9-daemon-set: 0
    Jan 23 13:56:44.860: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-t4jz9-daemon-set
    Jan 23 13:56:44.862: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"122729"},"items":null}

    Jan 23 13:56:44.863: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"122729"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:56:44.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-590" for this suite. 01/23/24 13:56:44.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:56:44.874
Jan 23 13:56:44.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename downward-api 01/23/24 13:56:44.874
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:44.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:44.884
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 01/23/24 13:56:44.886
Jan 23 13:56:44.902: INFO: Waiting up to 5m0s for pod "downward-api-9b5d7e03-ddaa-468e-ba99-23db7bef6d96" in namespace "downward-api-2315" to be "Succeeded or Failed"
Jan 23 13:56:44.903: INFO: Pod "downward-api-9b5d7e03-ddaa-468e-ba99-23db7bef6d96": Phase="Pending", Reason="", readiness=false. Elapsed: 1.270063ms
Jan 23 13:56:46.907: INFO: Pod "downward-api-9b5d7e03-ddaa-468e-ba99-23db7bef6d96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00526246s
Jan 23 13:56:48.906: INFO: Pod "downward-api-9b5d7e03-ddaa-468e-ba99-23db7bef6d96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003826292s
STEP: Saw pod success 01/23/24 13:56:48.906
Jan 23 13:56:48.906: INFO: Pod "downward-api-9b5d7e03-ddaa-468e-ba99-23db7bef6d96" satisfied condition "Succeeded or Failed"
Jan 23 13:56:48.908: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downward-api-9b5d7e03-ddaa-468e-ba99-23db7bef6d96 container dapi-container: <nil>
STEP: delete the pod 01/23/24 13:56:48.911
Jan 23 13:56:48.919: INFO: Waiting for pod downward-api-9b5d7e03-ddaa-468e-ba99-23db7bef6d96 to disappear
Jan 23 13:56:48.921: INFO: Pod downward-api-9b5d7e03-ddaa-468e-ba99-23db7bef6d96 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 23 13:56:48.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2315" for this suite. 01/23/24 13:56:48.923
------------------------------
• [4.052 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:56:44.874
    Jan 23 13:56:44.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename downward-api 01/23/24 13:56:44.874
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:44.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:44.884
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 01/23/24 13:56:44.886
    Jan 23 13:56:44.902: INFO: Waiting up to 5m0s for pod "downward-api-9b5d7e03-ddaa-468e-ba99-23db7bef6d96" in namespace "downward-api-2315" to be "Succeeded or Failed"
    Jan 23 13:56:44.903: INFO: Pod "downward-api-9b5d7e03-ddaa-468e-ba99-23db7bef6d96": Phase="Pending", Reason="", readiness=false. Elapsed: 1.270063ms
    Jan 23 13:56:46.907: INFO: Pod "downward-api-9b5d7e03-ddaa-468e-ba99-23db7bef6d96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00526246s
    Jan 23 13:56:48.906: INFO: Pod "downward-api-9b5d7e03-ddaa-468e-ba99-23db7bef6d96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003826292s
    STEP: Saw pod success 01/23/24 13:56:48.906
    Jan 23 13:56:48.906: INFO: Pod "downward-api-9b5d7e03-ddaa-468e-ba99-23db7bef6d96" satisfied condition "Succeeded or Failed"
    Jan 23 13:56:48.908: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downward-api-9b5d7e03-ddaa-468e-ba99-23db7bef6d96 container dapi-container: <nil>
    STEP: delete the pod 01/23/24 13:56:48.911
    Jan 23 13:56:48.919: INFO: Waiting for pod downward-api-9b5d7e03-ddaa-468e-ba99-23db7bef6d96 to disappear
    Jan 23 13:56:48.921: INFO: Pod downward-api-9b5d7e03-ddaa-468e-ba99-23db7bef6d96 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:56:48.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2315" for this suite. 01/23/24 13:56:48.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:56:48.926
Jan 23 13:56:48.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename services 01/23/24 13:56:48.927
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:48.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:48.935
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-9728 01/23/24 13:56:48.937
STEP: creating replication controller nodeport-test in namespace services-9728 01/23/24 13:56:48.945
I0123 13:56:48.949975      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9728, replica count: 2
I0123 13:56:52.001664      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 23 13:56:52.001: INFO: Creating new exec pod
Jan 23 13:56:52.011: INFO: Waiting up to 5m0s for pod "execpodrj2vn" in namespace "services-9728" to be "running"
Jan 23 13:56:52.012: INFO: Pod "execpodrj2vn": Phase="Pending", Reason="", readiness=false. Elapsed: 1.737453ms
Jan 23 13:56:54.015: INFO: Pod "execpodrj2vn": Phase="Running", Reason="", readiness=true. Elapsed: 2.004127149s
Jan 23 13:56:54.015: INFO: Pod "execpodrj2vn" satisfied condition "running"
Jan 23 13:56:55.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-9728 exec execpodrj2vn -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Jan 23 13:56:55.184: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 23 13:56:55.184: INFO: stdout: ""
Jan 23 13:56:55.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-9728 exec execpodrj2vn -- /bin/sh -x -c nc -v -z -w 2 10.233.43.161 80'
Jan 23 13:56:55.350: INFO: stderr: "+ nc -v -z -w 2 10.233.43.161 80\nConnection to 10.233.43.161 80 port [tcp/http] succeeded!\n"
Jan 23 13:56:55.350: INFO: stdout: ""
Jan 23 13:56:55.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-9728 exec execpodrj2vn -- /bin/sh -x -c nc -v -z -w 2 172.31.11.40 32348'
Jan 23 13:56:55.528: INFO: stderr: "+ nc -v -z -w 2 172.31.11.40 32348\nConnection to 172.31.11.40 32348 port [tcp/*] succeeded!\n"
Jan 23 13:56:55.528: INFO: stdout: ""
Jan 23 13:56:55.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-9728 exec execpodrj2vn -- /bin/sh -x -c nc -v -z -w 2 172.31.11.67 32348'
Jan 23 13:56:55.702: INFO: stderr: "+ nc -v -z -w 2 172.31.11.67 32348\nConnection to 172.31.11.67 32348 port [tcp/*] succeeded!\n"
Jan 23 13:56:55.702: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 23 13:56:55.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9728" for this suite. 01/23/24 13:56:55.705
------------------------------
• [SLOW TEST] [6.783 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:56:48.926
    Jan 23 13:56:48.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename services 01/23/24 13:56:48.927
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:48.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:48.935
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-9728 01/23/24 13:56:48.937
    STEP: creating replication controller nodeport-test in namespace services-9728 01/23/24 13:56:48.945
    I0123 13:56:48.949975      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9728, replica count: 2
    I0123 13:56:52.001664      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 23 13:56:52.001: INFO: Creating new exec pod
    Jan 23 13:56:52.011: INFO: Waiting up to 5m0s for pod "execpodrj2vn" in namespace "services-9728" to be "running"
    Jan 23 13:56:52.012: INFO: Pod "execpodrj2vn": Phase="Pending", Reason="", readiness=false. Elapsed: 1.737453ms
    Jan 23 13:56:54.015: INFO: Pod "execpodrj2vn": Phase="Running", Reason="", readiness=true. Elapsed: 2.004127149s
    Jan 23 13:56:54.015: INFO: Pod "execpodrj2vn" satisfied condition "running"
    Jan 23 13:56:55.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-9728 exec execpodrj2vn -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Jan 23 13:56:55.184: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 23 13:56:55.184: INFO: stdout: ""
    Jan 23 13:56:55.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-9728 exec execpodrj2vn -- /bin/sh -x -c nc -v -z -w 2 10.233.43.161 80'
    Jan 23 13:56:55.350: INFO: stderr: "+ nc -v -z -w 2 10.233.43.161 80\nConnection to 10.233.43.161 80 port [tcp/http] succeeded!\n"
    Jan 23 13:56:55.350: INFO: stdout: ""
    Jan 23 13:56:55.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-9728 exec execpodrj2vn -- /bin/sh -x -c nc -v -z -w 2 172.31.11.40 32348'
    Jan 23 13:56:55.528: INFO: stderr: "+ nc -v -z -w 2 172.31.11.40 32348\nConnection to 172.31.11.40 32348 port [tcp/*] succeeded!\n"
    Jan 23 13:56:55.528: INFO: stdout: ""
    Jan 23 13:56:55.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-9728 exec execpodrj2vn -- /bin/sh -x -c nc -v -z -w 2 172.31.11.67 32348'
    Jan 23 13:56:55.702: INFO: stderr: "+ nc -v -z -w 2 172.31.11.67 32348\nConnection to 172.31.11.67 32348 port [tcp/*] succeeded!\n"
    Jan 23 13:56:55.702: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:56:55.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9728" for this suite. 01/23/24 13:56:55.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:56:55.71
Jan 23 13:56:55.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename dns 01/23/24 13:56:55.71
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:55.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:55.72
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/23/24 13:56:55.722
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/23/24 13:56:55.722
STEP: creating a pod to probe DNS 01/23/24 13:56:55.722
STEP: submitting the pod to kubernetes 01/23/24 13:56:55.722
Jan 23 13:56:55.743: INFO: Waiting up to 15m0s for pod "dns-test-f53f9f82-1697-4e2e-890a-5fd307003025" in namespace "dns-8311" to be "running"
Jan 23 13:56:55.745: INFO: Pod "dns-test-f53f9f82-1697-4e2e-890a-5fd307003025": Phase="Pending", Reason="", readiness=false. Elapsed: 1.857176ms
Jan 23 13:56:57.748: INFO: Pod "dns-test-f53f9f82-1697-4e2e-890a-5fd307003025": Phase="Running", Reason="", readiness=true. Elapsed: 2.005003617s
Jan 23 13:56:57.748: INFO: Pod "dns-test-f53f9f82-1697-4e2e-890a-5fd307003025" satisfied condition "running"
STEP: retrieving the pod 01/23/24 13:56:57.748
STEP: looking for the results for each expected name from probers 01/23/24 13:56:57.75
Jan 23 13:56:57.757: INFO: DNS probes using dns-8311/dns-test-f53f9f82-1697-4e2e-890a-5fd307003025 succeeded

STEP: deleting the pod 01/23/24 13:56:57.757
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 23 13:56:57.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8311" for this suite. 01/23/24 13:56:57.767
------------------------------
• [2.060 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:56:55.71
    Jan 23 13:56:55.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename dns 01/23/24 13:56:55.71
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:55.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:55.72
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/23/24 13:56:55.722
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/23/24 13:56:55.722
    STEP: creating a pod to probe DNS 01/23/24 13:56:55.722
    STEP: submitting the pod to kubernetes 01/23/24 13:56:55.722
    Jan 23 13:56:55.743: INFO: Waiting up to 15m0s for pod "dns-test-f53f9f82-1697-4e2e-890a-5fd307003025" in namespace "dns-8311" to be "running"
    Jan 23 13:56:55.745: INFO: Pod "dns-test-f53f9f82-1697-4e2e-890a-5fd307003025": Phase="Pending", Reason="", readiness=false. Elapsed: 1.857176ms
    Jan 23 13:56:57.748: INFO: Pod "dns-test-f53f9f82-1697-4e2e-890a-5fd307003025": Phase="Running", Reason="", readiness=true. Elapsed: 2.005003617s
    Jan 23 13:56:57.748: INFO: Pod "dns-test-f53f9f82-1697-4e2e-890a-5fd307003025" satisfied condition "running"
    STEP: retrieving the pod 01/23/24 13:56:57.748
    STEP: looking for the results for each expected name from probers 01/23/24 13:56:57.75
    Jan 23 13:56:57.757: INFO: DNS probes using dns-8311/dns-test-f53f9f82-1697-4e2e-890a-5fd307003025 succeeded

    STEP: deleting the pod 01/23/24 13:56:57.757
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:56:57.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8311" for this suite. 01/23/24 13:56:57.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:56:57.771
Jan 23 13:56:57.771: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename configmap 01/23/24 13:56:57.771
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:57.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:57.78
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-affce38e-088c-416f-a84e-7c334c55409a 01/23/24 13:56:57.781
STEP: Creating a pod to test consume configMaps 01/23/24 13:56:57.785
Jan 23 13:56:57.796: INFO: Waiting up to 5m0s for pod "pod-configmaps-90ebba24-18e4-463b-bfcd-c4fe9de1c617" in namespace "configmap-7592" to be "Succeeded or Failed"
Jan 23 13:56:57.798: INFO: Pod "pod-configmaps-90ebba24-18e4-463b-bfcd-c4fe9de1c617": Phase="Pending", Reason="", readiness=false. Elapsed: 1.119287ms
Jan 23 13:56:59.801: INFO: Pod "pod-configmaps-90ebba24-18e4-463b-bfcd-c4fe9de1c617": Phase="Running", Reason="", readiness=true. Elapsed: 2.004132986s
Jan 23 13:57:01.800: INFO: Pod "pod-configmaps-90ebba24-18e4-463b-bfcd-c4fe9de1c617": Phase="Running", Reason="", readiness=false. Elapsed: 4.003315143s
Jan 23 13:57:03.800: INFO: Pod "pod-configmaps-90ebba24-18e4-463b-bfcd-c4fe9de1c617": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.003501912s
STEP: Saw pod success 01/23/24 13:57:03.8
Jan 23 13:57:03.800: INFO: Pod "pod-configmaps-90ebba24-18e4-463b-bfcd-c4fe9de1c617" satisfied condition "Succeeded or Failed"
Jan 23 13:57:03.801: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-90ebba24-18e4-463b-bfcd-c4fe9de1c617 container agnhost-container: <nil>
STEP: delete the pod 01/23/24 13:57:03.805
Jan 23 13:57:03.810: INFO: Waiting for pod pod-configmaps-90ebba24-18e4-463b-bfcd-c4fe9de1c617 to disappear
Jan 23 13:57:03.811: INFO: Pod pod-configmaps-90ebba24-18e4-463b-bfcd-c4fe9de1c617 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 23 13:57:03.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7592" for this suite. 01/23/24 13:57:03.813
------------------------------
• [SLOW TEST] [6.045 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:56:57.771
    Jan 23 13:56:57.771: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename configmap 01/23/24 13:56:57.771
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:56:57.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:56:57.78
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-affce38e-088c-416f-a84e-7c334c55409a 01/23/24 13:56:57.781
    STEP: Creating a pod to test consume configMaps 01/23/24 13:56:57.785
    Jan 23 13:56:57.796: INFO: Waiting up to 5m0s for pod "pod-configmaps-90ebba24-18e4-463b-bfcd-c4fe9de1c617" in namespace "configmap-7592" to be "Succeeded or Failed"
    Jan 23 13:56:57.798: INFO: Pod "pod-configmaps-90ebba24-18e4-463b-bfcd-c4fe9de1c617": Phase="Pending", Reason="", readiness=false. Elapsed: 1.119287ms
    Jan 23 13:56:59.801: INFO: Pod "pod-configmaps-90ebba24-18e4-463b-bfcd-c4fe9de1c617": Phase="Running", Reason="", readiness=true. Elapsed: 2.004132986s
    Jan 23 13:57:01.800: INFO: Pod "pod-configmaps-90ebba24-18e4-463b-bfcd-c4fe9de1c617": Phase="Running", Reason="", readiness=false. Elapsed: 4.003315143s
    Jan 23 13:57:03.800: INFO: Pod "pod-configmaps-90ebba24-18e4-463b-bfcd-c4fe9de1c617": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.003501912s
    STEP: Saw pod success 01/23/24 13:57:03.8
    Jan 23 13:57:03.800: INFO: Pod "pod-configmaps-90ebba24-18e4-463b-bfcd-c4fe9de1c617" satisfied condition "Succeeded or Failed"
    Jan 23 13:57:03.801: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-90ebba24-18e4-463b-bfcd-c4fe9de1c617 container agnhost-container: <nil>
    STEP: delete the pod 01/23/24 13:57:03.805
    Jan 23 13:57:03.810: INFO: Waiting for pod pod-configmaps-90ebba24-18e4-463b-bfcd-c4fe9de1c617 to disappear
    Jan 23 13:57:03.811: INFO: Pod pod-configmaps-90ebba24-18e4-463b-bfcd-c4fe9de1c617 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:57:03.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7592" for this suite. 01/23/24 13:57:03.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:57:03.816
Jan 23 13:57:03.816: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename endpointslice 01/23/24 13:57:03.818
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:57:03.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:57:03.832
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 01/23/24 13:57:08.884
STEP: referencing matching pods with named port 01/23/24 13:57:13.889
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/23/24 13:57:18.894
STEP: recreating EndpointSlices after they've been deleted 01/23/24 13:57:23.902
Jan 23 13:57:23.909: INFO: EndpointSlice for Service endpointslice-5036/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 23 13:57:33.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5036" for this suite. 01/23/24 13:57:33.918
------------------------------
• [SLOW TEST] [30.105 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:57:03.816
    Jan 23 13:57:03.816: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename endpointslice 01/23/24 13:57:03.818
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:57:03.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:57:03.832
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 01/23/24 13:57:08.884
    STEP: referencing matching pods with named port 01/23/24 13:57:13.889
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/23/24 13:57:18.894
    STEP: recreating EndpointSlices after they've been deleted 01/23/24 13:57:23.902
    Jan 23 13:57:23.909: INFO: EndpointSlice for Service endpointslice-5036/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:57:33.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5036" for this suite. 01/23/24 13:57:33.918
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:57:33.922
Jan 23 13:57:33.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename sched-preemption 01/23/24 13:57:33.923
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:57:33.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:57:33.933
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jan 23 13:57:33.942: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 23 13:58:33.985: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:58:33.987
Jan 23 13:58:33.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename sched-preemption-path 01/23/24 13:58:33.988
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:58:33.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:58:33.997
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Jan 23 13:58:34.007: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan 23 13:58:34.009: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Jan 23 13:58:34.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:58:34.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-7656" for this suite. 01/23/24 13:58:34.045
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6578" for this suite. 01/23/24 13:58:34.048
------------------------------
• [SLOW TEST] [60.128 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:57:33.922
    Jan 23 13:57:33.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename sched-preemption 01/23/24 13:57:33.923
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:57:33.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:57:33.933
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jan 23 13:57:33.942: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 23 13:58:33.985: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:58:33.987
    Jan 23 13:58:33.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename sched-preemption-path 01/23/24 13:58:33.988
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:58:33.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:58:33.997
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Jan 23 13:58:34.007: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan 23 13:58:34.009: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:58:34.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:58:34.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-7656" for this suite. 01/23/24 13:58:34.045
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6578" for this suite. 01/23/24 13:58:34.048
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:58:34.051
Jan 23 13:58:34.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename replication-controller 01/23/24 13:58:34.051
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:58:34.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:58:34.059
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d 01/23/24 13:58:34.061
Jan 23 13:58:34.067: INFO: Pod name my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d: Found 0 pods out of 1
Jan 23 13:58:39.071: INFO: Pod name my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d: Found 1 pods out of 1
Jan 23 13:58:39.071: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d" are running
Jan 23 13:58:39.071: INFO: Waiting up to 5m0s for pod "my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d-rsqjn" in namespace "replication-controller-4625" to be "running"
Jan 23 13:58:39.073: INFO: Pod "my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d-rsqjn": Phase="Running", Reason="", readiness=true. Elapsed: 1.669383ms
Jan 23 13:58:39.073: INFO: Pod "my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d-rsqjn" satisfied condition "running"
Jan 23 13:58:39.073: INFO: Pod "my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d-rsqjn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-23 13:58:34 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-23 13:58:35 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-23 13:58:35 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-23 13:58:34 +0000 UTC Reason: Message:}])
Jan 23 13:58:39.073: INFO: Trying to dial the pod
Jan 23 13:58:44.081: INFO: Controller my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d: Got expected result from replica 1 [my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d-rsqjn]: "my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d-rsqjn", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 23 13:58:44.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4625" for this suite. 01/23/24 13:58:44.083
------------------------------
• [SLOW TEST] [10.035 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:58:34.051
    Jan 23 13:58:34.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename replication-controller 01/23/24 13:58:34.051
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:58:34.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:58:34.059
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d 01/23/24 13:58:34.061
    Jan 23 13:58:34.067: INFO: Pod name my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d: Found 0 pods out of 1
    Jan 23 13:58:39.071: INFO: Pod name my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d: Found 1 pods out of 1
    Jan 23 13:58:39.071: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d" are running
    Jan 23 13:58:39.071: INFO: Waiting up to 5m0s for pod "my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d-rsqjn" in namespace "replication-controller-4625" to be "running"
    Jan 23 13:58:39.073: INFO: Pod "my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d-rsqjn": Phase="Running", Reason="", readiness=true. Elapsed: 1.669383ms
    Jan 23 13:58:39.073: INFO: Pod "my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d-rsqjn" satisfied condition "running"
    Jan 23 13:58:39.073: INFO: Pod "my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d-rsqjn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-23 13:58:34 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-23 13:58:35 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-23 13:58:35 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-23 13:58:34 +0000 UTC Reason: Message:}])
    Jan 23 13:58:39.073: INFO: Trying to dial the pod
    Jan 23 13:58:44.081: INFO: Controller my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d: Got expected result from replica 1 [my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d-rsqjn]: "my-hostname-basic-28058245-2e84-4a71-8805-87aaccf2477d-rsqjn", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:58:44.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4625" for this suite. 01/23/24 13:58:44.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:58:44.087
Jan 23 13:58:44.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename disruption 01/23/24 13:58:44.087
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:58:44.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:58:44.097
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 01/23/24 13:58:44.099
STEP: Waiting for the pdb to be processed 01/23/24 13:58:44.101
STEP: updating the pdb 01/23/24 13:58:46.106
STEP: Waiting for the pdb to be processed 01/23/24 13:58:46.11
STEP: patching the pdb 01/23/24 13:58:48.115
STEP: Waiting for the pdb to be processed 01/23/24 13:58:48.121
STEP: Waiting for the pdb to be deleted 01/23/24 13:58:50.128
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 23 13:58:50.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4375" for this suite. 01/23/24 13:58:50.132
------------------------------
• [SLOW TEST] [6.049 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:58:44.087
    Jan 23 13:58:44.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename disruption 01/23/24 13:58:44.087
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:58:44.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:58:44.097
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 01/23/24 13:58:44.099
    STEP: Waiting for the pdb to be processed 01/23/24 13:58:44.101
    STEP: updating the pdb 01/23/24 13:58:46.106
    STEP: Waiting for the pdb to be processed 01/23/24 13:58:46.11
    STEP: patching the pdb 01/23/24 13:58:48.115
    STEP: Waiting for the pdb to be processed 01/23/24 13:58:48.121
    STEP: Waiting for the pdb to be deleted 01/23/24 13:58:50.128
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:58:50.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4375" for this suite. 01/23/24 13:58:50.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:58:50.136
Jan 23 13:58:50.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename deployment 01/23/24 13:58:50.137
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:58:50.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:58:50.145
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan 23 13:58:50.147: INFO: Creating deployment "webserver-deployment"
Jan 23 13:58:50.151: INFO: Waiting for observed generation 1
Jan 23 13:58:52.155: INFO: Waiting for all required pods to come up
Jan 23 13:58:52.157: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/23/24 13:58:52.157
Jan 23 13:58:52.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4hwjp" in namespace "deployment-2055" to be "running"
Jan 23 13:58:52.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-ckb4x" in namespace "deployment-2055" to be "running"
Jan 23 13:58:52.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-26t8h" in namespace "deployment-2055" to be "running"
Jan 23 13:58:52.158: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jncwv" in namespace "deployment-2055" to be "running"
Jan 23 13:58:52.158: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2wljz" in namespace "deployment-2055" to be "running"
Jan 23 13:58:52.158: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-crh8r" in namespace "deployment-2055" to be "running"
Jan 23 13:58:52.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-sq4xv" in namespace "deployment-2055" to be "running"
Jan 23 13:58:52.158: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-knwbb" in namespace "deployment-2055" to be "running"
Jan 23 13:58:52.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-859cv" in namespace "deployment-2055" to be "running"
Jan 23 13:58:52.158: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-nkmkc" in namespace "deployment-2055" to be "running"
Jan 23 13:58:52.159: INFO: Pod "webserver-deployment-7f5969cbc7-sq4xv": Phase="Pending", Reason="", readiness=false. Elapsed: 1.558445ms
Jan 23 13:58:52.159: INFO: Pod "webserver-deployment-7f5969cbc7-ckb4x": Phase="Pending", Reason="", readiness=false. Elapsed: 1.716113ms
Jan 23 13:58:52.159: INFO: Pod "webserver-deployment-7f5969cbc7-crh8r": Phase="Pending", Reason="", readiness=false. Elapsed: 1.582933ms
Jan 23 13:58:52.160: INFO: Pod "webserver-deployment-7f5969cbc7-26t8h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.479044ms
Jan 23 13:58:52.160: INFO: Pod "webserver-deployment-7f5969cbc7-4hwjp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.853388ms
Jan 23 13:58:52.161: INFO: Pod "webserver-deployment-7f5969cbc7-859cv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.978298ms
Jan 23 13:58:52.161: INFO: Pod "webserver-deployment-7f5969cbc7-knwbb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033667ms
Jan 23 13:58:52.161: INFO: Pod "webserver-deployment-7f5969cbc7-jncwv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.12882ms
Jan 23 13:58:52.161: INFO: Pod "webserver-deployment-7f5969cbc7-2wljz": Phase="Pending", Reason="", readiness=false. Elapsed: 3.136019ms
Jan 23 13:58:52.161: INFO: Pod "webserver-deployment-7f5969cbc7-nkmkc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.068391ms
Jan 23 13:58:54.163: INFO: Pod "webserver-deployment-7f5969cbc7-ckb4x": Phase="Running", Reason="", readiness=true. Elapsed: 2.005992011s
Jan 23 13:58:54.163: INFO: Pod "webserver-deployment-7f5969cbc7-ckb4x" satisfied condition "running"
Jan 23 13:58:54.163: INFO: Pod "webserver-deployment-7f5969cbc7-crh8r": Phase="Running", Reason="", readiness=true. Elapsed: 2.005907092s
Jan 23 13:58:54.163: INFO: Pod "webserver-deployment-7f5969cbc7-crh8r" satisfied condition "running"
Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-jncwv": Phase="Running", Reason="", readiness=true. Elapsed: 2.005973487s
Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-jncwv" satisfied condition "running"
Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-sq4xv": Phase="Running", Reason="", readiness=true. Elapsed: 2.005941588s
Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-sq4xv" satisfied condition "running"
Jan 23 13:58:54.163: INFO: Pod "webserver-deployment-7f5969cbc7-knwbb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005850999s
Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-knwbb" satisfied condition "running"
Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-nkmkc": Phase="Running", Reason="", readiness=true. Elapsed: 2.005950932s
Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-nkmkc" satisfied condition "running"
Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-26t8h": Phase="Running", Reason="", readiness=true. Elapsed: 2.006148759s
Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-2wljz": Phase="Running", Reason="", readiness=true. Elapsed: 2.006100654s
Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-26t8h" satisfied condition "running"
Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-2wljz" satisfied condition "running"
Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-4hwjp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006208369s
Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-859cv": Phase="Running", Reason="", readiness=true. Elapsed: 2.006092768s
Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-859cv" satisfied condition "running"
Jan 23 13:58:56.164: INFO: Pod "webserver-deployment-7f5969cbc7-4hwjp": Phase="Running", Reason="", readiness=true. Elapsed: 4.006366143s
Jan 23 13:58:56.164: INFO: Pod "webserver-deployment-7f5969cbc7-4hwjp" satisfied condition "running"
Jan 23 13:58:56.164: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 23 13:58:56.167: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 23 13:58:56.171: INFO: Updating deployment webserver-deployment
Jan 23 13:58:56.171: INFO: Waiting for observed generation 2
Jan 23 13:58:58.176: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 23 13:58:58.178: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 23 13:58:58.179: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 23 13:58:58.183: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 23 13:58:58.183: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 23 13:58:58.184: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 23 13:58:58.186: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 23 13:58:58.186: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 23 13:58:58.191: INFO: Updating deployment webserver-deployment
Jan 23 13:58:58.191: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 23 13:58:58.194: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 23 13:58:58.196: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 23 13:59:00.201: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2055  64f6c2d2-07dc-4301-a59f-40dfaa37d503 124216 3 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043db098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-01-23 13:58:58 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2024-01-23 13:58:58 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 23 13:59:00.202: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-2055  44e04e89-8d46-47b9-8156-efa62cf108ac 124215 3 2024-01-23 13:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 64f6c2d2-07dc-4301-a59f-40dfaa37d503 0xc006a2f937 0xc006a2f938}] [] [{kube-controller-manager Update apps/v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"64f6c2d2-07dc-4301-a59f-40dfaa37d503\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a2f9d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 23 13:59:00.202: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 23 13:59:00.202: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-2055  581b7b3d-c9d1-4e8b-9c4b-485723a42407 124196 3 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 64f6c2d2-07dc-4301-a59f-40dfaa37d503 0xc006a2f847 0xc006a2f848}] [] [{kube-controller-manager Update apps/v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"64f6c2d2-07dc-4301-a59f-40dfaa37d503\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a2f8d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 23 13:59:00.206: INFO: Pod "webserver-deployment-7f5969cbc7-26t8h" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-26t8h webserver-deployment-7f5969cbc7- deployment-2055  d005f40d-b4d0-4783-adc6-46395743e52d 124028 0 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6a50513329242c852dfa56139ba1d8b05f1ec8d6e7c65d3faa66e48351db3ad6 cni.projectcalico.org/podIP:10.233.87.117/32 cni.projectcalico.org/podIPs:10.233.87.117/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc006a2feb7 0xc006a2feb8}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:58:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:58:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2s8gp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2s8gp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.117,StartTime:2024-01-23 13:58:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:58:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c56d5ceb28b88df277e5ea06be1efb529a5c88a23a73b7f354d31620611721a5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.206: INFO: Pod "webserver-deployment-7f5969cbc7-2wljz" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2wljz webserver-deployment-7f5969cbc7- deployment-2055  0d0d40f7-5637-4805-9f80-5479fa9f348d 123988 0 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0a09bd0a318d39f9f8638ff89ddd30e3b0f58c84a5e47ec1a6c8fe50f56ef87b cni.projectcalico.org/podIP:10.233.75.61/32 cni.projectcalico.org/podIPs:10.233.75.61/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc0041420d7 0xc0041420d8}] [] [{calico Update v1 2024-01-23 13:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-23 13:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qrqz9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qrqz9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:10.233.75.61,StartTime:2024-01-23 13:58:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:58:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0b3c7d3570df99e8875bb1082c8da8ab3a63a857c740236831373ccaab38e13b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.206: INFO: Pod "webserver-deployment-7f5969cbc7-4hwjp" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4hwjp webserver-deployment-7f5969cbc7- deployment-2055  855344c6-180a-44fa-9c5d-d57b2f3233e4 124031 0 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:fac65a495d30fa42b9146f41491e11e3bb3c808b5bb8dc3608249a3fcbd856e5 cni.projectcalico.org/podIP:10.233.87.119/32 cni.projectcalico.org/podIPs:10.233.87.119/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc0041422e7 0xc0041422e8}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:58:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:58:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w995r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w995r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.119,StartTime:2024-01-23 13:58:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:58:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2e7c20cf5273418393d5aa9522d5cced0dfa3875cb76a048f2ceeac386211a1a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.206: INFO: Pod "webserver-deployment-7f5969cbc7-5d274" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5d274 webserver-deployment-7f5969cbc7- deployment-2055  c77596b2-af80-481c-86e5-ca3ed67059e1 124288 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc0041424e7 0xc0041424e8}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:59:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lfxvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lfxvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.206: INFO: Pod "webserver-deployment-7f5969cbc7-b9ww6" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-b9ww6 webserver-deployment-7f5969cbc7- deployment-2055  f3fb8456-a8cc-4ba5-9056-8a0175bbc207 124189 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc0041426a7 0xc0041426a8}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-65vqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-65vqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.206: INFO: Pod "webserver-deployment-7f5969cbc7-bbqc7" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bbqc7 webserver-deployment-7f5969cbc7- deployment-2055  9ba000fb-1110-411c-95c0-45554e410d4e 124182 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004142800 0xc004142801}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wkjdr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wkjdr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.207: INFO: Pod "webserver-deployment-7f5969cbc7-bmqsk" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bmqsk webserver-deployment-7f5969cbc7- deployment-2055  c1ba7053-2390-44a0-985e-49a4b865c98e 124238 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004142950 0xc004142951}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gb646,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gb646,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.207: INFO: Pod "webserver-deployment-7f5969cbc7-ckb4x" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ckb4x webserver-deployment-7f5969cbc7- deployment-2055  2179298e-42a3-43e8-ab6a-d438bac720ea 123985 0 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4cd52a770a4bb02883bb0454d9db2eb94f30bd72e7dcdc01f45b87bad27d9460 cni.projectcalico.org/podIP:10.233.75.74/32 cni.projectcalico.org/podIPs:10.233.75.74/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004142b27 0xc004142b28}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:58:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:58:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-chdh7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-chdh7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:10.233.75.74,StartTime:2024-01-23 13:58:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:58:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a8f2aaf735fb09a536cf26864480d454a8696b7fed59551f8c6af6925119b740,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.207: INFO: Pod "webserver-deployment-7f5969cbc7-crh8r" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-crh8r webserver-deployment-7f5969cbc7- deployment-2055  d527794c-8cbc-4ee5-bef1-e05d2e383dbf 124025 0 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:409ed2c62ac0140a735da5cb17083c8871912b4be6dbeab1571f68ffbf8dd567 cni.projectcalico.org/podIP:10.233.87.111/32 cni.projectcalico.org/podIPs:10.233.87.111/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004142d47 0xc004142d48}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:58:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:58:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2wk4x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2wk4x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.111,StartTime:2024-01-23 13:58:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:58:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2542cae869e647bcd73c1cf0a1880ab6138e3590896459ec3957581628411d23,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.207: INFO: Pod "webserver-deployment-7f5969cbc7-j8gnk" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-j8gnk webserver-deployment-7f5969cbc7- deployment-2055  6020f031-de79-446d-9500-31e4e4d65108 124271 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:df158b4e80672c4994441bd4b278b8209ebda10e97c804be16a616d561021174 cni.projectcalico.org/podIP:10.233.87.121/32 cni.projectcalico.org/podIPs:10.233.87.121/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004142f67 0xc004142f68}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-23 13:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wpwth,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wpwth,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.207: INFO: Pod "webserver-deployment-7f5969cbc7-jncwv" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jncwv webserver-deployment-7f5969cbc7- deployment-2055  0d2b9720-4f00-4493-9aae-1743a1901805 123982 0 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3a1a0c381074d942c59ade21a65ad2c4cf51863919c3118d9014c89368e7adcd cni.projectcalico.org/podIP:10.233.75.49/32 cni.projectcalico.org/podIPs:10.233.75.49/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004143167 0xc004143168}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:58:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:58:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sjmpk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sjmpk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:10.233.75.49,StartTime:2024-01-23 13:58:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:58:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://414fe9d1225a10f99338d6a738aa1592ae0b0a7e512f6f6fd0d0d845d257fd57,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.207: INFO: Pod "webserver-deployment-7f5969cbc7-knwbb" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-knwbb webserver-deployment-7f5969cbc7- deployment-2055  1c6c72ff-1594-49de-b599-21cafa5aa2d8 123995 0 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:52969238cc3d8dc772fb04321ebe8a9afb85d003477b918af514cee3e52776c1 cni.projectcalico.org/podIP:10.233.75.76/32 cni.projectcalico.org/podIPs:10.233.75.76/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004143387 0xc004143388}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:58:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:58:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6twg8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6twg8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:10.233.75.76,StartTime:2024-01-23 13:58:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:58:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9c67f16fe2903bcf32763006bd8b5967883005e320ca291b638cb66c2cfe569e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.207: INFO: Pod "webserver-deployment-7f5969cbc7-ksqcv" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ksqcv webserver-deployment-7f5969cbc7- deployment-2055  13bba593-b83e-4da0-98e0-73335eb8bec5 124186 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004143587 0xc004143588}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cgs6g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cgs6g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.208: INFO: Pod "webserver-deployment-7f5969cbc7-nkmkc" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nkmkc webserver-deployment-7f5969cbc7- deployment-2055  56c4a3a9-04b6-4d6f-b800-f839226a049a 123992 0 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1c06915074b25d10413af784e5264ded79f246c018de28c25ab2b69cce1672a7 cni.projectcalico.org/podIP:10.233.87.115/32 cni.projectcalico.org/podIPs:10.233.87.115/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004143767 0xc004143768}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:58:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:58:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dffm8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dffm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.115,StartTime:2024-01-23 13:58:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:58:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f54e711fa7b026c5050114aa25b09807de9aa67b2d3ce7c4a9cd011f781b0c93,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.208: INFO: Pod "webserver-deployment-7f5969cbc7-pw58g" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pw58g webserver-deployment-7f5969cbc7- deployment-2055  23232e9d-0d6c-4f06-8796-ed73332ceedc 124195 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004143967 0xc004143968}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kmqmw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kmqmw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.208: INFO: Pod "webserver-deployment-7f5969cbc7-v6jrw" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-v6jrw webserver-deployment-7f5969cbc7- deployment-2055  2b154a3c-6ff8-4eae-ab81-3bbc0f8b97aa 124237 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6fd94485661fe680ff848ccc12098398ebe2ed4541a25d46b28db6954bba34d7 cni.projectcalico.org/podIP:10.233.75.44/32 cni.projectcalico.org/podIPs:10.233.75.44/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004143b47 0xc004143b48}] [] [{calico Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5zfv7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5zfv7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.208: INFO: Pod "webserver-deployment-7f5969cbc7-vcgnh" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vcgnh webserver-deployment-7f5969cbc7- deployment-2055  1448e0cb-83b6-4935-bd94-2c0b053324a5 124286 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3e8865ebba84526513de0e46e8388bd8c2ca34b25d65de1adc999b4e40883d5b cni.projectcalico.org/podIP:10.233.75.78/32 cni.projectcalico.org/podIPs:10.233.75.78/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004143cd0 0xc004143cd1}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v52bp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v52bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.208: INFO: Pod "webserver-deployment-7f5969cbc7-wjk9m" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wjk9m webserver-deployment-7f5969cbc7- deployment-2055  ca5b1256-f396-429f-9827-be60b909468f 124185 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004143e40 0xc004143e41}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mtfqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mtfqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.208: INFO: Pod "webserver-deployment-7f5969cbc7-wnp8s" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wnp8s webserver-deployment-7f5969cbc7- deployment-2055  0b991c61-9b5b-4f90-a80e-d62f1427c2cf 124181 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004143f90 0xc004143f91}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8x8ns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8x8ns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.208: INFO: Pod "webserver-deployment-7f5969cbc7-zv7vw" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zv7vw webserver-deployment-7f5969cbc7- deployment-2055  06b47c8c-efc0-4679-a6bb-22ca9879a555 124228 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5c14e64081a3f11f4a052fa5f807c03a64ad8e35d444b91e241d039857a1dff7 cni.projectcalico.org/podIP:10.233.75.53/32 cni.projectcalico.org/podIPs:10.233.75.53/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc00418c167 0xc00418c168}] [] [{calico Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-js8qr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-js8qr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.208: INFO: Pod "webserver-deployment-d9f79cb5-5b6ch" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5b6ch webserver-deployment-d9f79cb5- deployment-2055  c9915fb7-cb8c-4642-b465-78995dd44259 124190 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418c337 0xc00418c338}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wzgkx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wzgkx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.209: INFO: Pod "webserver-deployment-d9f79cb5-92h5m" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-92h5m webserver-deployment-d9f79cb5- deployment-2055  5bc27671-8d3e-417e-8b32-d879dd0c414f 124252 0 2024-01-23 13:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3dbc4fb47ca8d6f2d5572144df2805c32bfecb4386331723be8dff36b6a748ba cni.projectcalico.org/podIP:10.233.87.120/32 cni.projectcalico.org/podIPs:10.233.87.120/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418c48f 0xc00418c4c0}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-23 13:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l9lfj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l9lfj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:,StartTime:2024-01-23 13:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.209: INFO: Pod "webserver-deployment-d9f79cb5-cjkp7" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-cjkp7 webserver-deployment-d9f79cb5- deployment-2055  98af03bc-72db-4860-9cff-b5c5192a9e19 124110 0 2024-01-23 13:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:52c39b9f3883e36a8a2492aa70a8ea4bb697dda9d09d24b8d253ec46b81e8716 cni.projectcalico.org/podIP:10.233.75.56/32 cni.projectcalico.org/podIPs:10.233.75.56/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418c6d7 0xc00418c6d8}] [] [{calico Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c4pgn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c4pgn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:,StartTime:2024-01-23 13:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.209: INFO: Pod "webserver-deployment-d9f79cb5-flg5z" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-flg5z webserver-deployment-d9f79cb5- deployment-2055  bbf7b87f-8b09-46a6-83f6-1d14c534a7b1 124177 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418c8c7 0xc00418c8c8}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qrhgq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qrhgq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.209: INFO: Pod "webserver-deployment-d9f79cb5-jw72s" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jw72s webserver-deployment-d9f79cb5- deployment-2055  35d0aca8-1e5a-4a72-a745-51a1bd8d1950 124107 0 2024-01-23 13:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6d09de2934b923f59061024e78eacaa43255fa5f1400a49973fb6db334061ac4 cni.projectcalico.org/podIP:10.233.75.70/32 cni.projectcalico.org/podIPs:10.233.75.70/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418ca1f 0xc00418ca50}] [] [{calico Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rhqvg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rhqvg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:,StartTime:2024-01-23 13:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.209: INFO: Pod "webserver-deployment-d9f79cb5-m5hgh" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-m5hgh webserver-deployment-d9f79cb5- deployment-2055  23e8f594-c9fe-4e10-b22f-a81c319472c2 124260 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:fa7627779894765291a9ff55c976588fd77cb535ee0364f54a29a75de8e72b85 cni.projectcalico.org/podIP:10.233.75.42/32 cni.projectcalico.org/podIPs:10.233.75.42/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418cc67 0xc00418cc68}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-23 13:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4jwzk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4jwzk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.209: INFO: Pod "webserver-deployment-d9f79cb5-p2b2f" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-p2b2f webserver-deployment-d9f79cb5- deployment-2055  6d84fdae-4529-4460-800e-73dd14866dbe 124268 0 2024-01-23 13:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2d4e75af42b2b4a4e473651153fae75262d16d0f6f17b4e77f87b254e4e5fa2c cni.projectcalico.org/podIP:10.233.87.122/32 cni.projectcalico.org/podIPs:10.233.87.122/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418ce87 0xc00418ce88}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-23 13:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ksn87,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ksn87,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:,StartTime:2024-01-23 13:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.209: INFO: Pod "webserver-deployment-d9f79cb5-pvgf2" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pvgf2 webserver-deployment-d9f79cb5- deployment-2055  650e0351-7410-4c0c-8664-43f787f023f4 124276 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:5df0a9f5ed83f83999273d65c407c117b4468ff8afe697ebd1da55c8cc225b54 cni.projectcalico.org/podIP:10.233.87.124/32 cni.projectcalico.org/podIPs:10.233.87.124/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418d0a7 0xc00418d0a8}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-23 13:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-frhgl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-frhgl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.209: INFO: Pod "webserver-deployment-d9f79cb5-pxmfp" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pxmfp webserver-deployment-d9f79cb5- deployment-2055  779e9cef-442a-4c9d-a471-c23604f751dd 124267 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ebbaa1a8267effac88c5a7ea29040adc598e27a71172b80aa1ce8d3102b71cb0 cni.projectcalico.org/podIP:10.233.75.43/32 cni.projectcalico.org/podIPs:10.233.75.43/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418d2c7 0xc00418d2c8}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-23 13:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pkmzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pkmzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.209: INFO: Pod "webserver-deployment-d9f79cb5-sslsp" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sslsp webserver-deployment-d9f79cb5- deployment-2055  7851a8a5-9f4e-42dc-a79a-6c65b5217a28 124173 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418d4c7 0xc00418d4c8}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8spp2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8spp2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.210: INFO: Pod "webserver-deployment-d9f79cb5-t7hh4" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-t7hh4 webserver-deployment-d9f79cb5- deployment-2055  ff133593-7029-486c-9fd5-bada997addb7 124171 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418d61f 0xc00418d630}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7crpb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7crpb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.210: INFO: Pod "webserver-deployment-d9f79cb5-t94zh" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-t94zh webserver-deployment-d9f79cb5- deployment-2055  5b0e195d-7242-4398-81e5-17fa475d6e86 124172 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418d77f 0xc00418d790}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wz4hf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wz4hf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 13:59:00.210: INFO: Pod "webserver-deployment-d9f79cb5-wdghz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wdghz webserver-deployment-d9f79cb5- deployment-2055  8ee8801f-55ad-4664-9c7f-638c31c0bec3 124259 0 2024-01-23 13:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6a65fb86f7b807f8e293aae5b20546cd9de68e4c8470fc6e818f69606f35caf9 cni.projectcalico.org/podIP:10.233.87.123/32 cni.projectcalico.org/podIPs:10.233.87.123/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418d8df 0xc00418d910}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-23 13:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gbkrg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gbkrg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:,StartTime:2024-01-23 13:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 23 13:59:00.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2055" for this suite. 01/23/24 13:59:00.214
------------------------------
• [SLOW TEST] [10.087 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:58:50.136
    Jan 23 13:58:50.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename deployment 01/23/24 13:58:50.137
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:58:50.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:58:50.145
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan 23 13:58:50.147: INFO: Creating deployment "webserver-deployment"
    Jan 23 13:58:50.151: INFO: Waiting for observed generation 1
    Jan 23 13:58:52.155: INFO: Waiting for all required pods to come up
    Jan 23 13:58:52.157: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/23/24 13:58:52.157
    Jan 23 13:58:52.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4hwjp" in namespace "deployment-2055" to be "running"
    Jan 23 13:58:52.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-ckb4x" in namespace "deployment-2055" to be "running"
    Jan 23 13:58:52.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-26t8h" in namespace "deployment-2055" to be "running"
    Jan 23 13:58:52.158: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jncwv" in namespace "deployment-2055" to be "running"
    Jan 23 13:58:52.158: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2wljz" in namespace "deployment-2055" to be "running"
    Jan 23 13:58:52.158: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-crh8r" in namespace "deployment-2055" to be "running"
    Jan 23 13:58:52.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-sq4xv" in namespace "deployment-2055" to be "running"
    Jan 23 13:58:52.158: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-knwbb" in namespace "deployment-2055" to be "running"
    Jan 23 13:58:52.157: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-859cv" in namespace "deployment-2055" to be "running"
    Jan 23 13:58:52.158: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-nkmkc" in namespace "deployment-2055" to be "running"
    Jan 23 13:58:52.159: INFO: Pod "webserver-deployment-7f5969cbc7-sq4xv": Phase="Pending", Reason="", readiness=false. Elapsed: 1.558445ms
    Jan 23 13:58:52.159: INFO: Pod "webserver-deployment-7f5969cbc7-ckb4x": Phase="Pending", Reason="", readiness=false. Elapsed: 1.716113ms
    Jan 23 13:58:52.159: INFO: Pod "webserver-deployment-7f5969cbc7-crh8r": Phase="Pending", Reason="", readiness=false. Elapsed: 1.582933ms
    Jan 23 13:58:52.160: INFO: Pod "webserver-deployment-7f5969cbc7-26t8h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.479044ms
    Jan 23 13:58:52.160: INFO: Pod "webserver-deployment-7f5969cbc7-4hwjp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.853388ms
    Jan 23 13:58:52.161: INFO: Pod "webserver-deployment-7f5969cbc7-859cv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.978298ms
    Jan 23 13:58:52.161: INFO: Pod "webserver-deployment-7f5969cbc7-knwbb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033667ms
    Jan 23 13:58:52.161: INFO: Pod "webserver-deployment-7f5969cbc7-jncwv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.12882ms
    Jan 23 13:58:52.161: INFO: Pod "webserver-deployment-7f5969cbc7-2wljz": Phase="Pending", Reason="", readiness=false. Elapsed: 3.136019ms
    Jan 23 13:58:52.161: INFO: Pod "webserver-deployment-7f5969cbc7-nkmkc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.068391ms
    Jan 23 13:58:54.163: INFO: Pod "webserver-deployment-7f5969cbc7-ckb4x": Phase="Running", Reason="", readiness=true. Elapsed: 2.005992011s
    Jan 23 13:58:54.163: INFO: Pod "webserver-deployment-7f5969cbc7-ckb4x" satisfied condition "running"
    Jan 23 13:58:54.163: INFO: Pod "webserver-deployment-7f5969cbc7-crh8r": Phase="Running", Reason="", readiness=true. Elapsed: 2.005907092s
    Jan 23 13:58:54.163: INFO: Pod "webserver-deployment-7f5969cbc7-crh8r" satisfied condition "running"
    Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-jncwv": Phase="Running", Reason="", readiness=true. Elapsed: 2.005973487s
    Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-jncwv" satisfied condition "running"
    Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-sq4xv": Phase="Running", Reason="", readiness=true. Elapsed: 2.005941588s
    Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-sq4xv" satisfied condition "running"
    Jan 23 13:58:54.163: INFO: Pod "webserver-deployment-7f5969cbc7-knwbb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005850999s
    Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-knwbb" satisfied condition "running"
    Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-nkmkc": Phase="Running", Reason="", readiness=true. Elapsed: 2.005950932s
    Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-nkmkc" satisfied condition "running"
    Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-26t8h": Phase="Running", Reason="", readiness=true. Elapsed: 2.006148759s
    Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-2wljz": Phase="Running", Reason="", readiness=true. Elapsed: 2.006100654s
    Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-26t8h" satisfied condition "running"
    Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-2wljz" satisfied condition "running"
    Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-4hwjp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006208369s
    Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-859cv": Phase="Running", Reason="", readiness=true. Elapsed: 2.006092768s
    Jan 23 13:58:54.164: INFO: Pod "webserver-deployment-7f5969cbc7-859cv" satisfied condition "running"
    Jan 23 13:58:56.164: INFO: Pod "webserver-deployment-7f5969cbc7-4hwjp": Phase="Running", Reason="", readiness=true. Elapsed: 4.006366143s
    Jan 23 13:58:56.164: INFO: Pod "webserver-deployment-7f5969cbc7-4hwjp" satisfied condition "running"
    Jan 23 13:58:56.164: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan 23 13:58:56.167: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan 23 13:58:56.171: INFO: Updating deployment webserver-deployment
    Jan 23 13:58:56.171: INFO: Waiting for observed generation 2
    Jan 23 13:58:58.176: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan 23 13:58:58.178: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan 23 13:58:58.179: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 23 13:58:58.183: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan 23 13:58:58.183: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan 23 13:58:58.184: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 23 13:58:58.186: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan 23 13:58:58.186: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan 23 13:58:58.191: INFO: Updating deployment webserver-deployment
    Jan 23 13:58:58.191: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan 23 13:58:58.194: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan 23 13:58:58.196: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 23 13:59:00.201: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-2055  64f6c2d2-07dc-4301-a59f-40dfaa37d503 124216 3 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043db098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-01-23 13:58:58 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2024-01-23 13:58:58 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan 23 13:59:00.202: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-2055  44e04e89-8d46-47b9-8156-efa62cf108ac 124215 3 2024-01-23 13:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 64f6c2d2-07dc-4301-a59f-40dfaa37d503 0xc006a2f937 0xc006a2f938}] [] [{kube-controller-manager Update apps/v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"64f6c2d2-07dc-4301-a59f-40dfaa37d503\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a2f9d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 23 13:59:00.202: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan 23 13:59:00.202: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-2055  581b7b3d-c9d1-4e8b-9c4b-485723a42407 124196 3 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 64f6c2d2-07dc-4301-a59f-40dfaa37d503 0xc006a2f847 0xc006a2f848}] [] [{kube-controller-manager Update apps/v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"64f6c2d2-07dc-4301-a59f-40dfaa37d503\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a2f8d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan 23 13:59:00.206: INFO: Pod "webserver-deployment-7f5969cbc7-26t8h" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-26t8h webserver-deployment-7f5969cbc7- deployment-2055  d005f40d-b4d0-4783-adc6-46395743e52d 124028 0 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6a50513329242c852dfa56139ba1d8b05f1ec8d6e7c65d3faa66e48351db3ad6 cni.projectcalico.org/podIP:10.233.87.117/32 cni.projectcalico.org/podIPs:10.233.87.117/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc006a2feb7 0xc006a2feb8}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:58:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:58:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2s8gp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2s8gp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.117,StartTime:2024-01-23 13:58:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:58:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c56d5ceb28b88df277e5ea06be1efb529a5c88a23a73b7f354d31620611721a5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.206: INFO: Pod "webserver-deployment-7f5969cbc7-2wljz" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2wljz webserver-deployment-7f5969cbc7- deployment-2055  0d0d40f7-5637-4805-9f80-5479fa9f348d 123988 0 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0a09bd0a318d39f9f8638ff89ddd30e3b0f58c84a5e47ec1a6c8fe50f56ef87b cni.projectcalico.org/podIP:10.233.75.61/32 cni.projectcalico.org/podIPs:10.233.75.61/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc0041420d7 0xc0041420d8}] [] [{calico Update v1 2024-01-23 13:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-23 13:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qrqz9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qrqz9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:10.233.75.61,StartTime:2024-01-23 13:58:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:58:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0b3c7d3570df99e8875bb1082c8da8ab3a63a857c740236831373ccaab38e13b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.206: INFO: Pod "webserver-deployment-7f5969cbc7-4hwjp" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4hwjp webserver-deployment-7f5969cbc7- deployment-2055  855344c6-180a-44fa-9c5d-d57b2f3233e4 124031 0 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:fac65a495d30fa42b9146f41491e11e3bb3c808b5bb8dc3608249a3fcbd856e5 cni.projectcalico.org/podIP:10.233.87.119/32 cni.projectcalico.org/podIPs:10.233.87.119/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc0041422e7 0xc0041422e8}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:58:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:58:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w995r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w995r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.119,StartTime:2024-01-23 13:58:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:58:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2e7c20cf5273418393d5aa9522d5cced0dfa3875cb76a048f2ceeac386211a1a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.206: INFO: Pod "webserver-deployment-7f5969cbc7-5d274" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5d274 webserver-deployment-7f5969cbc7- deployment-2055  c77596b2-af80-481c-86e5-ca3ed67059e1 124288 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc0041424e7 0xc0041424e8}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:59:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lfxvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lfxvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.206: INFO: Pod "webserver-deployment-7f5969cbc7-b9ww6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-b9ww6 webserver-deployment-7f5969cbc7- deployment-2055  f3fb8456-a8cc-4ba5-9056-8a0175bbc207 124189 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc0041426a7 0xc0041426a8}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-65vqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-65vqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.206: INFO: Pod "webserver-deployment-7f5969cbc7-bbqc7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bbqc7 webserver-deployment-7f5969cbc7- deployment-2055  9ba000fb-1110-411c-95c0-45554e410d4e 124182 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004142800 0xc004142801}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wkjdr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wkjdr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.207: INFO: Pod "webserver-deployment-7f5969cbc7-bmqsk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bmqsk webserver-deployment-7f5969cbc7- deployment-2055  c1ba7053-2390-44a0-985e-49a4b865c98e 124238 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004142950 0xc004142951}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gb646,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gb646,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.207: INFO: Pod "webserver-deployment-7f5969cbc7-ckb4x" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ckb4x webserver-deployment-7f5969cbc7- deployment-2055  2179298e-42a3-43e8-ab6a-d438bac720ea 123985 0 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4cd52a770a4bb02883bb0454d9db2eb94f30bd72e7dcdc01f45b87bad27d9460 cni.projectcalico.org/podIP:10.233.75.74/32 cni.projectcalico.org/podIPs:10.233.75.74/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004142b27 0xc004142b28}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:58:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:58:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-chdh7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-chdh7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:10.233.75.74,StartTime:2024-01-23 13:58:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:58:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a8f2aaf735fb09a536cf26864480d454a8696b7fed59551f8c6af6925119b740,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.207: INFO: Pod "webserver-deployment-7f5969cbc7-crh8r" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-crh8r webserver-deployment-7f5969cbc7- deployment-2055  d527794c-8cbc-4ee5-bef1-e05d2e383dbf 124025 0 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:409ed2c62ac0140a735da5cb17083c8871912b4be6dbeab1571f68ffbf8dd567 cni.projectcalico.org/podIP:10.233.87.111/32 cni.projectcalico.org/podIPs:10.233.87.111/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004142d47 0xc004142d48}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:58:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:58:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2wk4x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2wk4x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.111,StartTime:2024-01-23 13:58:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:58:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2542cae869e647bcd73c1cf0a1880ab6138e3590896459ec3957581628411d23,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.207: INFO: Pod "webserver-deployment-7f5969cbc7-j8gnk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-j8gnk webserver-deployment-7f5969cbc7- deployment-2055  6020f031-de79-446d-9500-31e4e4d65108 124271 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:df158b4e80672c4994441bd4b278b8209ebda10e97c804be16a616d561021174 cni.projectcalico.org/podIP:10.233.87.121/32 cni.projectcalico.org/podIPs:10.233.87.121/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004142f67 0xc004142f68}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-23 13:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wpwth,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wpwth,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.207: INFO: Pod "webserver-deployment-7f5969cbc7-jncwv" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jncwv webserver-deployment-7f5969cbc7- deployment-2055  0d2b9720-4f00-4493-9aae-1743a1901805 123982 0 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3a1a0c381074d942c59ade21a65ad2c4cf51863919c3118d9014c89368e7adcd cni.projectcalico.org/podIP:10.233.75.49/32 cni.projectcalico.org/podIPs:10.233.75.49/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004143167 0xc004143168}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:58:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:58:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sjmpk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sjmpk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:10.233.75.49,StartTime:2024-01-23 13:58:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:58:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://414fe9d1225a10f99338d6a738aa1592ae0b0a7e512f6f6fd0d0d845d257fd57,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.207: INFO: Pod "webserver-deployment-7f5969cbc7-knwbb" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-knwbb webserver-deployment-7f5969cbc7- deployment-2055  1c6c72ff-1594-49de-b599-21cafa5aa2d8 123995 0 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:52969238cc3d8dc772fb04321ebe8a9afb85d003477b918af514cee3e52776c1 cni.projectcalico.org/podIP:10.233.75.76/32 cni.projectcalico.org/podIPs:10.233.75.76/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004143387 0xc004143388}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:58:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:58:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6twg8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6twg8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:10.233.75.76,StartTime:2024-01-23 13:58:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:58:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9c67f16fe2903bcf32763006bd8b5967883005e320ca291b638cb66c2cfe569e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.207: INFO: Pod "webserver-deployment-7f5969cbc7-ksqcv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ksqcv webserver-deployment-7f5969cbc7- deployment-2055  13bba593-b83e-4da0-98e0-73335eb8bec5 124186 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004143587 0xc004143588}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cgs6g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cgs6g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.208: INFO: Pod "webserver-deployment-7f5969cbc7-nkmkc" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nkmkc webserver-deployment-7f5969cbc7- deployment-2055  56c4a3a9-04b6-4d6f-b800-f839226a049a 123992 0 2024-01-23 13:58:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1c06915074b25d10413af784e5264ded79f246c018de28c25ab2b69cce1672a7 cni.projectcalico.org/podIP:10.233.87.115/32 cni.projectcalico.org/podIPs:10.233.87.115/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004143767 0xc004143768}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:58:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:58:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dffm8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dffm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.115,StartTime:2024-01-23 13:58:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:58:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f54e711fa7b026c5050114aa25b09807de9aa67b2d3ce7c4a9cd011f781b0c93,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.208: INFO: Pod "webserver-deployment-7f5969cbc7-pw58g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pw58g webserver-deployment-7f5969cbc7- deployment-2055  23232e9d-0d6c-4f06-8796-ed73332ceedc 124195 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004143967 0xc004143968}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kmqmw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kmqmw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.208: INFO: Pod "webserver-deployment-7f5969cbc7-v6jrw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-v6jrw webserver-deployment-7f5969cbc7- deployment-2055  2b154a3c-6ff8-4eae-ab81-3bbc0f8b97aa 124237 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6fd94485661fe680ff848ccc12098398ebe2ed4541a25d46b28db6954bba34d7 cni.projectcalico.org/podIP:10.233.75.44/32 cni.projectcalico.org/podIPs:10.233.75.44/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004143b47 0xc004143b48}] [] [{calico Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5zfv7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5zfv7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.208: INFO: Pod "webserver-deployment-7f5969cbc7-vcgnh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vcgnh webserver-deployment-7f5969cbc7- deployment-2055  1448e0cb-83b6-4935-bd94-2c0b053324a5 124286 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3e8865ebba84526513de0e46e8388bd8c2ca34b25d65de1adc999b4e40883d5b cni.projectcalico.org/podIP:10.233.75.78/32 cni.projectcalico.org/podIPs:10.233.75.78/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004143cd0 0xc004143cd1}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v52bp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v52bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.208: INFO: Pod "webserver-deployment-7f5969cbc7-wjk9m" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wjk9m webserver-deployment-7f5969cbc7- deployment-2055  ca5b1256-f396-429f-9827-be60b909468f 124185 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004143e40 0xc004143e41}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mtfqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mtfqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.208: INFO: Pod "webserver-deployment-7f5969cbc7-wnp8s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wnp8s webserver-deployment-7f5969cbc7- deployment-2055  0b991c61-9b5b-4f90-a80e-d62f1427c2cf 124181 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc004143f90 0xc004143f91}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8x8ns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8x8ns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.208: INFO: Pod "webserver-deployment-7f5969cbc7-zv7vw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zv7vw webserver-deployment-7f5969cbc7- deployment-2055  06b47c8c-efc0-4679-a6bb-22ca9879a555 124228 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5c14e64081a3f11f4a052fa5f807c03a64ad8e35d444b91e241d039857a1dff7 cni.projectcalico.org/podIP:10.233.75.53/32 cni.projectcalico.org/podIPs:10.233.75.53/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 581b7b3d-c9d1-4e8b-9c4b-485723a42407 0xc00418c167 0xc00418c168}] [] [{calico Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"581b7b3d-c9d1-4e8b-9c4b-485723a42407\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-js8qr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-js8qr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.208: INFO: Pod "webserver-deployment-d9f79cb5-5b6ch" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5b6ch webserver-deployment-d9f79cb5- deployment-2055  c9915fb7-cb8c-4642-b465-78995dd44259 124190 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418c337 0xc00418c338}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wzgkx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wzgkx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.209: INFO: Pod "webserver-deployment-d9f79cb5-92h5m" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-92h5m webserver-deployment-d9f79cb5- deployment-2055  5bc27671-8d3e-417e-8b32-d879dd0c414f 124252 0 2024-01-23 13:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3dbc4fb47ca8d6f2d5572144df2805c32bfecb4386331723be8dff36b6a748ba cni.projectcalico.org/podIP:10.233.87.120/32 cni.projectcalico.org/podIPs:10.233.87.120/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418c48f 0xc00418c4c0}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-23 13:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l9lfj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l9lfj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:,StartTime:2024-01-23 13:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.209: INFO: Pod "webserver-deployment-d9f79cb5-cjkp7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-cjkp7 webserver-deployment-d9f79cb5- deployment-2055  98af03bc-72db-4860-9cff-b5c5192a9e19 124110 0 2024-01-23 13:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:52c39b9f3883e36a8a2492aa70a8ea4bb697dda9d09d24b8d253ec46b81e8716 cni.projectcalico.org/podIP:10.233.75.56/32 cni.projectcalico.org/podIPs:10.233.75.56/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418c6d7 0xc00418c6d8}] [] [{calico Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c4pgn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c4pgn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:,StartTime:2024-01-23 13:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.209: INFO: Pod "webserver-deployment-d9f79cb5-flg5z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-flg5z webserver-deployment-d9f79cb5- deployment-2055  bbf7b87f-8b09-46a6-83f6-1d14c534a7b1 124177 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418c8c7 0xc00418c8c8}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qrhgq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qrhgq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.209: INFO: Pod "webserver-deployment-d9f79cb5-jw72s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jw72s webserver-deployment-d9f79cb5- deployment-2055  35d0aca8-1e5a-4a72-a745-51a1bd8d1950 124107 0 2024-01-23 13:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6d09de2934b923f59061024e78eacaa43255fa5f1400a49973fb6db334061ac4 cni.projectcalico.org/podIP:10.233.75.70/32 cni.projectcalico.org/podIPs:10.233.75.70/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418ca1f 0xc00418ca50}] [] [{calico Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rhqvg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rhqvg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:,StartTime:2024-01-23 13:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.209: INFO: Pod "webserver-deployment-d9f79cb5-m5hgh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-m5hgh webserver-deployment-d9f79cb5- deployment-2055  23e8f594-c9fe-4e10-b22f-a81c319472c2 124260 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:fa7627779894765291a9ff55c976588fd77cb535ee0364f54a29a75de8e72b85 cni.projectcalico.org/podIP:10.233.75.42/32 cni.projectcalico.org/podIPs:10.233.75.42/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418cc67 0xc00418cc68}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-23 13:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4jwzk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4jwzk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.209: INFO: Pod "webserver-deployment-d9f79cb5-p2b2f" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-p2b2f webserver-deployment-d9f79cb5- deployment-2055  6d84fdae-4529-4460-800e-73dd14866dbe 124268 0 2024-01-23 13:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2d4e75af42b2b4a4e473651153fae75262d16d0f6f17b4e77f87b254e4e5fa2c cni.projectcalico.org/podIP:10.233.87.122/32 cni.projectcalico.org/podIPs:10.233.87.122/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418ce87 0xc00418ce88}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-23 13:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ksn87,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ksn87,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:,StartTime:2024-01-23 13:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.209: INFO: Pod "webserver-deployment-d9f79cb5-pvgf2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pvgf2 webserver-deployment-d9f79cb5- deployment-2055  650e0351-7410-4c0c-8664-43f787f023f4 124276 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:5df0a9f5ed83f83999273d65c407c117b4468ff8afe697ebd1da55c8cc225b54 cni.projectcalico.org/podIP:10.233.87.124/32 cni.projectcalico.org/podIPs:10.233.87.124/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418d0a7 0xc00418d0a8}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-23 13:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-frhgl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-frhgl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.209: INFO: Pod "webserver-deployment-d9f79cb5-pxmfp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pxmfp webserver-deployment-d9f79cb5- deployment-2055  779e9cef-442a-4c9d-a471-c23604f751dd 124267 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ebbaa1a8267effac88c5a7ea29040adc598e27a71172b80aa1ce8d3102b71cb0 cni.projectcalico.org/podIP:10.233.75.43/32 cni.projectcalico.org/podIPs:10.233.75.43/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418d2c7 0xc00418d2c8}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-23 13:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pkmzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pkmzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:,StartTime:2024-01-23 13:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.209: INFO: Pod "webserver-deployment-d9f79cb5-sslsp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sslsp webserver-deployment-d9f79cb5- deployment-2055  7851a8a5-9f4e-42dc-a79a-6c65b5217a28 124173 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418d4c7 0xc00418d4c8}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8spp2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8spp2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.210: INFO: Pod "webserver-deployment-d9f79cb5-t7hh4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-t7hh4 webserver-deployment-d9f79cb5- deployment-2055  ff133593-7029-486c-9fd5-bada997addb7 124171 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418d61f 0xc00418d630}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7crpb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7crpb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.210: INFO: Pod "webserver-deployment-d9f79cb5-t94zh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-t94zh webserver-deployment-d9f79cb5- deployment-2055  5b0e195d-7242-4398-81e5-17fa475d6e86 124172 0 2024-01-23 13:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418d77f 0xc00418d790}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wz4hf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wz4hf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 13:59:00.210: INFO: Pod "webserver-deployment-d9f79cb5-wdghz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wdghz webserver-deployment-d9f79cb5- deployment-2055  8ee8801f-55ad-4664-9c7f-638c31c0bec3 124259 0 2024-01-23 13:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6a65fb86f7b807f8e293aae5b20546cd9de68e4c8470fc6e818f69606f35caf9 cni.projectcalico.org/podIP:10.233.87.123/32 cni.projectcalico.org/podIPs:10.233.87.123/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 44e04e89-8d46-47b9-8156-efa62cf108ac 0xc00418d8df 0xc00418d910}] [] [{kube-controller-manager Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44e04e89-8d46-47b9-8156-efa62cf108ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:58:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-23 13:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gbkrg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gbkrg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:58:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:,StartTime:2024-01-23 13:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:59:00.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2055" for this suite. 01/23/24 13:59:00.214
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:59:00.225
Jan 23 13:59:00.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename pods 01/23/24 13:59:00.226
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:59:00.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:59:00.235
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Jan 23 13:59:00.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: creating the pod 01/23/24 13:59:00.24
STEP: submitting the pod to kubernetes 01/23/24 13:59:00.24
Jan 23 13:59:00.254: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca" in namespace "pods-5452" to be "running and ready"
Jan 23 13:59:00.260: INFO: Pod "pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca": Phase="Pending", Reason="", readiness=false. Elapsed: 5.268685ms
Jan 23 13:59:00.260: INFO: The phase of Pod pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca is Pending, waiting for it to be Running (with Ready = true)
Jan 23 13:59:02.263: INFO: Pod "pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008823881s
Jan 23 13:59:02.263: INFO: The phase of Pod pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca is Pending, waiting for it to be Running (with Ready = true)
Jan 23 13:59:04.262: INFO: Pod "pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007792373s
Jan 23 13:59:04.262: INFO: The phase of Pod pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca is Pending, waiting for it to be Running (with Ready = true)
Jan 23 13:59:06.262: INFO: Pod "pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca": Phase="Running", Reason="", readiness=true. Elapsed: 6.007462315s
Jan 23 13:59:06.262: INFO: The phase of Pod pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca is Running (Ready = true)
Jan 23 13:59:06.262: INFO: Pod "pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 23 13:59:08.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5452" for this suite. 01/23/24 13:59:08.75
------------------------------
• [SLOW TEST] [8.529 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:59:00.225
    Jan 23 13:59:00.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename pods 01/23/24 13:59:00.226
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:59:00.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:59:00.235
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Jan 23 13:59:00.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: creating the pod 01/23/24 13:59:00.24
    STEP: submitting the pod to kubernetes 01/23/24 13:59:00.24
    Jan 23 13:59:00.254: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca" in namespace "pods-5452" to be "running and ready"
    Jan 23 13:59:00.260: INFO: Pod "pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca": Phase="Pending", Reason="", readiness=false. Elapsed: 5.268685ms
    Jan 23 13:59:00.260: INFO: The phase of Pod pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 13:59:02.263: INFO: Pod "pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008823881s
    Jan 23 13:59:02.263: INFO: The phase of Pod pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 13:59:04.262: INFO: Pod "pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007792373s
    Jan 23 13:59:04.262: INFO: The phase of Pod pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 13:59:06.262: INFO: Pod "pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca": Phase="Running", Reason="", readiness=true. Elapsed: 6.007462315s
    Jan 23 13:59:06.262: INFO: The phase of Pod pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca is Running (Ready = true)
    Jan 23 13:59:06.262: INFO: Pod "pod-exec-websocket-77e40b6c-7777-4663-b856-8da8d6c774ca" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:59:08.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5452" for this suite. 01/23/24 13:59:08.75
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:59:08.753
Jan 23 13:59:08.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename daemonsets 01/23/24 13:59:08.754
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:59:08.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:59:08.764
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
STEP: Creating a simple DaemonSet "daemon-set" 01/23/24 13:59:08.774
STEP: Check that daemon pods launch on every node of the cluster. 01/23/24 13:59:08.777
Jan 23 13:59:08.780: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 13:59:08.781: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 13:59:08.781: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 13:59:09.785: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 13:59:09.786: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 13:59:09.786: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 13:59:10.784: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 13:59:10.786: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 13:59:10.786: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 13:59:11.785: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 13:59:11.787: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 13:59:11.787: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 13:59:12.784: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 13:59:12.786: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 23 13:59:12.786: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 13:59:13.784: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 13:59:13.785: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 23 13:59:13.785: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 13:59:14.785: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 13:59:14.787: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 23 13:59:14.787: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 13:59:15.784: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 13:59:15.786: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 23 13:59:15.786: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/23/24 13:59:15.787
Jan 23 13:59:15.798: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 13:59:15.801: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 23 13:59:15.801: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 13:59:16.804: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 13:59:16.805: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 23 13:59:16.806: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 13:59:17.804: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 13:59:17.806: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 23 13:59:17.806: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/23/24 13:59:17.806
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 01/23/24 13:59:17.809
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3722, will wait for the garbage collector to delete the pods 01/23/24 13:59:17.809
Jan 23 13:59:17.865: INFO: Deleting DaemonSet.extensions daemon-set took: 4.439027ms
Jan 23 13:59:17.966: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.935298ms
Jan 23 13:59:20.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 13:59:20.268: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 23 13:59:20.270: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"124836"},"items":null}

Jan 23 13:59:20.271: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"124836"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:59:20.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3722" for this suite. 01/23/24 13:59:20.279
------------------------------
• [SLOW TEST] [11.529 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:59:08.753
    Jan 23 13:59:08.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename daemonsets 01/23/24 13:59:08.754
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:59:08.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:59:08.764
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:305
    STEP: Creating a simple DaemonSet "daemon-set" 01/23/24 13:59:08.774
    STEP: Check that daemon pods launch on every node of the cluster. 01/23/24 13:59:08.777
    Jan 23 13:59:08.780: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 13:59:08.781: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 13:59:08.781: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 13:59:09.785: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 13:59:09.786: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 13:59:09.786: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 13:59:10.784: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 13:59:10.786: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 13:59:10.786: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 13:59:11.785: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 13:59:11.787: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 13:59:11.787: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 13:59:12.784: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 13:59:12.786: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 23 13:59:12.786: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 13:59:13.784: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 13:59:13.785: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 23 13:59:13.785: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 13:59:14.785: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 13:59:14.787: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 23 13:59:14.787: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 13:59:15.784: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 13:59:15.786: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 23 13:59:15.786: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/23/24 13:59:15.787
    Jan 23 13:59:15.798: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 13:59:15.801: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 23 13:59:15.801: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 13:59:16.804: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 13:59:16.805: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 23 13:59:16.806: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 13:59:17.804: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 13:59:17.806: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 23 13:59:17.806: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/23/24 13:59:17.806
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 01/23/24 13:59:17.809
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3722, will wait for the garbage collector to delete the pods 01/23/24 13:59:17.809
    Jan 23 13:59:17.865: INFO: Deleting DaemonSet.extensions daemon-set took: 4.439027ms
    Jan 23 13:59:17.966: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.935298ms
    Jan 23 13:59:20.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 13:59:20.268: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 23 13:59:20.270: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"124836"},"items":null}

    Jan 23 13:59:20.271: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"124836"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:59:20.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3722" for this suite. 01/23/24 13:59:20.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:59:20.283
Jan 23 13:59:20.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename deployment 01/23/24 13:59:20.284
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:59:20.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:59:20.294
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/23/24 13:59:20.301
STEP: waiting for Deployment to be created 01/23/24 13:59:20.305
STEP: waiting for all Replicas to be Ready 01/23/24 13:59:20.307
Jan 23 13:59:20.308: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 23 13:59:20.308: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 23 13:59:20.327: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 23 13:59:20.327: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 23 13:59:20.331: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 23 13:59:20.331: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 23 13:59:20.357: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 23 13:59:20.357: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 23 13:59:21.970: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 23 13:59:21.970: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 23 13:59:22.481: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/23/24 13:59:22.481
W0123 13:59:22.486358      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 23 13:59:22.487: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/23/24 13:59:22.487
Jan 23 13:59:22.488: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0
Jan 23 13:59:22.488: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0
Jan 23 13:59:22.488: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0
Jan 23 13:59:22.488: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0
Jan 23 13:59:22.488: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0
Jan 23 13:59:22.488: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0
Jan 23 13:59:22.489: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0
Jan 23 13:59:22.489: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0
Jan 23 13:59:22.489: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
Jan 23 13:59:22.489: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
Jan 23 13:59:22.489: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
Jan 23 13:59:22.489: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
Jan 23 13:59:22.489: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
Jan 23 13:59:22.489: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
Jan 23 13:59:22.496: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
Jan 23 13:59:22.496: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
Jan 23 13:59:22.508: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
Jan 23 13:59:22.508: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
Jan 23 13:59:22.514: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
Jan 23 13:59:22.514: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
Jan 23 13:59:22.529: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
Jan 23 13:59:22.529: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
Jan 23 13:59:23.980: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
Jan 23 13:59:23.980: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
Jan 23 13:59:23.994: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
STEP: listing Deployments 01/23/24 13:59:23.995
Jan 23 13:59:23.998: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/23/24 13:59:23.998
Jan 23 13:59:24.008: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/23/24 13:59:24.008
Jan 23 13:59:24.012: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 23 13:59:24.015: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 23 13:59:24.026: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 23 13:59:24.041: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 23 13:59:24.046: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 23 13:59:24.049: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 23 13:59:26.001: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 23 13:59:26.027: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 23 13:59:26.045: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 23 13:59:26.045: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 23 13:59:26.048: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 23 13:59:28.531: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/23/24 13:59:28.541
STEP: fetching the DeploymentStatus 01/23/24 13:59:28.545
Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 3
STEP: deleting the Deployment 01/23/24 13:59:28.547
Jan 23 13:59:28.551: INFO: observed event type MODIFIED
Jan 23 13:59:28.551: INFO: observed event type MODIFIED
Jan 23 13:59:28.551: INFO: observed event type MODIFIED
Jan 23 13:59:28.552: INFO: observed event type MODIFIED
Jan 23 13:59:28.552: INFO: observed event type MODIFIED
Jan 23 13:59:28.552: INFO: observed event type MODIFIED
Jan 23 13:59:28.552: INFO: observed event type MODIFIED
Jan 23 13:59:28.552: INFO: observed event type MODIFIED
Jan 23 13:59:28.552: INFO: observed event type MODIFIED
Jan 23 13:59:28.552: INFO: observed event type MODIFIED
Jan 23 13:59:28.552: INFO: observed event type MODIFIED
Jan 23 13:59:28.552: INFO: observed event type MODIFIED
Jan 23 13:59:28.552: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 23 13:59:28.560: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 23 13:59:28.567: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-450  b29079de-43b0-48a3-b348-175f11581c4b 125069 2 2024-01-23 13:59:24 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 7b814053-6807-4670-847d-c4261ccbe134 0xc004c8fb57 0xc004c8fb58}] [] [{kube-controller-manager Update apps/v1 2024-01-23 13:59:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b814053-6807-4670-847d-c4261ccbe134\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 13:59:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c8fbe0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 23 13:59:28.569: INFO: pod: "test-deployment-7b7876f9d6-z2cdn":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-z2cdn test-deployment-7b7876f9d6- deployment-450  86fe8d49-bcd0-4ee1-943b-fae0b7aaab8f 125068 0 2024-01-23 13:59:26 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:2137cfb005c32631c100bfb71f69311ac9115798f138d99ca82d3cd2ec413fce cni.projectcalico.org/podIP:10.233.75.39/32 cni.projectcalico.org/podIPs:10.233.75.39/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 b29079de-43b0-48a3-b348-175f11581c4b 0xc00506e097 0xc00506e098}] [] [{calico Update v1 2024-01-23 13:59:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-23 13:59:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b29079de-43b0-48a3-b348-175f11581c4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:59:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2mcqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2mcqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:10.233.75.39,StartTime:2024-01-23 13:59:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:59:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cf0f303330d63d124a22da83ca3dcf1db02357584a4cd95e3e4ee5bc6c5dba70,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.39,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 23 13:59:28.569: INFO: pod: "test-deployment-7b7876f9d6-zvwzx":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-zvwzx test-deployment-7b7876f9d6- deployment-450  9f1724b0-986f-4f92-a09a-d0af2722ee48 125014 0 2024-01-23 13:59:24 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:34d48d8a193e82fb794f4be01885436227c6f5972bc71d3a6b8b6c5e60d57231 cni.projectcalico.org/podIP:10.233.87.135/32 cni.projectcalico.org/podIPs:10.233.87.135/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 b29079de-43b0-48a3-b348-175f11581c4b 0xc00506e2b7 0xc00506e2b8}] [] [{kube-controller-manager Update v1 2024-01-23 13:59:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b29079de-43b0-48a3-b348-175f11581c4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:59:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:59:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9pmjf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9pmjf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.135,StartTime:2024-01-23 13:59:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:59:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://afca48a78bd97132346c70e7c0fa96c69069e3ff9a87517f4e5884965132ede8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 23 13:59:28.569: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-450  cdd857a6-8fc5-47bf-8c00-eefc71ffacff 125077 4 2024-01-23 13:59:22 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 7b814053-6807-4670-847d-c4261ccbe134 0xc004c8fc47 0xc004c8fc48}] [] [{kube-controller-manager Update apps/v1 2024-01-23 13:59:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b814053-6807-4670-847d-c4261ccbe134\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 13:59:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c8fcd0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 23 13:59:28.571: INFO: pod: "test-deployment-7df74c55ff-vcj4n":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-vcj4n test-deployment-7df74c55ff- deployment-450  c1379496-acfe-45aa-8d7b-a2662621ac7c 125072 0 2024-01-23 13:59:22 +0000 UTC 2024-01-23 13:59:29 +0000 UTC 0xc004d0b168 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:40c62355ffc7a22f3b632aa068af3d157816c51f6eb6e649eefe9da7d4e52833 cni.projectcalico.org/podIP:10.233.87.136/32 cni.projectcalico.org/podIPs:10.233.87.136/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff cdd857a6-8fc5-47bf-8c00-eefc71ffacff 0xc004d0b1b7 0xc004d0b1b8}] [] [{kube-controller-manager Update v1 2024-01-23 13:59:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cdd857a6-8fc5-47bf-8c00-eefc71ffacff\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:59:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jcljf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jcljf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.136,StartTime:2024-01-23 13:59:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:59:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:hub.nova-platform.io/registry/pause:3.9,ImageID:hub.nova-platform.io/registry/pause@sha256:8d4106c88ec0bd28001e34c975d65175d994072d65341f62a8ab0754b0fafe10,ContainerID:containerd://6d9a9f411903a71cf183aa858d39d94fdbc1ee7faf6fac2e333c75fa50f7d1f1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 23 13:59:28.571: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-450  fe7a1437-3bf8-45e7-a3e5-97f8ad787d9f 124933 3 2024-01-23 13:59:20 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 7b814053-6807-4670-847d-c4261ccbe134 0xc004c8fd37 0xc004c8fd38}] [] [{kube-controller-manager Update apps/v1 2024-01-23 13:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b814053-6807-4670-847d-c4261ccbe134\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 13:59:23 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c8fdc0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 23 13:59:28.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-450" for this suite. 01/23/24 13:59:28.575
------------------------------
• [SLOW TEST] [8.295 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:59:20.283
    Jan 23 13:59:20.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename deployment 01/23/24 13:59:20.284
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:59:20.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:59:20.294
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/23/24 13:59:20.301
    STEP: waiting for Deployment to be created 01/23/24 13:59:20.305
    STEP: waiting for all Replicas to be Ready 01/23/24 13:59:20.307
    Jan 23 13:59:20.308: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 23 13:59:20.308: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 23 13:59:20.327: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 23 13:59:20.327: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 23 13:59:20.331: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 23 13:59:20.331: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 23 13:59:20.357: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 23 13:59:20.357: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 23 13:59:21.970: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 23 13:59:21.970: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 23 13:59:22.481: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/23/24 13:59:22.481
    W0123 13:59:22.486358      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 23 13:59:22.487: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/23/24 13:59:22.487
    Jan 23 13:59:22.488: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0
    Jan 23 13:59:22.488: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0
    Jan 23 13:59:22.488: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0
    Jan 23 13:59:22.488: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0
    Jan 23 13:59:22.488: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0
    Jan 23 13:59:22.488: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0
    Jan 23 13:59:22.489: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0
    Jan 23 13:59:22.489: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 0
    Jan 23 13:59:22.489: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
    Jan 23 13:59:22.489: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
    Jan 23 13:59:22.489: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
    Jan 23 13:59:22.489: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
    Jan 23 13:59:22.489: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
    Jan 23 13:59:22.489: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
    Jan 23 13:59:22.496: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
    Jan 23 13:59:22.496: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
    Jan 23 13:59:22.508: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
    Jan 23 13:59:22.508: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
    Jan 23 13:59:22.514: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
    Jan 23 13:59:22.514: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
    Jan 23 13:59:22.529: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
    Jan 23 13:59:22.529: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
    Jan 23 13:59:23.980: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
    Jan 23 13:59:23.980: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
    Jan 23 13:59:23.994: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
    STEP: listing Deployments 01/23/24 13:59:23.995
    Jan 23 13:59:23.998: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/23/24 13:59:23.998
    Jan 23 13:59:24.008: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/23/24 13:59:24.008
    Jan 23 13:59:24.012: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 23 13:59:24.015: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 23 13:59:24.026: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 23 13:59:24.041: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 23 13:59:24.046: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 23 13:59:24.049: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 23 13:59:26.001: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 23 13:59:26.027: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 23 13:59:26.045: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 23 13:59:26.045: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 23 13:59:26.048: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 23 13:59:28.531: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/23/24 13:59:28.541
    STEP: fetching the DeploymentStatus 01/23/24 13:59:28.545
    Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
    Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
    Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
    Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
    Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
    Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 1
    Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
    Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
    Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
    Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
    Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 2
    Jan 23 13:59:28.547: INFO: observed Deployment test-deployment in namespace deployment-450 with ReadyReplicas 3
    STEP: deleting the Deployment 01/23/24 13:59:28.547
    Jan 23 13:59:28.551: INFO: observed event type MODIFIED
    Jan 23 13:59:28.551: INFO: observed event type MODIFIED
    Jan 23 13:59:28.551: INFO: observed event type MODIFIED
    Jan 23 13:59:28.552: INFO: observed event type MODIFIED
    Jan 23 13:59:28.552: INFO: observed event type MODIFIED
    Jan 23 13:59:28.552: INFO: observed event type MODIFIED
    Jan 23 13:59:28.552: INFO: observed event type MODIFIED
    Jan 23 13:59:28.552: INFO: observed event type MODIFIED
    Jan 23 13:59:28.552: INFO: observed event type MODIFIED
    Jan 23 13:59:28.552: INFO: observed event type MODIFIED
    Jan 23 13:59:28.552: INFO: observed event type MODIFIED
    Jan 23 13:59:28.552: INFO: observed event type MODIFIED
    Jan 23 13:59:28.552: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 23 13:59:28.560: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan 23 13:59:28.567: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-450  b29079de-43b0-48a3-b348-175f11581c4b 125069 2 2024-01-23 13:59:24 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 7b814053-6807-4670-847d-c4261ccbe134 0xc004c8fb57 0xc004c8fb58}] [] [{kube-controller-manager Update apps/v1 2024-01-23 13:59:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b814053-6807-4670-847d-c4261ccbe134\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 13:59:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c8fbe0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan 23 13:59:28.569: INFO: pod: "test-deployment-7b7876f9d6-z2cdn":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-z2cdn test-deployment-7b7876f9d6- deployment-450  86fe8d49-bcd0-4ee1-943b-fae0b7aaab8f 125068 0 2024-01-23 13:59:26 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:2137cfb005c32631c100bfb71f69311ac9115798f138d99ca82d3cd2ec413fce cni.projectcalico.org/podIP:10.233.75.39/32 cni.projectcalico.org/podIPs:10.233.75.39/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 b29079de-43b0-48a3-b348-175f11581c4b 0xc00506e097 0xc00506e098}] [] [{calico Update v1 2024-01-23 13:59:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-23 13:59:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b29079de-43b0-48a3-b348-175f11581c4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 13:59:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2mcqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2mcqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-hohyvwot.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.40,PodIP:10.233.75.39,StartTime:2024-01-23 13:59:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:59:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cf0f303330d63d124a22da83ca3dcf1db02357584a4cd95e3e4ee5bc6c5dba70,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.39,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 23 13:59:28.569: INFO: pod: "test-deployment-7b7876f9d6-zvwzx":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-zvwzx test-deployment-7b7876f9d6- deployment-450  9f1724b0-986f-4f92-a09a-d0af2722ee48 125014 0 2024-01-23 13:59:24 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:34d48d8a193e82fb794f4be01885436227c6f5972bc71d3a6b8b6c5e60d57231 cni.projectcalico.org/podIP:10.233.87.135/32 cni.projectcalico.org/podIPs:10.233.87.135/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 b29079de-43b0-48a3-b348-175f11581c4b 0xc00506e2b7 0xc00506e2b8}] [] [{kube-controller-manager Update v1 2024-01-23 13:59:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b29079de-43b0-48a3-b348-175f11581c4b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:59:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:59:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9pmjf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9pmjf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.135,StartTime:2024-01-23 13:59:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:59:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://afca48a78bd97132346c70e7c0fa96c69069e3ff9a87517f4e5884965132ede8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 23 13:59:28.569: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-450  cdd857a6-8fc5-47bf-8c00-eefc71ffacff 125077 4 2024-01-23 13:59:22 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 7b814053-6807-4670-847d-c4261ccbe134 0xc004c8fc47 0xc004c8fc48}] [] [{kube-controller-manager Update apps/v1 2024-01-23 13:59:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b814053-6807-4670-847d-c4261ccbe134\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 13:59:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c8fcd0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan 23 13:59:28.571: INFO: pod: "test-deployment-7df74c55ff-vcj4n":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-vcj4n test-deployment-7df74c55ff- deployment-450  c1379496-acfe-45aa-8d7b-a2662621ac7c 125072 0 2024-01-23 13:59:22 +0000 UTC 2024-01-23 13:59:29 +0000 UTC 0xc004d0b168 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:40c62355ffc7a22f3b632aa068af3d157816c51f6eb6e649eefe9da7d4e52833 cni.projectcalico.org/podIP:10.233.87.136/32 cni.projectcalico.org/podIPs:10.233.87.136/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff cdd857a6-8fc5-47bf-8c00-eefc71ffacff 0xc004d0b1b7 0xc004d0b1b8}] [] [{kube-controller-manager Update v1 2024-01-23 13:59:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cdd857a6-8fc5-47bf-8c00-eefc71ffacff\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-23 13:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-23 13:59:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jcljf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jcljf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 13:59:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.136,StartTime:2024-01-23 13:59:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 13:59:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:hub.nova-platform.io/registry/pause:3.9,ImageID:hub.nova-platform.io/registry/pause@sha256:8d4106c88ec0bd28001e34c975d65175d994072d65341f62a8ab0754b0fafe10,ContainerID:containerd://6d9a9f411903a71cf183aa858d39d94fdbc1ee7faf6fac2e333c75fa50f7d1f1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 23 13:59:28.571: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-450  fe7a1437-3bf8-45e7-a3e5-97f8ad787d9f 124933 3 2024-01-23 13:59:20 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 7b814053-6807-4670-847d-c4261ccbe134 0xc004c8fd37 0xc004c8fd38}] [] [{kube-controller-manager Update apps/v1 2024-01-23 13:59:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b814053-6807-4670-847d-c4261ccbe134\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 13:59:23 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c8fdc0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:59:28.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-450" for this suite. 01/23/24 13:59:28.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:59:28.579
Jan 23 13:59:28.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 13:59:28.579
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:59:28.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:59:28.591
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-480eea5b-0049-4039-8888-f9c1bd65d796 01/23/24 13:59:28.593
STEP: Creating a pod to test consume secrets 01/23/24 13:59:28.599
Jan 23 13:59:28.610: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bd25cf68-1ee9-4017-bc8f-2162a4dd9c50" in namespace "projected-550" to be "Succeeded or Failed"
Jan 23 13:59:28.612: INFO: Pod "pod-projected-secrets-bd25cf68-1ee9-4017-bc8f-2162a4dd9c50": Phase="Pending", Reason="", readiness=false. Elapsed: 1.265951ms
Jan 23 13:59:30.615: INFO: Pod "pod-projected-secrets-bd25cf68-1ee9-4017-bc8f-2162a4dd9c50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00502485s
Jan 23 13:59:32.616: INFO: Pod "pod-projected-secrets-bd25cf68-1ee9-4017-bc8f-2162a4dd9c50": Phase="Running", Reason="", readiness=false. Elapsed: 4.005743233s
Jan 23 13:59:34.616: INFO: Pod "pod-projected-secrets-bd25cf68-1ee9-4017-bc8f-2162a4dd9c50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005354093s
STEP: Saw pod success 01/23/24 13:59:34.616
Jan 23 13:59:34.616: INFO: Pod "pod-projected-secrets-bd25cf68-1ee9-4017-bc8f-2162a4dd9c50" satisfied condition "Succeeded or Failed"
Jan 23 13:59:34.617: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-secrets-bd25cf68-1ee9-4017-bc8f-2162a4dd9c50 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/23/24 13:59:34.627
Jan 23 13:59:34.633: INFO: Waiting for pod pod-projected-secrets-bd25cf68-1ee9-4017-bc8f-2162a4dd9c50 to disappear
Jan 23 13:59:34.634: INFO: Pod pod-projected-secrets-bd25cf68-1ee9-4017-bc8f-2162a4dd9c50 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 23 13:59:34.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-550" for this suite. 01/23/24 13:59:34.636
------------------------------
• [SLOW TEST] [6.061 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:59:28.579
    Jan 23 13:59:28.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 13:59:28.579
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:59:28.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:59:28.591
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-480eea5b-0049-4039-8888-f9c1bd65d796 01/23/24 13:59:28.593
    STEP: Creating a pod to test consume secrets 01/23/24 13:59:28.599
    Jan 23 13:59:28.610: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bd25cf68-1ee9-4017-bc8f-2162a4dd9c50" in namespace "projected-550" to be "Succeeded or Failed"
    Jan 23 13:59:28.612: INFO: Pod "pod-projected-secrets-bd25cf68-1ee9-4017-bc8f-2162a4dd9c50": Phase="Pending", Reason="", readiness=false. Elapsed: 1.265951ms
    Jan 23 13:59:30.615: INFO: Pod "pod-projected-secrets-bd25cf68-1ee9-4017-bc8f-2162a4dd9c50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00502485s
    Jan 23 13:59:32.616: INFO: Pod "pod-projected-secrets-bd25cf68-1ee9-4017-bc8f-2162a4dd9c50": Phase="Running", Reason="", readiness=false. Elapsed: 4.005743233s
    Jan 23 13:59:34.616: INFO: Pod "pod-projected-secrets-bd25cf68-1ee9-4017-bc8f-2162a4dd9c50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005354093s
    STEP: Saw pod success 01/23/24 13:59:34.616
    Jan 23 13:59:34.616: INFO: Pod "pod-projected-secrets-bd25cf68-1ee9-4017-bc8f-2162a4dd9c50" satisfied condition "Succeeded or Failed"
    Jan 23 13:59:34.617: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-secrets-bd25cf68-1ee9-4017-bc8f-2162a4dd9c50 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/23/24 13:59:34.627
    Jan 23 13:59:34.633: INFO: Waiting for pod pod-projected-secrets-bd25cf68-1ee9-4017-bc8f-2162a4dd9c50 to disappear
    Jan 23 13:59:34.634: INFO: Pod pod-projected-secrets-bd25cf68-1ee9-4017-bc8f-2162a4dd9c50 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:59:34.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-550" for this suite. 01/23/24 13:59:34.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:59:34.64
Jan 23 13:59:34.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename replication-controller 01/23/24 13:59:34.64
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:59:34.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:59:34.648
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Jan 23 13:59:34.650: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/23/24 13:59:35.656
STEP: Checking rc "condition-test" has the desired failure condition set 01/23/24 13:59:35.66
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/23/24 13:59:36.666
Jan 23 13:59:36.673: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/23/24 13:59:36.673
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 23 13:59:37.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8722" for this suite. 01/23/24 13:59:37.68
------------------------------
• [3.043 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:59:34.64
    Jan 23 13:59:34.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename replication-controller 01/23/24 13:59:34.64
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:59:34.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:59:34.648
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Jan 23 13:59:34.650: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/23/24 13:59:35.656
    STEP: Checking rc "condition-test" has the desired failure condition set 01/23/24 13:59:35.66
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/23/24 13:59:36.666
    Jan 23 13:59:36.673: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/23/24 13:59:36.673
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:59:37.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8722" for this suite. 01/23/24 13:59:37.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:59:37.683
Jan 23 13:59:37.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename configmap 01/23/24 13:59:37.684
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:59:37.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:59:37.692
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 23 13:59:37.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5994" for this suite. 01/23/24 13:59:37.731
------------------------------
• [0.052 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:59:37.683
    Jan 23 13:59:37.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename configmap 01/23/24 13:59:37.684
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:59:37.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:59:37.692
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:59:37.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5994" for this suite. 01/23/24 13:59:37.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:59:37.736
Jan 23 13:59:37.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename daemonsets 01/23/24 13:59:37.736
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:59:37.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:59:37.744
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
STEP: Creating simple DaemonSet "daemon-set" 01/23/24 13:59:37.755
STEP: Check that daemon pods launch on every node of the cluster. 01/23/24 13:59:37.758
Jan 23 13:59:37.760: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 13:59:37.762: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 13:59:37.762: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 13:59:38.765: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 13:59:38.767: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 13:59:38.767: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 13:59:39.765: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 13:59:39.766: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 23 13:59:39.766: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 01/23/24 13:59:39.768
Jan 23 13:59:39.770: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/23/24 13:59:39.77
Jan 23 13:59:39.776: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/23/24 13:59:39.776
Jan 23 13:59:39.778: INFO: Observed &DaemonSet event: ADDED
Jan 23 13:59:39.778: INFO: Observed &DaemonSet event: MODIFIED
Jan 23 13:59:39.778: INFO: Observed &DaemonSet event: MODIFIED
Jan 23 13:59:39.778: INFO: Observed &DaemonSet event: MODIFIED
Jan 23 13:59:39.778: INFO: Found daemon set daemon-set in namespace daemonsets-5696 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 23 13:59:39.778: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/23/24 13:59:39.778
STEP: watching for the daemon set status to be patched 01/23/24 13:59:39.782
Jan 23 13:59:39.784: INFO: Observed &DaemonSet event: ADDED
Jan 23 13:59:39.784: INFO: Observed &DaemonSet event: MODIFIED
Jan 23 13:59:39.784: INFO: Observed &DaemonSet event: MODIFIED
Jan 23 13:59:39.784: INFO: Observed &DaemonSet event: MODIFIED
Jan 23 13:59:39.784: INFO: Observed daemon set daemon-set in namespace daemonsets-5696 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 23 13:59:39.784: INFO: Observed &DaemonSet event: MODIFIED
Jan 23 13:59:39.784: INFO: Found daemon set daemon-set in namespace daemonsets-5696 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 23 13:59:39.784: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 01/23/24 13:59:39.786
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5696, will wait for the garbage collector to delete the pods 01/23/24 13:59:39.787
Jan 23 13:59:39.842: INFO: Deleting DaemonSet.extensions daemon-set took: 2.871443ms
Jan 23 13:59:39.943: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.639918ms
Jan 23 13:59:43.646: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 13:59:43.646: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 23 13:59:43.647: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"125436"},"items":null}

Jan 23 13:59:43.648: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"125436"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:59:43.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5696" for this suite. 01/23/24 13:59:43.655
------------------------------
• [SLOW TEST] [5.922 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:59:37.736
    Jan 23 13:59:37.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename daemonsets 01/23/24 13:59:37.736
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:59:37.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:59:37.744
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:873
    STEP: Creating simple DaemonSet "daemon-set" 01/23/24 13:59:37.755
    STEP: Check that daemon pods launch on every node of the cluster. 01/23/24 13:59:37.758
    Jan 23 13:59:37.760: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 13:59:37.762: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 13:59:37.762: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 13:59:38.765: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 13:59:38.767: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 13:59:38.767: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 13:59:39.765: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 13:59:39.766: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 23 13:59:39.766: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 01/23/24 13:59:39.768
    Jan 23 13:59:39.770: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/23/24 13:59:39.77
    Jan 23 13:59:39.776: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/23/24 13:59:39.776
    Jan 23 13:59:39.778: INFO: Observed &DaemonSet event: ADDED
    Jan 23 13:59:39.778: INFO: Observed &DaemonSet event: MODIFIED
    Jan 23 13:59:39.778: INFO: Observed &DaemonSet event: MODIFIED
    Jan 23 13:59:39.778: INFO: Observed &DaemonSet event: MODIFIED
    Jan 23 13:59:39.778: INFO: Found daemon set daemon-set in namespace daemonsets-5696 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 23 13:59:39.778: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/23/24 13:59:39.778
    STEP: watching for the daemon set status to be patched 01/23/24 13:59:39.782
    Jan 23 13:59:39.784: INFO: Observed &DaemonSet event: ADDED
    Jan 23 13:59:39.784: INFO: Observed &DaemonSet event: MODIFIED
    Jan 23 13:59:39.784: INFO: Observed &DaemonSet event: MODIFIED
    Jan 23 13:59:39.784: INFO: Observed &DaemonSet event: MODIFIED
    Jan 23 13:59:39.784: INFO: Observed daemon set daemon-set in namespace daemonsets-5696 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 23 13:59:39.784: INFO: Observed &DaemonSet event: MODIFIED
    Jan 23 13:59:39.784: INFO: Found daemon set daemon-set in namespace daemonsets-5696 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan 23 13:59:39.784: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 01/23/24 13:59:39.786
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5696, will wait for the garbage collector to delete the pods 01/23/24 13:59:39.787
    Jan 23 13:59:39.842: INFO: Deleting DaemonSet.extensions daemon-set took: 2.871443ms
    Jan 23 13:59:39.943: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.639918ms
    Jan 23 13:59:43.646: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 13:59:43.646: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 23 13:59:43.647: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"125436"},"items":null}

    Jan 23 13:59:43.648: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"125436"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:59:43.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5696" for this suite. 01/23/24 13:59:43.655
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:59:43.658
Jan 23 13:59:43.658: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename webhook 01/23/24 13:59:43.659
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:59:43.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:59:43.668
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/23/24 13:59:43.676
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 13:59:44.292
STEP: Deploying the webhook pod 01/23/24 13:59:44.298
STEP: Wait for the deployment to be ready 01/23/24 13:59:44.307
Jan 23 13:59:44.310: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/23/24 13:59:46.317
STEP: Verifying the service has paired with the endpoint 01/23/24 13:59:46.322
Jan 23 13:59:47.322: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Jan 23 13:59:47.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/23/24 13:59:52.831
STEP: Creating a custom resource that should be denied by the webhook 01/23/24 13:59:52.843
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/23/24 13:59:54.852
STEP: Updating the custom resource with disallowed data should be denied 01/23/24 13:59:54.856
STEP: Deleting the custom resource should be denied 01/23/24 13:59:54.861
STEP: Remove the offending key and value from the custom resource data 01/23/24 13:59:54.864
STEP: Deleting the updated custom resource should be successful 01/23/24 13:59:54.873
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 13:59:55.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1725" for this suite. 01/23/24 13:59:55.415
STEP: Destroying namespace "webhook-1725-markers" for this suite. 01/23/24 13:59:55.418
------------------------------
• [SLOW TEST] [11.766 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:59:43.658
    Jan 23 13:59:43.658: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename webhook 01/23/24 13:59:43.659
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:59:43.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:59:43.668
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/23/24 13:59:43.676
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 13:59:44.292
    STEP: Deploying the webhook pod 01/23/24 13:59:44.298
    STEP: Wait for the deployment to be ready 01/23/24 13:59:44.307
    Jan 23 13:59:44.310: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/23/24 13:59:46.317
    STEP: Verifying the service has paired with the endpoint 01/23/24 13:59:46.322
    Jan 23 13:59:47.322: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Jan 23 13:59:47.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/23/24 13:59:52.831
    STEP: Creating a custom resource that should be denied by the webhook 01/23/24 13:59:52.843
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/23/24 13:59:54.852
    STEP: Updating the custom resource with disallowed data should be denied 01/23/24 13:59:54.856
    STEP: Deleting the custom resource should be denied 01/23/24 13:59:54.861
    STEP: Remove the offending key and value from the custom resource data 01/23/24 13:59:54.864
    STEP: Deleting the updated custom resource should be successful 01/23/24 13:59:54.873
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 13:59:55.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1725" for this suite. 01/23/24 13:59:55.415
    STEP: Destroying namespace "webhook-1725-markers" for this suite. 01/23/24 13:59:55.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 13:59:55.426
Jan 23 13:59:55.426: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename configmap 01/23/24 13:59:55.427
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:59:55.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:59:55.451
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-89725b14-6da6-439b-9fe5-eab6be19a60a 01/23/24 13:59:55.454
STEP: Creating a pod to test consume configMaps 01/23/24 13:59:55.458
Jan 23 13:59:55.480: INFO: Waiting up to 5m0s for pod "pod-configmaps-800260db-bbbb-42c3-8770-db67d36d9aa3" in namespace "configmap-7241" to be "Succeeded or Failed"
Jan 23 13:59:55.483: INFO: Pod "pod-configmaps-800260db-bbbb-42c3-8770-db67d36d9aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.511525ms
Jan 23 13:59:57.487: INFO: Pod "pod-configmaps-800260db-bbbb-42c3-8770-db67d36d9aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007096415s
Jan 23 13:59:59.486: INFO: Pod "pod-configmaps-800260db-bbbb-42c3-8770-db67d36d9aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005708324s
Jan 23 14:00:01.486: INFO: Pod "pod-configmaps-800260db-bbbb-42c3-8770-db67d36d9aa3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00606162s
STEP: Saw pod success 01/23/24 14:00:01.486
Jan 23 14:00:01.486: INFO: Pod "pod-configmaps-800260db-bbbb-42c3-8770-db67d36d9aa3" satisfied condition "Succeeded or Failed"
Jan 23 14:00:01.488: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-800260db-bbbb-42c3-8770-db67d36d9aa3 container agnhost-container: <nil>
STEP: delete the pod 01/23/24 14:00:01.493
Jan 23 14:00:01.500: INFO: Waiting for pod pod-configmaps-800260db-bbbb-42c3-8770-db67d36d9aa3 to disappear
Jan 23 14:00:01.502: INFO: Pod pod-configmaps-800260db-bbbb-42c3-8770-db67d36d9aa3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 23 14:00:01.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7241" for this suite. 01/23/24 14:00:01.504
------------------------------
• [SLOW TEST] [6.081 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 13:59:55.426
    Jan 23 13:59:55.426: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename configmap 01/23/24 13:59:55.427
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 13:59:55.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 13:59:55.451
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-89725b14-6da6-439b-9fe5-eab6be19a60a 01/23/24 13:59:55.454
    STEP: Creating a pod to test consume configMaps 01/23/24 13:59:55.458
    Jan 23 13:59:55.480: INFO: Waiting up to 5m0s for pod "pod-configmaps-800260db-bbbb-42c3-8770-db67d36d9aa3" in namespace "configmap-7241" to be "Succeeded or Failed"
    Jan 23 13:59:55.483: INFO: Pod "pod-configmaps-800260db-bbbb-42c3-8770-db67d36d9aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.511525ms
    Jan 23 13:59:57.487: INFO: Pod "pod-configmaps-800260db-bbbb-42c3-8770-db67d36d9aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007096415s
    Jan 23 13:59:59.486: INFO: Pod "pod-configmaps-800260db-bbbb-42c3-8770-db67d36d9aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005708324s
    Jan 23 14:00:01.486: INFO: Pod "pod-configmaps-800260db-bbbb-42c3-8770-db67d36d9aa3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00606162s
    STEP: Saw pod success 01/23/24 14:00:01.486
    Jan 23 14:00:01.486: INFO: Pod "pod-configmaps-800260db-bbbb-42c3-8770-db67d36d9aa3" satisfied condition "Succeeded or Failed"
    Jan 23 14:00:01.488: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-800260db-bbbb-42c3-8770-db67d36d9aa3 container agnhost-container: <nil>
    STEP: delete the pod 01/23/24 14:00:01.493
    Jan 23 14:00:01.500: INFO: Waiting for pod pod-configmaps-800260db-bbbb-42c3-8770-db67d36d9aa3 to disappear
    Jan 23 14:00:01.502: INFO: Pod pod-configmaps-800260db-bbbb-42c3-8770-db67d36d9aa3 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:00:01.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7241" for this suite. 01/23/24 14:00:01.504
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:00:01.508
Jan 23 14:00:01.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubelet-test 01/23/24 14:00:01.508
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:00:01.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:00:01.52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan 23 14:00:01.531: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsd363d03e-39f1-4374-8c0a-0502748e7d5c" in namespace "kubelet-test-2869" to be "running and ready"
Jan 23 14:00:01.533: INFO: Pod "busybox-readonly-fsd363d03e-39f1-4374-8c0a-0502748e7d5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022532ms
Jan 23 14:00:01.533: INFO: The phase of Pod busybox-readonly-fsd363d03e-39f1-4374-8c0a-0502748e7d5c is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:00:03.536: INFO: Pod "busybox-readonly-fsd363d03e-39f1-4374-8c0a-0502748e7d5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004913489s
Jan 23 14:00:03.536: INFO: The phase of Pod busybox-readonly-fsd363d03e-39f1-4374-8c0a-0502748e7d5c is Running (Ready = true)
Jan 23 14:00:03.536: INFO: Pod "busybox-readonly-fsd363d03e-39f1-4374-8c0a-0502748e7d5c" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 23 14:00:03.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2869" for this suite. 01/23/24 14:00:03.544
------------------------------
• [2.041 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:00:01.508
    Jan 23 14:00:01.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubelet-test 01/23/24 14:00:01.508
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:00:01.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:00:01.52
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan 23 14:00:01.531: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsd363d03e-39f1-4374-8c0a-0502748e7d5c" in namespace "kubelet-test-2869" to be "running and ready"
    Jan 23 14:00:01.533: INFO: Pod "busybox-readonly-fsd363d03e-39f1-4374-8c0a-0502748e7d5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022532ms
    Jan 23 14:00:01.533: INFO: The phase of Pod busybox-readonly-fsd363d03e-39f1-4374-8c0a-0502748e7d5c is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:00:03.536: INFO: Pod "busybox-readonly-fsd363d03e-39f1-4374-8c0a-0502748e7d5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004913489s
    Jan 23 14:00:03.536: INFO: The phase of Pod busybox-readonly-fsd363d03e-39f1-4374-8c0a-0502748e7d5c is Running (Ready = true)
    Jan 23 14:00:03.536: INFO: Pod "busybox-readonly-fsd363d03e-39f1-4374-8c0a-0502748e7d5c" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:00:03.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2869" for this suite. 01/23/24 14:00:03.544
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:00:03.549
Jan 23 14:00:03.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename container-runtime 01/23/24 14:00:03.549
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:00:03.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:00:03.562
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/23/24 14:00:03.579
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/23/24 14:00:23.64
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/23/24 14:00:23.641
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/23/24 14:00:23.644
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/23/24 14:00:23.644
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/23/24 14:00:23.665
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/23/24 14:00:28.687
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/23/24 14:00:30.695
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/23/24 14:00:30.698
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/23/24 14:00:30.698
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/23/24 14:00:30.746
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/23/24 14:00:31.753
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/23/24 14:00:35.766
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/23/24 14:00:35.769
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/23/24 14:00:35.769
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 23 14:00:35.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-171" for this suite. 01/23/24 14:00:35.783
------------------------------
• [SLOW TEST] [32.237 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:00:03.549
    Jan 23 14:00:03.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename container-runtime 01/23/24 14:00:03.549
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:00:03.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:00:03.562
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/23/24 14:00:03.579
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/23/24 14:00:23.64
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/23/24 14:00:23.641
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/23/24 14:00:23.644
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/23/24 14:00:23.644
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/23/24 14:00:23.665
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/23/24 14:00:28.687
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/23/24 14:00:30.695
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/23/24 14:00:30.698
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/23/24 14:00:30.698
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/23/24 14:00:30.746
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/23/24 14:00:31.753
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/23/24 14:00:35.766
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/23/24 14:00:35.769
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/23/24 14:00:35.769
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:00:35.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-171" for this suite. 01/23/24 14:00:35.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:00:35.786
Jan 23 14:00:35.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename services 01/23/24 14:00:35.787
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:00:35.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:00:35.799
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 01/23/24 14:00:35.801
Jan 23 14:00:35.801: INFO: Creating e2e-svc-a-prfdr
Jan 23 14:00:35.806: INFO: Creating e2e-svc-b-2m947
Jan 23 14:00:35.811: INFO: Creating e2e-svc-c-dgm57
STEP: deleting service collection 01/23/24 14:00:35.819
Jan 23 14:00:35.837: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 23 14:00:35.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2907" for this suite. 01/23/24 14:00:35.84
------------------------------
• [0.057 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:00:35.786
    Jan 23 14:00:35.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename services 01/23/24 14:00:35.787
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:00:35.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:00:35.799
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 01/23/24 14:00:35.801
    Jan 23 14:00:35.801: INFO: Creating e2e-svc-a-prfdr
    Jan 23 14:00:35.806: INFO: Creating e2e-svc-b-2m947
    Jan 23 14:00:35.811: INFO: Creating e2e-svc-c-dgm57
    STEP: deleting service collection 01/23/24 14:00:35.819
    Jan 23 14:00:35.837: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:00:35.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2907" for this suite. 01/23/24 14:00:35.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:00:35.844
Jan 23 14:00:35.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename secrets 01/23/24 14:00:35.845
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:00:35.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:00:35.856
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-2434d3ba-f954-4d72-b8c0-b63875ea9b83 01/23/24 14:00:35.859
STEP: Creating a pod to test consume secrets 01/23/24 14:00:35.863
Jan 23 14:00:35.875: INFO: Waiting up to 5m0s for pod "pod-secrets-9a602b64-266f-4fdf-b077-5f17dac92811" in namespace "secrets-1774" to be "Succeeded or Failed"
Jan 23 14:00:35.878: INFO: Pod "pod-secrets-9a602b64-266f-4fdf-b077-5f17dac92811": Phase="Pending", Reason="", readiness=false. Elapsed: 3.800788ms
Jan 23 14:00:37.882: INFO: Pod "pod-secrets-9a602b64-266f-4fdf-b077-5f17dac92811": Phase="Running", Reason="", readiness=true. Elapsed: 2.007152072s
Jan 23 14:00:39.882: INFO: Pod "pod-secrets-9a602b64-266f-4fdf-b077-5f17dac92811": Phase="Running", Reason="", readiness=false. Elapsed: 4.00742862s
Jan 23 14:00:41.881: INFO: Pod "pod-secrets-9a602b64-266f-4fdf-b077-5f17dac92811": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006668031s
STEP: Saw pod success 01/23/24 14:00:41.881
Jan 23 14:00:41.881: INFO: Pod "pod-secrets-9a602b64-266f-4fdf-b077-5f17dac92811" satisfied condition "Succeeded or Failed"
Jan 23 14:00:41.883: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-secrets-9a602b64-266f-4fdf-b077-5f17dac92811 container secret-volume-test: <nil>
STEP: delete the pod 01/23/24 14:00:41.887
Jan 23 14:00:41.894: INFO: Waiting for pod pod-secrets-9a602b64-266f-4fdf-b077-5f17dac92811 to disappear
Jan 23 14:00:41.896: INFO: Pod pod-secrets-9a602b64-266f-4fdf-b077-5f17dac92811 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 23 14:00:41.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1774" for this suite. 01/23/24 14:00:41.898
------------------------------
• [SLOW TEST] [6.057 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:00:35.844
    Jan 23 14:00:35.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename secrets 01/23/24 14:00:35.845
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:00:35.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:00:35.856
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-2434d3ba-f954-4d72-b8c0-b63875ea9b83 01/23/24 14:00:35.859
    STEP: Creating a pod to test consume secrets 01/23/24 14:00:35.863
    Jan 23 14:00:35.875: INFO: Waiting up to 5m0s for pod "pod-secrets-9a602b64-266f-4fdf-b077-5f17dac92811" in namespace "secrets-1774" to be "Succeeded or Failed"
    Jan 23 14:00:35.878: INFO: Pod "pod-secrets-9a602b64-266f-4fdf-b077-5f17dac92811": Phase="Pending", Reason="", readiness=false. Elapsed: 3.800788ms
    Jan 23 14:00:37.882: INFO: Pod "pod-secrets-9a602b64-266f-4fdf-b077-5f17dac92811": Phase="Running", Reason="", readiness=true. Elapsed: 2.007152072s
    Jan 23 14:00:39.882: INFO: Pod "pod-secrets-9a602b64-266f-4fdf-b077-5f17dac92811": Phase="Running", Reason="", readiness=false. Elapsed: 4.00742862s
    Jan 23 14:00:41.881: INFO: Pod "pod-secrets-9a602b64-266f-4fdf-b077-5f17dac92811": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006668031s
    STEP: Saw pod success 01/23/24 14:00:41.881
    Jan 23 14:00:41.881: INFO: Pod "pod-secrets-9a602b64-266f-4fdf-b077-5f17dac92811" satisfied condition "Succeeded or Failed"
    Jan 23 14:00:41.883: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-secrets-9a602b64-266f-4fdf-b077-5f17dac92811 container secret-volume-test: <nil>
    STEP: delete the pod 01/23/24 14:00:41.887
    Jan 23 14:00:41.894: INFO: Waiting for pod pod-secrets-9a602b64-266f-4fdf-b077-5f17dac92811 to disappear
    Jan 23 14:00:41.896: INFO: Pod pod-secrets-9a602b64-266f-4fdf-b077-5f17dac92811 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:00:41.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1774" for this suite. 01/23/24 14:00:41.898
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:00:41.901
Jan 23 14:00:41.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename emptydir 01/23/24 14:00:41.902
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:00:41.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:00:41.909
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 01/23/24 14:00:41.913
Jan 23 14:00:41.925: INFO: Waiting up to 5m0s for pod "pod-12304c3b-3e59-4780-8913-44ac752a3da7" in namespace "emptydir-7737" to be "Succeeded or Failed"
Jan 23 14:00:41.926: INFO: Pod "pod-12304c3b-3e59-4780-8913-44ac752a3da7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.153141ms
Jan 23 14:00:43.929: INFO: Pod "pod-12304c3b-3e59-4780-8913-44ac752a3da7": Phase="Running", Reason="", readiness=true. Elapsed: 2.004196544s
Jan 23 14:00:45.929: INFO: Pod "pod-12304c3b-3e59-4780-8913-44ac752a3da7": Phase="Running", Reason="", readiness=false. Elapsed: 4.004076573s
Jan 23 14:00:47.931: INFO: Pod "pod-12304c3b-3e59-4780-8913-44ac752a3da7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005998185s
STEP: Saw pod success 01/23/24 14:00:47.931
Jan 23 14:00:47.931: INFO: Pod "pod-12304c3b-3e59-4780-8913-44ac752a3da7" satisfied condition "Succeeded or Failed"
Jan 23 14:00:47.933: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-12304c3b-3e59-4780-8913-44ac752a3da7 container test-container: <nil>
STEP: delete the pod 01/23/24 14:00:47.936
Jan 23 14:00:47.943: INFO: Waiting for pod pod-12304c3b-3e59-4780-8913-44ac752a3da7 to disappear
Jan 23 14:00:47.944: INFO: Pod pod-12304c3b-3e59-4780-8913-44ac752a3da7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 23 14:00:47.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7737" for this suite. 01/23/24 14:00:47.946
------------------------------
• [SLOW TEST] [6.049 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:00:41.901
    Jan 23 14:00:41.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename emptydir 01/23/24 14:00:41.902
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:00:41.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:00:41.909
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 01/23/24 14:00:41.913
    Jan 23 14:00:41.925: INFO: Waiting up to 5m0s for pod "pod-12304c3b-3e59-4780-8913-44ac752a3da7" in namespace "emptydir-7737" to be "Succeeded or Failed"
    Jan 23 14:00:41.926: INFO: Pod "pod-12304c3b-3e59-4780-8913-44ac752a3da7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.153141ms
    Jan 23 14:00:43.929: INFO: Pod "pod-12304c3b-3e59-4780-8913-44ac752a3da7": Phase="Running", Reason="", readiness=true. Elapsed: 2.004196544s
    Jan 23 14:00:45.929: INFO: Pod "pod-12304c3b-3e59-4780-8913-44ac752a3da7": Phase="Running", Reason="", readiness=false. Elapsed: 4.004076573s
    Jan 23 14:00:47.931: INFO: Pod "pod-12304c3b-3e59-4780-8913-44ac752a3da7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005998185s
    STEP: Saw pod success 01/23/24 14:00:47.931
    Jan 23 14:00:47.931: INFO: Pod "pod-12304c3b-3e59-4780-8913-44ac752a3da7" satisfied condition "Succeeded or Failed"
    Jan 23 14:00:47.933: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-12304c3b-3e59-4780-8913-44ac752a3da7 container test-container: <nil>
    STEP: delete the pod 01/23/24 14:00:47.936
    Jan 23 14:00:47.943: INFO: Waiting for pod pod-12304c3b-3e59-4780-8913-44ac752a3da7 to disappear
    Jan 23 14:00:47.944: INFO: Pod pod-12304c3b-3e59-4780-8913-44ac752a3da7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:00:47.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7737" for this suite. 01/23/24 14:00:47.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:00:47.95
Jan 23 14:00:47.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename container-lifecycle-hook 01/23/24 14:00:47.95
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:00:47.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:00:47.959
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/23/24 14:00:47.965
Jan 23 14:00:47.980: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7323" to be "running and ready"
Jan 23 14:00:47.981: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.654984ms
Jan 23 14:00:47.981: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:00:49.984: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004434545s
Jan 23 14:00:49.984: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 23 14:00:49.984: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 01/23/24 14:00:49.986
Jan 23 14:00:49.996: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7323" to be "running and ready"
Jan 23 14:00:49.997: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.758499ms
Jan 23 14:00:49.998: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:00:52.001: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004999822s
Jan 23 14:00:52.001: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan 23 14:00:52.001: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/23/24 14:00:52.002
Jan 23 14:00:52.006: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 23 14:00:52.008: INFO: Pod pod-with-prestop-http-hook still exists
Jan 23 14:00:54.008: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 23 14:00:54.011: INFO: Pod pod-with-prestop-http-hook still exists
Jan 23 14:00:56.009: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 23 14:00:56.011: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/23/24 14:00:56.011
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 23 14:00:56.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-7323" for this suite. 01/23/24 14:00:56.018
------------------------------
• [SLOW TEST] [8.071 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:00:47.95
    Jan 23 14:00:47.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/23/24 14:00:47.95
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:00:47.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:00:47.959
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/23/24 14:00:47.965
    Jan 23 14:00:47.980: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7323" to be "running and ready"
    Jan 23 14:00:47.981: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.654984ms
    Jan 23 14:00:47.981: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:00:49.984: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004434545s
    Jan 23 14:00:49.984: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 23 14:00:49.984: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 01/23/24 14:00:49.986
    Jan 23 14:00:49.996: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7323" to be "running and ready"
    Jan 23 14:00:49.997: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.758499ms
    Jan 23 14:00:49.998: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:00:52.001: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004999822s
    Jan 23 14:00:52.001: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan 23 14:00:52.001: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/23/24 14:00:52.002
    Jan 23 14:00:52.006: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 23 14:00:52.008: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 23 14:00:54.008: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 23 14:00:54.011: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 23 14:00:56.009: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 23 14:00:56.011: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/23/24 14:00:56.011
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:00:56.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-7323" for this suite. 01/23/24 14:00:56.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:00:56.022
Jan 23 14:00:56.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename dns 01/23/24 14:00:56.023
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:00:56.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:00:56.032
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2850.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2850.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/23/24 14:00:56.033
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2850.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2850.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/23/24 14:00:56.033
STEP: creating a pod to probe /etc/hosts 01/23/24 14:00:56.033
STEP: submitting the pod to kubernetes 01/23/24 14:00:56.034
Jan 23 14:00:56.054: INFO: Waiting up to 15m0s for pod "dns-test-33430cf6-26f4-4dd6-9775-7a58961bb860" in namespace "dns-2850" to be "running"
Jan 23 14:00:56.055: INFO: Pod "dns-test-33430cf6-26f4-4dd6-9775-7a58961bb860": Phase="Pending", Reason="", readiness=false. Elapsed: 1.298121ms
Jan 23 14:00:58.058: INFO: Pod "dns-test-33430cf6-26f4-4dd6-9775-7a58961bb860": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004663331s
Jan 23 14:01:00.059: INFO: Pod "dns-test-33430cf6-26f4-4dd6-9775-7a58961bb860": Phase="Running", Reason="", readiness=true. Elapsed: 4.005118791s
Jan 23 14:01:00.059: INFO: Pod "dns-test-33430cf6-26f4-4dd6-9775-7a58961bb860" satisfied condition "running"
STEP: retrieving the pod 01/23/24 14:01:00.059
STEP: looking for the results for each expected name from probers 01/23/24 14:01:00.061
Jan 23 14:01:00.069: INFO: DNS probes using dns-2850/dns-test-33430cf6-26f4-4dd6-9775-7a58961bb860 succeeded

STEP: deleting the pod 01/23/24 14:01:00.069
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 23 14:01:00.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2850" for this suite. 01/23/24 14:01:00.078
------------------------------
• [4.060 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:00:56.022
    Jan 23 14:00:56.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename dns 01/23/24 14:00:56.023
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:00:56.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:00:56.032
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2850.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2850.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/23/24 14:00:56.033
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2850.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2850.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/23/24 14:00:56.033
    STEP: creating a pod to probe /etc/hosts 01/23/24 14:00:56.033
    STEP: submitting the pod to kubernetes 01/23/24 14:00:56.034
    Jan 23 14:00:56.054: INFO: Waiting up to 15m0s for pod "dns-test-33430cf6-26f4-4dd6-9775-7a58961bb860" in namespace "dns-2850" to be "running"
    Jan 23 14:00:56.055: INFO: Pod "dns-test-33430cf6-26f4-4dd6-9775-7a58961bb860": Phase="Pending", Reason="", readiness=false. Elapsed: 1.298121ms
    Jan 23 14:00:58.058: INFO: Pod "dns-test-33430cf6-26f4-4dd6-9775-7a58961bb860": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004663331s
    Jan 23 14:01:00.059: INFO: Pod "dns-test-33430cf6-26f4-4dd6-9775-7a58961bb860": Phase="Running", Reason="", readiness=true. Elapsed: 4.005118791s
    Jan 23 14:01:00.059: INFO: Pod "dns-test-33430cf6-26f4-4dd6-9775-7a58961bb860" satisfied condition "running"
    STEP: retrieving the pod 01/23/24 14:01:00.059
    STEP: looking for the results for each expected name from probers 01/23/24 14:01:00.061
    Jan 23 14:01:00.069: INFO: DNS probes using dns-2850/dns-test-33430cf6-26f4-4dd6-9775-7a58961bb860 succeeded

    STEP: deleting the pod 01/23/24 14:01:00.069
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:01:00.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2850" for this suite. 01/23/24 14:01:00.078
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:01:00.082
Jan 23 14:01:00.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename dns 01/23/24 14:01:00.083
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:01:00.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:01:00.092
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/23/24 14:01:00.093
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1798 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1798;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1798 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1798;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1798.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1798.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1798.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1798.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1798.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1798.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1798.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1798.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1798.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1798.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1798.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1798.svc;check="$$(dig +notcp +noall +answer +search 73.29.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.29.73_udp@PTR;check="$$(dig +tcp +noall +answer +search 73.29.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.29.73_tcp@PTR;sleep 1; done
 01/23/24 14:01:00.106
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1798 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1798;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1798 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1798;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1798.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1798.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1798.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1798.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1798.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1798.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1798.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1798.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1798.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1798.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1798.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1798.svc;check="$$(dig +notcp +noall +answer +search 73.29.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.29.73_udp@PTR;check="$$(dig +tcp +noall +answer +search 73.29.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.29.73_tcp@PTR;sleep 1; done
 01/23/24 14:01:00.106
STEP: creating a pod to probe DNS 01/23/24 14:01:00.106
STEP: submitting the pod to kubernetes 01/23/24 14:01:00.106
Jan 23 14:01:00.127: INFO: Waiting up to 15m0s for pod "dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345" in namespace "dns-1798" to be "running"
Jan 23 14:01:00.130: INFO: Pod "dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345": Phase="Pending", Reason="", readiness=false. Elapsed: 2.286625ms
Jan 23 14:01:02.133: INFO: Pod "dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0055616s
Jan 23 14:01:04.133: INFO: Pod "dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005325604s
Jan 23 14:01:06.133: INFO: Pod "dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345": Phase="Running", Reason="", readiness=true. Elapsed: 6.005515587s
Jan 23 14:01:06.133: INFO: Pod "dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345" satisfied condition "running"
STEP: retrieving the pod 01/23/24 14:01:06.133
STEP: looking for the results for each expected name from probers 01/23/24 14:01:06.135
Jan 23 14:01:06.137: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
Jan 23 14:01:06.139: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
Jan 23 14:01:06.140: INFO: Unable to read wheezy_udp@dns-test-service.dns-1798 from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
Jan 23 14:01:06.142: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1798 from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
Jan 23 14:01:06.143: INFO: Unable to read wheezy_udp@dns-test-service.dns-1798.svc from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
Jan 23 14:01:06.145: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1798.svc from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
Jan 23 14:01:06.146: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1798.svc from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
Jan 23 14:01:06.148: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1798.svc from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
Jan 23 14:01:06.154: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
Jan 23 14:01:06.156: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
Jan 23 14:01:06.157: INFO: Unable to read jessie_udp@dns-test-service.dns-1798 from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
Jan 23 14:01:06.159: INFO: Unable to read jessie_tcp@dns-test-service.dns-1798 from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
Jan 23 14:01:06.160: INFO: Unable to read jessie_udp@dns-test-service.dns-1798.svc from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
Jan 23 14:01:06.161: INFO: Unable to read jessie_tcp@dns-test-service.dns-1798.svc from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
Jan 23 14:01:06.163: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1798.svc from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
Jan 23 14:01:06.164: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1798.svc from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
Jan 23 14:01:06.171: INFO: Lookups using dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1798 wheezy_tcp@dns-test-service.dns-1798 wheezy_udp@dns-test-service.dns-1798.svc wheezy_tcp@dns-test-service.dns-1798.svc wheezy_udp@_http._tcp.dns-test-service.dns-1798.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1798.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1798 jessie_tcp@dns-test-service.dns-1798 jessie_udp@dns-test-service.dns-1798.svc jessie_tcp@dns-test-service.dns-1798.svc jessie_udp@_http._tcp.dns-test-service.dns-1798.svc jessie_tcp@_http._tcp.dns-test-service.dns-1798.svc]

Jan 23 14:01:11.212: INFO: DNS probes using dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345 succeeded

STEP: deleting the pod 01/23/24 14:01:11.213
STEP: deleting the test service 01/23/24 14:01:11.221
STEP: deleting the test headless service 01/23/24 14:01:11.241
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 23 14:01:11.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1798" for this suite. 01/23/24 14:01:11.25
------------------------------
• [SLOW TEST] [11.171 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:01:00.082
    Jan 23 14:01:00.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename dns 01/23/24 14:01:00.083
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:01:00.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:01:00.092
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/23/24 14:01:00.093
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1798 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1798;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1798 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1798;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1798.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1798.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1798.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1798.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1798.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1798.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1798.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1798.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1798.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1798.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1798.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1798.svc;check="$$(dig +notcp +noall +answer +search 73.29.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.29.73_udp@PTR;check="$$(dig +tcp +noall +answer +search 73.29.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.29.73_tcp@PTR;sleep 1; done
     01/23/24 14:01:00.106
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1798 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1798;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1798 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1798;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1798.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1798.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1798.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1798.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1798.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1798.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1798.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1798.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1798.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1798.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1798.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1798.svc;check="$$(dig +notcp +noall +answer +search 73.29.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.29.73_udp@PTR;check="$$(dig +tcp +noall +answer +search 73.29.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.29.73_tcp@PTR;sleep 1; done
     01/23/24 14:01:00.106
    STEP: creating a pod to probe DNS 01/23/24 14:01:00.106
    STEP: submitting the pod to kubernetes 01/23/24 14:01:00.106
    Jan 23 14:01:00.127: INFO: Waiting up to 15m0s for pod "dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345" in namespace "dns-1798" to be "running"
    Jan 23 14:01:00.130: INFO: Pod "dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345": Phase="Pending", Reason="", readiness=false. Elapsed: 2.286625ms
    Jan 23 14:01:02.133: INFO: Pod "dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0055616s
    Jan 23 14:01:04.133: INFO: Pod "dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005325604s
    Jan 23 14:01:06.133: INFO: Pod "dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345": Phase="Running", Reason="", readiness=true. Elapsed: 6.005515587s
    Jan 23 14:01:06.133: INFO: Pod "dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345" satisfied condition "running"
    STEP: retrieving the pod 01/23/24 14:01:06.133
    STEP: looking for the results for each expected name from probers 01/23/24 14:01:06.135
    Jan 23 14:01:06.137: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
    Jan 23 14:01:06.139: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
    Jan 23 14:01:06.140: INFO: Unable to read wheezy_udp@dns-test-service.dns-1798 from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
    Jan 23 14:01:06.142: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1798 from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
    Jan 23 14:01:06.143: INFO: Unable to read wheezy_udp@dns-test-service.dns-1798.svc from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
    Jan 23 14:01:06.145: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1798.svc from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
    Jan 23 14:01:06.146: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1798.svc from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
    Jan 23 14:01:06.148: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1798.svc from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
    Jan 23 14:01:06.154: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
    Jan 23 14:01:06.156: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
    Jan 23 14:01:06.157: INFO: Unable to read jessie_udp@dns-test-service.dns-1798 from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
    Jan 23 14:01:06.159: INFO: Unable to read jessie_tcp@dns-test-service.dns-1798 from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
    Jan 23 14:01:06.160: INFO: Unable to read jessie_udp@dns-test-service.dns-1798.svc from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
    Jan 23 14:01:06.161: INFO: Unable to read jessie_tcp@dns-test-service.dns-1798.svc from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
    Jan 23 14:01:06.163: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1798.svc from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
    Jan 23 14:01:06.164: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1798.svc from pod dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345: the server could not find the requested resource (get pods dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345)
    Jan 23 14:01:06.171: INFO: Lookups using dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1798 wheezy_tcp@dns-test-service.dns-1798 wheezy_udp@dns-test-service.dns-1798.svc wheezy_tcp@dns-test-service.dns-1798.svc wheezy_udp@_http._tcp.dns-test-service.dns-1798.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1798.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1798 jessie_tcp@dns-test-service.dns-1798 jessie_udp@dns-test-service.dns-1798.svc jessie_tcp@dns-test-service.dns-1798.svc jessie_udp@_http._tcp.dns-test-service.dns-1798.svc jessie_tcp@_http._tcp.dns-test-service.dns-1798.svc]

    Jan 23 14:01:11.212: INFO: DNS probes using dns-1798/dns-test-8f67be63-3aa2-44eb-acf3-9ecc86284345 succeeded

    STEP: deleting the pod 01/23/24 14:01:11.213
    STEP: deleting the test service 01/23/24 14:01:11.221
    STEP: deleting the test headless service 01/23/24 14:01:11.241
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:01:11.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1798" for this suite. 01/23/24 14:01:11.25
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:01:11.253
Jan 23 14:01:11.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename job 01/23/24 14:01:11.254
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:01:11.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:01:11.265
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 01/23/24 14:01:11.267
STEP: Ensuring job reaches completions 01/23/24 14:01:11.271
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 23 14:01:27.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9626" for this suite. 01/23/24 14:01:27.277
------------------------------
• [SLOW TEST] [16.030 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:01:11.253
    Jan 23 14:01:11.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename job 01/23/24 14:01:11.254
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:01:11.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:01:11.265
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 01/23/24 14:01:11.267
    STEP: Ensuring job reaches completions 01/23/24 14:01:11.271
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:01:27.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9626" for this suite. 01/23/24 14:01:27.277
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:01:27.284
Jan 23 14:01:27.284: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename pods 01/23/24 14:01:27.284
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:01:27.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:01:27.298
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 01/23/24 14:01:27.3
Jan 23 14:01:27.315: INFO: Waiting up to 5m0s for pod "pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68" in namespace "pods-7467" to be "running and ready"
Jan 23 14:01:27.319: INFO: Pod "pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68": Phase="Pending", Reason="", readiness=false. Elapsed: 4.108759ms
Jan 23 14:01:27.319: INFO: The phase of Pod pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:01:29.321: INFO: Pod "pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006795402s
Jan 23 14:01:29.321: INFO: The phase of Pod pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:01:31.324: INFO: Pod "pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68": Phase="Running", Reason="", readiness=true. Elapsed: 4.009238125s
Jan 23 14:01:31.324: INFO: The phase of Pod pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68 is Running (Ready = true)
Jan 23 14:01:31.324: INFO: Pod "pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68" satisfied condition "running and ready"
Jan 23 14:01:31.327: INFO: Pod pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68 has hostIP: 172.31.11.67
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 23 14:01:31.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7467" for this suite. 01/23/24 14:01:31.329
------------------------------
• [4.049 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:01:27.284
    Jan 23 14:01:27.284: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename pods 01/23/24 14:01:27.284
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:01:27.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:01:27.298
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 01/23/24 14:01:27.3
    Jan 23 14:01:27.315: INFO: Waiting up to 5m0s for pod "pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68" in namespace "pods-7467" to be "running and ready"
    Jan 23 14:01:27.319: INFO: Pod "pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68": Phase="Pending", Reason="", readiness=false. Elapsed: 4.108759ms
    Jan 23 14:01:27.319: INFO: The phase of Pod pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:01:29.321: INFO: Pod "pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006795402s
    Jan 23 14:01:29.321: INFO: The phase of Pod pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:01:31.324: INFO: Pod "pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68": Phase="Running", Reason="", readiness=true. Elapsed: 4.009238125s
    Jan 23 14:01:31.324: INFO: The phase of Pod pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68 is Running (Ready = true)
    Jan 23 14:01:31.324: INFO: Pod "pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68" satisfied condition "running and ready"
    Jan 23 14:01:31.327: INFO: Pod pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68 has hostIP: 172.31.11.67
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:01:31.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7467" for this suite. 01/23/24 14:01:31.329
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:01:31.333
Jan 23 14:01:31.333: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename security-context-test 01/23/24 14:01:31.334
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:01:31.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:01:31.357
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Jan 23 14:01:31.371: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-27e76a33-555a-4176-a7ed-b72337ec5227" in namespace "security-context-test-3869" to be "Succeeded or Failed"
Jan 23 14:01:31.373: INFO: Pod "alpine-nnp-false-27e76a33-555a-4176-a7ed-b72337ec5227": Phase="Pending", Reason="", readiness=false. Elapsed: 1.767349ms
Jan 23 14:01:33.375: INFO: Pod "alpine-nnp-false-27e76a33-555a-4176-a7ed-b72337ec5227": Phase="Running", Reason="", readiness=false. Elapsed: 2.004613323s
Jan 23 14:01:35.375: INFO: Pod "alpine-nnp-false-27e76a33-555a-4176-a7ed-b72337ec5227": Phase="Running", Reason="", readiness=false. Elapsed: 4.003991699s
Jan 23 14:01:37.375: INFO: Pod "alpine-nnp-false-27e76a33-555a-4176-a7ed-b72337ec5227": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004497583s
Jan 23 14:01:37.375: INFO: Pod "alpine-nnp-false-27e76a33-555a-4176-a7ed-b72337ec5227" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 23 14:01:37.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3869" for this suite. 01/23/24 14:01:37.384
------------------------------
• [SLOW TEST] [6.058 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:01:31.333
    Jan 23 14:01:31.333: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename security-context-test 01/23/24 14:01:31.334
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:01:31.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:01:31.357
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Jan 23 14:01:31.371: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-27e76a33-555a-4176-a7ed-b72337ec5227" in namespace "security-context-test-3869" to be "Succeeded or Failed"
    Jan 23 14:01:31.373: INFO: Pod "alpine-nnp-false-27e76a33-555a-4176-a7ed-b72337ec5227": Phase="Pending", Reason="", readiness=false. Elapsed: 1.767349ms
    Jan 23 14:01:33.375: INFO: Pod "alpine-nnp-false-27e76a33-555a-4176-a7ed-b72337ec5227": Phase="Running", Reason="", readiness=false. Elapsed: 2.004613323s
    Jan 23 14:01:35.375: INFO: Pod "alpine-nnp-false-27e76a33-555a-4176-a7ed-b72337ec5227": Phase="Running", Reason="", readiness=false. Elapsed: 4.003991699s
    Jan 23 14:01:37.375: INFO: Pod "alpine-nnp-false-27e76a33-555a-4176-a7ed-b72337ec5227": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004497583s
    Jan 23 14:01:37.375: INFO: Pod "alpine-nnp-false-27e76a33-555a-4176-a7ed-b72337ec5227" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:01:37.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3869" for this suite. 01/23/24 14:01:37.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:01:37.397
Jan 23 14:01:37.397: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename sched-pred 01/23/24 14:01:37.398
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:01:37.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:01:37.41
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 23 14:01:37.412: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 23 14:01:37.418: INFO: Waiting for terminating namespaces to be deleted...
Jan 23 14:01:37.420: INFO: 
Logging pods the apiserver thinks is on node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local before test
Jan 23 14:01:37.437: INFO: calico-kube-controllers-7d4c856855-qrf8w from kube-system started at 2024-01-23 09:02:23 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container calico-kube-controllers ready: true, restart count 4
Jan 23 14:01:37.437: INFO: calico-node-hx9hg from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container calico-node ready: true, restart count 1
Jan 23 14:01:37.437: INFO: coredns-8446d7bc66-zglp7 from kube-system started at 2024-01-23 09:02:28 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container coredns ready: true, restart count 1
Jan 23 14:01:37.437: INFO: kube-proxy-5p4pt from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container kube-proxy ready: true, restart count 1
Jan 23 14:01:37.437: INFO: nginx-proxy-node-worker-hohyvwot.nova-ht9xu6tk2ptb.local from kube-system started at 2024-01-23 09:02:32 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container nginx-proxy ready: true, restart count 1
Jan 23 14:01:37.437: INFO: nova-dns-667b6f9dd9-f4wkr from kube-system started at 2024-01-23 09:02:53 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container nova-dns ready: true, restart count 1
Jan 23 14:01:37.437: INFO: nova-dns-667b6f9dd9-fc8vq from kube-system started at 2024-01-23 09:02:53 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container nova-dns ready: true, restart count 1
Jan 23 14:01:37.437: INFO: nova-reflector-7bc9b5d4dd-vgml8 from nova-automation started at 2024-01-23 09:06:13 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container reflector ready: true, restart count 1
Jan 23 14:01:37.437: INFO: nova-release-git-main-0 from nova-automation started at 2024-01-23 09:08:44 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container gitea ready: true, restart count 1
Jan 23 14:01:37.437: INFO: nova-reloader-744fcf7b8f-ztn6c from nova-automation started at 2024-01-23 09:06:13 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container nova-reloader ready: true, restart count 1
Jan 23 14:01:37.437: INFO: nova-cert-manager-74fb9fd7f9-q6q9f from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container cert-manager-controller ready: true, restart count 2
Jan 23 14:01:37.437: INFO: nova-cert-manager-cainjector-74465474c6-pjhbz from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container cert-manager-cainjector ready: true, restart count 4
Jan 23 14:01:37.437: INFO: nova-cert-manager-webhook-74b6ccdf8-f5bt2 from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container cert-manager-webhook ready: true, restart count 1
Jan 23 14:01:37.437: INFO: nova-console-675844f8c4-p92th from nova-console started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container nova-console ready: true, restart count 1
Jan 23 14:01:37.437: INFO: nova-local-path-provisioner-59754bbcb5-nm6ps from nova-csi-drivers started at 2024-01-23 09:06:07 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container local-path-provisioner ready: true, restart count 1
Jan 23 14:01:37.437: INFO: nova-oauth-csi-provider-msq2s from nova-csi-drivers started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container vault-csi-provider ready: true, restart count 1
Jan 23 14:01:37.437: INFO: nova-secrets-store-csi-driver-s4f8p from nova-csi-drivers started at 2024-01-23 09:03:57 +0000 UTC (3 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container liveness-probe ready: true, restart count 1
Jan 23 14:01:37.437: INFO: 	Container node-driver-registrar ready: true, restart count 1
Jan 23 14:01:37.437: INFO: 	Container secrets-store ready: true, restart count 1
Jan 23 14:01:37.437: INFO: secrets-store-csi-driver-upgrade-crds-jvnt9 from nova-csi-drivers started at 2024-01-23 09:03:57 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container crds-upgrade ready: false, restart count 0
Jan 23 14:01:37.437: INFO: nova-descheduler-575f46487d-wcml5 from nova-descheduler started at 2024-01-23 09:06:07 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container descheduler ready: true, restart count 4
Jan 23 14:01:37.437: INFO: helm-controller-6ffdd7974c-ndrg6 from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container manager ready: true, restart count 5
Jan 23 14:01:37.437: INFO: image-automation-controller-79bb688dbd-sq8gm from nova-gitops started at 2024-01-23 09:09:27 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container manager ready: true, restart count 4
Jan 23 14:01:37.437: INFO: image-reflector-controller-6b744758c7-nwppg from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container manager ready: true, restart count 4
Jan 23 14:01:37.437: INFO: kustomize-controller-5d5bb4d48-99q8z from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container manager ready: true, restart count 4
Jan 23 14:01:37.437: INFO: notification-controller-5974fbb84-8wcdc from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container manager ready: true, restart count 4
Jan 23 14:01:37.437: INFO: source-controller-7f8d6bc9d7-dbb88 from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container manager ready: true, restart count 4
Jan 23 14:01:37.437: INFO: nova-ingress-internal-controller-g8rl8 from nova-ingress-internal started at 2024-01-23 09:08:09 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container controller ready: true, restart count 1
Jan 23 14:01:37.437: INFO: nova-logging-operator-7587849584-mq59r from nova-logging-operator started at 2024-01-23 09:06:18 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container logging-operator ready: true, restart count 5
Jan 23 14:01:37.437: INFO: alertmanager-main-0 from nova-monitoring started at 2024-01-23 09:09:10 +0000 UTC (3 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container alertmanager ready: true, restart count 1
Jan 23 14:01:37.437: INFO: 	Container config-reloader ready: true, restart count 1
Jan 23 14:01:37.437: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 23 14:01:37.437: INFO: monitoring-plugin-6dcd875fb-6sn96 from nova-monitoring started at 2024-01-23 09:08:38 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container monitoring-plugin ready: true, restart count 1
Jan 23 14:01:37.437: INFO: nova-cadvisor-jvkj7 from nova-monitoring started at 2024-01-23 09:09:34 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container nova-cadvisor ready: true, restart count 1
Jan 23 14:01:37.437: INFO: nova-grafana-5fcb766f99-xlrkr from nova-monitoring started at 2024-01-23 09:09:09 +0000 UTC (3 container statuses recorded)
Jan 23 14:01:37.437: INFO: 	Container grafana ready: true, restart count 1
Jan 23 14:01:37.437: INFO: 	Container nova-release-grafana-sc-dashboard ready: true, restart count 1
Jan 23 14:01:37.437: INFO: 	Container nova-release-grafana-sc-datasources ready: true, restart count 1
Jan 23 14:01:37.438: INFO: nova-kube-state-metrics-6c99956449-vwb5x from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.438: INFO: 	Container nova-release-kube-state-metrics ready: true, restart count 1
Jan 23 14:01:37.438: INFO: nova-metrics-server-6fc9cb6c86-6dfwd from nova-monitoring started at 2024-01-23 09:06:11 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.438: INFO: 	Container metrics-server ready: true, restart count 1
Jan 23 14:01:37.438: INFO: nova-prometheus-adapter-55b7c8779-b9ffb from nova-monitoring started at 2024-01-23 09:09:27 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.438: INFO: 	Container prometheus-adapter ready: true, restart count 1
Jan 23 14:01:37.438: INFO: nova-prometheus-main-operator-fcb966c79-2wnc9 from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.438: INFO: 	Container main ready: true, restart count 1
Jan 23 14:01:37.438: INFO: nova-prometheus-node-exporter-cc7w5 from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.438: INFO: 	Container node-exporter ready: true, restart count 1
Jan 23 14:01:37.438: INFO: prometheus-main-0 from nova-monitoring started at 2024-01-23 09:09:42 +0000 UTC (5 container statuses recorded)
Jan 23 14:01:37.438: INFO: 	Container config-reloader ready: true, restart count 1
Jan 23 14:01:37.438: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 23 14:01:37.438: INFO: 	Container prometheus ready: true, restart count 1
Jan 23 14:01:37.438: INFO: 	Container thanos-sidecar ready: true, restart count 1
Jan 23 14:01:37.438: INFO: 	Container vault-agent-auth ready: true, restart count 1
Jan 23 14:01:37.438: INFO: nova-oauth-secrets-webhook-gmmxd from nova-secrets-webhook started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.438: INFO: 	Container vault-secrets-webhook ready: true, restart count 1
Jan 23 14:01:37.438: INFO: sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-4htxf from sonobuoy started at 2024-01-23 13:21:23 +0000 UTC (2 container statuses recorded)
Jan 23 14:01:37.438: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 14:01:37.438: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 23 14:01:37.438: INFO: 
Logging pods the apiserver thinks is on node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local before test
Jan 23 14:01:37.447: INFO: calico-node-44dpv from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.447: INFO: 	Container calico-node ready: true, restart count 1
Jan 23 14:01:37.447: INFO: coredns-8446d7bc66-rpjs8 from kube-system started at 2024-01-23 09:02:28 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.447: INFO: 	Container coredns ready: true, restart count 1
Jan 23 14:01:37.447: INFO: kube-proxy-dvptq from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.447: INFO: 	Container kube-proxy ready: true, restart count 1
Jan 23 14:01:37.447: INFO: nginx-proxy-node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local from kube-system started at 2024-01-23 09:02:45 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.447: INFO: 	Container nginx-proxy ready: true, restart count 1
Jan 23 14:01:37.447: INFO: nova-oauth-csi-provider-pj45z from nova-csi-drivers started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.447: INFO: 	Container vault-csi-provider ready: true, restart count 1
Jan 23 14:01:37.447: INFO: nova-secrets-store-csi-driver-ft87l from nova-csi-drivers started at 2024-01-23 09:03:58 +0000 UTC (3 container statuses recorded)
Jan 23 14:01:37.447: INFO: 	Container liveness-probe ready: true, restart count 1
Jan 23 14:01:37.447: INFO: 	Container node-driver-registrar ready: true, restart count 1
Jan 23 14:01:37.447: INFO: 	Container secrets-store ready: true, restart count 1
Jan 23 14:01:37.447: INFO: nova-ingress-public-controller-b6dmc from nova-ingress-public started at 2024-01-23 13:24:57 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.447: INFO: 	Container controller ready: true, restart count 0
Jan 23 14:01:37.447: INFO: nova-cadvisor-tq2rp from nova-monitoring started at 2024-01-23 09:09:34 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.447: INFO: 	Container nova-cadvisor ready: true, restart count 1
Jan 23 14:01:37.447: INFO: nova-prometheus-node-exporter-lbfhw from nova-monitoring started at 2024-01-23 13:24:57 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.447: INFO: 	Container node-exporter ready: true, restart count 0
Jan 23 14:01:37.447: INFO: nova-oauth-secrets-webhook-hhwh4 from nova-secrets-webhook started at 2024-01-23 13:24:57 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.447: INFO: 	Container vault-secrets-webhook ready: true, restart count 0
Jan 23 14:01:37.447: INFO: pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68 from pods-7467 started at 2024-01-23 14:01:27 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.447: INFO: 	Container test ready: false, restart count 0
Jan 23 14:01:37.447: INFO: alpine-nnp-false-27e76a33-555a-4176-a7ed-b72337ec5227 from security-context-test-3869 started at 2024-01-23 14:01:31 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.447: INFO: 	Container alpine-nnp-false-27e76a33-555a-4176-a7ed-b72337ec5227 ready: false, restart count 0
Jan 23 14:01:37.447: INFO: sonobuoy from sonobuoy started at 2024-01-23 13:21:17 +0000 UTC (1 container statuses recorded)
Jan 23 14:01:37.447: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 23 14:01:37.447: INFO: sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-7csgd from sonobuoy started at 2024-01-23 13:21:23 +0000 UTC (2 container statuses recorded)
Jan 23 14:01:37.447: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 14:01:37.447: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/23/24 14:01:37.447
Jan 23 14:01:37.466: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4067" to be "running"
Jan 23 14:01:37.475: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.834468ms
Jan 23 14:01:39.477: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011418569s
Jan 23 14:01:39.477: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/23/24 14:01:39.479
STEP: Trying to apply a random label on the found node. 01/23/24 14:01:39.487
STEP: verifying the node has the label kubernetes.io/e2e-be74386b-e337-4e0d-b096-d9402d1af16e 42 01/23/24 14:01:39.496
STEP: Trying to relaunch the pod, now with labels. 01/23/24 14:01:39.499
Jan 23 14:01:39.511: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-4067" to be "not pending"
Jan 23 14:01:39.513: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 1.780483ms
Jan 23 14:01:41.516: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.004811579s
Jan 23 14:01:41.516: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-be74386b-e337-4e0d-b096-d9402d1af16e off the node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local 01/23/24 14:01:41.517
STEP: verifying the node doesn't have the label kubernetes.io/e2e-be74386b-e337-4e0d-b096-d9402d1af16e 01/23/24 14:01:41.525
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:01:41.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4067" for this suite. 01/23/24 14:01:41.529
------------------------------
• [4.137 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:01:37.397
    Jan 23 14:01:37.397: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename sched-pred 01/23/24 14:01:37.398
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:01:37.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:01:37.41
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 23 14:01:37.412: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 23 14:01:37.418: INFO: Waiting for terminating namespaces to be deleted...
    Jan 23 14:01:37.420: INFO: 
    Logging pods the apiserver thinks is on node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local before test
    Jan 23 14:01:37.437: INFO: calico-kube-controllers-7d4c856855-qrf8w from kube-system started at 2024-01-23 09:02:23 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container calico-kube-controllers ready: true, restart count 4
    Jan 23 14:01:37.437: INFO: calico-node-hx9hg from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container calico-node ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: coredns-8446d7bc66-zglp7 from kube-system started at 2024-01-23 09:02:28 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container coredns ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: kube-proxy-5p4pt from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container kube-proxy ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: nginx-proxy-node-worker-hohyvwot.nova-ht9xu6tk2ptb.local from kube-system started at 2024-01-23 09:02:32 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container nginx-proxy ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: nova-dns-667b6f9dd9-f4wkr from kube-system started at 2024-01-23 09:02:53 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container nova-dns ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: nova-dns-667b6f9dd9-fc8vq from kube-system started at 2024-01-23 09:02:53 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container nova-dns ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: nova-reflector-7bc9b5d4dd-vgml8 from nova-automation started at 2024-01-23 09:06:13 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container reflector ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: nova-release-git-main-0 from nova-automation started at 2024-01-23 09:08:44 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container gitea ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: nova-reloader-744fcf7b8f-ztn6c from nova-automation started at 2024-01-23 09:06:13 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container nova-reloader ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: nova-cert-manager-74fb9fd7f9-q6q9f from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container cert-manager-controller ready: true, restart count 2
    Jan 23 14:01:37.437: INFO: nova-cert-manager-cainjector-74465474c6-pjhbz from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container cert-manager-cainjector ready: true, restart count 4
    Jan 23 14:01:37.437: INFO: nova-cert-manager-webhook-74b6ccdf8-f5bt2 from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container cert-manager-webhook ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: nova-console-675844f8c4-p92th from nova-console started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container nova-console ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: nova-local-path-provisioner-59754bbcb5-nm6ps from nova-csi-drivers started at 2024-01-23 09:06:07 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container local-path-provisioner ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: nova-oauth-csi-provider-msq2s from nova-csi-drivers started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container vault-csi-provider ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: nova-secrets-store-csi-driver-s4f8p from nova-csi-drivers started at 2024-01-23 09:03:57 +0000 UTC (3 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container liveness-probe ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: 	Container node-driver-registrar ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: 	Container secrets-store ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: secrets-store-csi-driver-upgrade-crds-jvnt9 from nova-csi-drivers started at 2024-01-23 09:03:57 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container crds-upgrade ready: false, restart count 0
    Jan 23 14:01:37.437: INFO: nova-descheduler-575f46487d-wcml5 from nova-descheduler started at 2024-01-23 09:06:07 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container descheduler ready: true, restart count 4
    Jan 23 14:01:37.437: INFO: helm-controller-6ffdd7974c-ndrg6 from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container manager ready: true, restart count 5
    Jan 23 14:01:37.437: INFO: image-automation-controller-79bb688dbd-sq8gm from nova-gitops started at 2024-01-23 09:09:27 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container manager ready: true, restart count 4
    Jan 23 14:01:37.437: INFO: image-reflector-controller-6b744758c7-nwppg from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container manager ready: true, restart count 4
    Jan 23 14:01:37.437: INFO: kustomize-controller-5d5bb4d48-99q8z from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container manager ready: true, restart count 4
    Jan 23 14:01:37.437: INFO: notification-controller-5974fbb84-8wcdc from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container manager ready: true, restart count 4
    Jan 23 14:01:37.437: INFO: source-controller-7f8d6bc9d7-dbb88 from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container manager ready: true, restart count 4
    Jan 23 14:01:37.437: INFO: nova-ingress-internal-controller-g8rl8 from nova-ingress-internal started at 2024-01-23 09:08:09 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container controller ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: nova-logging-operator-7587849584-mq59r from nova-logging-operator started at 2024-01-23 09:06:18 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container logging-operator ready: true, restart count 5
    Jan 23 14:01:37.437: INFO: alertmanager-main-0 from nova-monitoring started at 2024-01-23 09:09:10 +0000 UTC (3 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: 	Container config-reloader ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: monitoring-plugin-6dcd875fb-6sn96 from nova-monitoring started at 2024-01-23 09:08:38 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container monitoring-plugin ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: nova-cadvisor-jvkj7 from nova-monitoring started at 2024-01-23 09:09:34 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container nova-cadvisor ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: nova-grafana-5fcb766f99-xlrkr from nova-monitoring started at 2024-01-23 09:09:09 +0000 UTC (3 container statuses recorded)
    Jan 23 14:01:37.437: INFO: 	Container grafana ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: 	Container nova-release-grafana-sc-dashboard ready: true, restart count 1
    Jan 23 14:01:37.437: INFO: 	Container nova-release-grafana-sc-datasources ready: true, restart count 1
    Jan 23 14:01:37.438: INFO: nova-kube-state-metrics-6c99956449-vwb5x from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.438: INFO: 	Container nova-release-kube-state-metrics ready: true, restart count 1
    Jan 23 14:01:37.438: INFO: nova-metrics-server-6fc9cb6c86-6dfwd from nova-monitoring started at 2024-01-23 09:06:11 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.438: INFO: 	Container metrics-server ready: true, restart count 1
    Jan 23 14:01:37.438: INFO: nova-prometheus-adapter-55b7c8779-b9ffb from nova-monitoring started at 2024-01-23 09:09:27 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.438: INFO: 	Container prometheus-adapter ready: true, restart count 1
    Jan 23 14:01:37.438: INFO: nova-prometheus-main-operator-fcb966c79-2wnc9 from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.438: INFO: 	Container main ready: true, restart count 1
    Jan 23 14:01:37.438: INFO: nova-prometheus-node-exporter-cc7w5 from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.438: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 23 14:01:37.438: INFO: prometheus-main-0 from nova-monitoring started at 2024-01-23 09:09:42 +0000 UTC (5 container statuses recorded)
    Jan 23 14:01:37.438: INFO: 	Container config-reloader ready: true, restart count 1
    Jan 23 14:01:37.438: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 23 14:01:37.438: INFO: 	Container prometheus ready: true, restart count 1
    Jan 23 14:01:37.438: INFO: 	Container thanos-sidecar ready: true, restart count 1
    Jan 23 14:01:37.438: INFO: 	Container vault-agent-auth ready: true, restart count 1
    Jan 23 14:01:37.438: INFO: nova-oauth-secrets-webhook-gmmxd from nova-secrets-webhook started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.438: INFO: 	Container vault-secrets-webhook ready: true, restart count 1
    Jan 23 14:01:37.438: INFO: sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-4htxf from sonobuoy started at 2024-01-23 13:21:23 +0000 UTC (2 container statuses recorded)
    Jan 23 14:01:37.438: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 23 14:01:37.438: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 23 14:01:37.438: INFO: 
    Logging pods the apiserver thinks is on node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local before test
    Jan 23 14:01:37.447: INFO: calico-node-44dpv from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.447: INFO: 	Container calico-node ready: true, restart count 1
    Jan 23 14:01:37.447: INFO: coredns-8446d7bc66-rpjs8 from kube-system started at 2024-01-23 09:02:28 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.447: INFO: 	Container coredns ready: true, restart count 1
    Jan 23 14:01:37.447: INFO: kube-proxy-dvptq from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.447: INFO: 	Container kube-proxy ready: true, restart count 1
    Jan 23 14:01:37.447: INFO: nginx-proxy-node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local from kube-system started at 2024-01-23 09:02:45 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.447: INFO: 	Container nginx-proxy ready: true, restart count 1
    Jan 23 14:01:37.447: INFO: nova-oauth-csi-provider-pj45z from nova-csi-drivers started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.447: INFO: 	Container vault-csi-provider ready: true, restart count 1
    Jan 23 14:01:37.447: INFO: nova-secrets-store-csi-driver-ft87l from nova-csi-drivers started at 2024-01-23 09:03:58 +0000 UTC (3 container statuses recorded)
    Jan 23 14:01:37.447: INFO: 	Container liveness-probe ready: true, restart count 1
    Jan 23 14:01:37.447: INFO: 	Container node-driver-registrar ready: true, restart count 1
    Jan 23 14:01:37.447: INFO: 	Container secrets-store ready: true, restart count 1
    Jan 23 14:01:37.447: INFO: nova-ingress-public-controller-b6dmc from nova-ingress-public started at 2024-01-23 13:24:57 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.447: INFO: 	Container controller ready: true, restart count 0
    Jan 23 14:01:37.447: INFO: nova-cadvisor-tq2rp from nova-monitoring started at 2024-01-23 09:09:34 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.447: INFO: 	Container nova-cadvisor ready: true, restart count 1
    Jan 23 14:01:37.447: INFO: nova-prometheus-node-exporter-lbfhw from nova-monitoring started at 2024-01-23 13:24:57 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.447: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 23 14:01:37.447: INFO: nova-oauth-secrets-webhook-hhwh4 from nova-secrets-webhook started at 2024-01-23 13:24:57 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.447: INFO: 	Container vault-secrets-webhook ready: true, restart count 0
    Jan 23 14:01:37.447: INFO: pod-hostip-73d8efe6-2ed8-4d0b-9549-326622908e68 from pods-7467 started at 2024-01-23 14:01:27 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.447: INFO: 	Container test ready: false, restart count 0
    Jan 23 14:01:37.447: INFO: alpine-nnp-false-27e76a33-555a-4176-a7ed-b72337ec5227 from security-context-test-3869 started at 2024-01-23 14:01:31 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.447: INFO: 	Container alpine-nnp-false-27e76a33-555a-4176-a7ed-b72337ec5227 ready: false, restart count 0
    Jan 23 14:01:37.447: INFO: sonobuoy from sonobuoy started at 2024-01-23 13:21:17 +0000 UTC (1 container statuses recorded)
    Jan 23 14:01:37.447: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 23 14:01:37.447: INFO: sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-7csgd from sonobuoy started at 2024-01-23 13:21:23 +0000 UTC (2 container statuses recorded)
    Jan 23 14:01:37.447: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 23 14:01:37.447: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/23/24 14:01:37.447
    Jan 23 14:01:37.466: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4067" to be "running"
    Jan 23 14:01:37.475: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.834468ms
    Jan 23 14:01:39.477: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011418569s
    Jan 23 14:01:39.477: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/23/24 14:01:39.479
    STEP: Trying to apply a random label on the found node. 01/23/24 14:01:39.487
    STEP: verifying the node has the label kubernetes.io/e2e-be74386b-e337-4e0d-b096-d9402d1af16e 42 01/23/24 14:01:39.496
    STEP: Trying to relaunch the pod, now with labels. 01/23/24 14:01:39.499
    Jan 23 14:01:39.511: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-4067" to be "not pending"
    Jan 23 14:01:39.513: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 1.780483ms
    Jan 23 14:01:41.516: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.004811579s
    Jan 23 14:01:41.516: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-be74386b-e337-4e0d-b096-d9402d1af16e off the node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local 01/23/24 14:01:41.517
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-be74386b-e337-4e0d-b096-d9402d1af16e 01/23/24 14:01:41.525
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:01:41.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4067" for this suite. 01/23/24 14:01:41.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:01:41.534
Jan 23 14:01:41.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename downward-api 01/23/24 14:01:41.535
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:01:41.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:01:41.543
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 01/23/24 14:01:41.545
Jan 23 14:01:41.559: INFO: Waiting up to 5m0s for pod "downward-api-bd13060d-19cb-43d6-beb1-506c7c21eaa9" in namespace "downward-api-2887" to be "Succeeded or Failed"
Jan 23 14:01:41.562: INFO: Pod "downward-api-bd13060d-19cb-43d6-beb1-506c7c21eaa9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.602724ms
Jan 23 14:01:43.565: INFO: Pod "downward-api-bd13060d-19cb-43d6-beb1-506c7c21eaa9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005697719s
Jan 23 14:01:45.564: INFO: Pod "downward-api-bd13060d-19cb-43d6-beb1-506c7c21eaa9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004694717s
STEP: Saw pod success 01/23/24 14:01:45.564
Jan 23 14:01:45.564: INFO: Pod "downward-api-bd13060d-19cb-43d6-beb1-506c7c21eaa9" satisfied condition "Succeeded or Failed"
Jan 23 14:01:45.565: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downward-api-bd13060d-19cb-43d6-beb1-506c7c21eaa9 container dapi-container: <nil>
STEP: delete the pod 01/23/24 14:01:45.57
Jan 23 14:01:45.576: INFO: Waiting for pod downward-api-bd13060d-19cb-43d6-beb1-506c7c21eaa9 to disappear
Jan 23 14:01:45.578: INFO: Pod downward-api-bd13060d-19cb-43d6-beb1-506c7c21eaa9 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 23 14:01:45.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2887" for this suite. 01/23/24 14:01:45.58
------------------------------
• [4.049 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:01:41.534
    Jan 23 14:01:41.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename downward-api 01/23/24 14:01:41.535
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:01:41.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:01:41.543
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 01/23/24 14:01:41.545
    Jan 23 14:01:41.559: INFO: Waiting up to 5m0s for pod "downward-api-bd13060d-19cb-43d6-beb1-506c7c21eaa9" in namespace "downward-api-2887" to be "Succeeded or Failed"
    Jan 23 14:01:41.562: INFO: Pod "downward-api-bd13060d-19cb-43d6-beb1-506c7c21eaa9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.602724ms
    Jan 23 14:01:43.565: INFO: Pod "downward-api-bd13060d-19cb-43d6-beb1-506c7c21eaa9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005697719s
    Jan 23 14:01:45.564: INFO: Pod "downward-api-bd13060d-19cb-43d6-beb1-506c7c21eaa9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004694717s
    STEP: Saw pod success 01/23/24 14:01:45.564
    Jan 23 14:01:45.564: INFO: Pod "downward-api-bd13060d-19cb-43d6-beb1-506c7c21eaa9" satisfied condition "Succeeded or Failed"
    Jan 23 14:01:45.565: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downward-api-bd13060d-19cb-43d6-beb1-506c7c21eaa9 container dapi-container: <nil>
    STEP: delete the pod 01/23/24 14:01:45.57
    Jan 23 14:01:45.576: INFO: Waiting for pod downward-api-bd13060d-19cb-43d6-beb1-506c7c21eaa9 to disappear
    Jan 23 14:01:45.578: INFO: Pod downward-api-bd13060d-19cb-43d6-beb1-506c7c21eaa9 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:01:45.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2887" for this suite. 01/23/24 14:01:45.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:01:45.583
Jan 23 14:01:45.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename container-probe 01/23/24 14:01:45.584
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:01:45.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:01:45.594
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 23 14:02:45.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3797" for this suite. 01/23/24 14:02:45.612
------------------------------
• [SLOW TEST] [60.033 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:01:45.583
    Jan 23 14:01:45.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename container-probe 01/23/24 14:01:45.584
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:01:45.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:01:45.594
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:02:45.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3797" for this suite. 01/23/24 14:02:45.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:02:45.616
Jan 23 14:02:45.616: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename csistoragecapacity 01/23/24 14:02:45.616
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:02:45.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:02:45.625
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/23/24 14:02:45.627
STEP: getting /apis/storage.k8s.io 01/23/24 14:02:45.629
STEP: getting /apis/storage.k8s.io/v1 01/23/24 14:02:45.63
STEP: creating 01/23/24 14:02:45.63
STEP: watching 01/23/24 14:02:45.639
Jan 23 14:02:45.639: INFO: starting watch
STEP: getting 01/23/24 14:02:45.642
STEP: listing in namespace 01/23/24 14:02:45.647
STEP: listing across namespaces 01/23/24 14:02:45.651
STEP: patching 01/23/24 14:02:45.653
STEP: updating 01/23/24 14:02:45.656
Jan 23 14:02:45.658: INFO: waiting for watch events with expected annotations in namespace
Jan 23 14:02:45.658: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/23/24 14:02:45.658
STEP: deleting a collection 01/23/24 14:02:45.664
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Jan 23 14:02:45.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-1957" for this suite. 01/23/24 14:02:45.671
------------------------------
• [0.059 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:02:45.616
    Jan 23 14:02:45.616: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename csistoragecapacity 01/23/24 14:02:45.616
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:02:45.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:02:45.625
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/23/24 14:02:45.627
    STEP: getting /apis/storage.k8s.io 01/23/24 14:02:45.629
    STEP: getting /apis/storage.k8s.io/v1 01/23/24 14:02:45.63
    STEP: creating 01/23/24 14:02:45.63
    STEP: watching 01/23/24 14:02:45.639
    Jan 23 14:02:45.639: INFO: starting watch
    STEP: getting 01/23/24 14:02:45.642
    STEP: listing in namespace 01/23/24 14:02:45.647
    STEP: listing across namespaces 01/23/24 14:02:45.651
    STEP: patching 01/23/24 14:02:45.653
    STEP: updating 01/23/24 14:02:45.656
    Jan 23 14:02:45.658: INFO: waiting for watch events with expected annotations in namespace
    Jan 23 14:02:45.658: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/23/24 14:02:45.658
    STEP: deleting a collection 01/23/24 14:02:45.664
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:02:45.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-1957" for this suite. 01/23/24 14:02:45.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:02:45.675
Jan 23 14:02:45.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename daemonsets 01/23/24 14:02:45.676
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:02:45.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:02:45.686
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
Jan 23 14:02:45.697: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/23/24 14:02:45.702
Jan 23 14:02:45.704: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 14:02:45.704: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/23/24 14:02:45.704
Jan 23 14:02:45.717: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 14:02:45.717: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:02:46.721: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 23 14:02:46.721: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/23/24 14:02:46.723
Jan 23 14:02:46.731: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 23 14:02:46.731: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 23 14:02:47.734: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 14:02:47.734: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/23/24 14:02:47.734
Jan 23 14:02:47.742: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 14:02:47.742: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:02:48.745: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 14:02:48.745: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:02:49.745: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 14:02:49.745: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:02:50.743: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 14:02:50.743: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:02:51.745: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 23 14:02:51.745: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 01/23/24 14:02:51.748
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2241, will wait for the garbage collector to delete the pods 01/23/24 14:02:51.748
Jan 23 14:02:51.805: INFO: Deleting DaemonSet.extensions daemon-set took: 5.245772ms
Jan 23 14:02:51.906: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.539652ms
Jan 23 14:02:54.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 14:02:54.909: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 23 14:02:54.910: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"127351"},"items":null}

Jan 23 14:02:54.911: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"127351"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:02:54.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2241" for this suite. 01/23/24 14:02:54.924
------------------------------
• [SLOW TEST] [9.252 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:02:45.675
    Jan 23 14:02:45.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename daemonsets 01/23/24 14:02:45.676
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:02:45.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:02:45.686
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:205
    Jan 23 14:02:45.697: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/23/24 14:02:45.702
    Jan 23 14:02:45.704: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 14:02:45.704: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/23/24 14:02:45.704
    Jan 23 14:02:45.717: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 14:02:45.717: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:02:46.721: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 23 14:02:46.721: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/23/24 14:02:46.723
    Jan 23 14:02:46.731: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 23 14:02:46.731: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jan 23 14:02:47.734: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 14:02:47.734: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/23/24 14:02:47.734
    Jan 23 14:02:47.742: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 14:02:47.742: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:02:48.745: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 14:02:48.745: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:02:49.745: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 14:02:49.745: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:02:50.743: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 14:02:50.743: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:02:51.745: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 23 14:02:51.745: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 01/23/24 14:02:51.748
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2241, will wait for the garbage collector to delete the pods 01/23/24 14:02:51.748
    Jan 23 14:02:51.805: INFO: Deleting DaemonSet.extensions daemon-set took: 5.245772ms
    Jan 23 14:02:51.906: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.539652ms
    Jan 23 14:02:54.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 14:02:54.909: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 23 14:02:54.910: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"127351"},"items":null}

    Jan 23 14:02:54.911: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"127351"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:02:54.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2241" for this suite. 01/23/24 14:02:54.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:02:54.928
Jan 23 14:02:54.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubectl 01/23/24 14:02:54.929
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:02:54.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:02:54.937
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 01/23/24 14:02:54.94
Jan 23 14:02:54.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-6369 create -f -'
Jan 23 14:02:56.693: INFO: stderr: ""
Jan 23 14:02:56.693: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/23/24 14:02:56.693
Jan 23 14:02:56.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-6369 diff -f -'
Jan 23 14:02:57.040: INFO: rc: 1
Jan 23 14:02:57.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-6369 delete -f -'
Jan 23 14:02:57.126: INFO: stderr: ""
Jan 23 14:02:57.126: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 23 14:02:57.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6369" for this suite. 01/23/24 14:02:57.129
------------------------------
• [2.204 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:02:54.928
    Jan 23 14:02:54.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubectl 01/23/24 14:02:54.929
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:02:54.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:02:54.937
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 01/23/24 14:02:54.94
    Jan 23 14:02:54.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-6369 create -f -'
    Jan 23 14:02:56.693: INFO: stderr: ""
    Jan 23 14:02:56.693: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/23/24 14:02:56.693
    Jan 23 14:02:56.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-6369 diff -f -'
    Jan 23 14:02:57.040: INFO: rc: 1
    Jan 23 14:02:57.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-6369 delete -f -'
    Jan 23 14:02:57.126: INFO: stderr: ""
    Jan 23 14:02:57.126: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:02:57.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6369" for this suite. 01/23/24 14:02:57.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:02:57.132
Jan 23 14:02:57.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename watch 01/23/24 14:02:57.133
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:02:57.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:02:57.142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/23/24 14:02:57.143
STEP: creating a new configmap 01/23/24 14:02:57.144
STEP: modifying the configmap once 01/23/24 14:02:57.147
STEP: closing the watch once it receives two notifications 01/23/24 14:02:57.153
Jan 23 14:02:57.153: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7950  46eb1dd6-d24c-4e68-9862-7b843227820b 127391 0 2024-01-23 14:02:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-23 14:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 23 14:02:57.153: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7950  46eb1dd6-d24c-4e68-9862-7b843227820b 127392 0 2024-01-23 14:02:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-23 14:02:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/23/24 14:02:57.153
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/23/24 14:02:57.161
STEP: deleting the configmap 01/23/24 14:02:57.161
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/23/24 14:02:57.165
Jan 23 14:02:57.165: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7950  46eb1dd6-d24c-4e68-9862-7b843227820b 127394 0 2024-01-23 14:02:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-23 14:02:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 23 14:02:57.165: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7950  46eb1dd6-d24c-4e68-9862-7b843227820b 127396 0 2024-01-23 14:02:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-23 14:02:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 23 14:02:57.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7950" for this suite. 01/23/24 14:02:57.167
------------------------------
• [0.038 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:02:57.132
    Jan 23 14:02:57.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename watch 01/23/24 14:02:57.133
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:02:57.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:02:57.142
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/23/24 14:02:57.143
    STEP: creating a new configmap 01/23/24 14:02:57.144
    STEP: modifying the configmap once 01/23/24 14:02:57.147
    STEP: closing the watch once it receives two notifications 01/23/24 14:02:57.153
    Jan 23 14:02:57.153: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7950  46eb1dd6-d24c-4e68-9862-7b843227820b 127391 0 2024-01-23 14:02:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-23 14:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 23 14:02:57.153: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7950  46eb1dd6-d24c-4e68-9862-7b843227820b 127392 0 2024-01-23 14:02:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-23 14:02:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/23/24 14:02:57.153
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/23/24 14:02:57.161
    STEP: deleting the configmap 01/23/24 14:02:57.161
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/23/24 14:02:57.165
    Jan 23 14:02:57.165: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7950  46eb1dd6-d24c-4e68-9862-7b843227820b 127394 0 2024-01-23 14:02:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-23 14:02:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 23 14:02:57.165: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7950  46eb1dd6-d24c-4e68-9862-7b843227820b 127396 0 2024-01-23 14:02:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-23 14:02:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:02:57.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7950" for this suite. 01/23/24 14:02:57.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:02:57.171
Jan 23 14:02:57.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename configmap 01/23/24 14:02:57.171
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:02:57.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:02:57.179
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-b3be484a-580b-46ad-a691-1ff787be4000 01/23/24 14:02:57.183
STEP: Creating the pod 01/23/24 14:02:57.188
Jan 23 14:02:57.203: INFO: Waiting up to 5m0s for pod "pod-configmaps-00024d7b-d201-44e9-bf3a-68374588e34c" in namespace "configmap-6069" to be "running and ready"
Jan 23 14:02:57.206: INFO: Pod "pod-configmaps-00024d7b-d201-44e9-bf3a-68374588e34c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.822618ms
Jan 23 14:02:57.206: INFO: The phase of Pod pod-configmaps-00024d7b-d201-44e9-bf3a-68374588e34c is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:02:59.210: INFO: Pod "pod-configmaps-00024d7b-d201-44e9-bf3a-68374588e34c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006293648s
Jan 23 14:02:59.210: INFO: The phase of Pod pod-configmaps-00024d7b-d201-44e9-bf3a-68374588e34c is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:03:01.210: INFO: Pod "pod-configmaps-00024d7b-d201-44e9-bf3a-68374588e34c": Phase="Running", Reason="", readiness=true. Elapsed: 4.006348635s
Jan 23 14:03:01.210: INFO: The phase of Pod pod-configmaps-00024d7b-d201-44e9-bf3a-68374588e34c is Running (Ready = true)
Jan 23 14:03:01.210: INFO: Pod "pod-configmaps-00024d7b-d201-44e9-bf3a-68374588e34c" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-b3be484a-580b-46ad-a691-1ff787be4000 01/23/24 14:03:01.215
STEP: waiting to observe update in volume 01/23/24 14:03:01.222
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 23 14:04:21.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6069" for this suite. 01/23/24 14:04:21.457
------------------------------
• [SLOW TEST] [84.291 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:02:57.171
    Jan 23 14:02:57.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename configmap 01/23/24 14:02:57.171
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:02:57.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:02:57.179
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-b3be484a-580b-46ad-a691-1ff787be4000 01/23/24 14:02:57.183
    STEP: Creating the pod 01/23/24 14:02:57.188
    Jan 23 14:02:57.203: INFO: Waiting up to 5m0s for pod "pod-configmaps-00024d7b-d201-44e9-bf3a-68374588e34c" in namespace "configmap-6069" to be "running and ready"
    Jan 23 14:02:57.206: INFO: Pod "pod-configmaps-00024d7b-d201-44e9-bf3a-68374588e34c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.822618ms
    Jan 23 14:02:57.206: INFO: The phase of Pod pod-configmaps-00024d7b-d201-44e9-bf3a-68374588e34c is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:02:59.210: INFO: Pod "pod-configmaps-00024d7b-d201-44e9-bf3a-68374588e34c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006293648s
    Jan 23 14:02:59.210: INFO: The phase of Pod pod-configmaps-00024d7b-d201-44e9-bf3a-68374588e34c is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:03:01.210: INFO: Pod "pod-configmaps-00024d7b-d201-44e9-bf3a-68374588e34c": Phase="Running", Reason="", readiness=true. Elapsed: 4.006348635s
    Jan 23 14:03:01.210: INFO: The phase of Pod pod-configmaps-00024d7b-d201-44e9-bf3a-68374588e34c is Running (Ready = true)
    Jan 23 14:03:01.210: INFO: Pod "pod-configmaps-00024d7b-d201-44e9-bf3a-68374588e34c" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-b3be484a-580b-46ad-a691-1ff787be4000 01/23/24 14:03:01.215
    STEP: waiting to observe update in volume 01/23/24 14:03:01.222
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:04:21.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6069" for this suite. 01/23/24 14:04:21.457
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:04:21.462
Jan 23 14:04:21.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename container-probe 01/23/24 14:04:21.462
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:04:21.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:04:21.473
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-da38a734-f600-4b25-950c-4d7eb3d09edd in namespace container-probe-6376 01/23/24 14:04:21.475
Jan 23 14:04:21.487: INFO: Waiting up to 5m0s for pod "liveness-da38a734-f600-4b25-950c-4d7eb3d09edd" in namespace "container-probe-6376" to be "not pending"
Jan 23 14:04:21.488: INFO: Pod "liveness-da38a734-f600-4b25-950c-4d7eb3d09edd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.306621ms
Jan 23 14:04:23.492: INFO: Pod "liveness-da38a734-f600-4b25-950c-4d7eb3d09edd": Phase="Running", Reason="", readiness=true. Elapsed: 2.005604164s
Jan 23 14:04:23.492: INFO: Pod "liveness-da38a734-f600-4b25-950c-4d7eb3d09edd" satisfied condition "not pending"
Jan 23 14:04:23.492: INFO: Started pod liveness-da38a734-f600-4b25-950c-4d7eb3d09edd in namespace container-probe-6376
STEP: checking the pod's current state and verifying that restartCount is present 01/23/24 14:04:23.492
Jan 23 14:04:23.494: INFO: Initial restart count of pod liveness-da38a734-f600-4b25-950c-4d7eb3d09edd is 0
STEP: deleting the pod 01/23/24 14:08:23.915
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 23 14:08:23.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6376" for this suite. 01/23/24 14:08:23.928
------------------------------
• [SLOW TEST] [242.469 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:04:21.462
    Jan 23 14:04:21.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename container-probe 01/23/24 14:04:21.462
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:04:21.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:04:21.473
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-da38a734-f600-4b25-950c-4d7eb3d09edd in namespace container-probe-6376 01/23/24 14:04:21.475
    Jan 23 14:04:21.487: INFO: Waiting up to 5m0s for pod "liveness-da38a734-f600-4b25-950c-4d7eb3d09edd" in namespace "container-probe-6376" to be "not pending"
    Jan 23 14:04:21.488: INFO: Pod "liveness-da38a734-f600-4b25-950c-4d7eb3d09edd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.306621ms
    Jan 23 14:04:23.492: INFO: Pod "liveness-da38a734-f600-4b25-950c-4d7eb3d09edd": Phase="Running", Reason="", readiness=true. Elapsed: 2.005604164s
    Jan 23 14:04:23.492: INFO: Pod "liveness-da38a734-f600-4b25-950c-4d7eb3d09edd" satisfied condition "not pending"
    Jan 23 14:04:23.492: INFO: Started pod liveness-da38a734-f600-4b25-950c-4d7eb3d09edd in namespace container-probe-6376
    STEP: checking the pod's current state and verifying that restartCount is present 01/23/24 14:04:23.492
    Jan 23 14:04:23.494: INFO: Initial restart count of pod liveness-da38a734-f600-4b25-950c-4d7eb3d09edd is 0
    STEP: deleting the pod 01/23/24 14:08:23.915
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:08:23.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6376" for this suite. 01/23/24 14:08:23.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:08:23.933
Jan 23 14:08:23.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename runtimeclass 01/23/24 14:08:23.933
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:08:23.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:08:23.942
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan 23 14:08:23.964: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2388 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 23 14:08:23.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2388" for this suite. 01/23/24 14:08:23.972
------------------------------
• [0.044 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:08:23.933
    Jan 23 14:08:23.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename runtimeclass 01/23/24 14:08:23.933
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:08:23.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:08:23.942
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan 23 14:08:23.964: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2388 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:08:23.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2388" for this suite. 01/23/24 14:08:23.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:08:23.977
Jan 23 14:08:23.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename configmap 01/23/24 14:08:23.977
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:08:23.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:08:23.989
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-2211bb50-9a6c-4041-be18-b99059b8428d 01/23/24 14:08:23.991
STEP: Creating a pod to test consume configMaps 01/23/24 14:08:23.995
Jan 23 14:08:24.009: INFO: Waiting up to 5m0s for pod "pod-configmaps-5aaa07cd-0bbb-4728-8a6e-0498eb536af2" in namespace "configmap-6791" to be "Succeeded or Failed"
Jan 23 14:08:24.011: INFO: Pod "pod-configmaps-5aaa07cd-0bbb-4728-8a6e-0498eb536af2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.566102ms
Jan 23 14:08:26.014: INFO: Pod "pod-configmaps-5aaa07cd-0bbb-4728-8a6e-0498eb536af2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005122999s
Jan 23 14:08:28.015: INFO: Pod "pod-configmaps-5aaa07cd-0bbb-4728-8a6e-0498eb536af2": Phase="Running", Reason="", readiness=false. Elapsed: 4.005681241s
Jan 23 14:08:30.015: INFO: Pod "pod-configmaps-5aaa07cd-0bbb-4728-8a6e-0498eb536af2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006171074s
STEP: Saw pod success 01/23/24 14:08:30.015
Jan 23 14:08:30.016: INFO: Pod "pod-configmaps-5aaa07cd-0bbb-4728-8a6e-0498eb536af2" satisfied condition "Succeeded or Failed"
Jan 23 14:08:30.017: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-5aaa07cd-0bbb-4728-8a6e-0498eb536af2 container configmap-volume-test: <nil>
STEP: delete the pod 01/23/24 14:08:30.028
Jan 23 14:08:30.036: INFO: Waiting for pod pod-configmaps-5aaa07cd-0bbb-4728-8a6e-0498eb536af2 to disappear
Jan 23 14:08:30.037: INFO: Pod pod-configmaps-5aaa07cd-0bbb-4728-8a6e-0498eb536af2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 23 14:08:30.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6791" for this suite. 01/23/24 14:08:30.039
------------------------------
• [SLOW TEST] [6.066 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:08:23.977
    Jan 23 14:08:23.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename configmap 01/23/24 14:08:23.977
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:08:23.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:08:23.989
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-2211bb50-9a6c-4041-be18-b99059b8428d 01/23/24 14:08:23.991
    STEP: Creating a pod to test consume configMaps 01/23/24 14:08:23.995
    Jan 23 14:08:24.009: INFO: Waiting up to 5m0s for pod "pod-configmaps-5aaa07cd-0bbb-4728-8a6e-0498eb536af2" in namespace "configmap-6791" to be "Succeeded or Failed"
    Jan 23 14:08:24.011: INFO: Pod "pod-configmaps-5aaa07cd-0bbb-4728-8a6e-0498eb536af2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.566102ms
    Jan 23 14:08:26.014: INFO: Pod "pod-configmaps-5aaa07cd-0bbb-4728-8a6e-0498eb536af2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005122999s
    Jan 23 14:08:28.015: INFO: Pod "pod-configmaps-5aaa07cd-0bbb-4728-8a6e-0498eb536af2": Phase="Running", Reason="", readiness=false. Elapsed: 4.005681241s
    Jan 23 14:08:30.015: INFO: Pod "pod-configmaps-5aaa07cd-0bbb-4728-8a6e-0498eb536af2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006171074s
    STEP: Saw pod success 01/23/24 14:08:30.015
    Jan 23 14:08:30.016: INFO: Pod "pod-configmaps-5aaa07cd-0bbb-4728-8a6e-0498eb536af2" satisfied condition "Succeeded or Failed"
    Jan 23 14:08:30.017: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-5aaa07cd-0bbb-4728-8a6e-0498eb536af2 container configmap-volume-test: <nil>
    STEP: delete the pod 01/23/24 14:08:30.028
    Jan 23 14:08:30.036: INFO: Waiting for pod pod-configmaps-5aaa07cd-0bbb-4728-8a6e-0498eb536af2 to disappear
    Jan 23 14:08:30.037: INFO: Pod pod-configmaps-5aaa07cd-0bbb-4728-8a6e-0498eb536af2 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:08:30.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6791" for this suite. 01/23/24 14:08:30.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:08:30.043
Jan 23 14:08:30.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename sysctl 01/23/24 14:08:30.044
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:08:30.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:08:30.058
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/23/24 14:08:30.06
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:08:30.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-7790" for this suite. 01/23/24 14:08:30.076
------------------------------
• [0.037 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:08:30.043
    Jan 23 14:08:30.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename sysctl 01/23/24 14:08:30.044
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:08:30.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:08:30.058
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/23/24 14:08:30.06
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:08:30.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-7790" for this suite. 01/23/24 14:08:30.076
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:08:30.081
Jan 23 14:08:30.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename job 01/23/24 14:08:30.082
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:08:30.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:08:30.09
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 01/23/24 14:08:30.095
STEP: Patching the Job 01/23/24 14:08:30.097
STEP: Watching for Job to be patched 01/23/24 14:08:30.109
Jan 23 14:08:30.110: INFO: Event ADDED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 23 14:08:30.110: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 23 14:08:30.110: INFO: Event MODIFIED found for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/23/24 14:08:30.11
STEP: Watching for Job to be updated 01/23/24 14:08:30.115
Jan 23 14:08:30.116: INFO: Event MODIFIED found for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 23 14:08:30.116: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/23/24 14:08:30.116
Jan 23 14:08:30.117: INFO: Job: e2e-jxztq as labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched]
STEP: Waiting for job to complete 01/23/24 14:08:30.117
STEP: Delete a job collection with a labelselector 01/23/24 14:08:40.121
STEP: Watching for Job to be deleted 01/23/24 14:08:40.125
Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 23 14:08:40.127: INFO: Event DELETED found for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/23/24 14:08:40.127
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 23 14:08:40.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1306" for this suite. 01/23/24 14:08:40.137
------------------------------
• [SLOW TEST] [10.059 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:08:30.081
    Jan 23 14:08:30.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename job 01/23/24 14:08:30.082
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:08:30.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:08:30.09
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 01/23/24 14:08:30.095
    STEP: Patching the Job 01/23/24 14:08:30.097
    STEP: Watching for Job to be patched 01/23/24 14:08:30.109
    Jan 23 14:08:30.110: INFO: Event ADDED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 23 14:08:30.110: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 23 14:08:30.110: INFO: Event MODIFIED found for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/23/24 14:08:30.11
    STEP: Watching for Job to be updated 01/23/24 14:08:30.115
    Jan 23 14:08:30.116: INFO: Event MODIFIED found for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 23 14:08:30.116: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/23/24 14:08:30.116
    Jan 23 14:08:30.117: INFO: Job: e2e-jxztq as labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched]
    STEP: Waiting for job to complete 01/23/24 14:08:30.117
    STEP: Delete a job collection with a labelselector 01/23/24 14:08:40.121
    STEP: Watching for Job to be deleted 01/23/24 14:08:40.125
    Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 23 14:08:40.127: INFO: Event MODIFIED observed for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 23 14:08:40.127: INFO: Event DELETED found for Job e2e-jxztq in namespace job-1306 with labels: map[e2e-job-label:e2e-jxztq e2e-jxztq:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/23/24 14:08:40.127
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:08:40.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1306" for this suite. 01/23/24 14:08:40.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:08:40.14
Jan 23 14:08:40.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename limitrange 01/23/24 14:08:40.141
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:08:40.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:08:40.153
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 01/23/24 14:08:40.154
STEP: Setting up watch 01/23/24 14:08:40.155
STEP: Submitting a LimitRange 01/23/24 14:08:40.164
STEP: Verifying LimitRange creation was observed 01/23/24 14:08:40.168
STEP: Fetching the LimitRange to ensure it has proper values 01/23/24 14:08:40.168
Jan 23 14:08:40.174: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 23 14:08:40.174: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/23/24 14:08:40.174
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/23/24 14:08:40.184
Jan 23 14:08:40.186: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 23 14:08:40.186: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/23/24 14:08:40.186
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/23/24 14:08:40.198
Jan 23 14:08:40.200: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 23 14:08:40.200: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/23/24 14:08:40.2
STEP: Failing to create a Pod with more than max resources 01/23/24 14:08:40.208
STEP: Updating a LimitRange 01/23/24 14:08:40.215
STEP: Verifying LimitRange updating is effective 01/23/24 14:08:40.219
STEP: Creating a Pod with less than former min resources 01/23/24 14:08:42.222
STEP: Failing to create a Pod with more than max resources 01/23/24 14:08:42.235
STEP: Deleting a LimitRange 01/23/24 14:08:42.242
STEP: Verifying the LimitRange was deleted 01/23/24 14:08:42.246
Jan 23 14:08:47.248: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/23/24 14:08:47.248
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan 23 14:08:47.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-1702" for this suite. 01/23/24 14:08:47.261
------------------------------
• [SLOW TEST] [7.125 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:08:40.14
    Jan 23 14:08:40.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename limitrange 01/23/24 14:08:40.141
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:08:40.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:08:40.153
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 01/23/24 14:08:40.154
    STEP: Setting up watch 01/23/24 14:08:40.155
    STEP: Submitting a LimitRange 01/23/24 14:08:40.164
    STEP: Verifying LimitRange creation was observed 01/23/24 14:08:40.168
    STEP: Fetching the LimitRange to ensure it has proper values 01/23/24 14:08:40.168
    Jan 23 14:08:40.174: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 23 14:08:40.174: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/23/24 14:08:40.174
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/23/24 14:08:40.184
    Jan 23 14:08:40.186: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 23 14:08:40.186: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/23/24 14:08:40.186
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/23/24 14:08:40.198
    Jan 23 14:08:40.200: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan 23 14:08:40.200: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/23/24 14:08:40.2
    STEP: Failing to create a Pod with more than max resources 01/23/24 14:08:40.208
    STEP: Updating a LimitRange 01/23/24 14:08:40.215
    STEP: Verifying LimitRange updating is effective 01/23/24 14:08:40.219
    STEP: Creating a Pod with less than former min resources 01/23/24 14:08:42.222
    STEP: Failing to create a Pod with more than max resources 01/23/24 14:08:42.235
    STEP: Deleting a LimitRange 01/23/24 14:08:42.242
    STEP: Verifying the LimitRange was deleted 01/23/24 14:08:42.246
    Jan 23 14:08:47.248: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/23/24 14:08:47.248
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:08:47.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-1702" for this suite. 01/23/24 14:08:47.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:08:47.266
Jan 23 14:08:47.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename secrets 01/23/24 14:08:47.266
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:08:47.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:08:47.275
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 01/23/24 14:08:47.277
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/23/24 14:08:47.28
STEP: patching the secret 01/23/24 14:08:47.285
STEP: deleting the secret using a LabelSelector 01/23/24 14:08:47.292
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/23/24 14:08:47.298
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 23 14:08:47.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2120" for this suite. 01/23/24 14:08:47.306
------------------------------
• [0.043 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:08:47.266
    Jan 23 14:08:47.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename secrets 01/23/24 14:08:47.266
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:08:47.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:08:47.275
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 01/23/24 14:08:47.277
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/23/24 14:08:47.28
    STEP: patching the secret 01/23/24 14:08:47.285
    STEP: deleting the secret using a LabelSelector 01/23/24 14:08:47.292
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/23/24 14:08:47.298
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:08:47.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2120" for this suite. 01/23/24 14:08:47.306
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:08:47.309
Jan 23 14:08:47.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename crd-watch 01/23/24 14:08:47.309
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:08:47.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:08:47.319
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan 23 14:08:47.321: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Creating first CR  01/23/24 14:08:54.389
Jan 23 14:08:54.393: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-23T14:08:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-23T14:08:54Z]] name:name1 resourceVersion:129555 uid:d4fe7443-c3d6-4932-b34f-6ddf4631df7e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/23/24 14:09:04.394
Jan 23 14:09:04.398: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-23T14:09:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-23T14:09:04Z]] name:name2 resourceVersion:129598 uid:80628f0b-a7e0-46f9-a5b0-fde862e60b08] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/23/24 14:09:14.4
Jan 23 14:09:14.406: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-23T14:08:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-23T14:09:14Z]] name:name1 resourceVersion:129644 uid:d4fe7443-c3d6-4932-b34f-6ddf4631df7e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/23/24 14:09:24.406
Jan 23 14:09:24.411: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-23T14:09:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-23T14:09:24Z]] name:name2 resourceVersion:129690 uid:80628f0b-a7e0-46f9-a5b0-fde862e60b08] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/23/24 14:09:34.412
Jan 23 14:09:34.417: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-23T14:08:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-23T14:09:14Z]] name:name1 resourceVersion:129733 uid:d4fe7443-c3d6-4932-b34f-6ddf4631df7e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/23/24 14:09:44.417
Jan 23 14:09:44.422: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-23T14:09:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-23T14:09:24Z]] name:name2 resourceVersion:129779 uid:80628f0b-a7e0-46f9-a5b0-fde862e60b08] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:09:54.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-3980" for this suite. 01/23/24 14:09:54.934
------------------------------
• [SLOW TEST] [67.629 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:08:47.309
    Jan 23 14:08:47.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename crd-watch 01/23/24 14:08:47.309
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:08:47.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:08:47.319
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan 23 14:08:47.321: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Creating first CR  01/23/24 14:08:54.389
    Jan 23 14:08:54.393: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-23T14:08:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-23T14:08:54Z]] name:name1 resourceVersion:129555 uid:d4fe7443-c3d6-4932-b34f-6ddf4631df7e] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/23/24 14:09:04.394
    Jan 23 14:09:04.398: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-23T14:09:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-23T14:09:04Z]] name:name2 resourceVersion:129598 uid:80628f0b-a7e0-46f9-a5b0-fde862e60b08] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/23/24 14:09:14.4
    Jan 23 14:09:14.406: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-23T14:08:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-23T14:09:14Z]] name:name1 resourceVersion:129644 uid:d4fe7443-c3d6-4932-b34f-6ddf4631df7e] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/23/24 14:09:24.406
    Jan 23 14:09:24.411: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-23T14:09:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-23T14:09:24Z]] name:name2 resourceVersion:129690 uid:80628f0b-a7e0-46f9-a5b0-fde862e60b08] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/23/24 14:09:34.412
    Jan 23 14:09:34.417: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-23T14:08:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-23T14:09:14Z]] name:name1 resourceVersion:129733 uid:d4fe7443-c3d6-4932-b34f-6ddf4631df7e] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/23/24 14:09:44.417
    Jan 23 14:09:44.422: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-23T14:09:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-23T14:09:24Z]] name:name2 resourceVersion:129779 uid:80628f0b-a7e0-46f9-a5b0-fde862e60b08] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:09:54.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-3980" for this suite. 01/23/24 14:09:54.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:09:54.938
Jan 23 14:09:54.938: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename svcaccounts 01/23/24 14:09:54.938
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:09:54.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:09:54.954
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 01/23/24 14:09:54.958
STEP: watching for the ServiceAccount to be added 01/23/24 14:09:54.961
STEP: patching the ServiceAccount 01/23/24 14:09:54.962
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/23/24 14:09:54.966
STEP: deleting the ServiceAccount 01/23/24 14:09:54.969
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 23 14:09:54.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1941" for this suite. 01/23/24 14:09:54.976
------------------------------
• [0.040 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:09:54.938
    Jan 23 14:09:54.938: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename svcaccounts 01/23/24 14:09:54.938
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:09:54.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:09:54.954
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 01/23/24 14:09:54.958
    STEP: watching for the ServiceAccount to be added 01/23/24 14:09:54.961
    STEP: patching the ServiceAccount 01/23/24 14:09:54.962
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/23/24 14:09:54.966
    STEP: deleting the ServiceAccount 01/23/24 14:09:54.969
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:09:54.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1941" for this suite. 01/23/24 14:09:54.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:09:54.978
Jan 23 14:09:54.979: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename container-lifecycle-hook 01/23/24 14:09:54.979
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:09:54.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:09:54.987
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/23/24 14:09:54.995
Jan 23 14:09:55.010: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-169" to be "running and ready"
Jan 23 14:09:55.012: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.24914ms
Jan 23 14:09:55.012: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:09:57.015: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004543719s
Jan 23 14:09:57.015: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 23 14:09:57.015: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 01/23/24 14:09:57.016
Jan 23 14:09:57.025: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-169" to be "running and ready"
Jan 23 14:09:57.027: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137679ms
Jan 23 14:09:57.027: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:09:59.029: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004472584s
Jan 23 14:09:59.029: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan 23 14:09:59.029: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/23/24 14:09:59.031
STEP: delete the pod with lifecycle hook 01/23/24 14:09:59.034
Jan 23 14:09:59.039: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 14:09:59.041: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 14:10:01.041: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 14:10:01.043: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 14:10:03.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 14:10:03.044: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 23 14:10:03.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-169" for this suite. 01/23/24 14:10:03.047
------------------------------
• [SLOW TEST] [8.074 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:09:54.978
    Jan 23 14:09:54.979: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/23/24 14:09:54.979
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:09:54.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:09:54.987
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/23/24 14:09:54.995
    Jan 23 14:09:55.010: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-169" to be "running and ready"
    Jan 23 14:09:55.012: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.24914ms
    Jan 23 14:09:55.012: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:09:57.015: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004543719s
    Jan 23 14:09:57.015: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 23 14:09:57.015: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 01/23/24 14:09:57.016
    Jan 23 14:09:57.025: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-169" to be "running and ready"
    Jan 23 14:09:57.027: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137679ms
    Jan 23 14:09:57.027: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:09:59.029: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004472584s
    Jan 23 14:09:59.029: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan 23 14:09:59.029: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/23/24 14:09:59.031
    STEP: delete the pod with lifecycle hook 01/23/24 14:09:59.034
    Jan 23 14:09:59.039: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 23 14:09:59.041: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 23 14:10:01.041: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 23 14:10:01.043: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 23 14:10:03.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 23 14:10:03.044: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:10:03.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-169" for this suite. 01/23/24 14:10:03.047
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:10:03.053
Jan 23 14:10:03.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename replicaset 01/23/24 14:10:03.054
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:10:03.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:10:03.062
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan 23 14:10:03.071: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 23 14:10:08.075: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/23/24 14:10:08.075
STEP: Scaling up "test-rs" replicaset  01/23/24 14:10:08.075
Jan 23 14:10:08.081: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/23/24 14:10:08.081
W0123 14:10:08.085602      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 23 14:10:08.086: INFO: observed ReplicaSet test-rs in namespace replicaset-7140 with ReadyReplicas 1, AvailableReplicas 1
Jan 23 14:10:08.098: INFO: observed ReplicaSet test-rs in namespace replicaset-7140 with ReadyReplicas 1, AvailableReplicas 1
Jan 23 14:10:08.115: INFO: observed ReplicaSet test-rs in namespace replicaset-7140 with ReadyReplicas 1, AvailableReplicas 1
Jan 23 14:10:08.120: INFO: observed ReplicaSet test-rs in namespace replicaset-7140 with ReadyReplicas 1, AvailableReplicas 1
Jan 23 14:10:09.069: INFO: observed ReplicaSet test-rs in namespace replicaset-7140 with ReadyReplicas 2, AvailableReplicas 2
Jan 23 14:10:10.465: INFO: observed Replicaset test-rs in namespace replicaset-7140 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 23 14:10:10.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7140" for this suite. 01/23/24 14:10:10.468
------------------------------
• [SLOW TEST] [7.418 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:10:03.053
    Jan 23 14:10:03.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename replicaset 01/23/24 14:10:03.054
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:10:03.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:10:03.062
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan 23 14:10:03.071: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 23 14:10:08.075: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/23/24 14:10:08.075
    STEP: Scaling up "test-rs" replicaset  01/23/24 14:10:08.075
    Jan 23 14:10:08.081: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/23/24 14:10:08.081
    W0123 14:10:08.085602      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 23 14:10:08.086: INFO: observed ReplicaSet test-rs in namespace replicaset-7140 with ReadyReplicas 1, AvailableReplicas 1
    Jan 23 14:10:08.098: INFO: observed ReplicaSet test-rs in namespace replicaset-7140 with ReadyReplicas 1, AvailableReplicas 1
    Jan 23 14:10:08.115: INFO: observed ReplicaSet test-rs in namespace replicaset-7140 with ReadyReplicas 1, AvailableReplicas 1
    Jan 23 14:10:08.120: INFO: observed ReplicaSet test-rs in namespace replicaset-7140 with ReadyReplicas 1, AvailableReplicas 1
    Jan 23 14:10:09.069: INFO: observed ReplicaSet test-rs in namespace replicaset-7140 with ReadyReplicas 2, AvailableReplicas 2
    Jan 23 14:10:10.465: INFO: observed Replicaset test-rs in namespace replicaset-7140 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:10:10.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7140" for this suite. 01/23/24 14:10:10.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:10:10.472
Jan 23 14:10:10.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename deployment 01/23/24 14:10:10.473
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:10:10.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:10:10.482
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan 23 14:10:10.485: INFO: Creating deployment "test-recreate-deployment"
Jan 23 14:10:10.489: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 23 14:10:10.493: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jan 23 14:10:12.498: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 23 14:10:12.499: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 23 14:10:12.504: INFO: Updating deployment test-recreate-deployment
Jan 23 14:10:12.504: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 23 14:10:12.556: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5270  65cdfef3-fb70-439e-9e67-869724ed58fd 130105 2 2024-01-23 14:10:10 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-23 14:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00186b768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-01-23 14:10:12 +0000 UTC,LastTransitionTime:2024-01-23 14:10:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2024-01-23 14:10:12 +0000 UTC,LastTransitionTime:2024-01-23 14:10:10 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 23 14:10:12.557: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-5270  31750653-45ab-48bb-803e-6a93cb2b3f8a 130103 1 2024-01-23 14:10:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 65cdfef3-fb70-439e-9e67-869724ed58fd 0xc00186bc50 0xc00186bc51}] [] [{kube-controller-manager Update apps/v1 2024-01-23 14:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"65cdfef3-fb70-439e-9e67-869724ed58fd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:10:12 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00186bce8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 23 14:10:12.557: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 23 14:10:12.557: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-5270  4471fdd6-26e0-4e83-9c86-9c2c1e344417 130093 2 2024-01-23 14:10:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 65cdfef3-fb70-439e-9e67-869724ed58fd 0xc00186bb37 0xc00186bb38}] [] [{kube-controller-manager Update apps/v1 2024-01-23 14:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"65cdfef3-fb70-439e-9e67-869724ed58fd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:10:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00186bbe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 23 14:10:12.559: INFO: Pod "test-recreate-deployment-cff6dc657-v98dw" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-v98dw test-recreate-deployment-cff6dc657- deployment-5270  b39f0f47-87ff-46e7-b9ad-694c35ee2a4f 130104 0 2024-01-23 14:10:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 31750653-45ab-48bb-803e-6a93cb2b3f8a 0xc004c707d0 0xc004c707d1}] [] [{kube-controller-manager Update v1 2024-01-23 14:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"31750653-45ab-48bb-803e-6a93cb2b3f8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 14:10:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rpqw4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rpqw4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:10:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:10:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:10:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:10:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:,StartTime:2024-01-23 14:10:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 23 14:10:12.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5270" for this suite. 01/23/24 14:10:12.561
------------------------------
• [2.092 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:10:10.472
    Jan 23 14:10:10.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename deployment 01/23/24 14:10:10.473
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:10:10.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:10:10.482
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan 23 14:10:10.485: INFO: Creating deployment "test-recreate-deployment"
    Jan 23 14:10:10.489: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan 23 14:10:10.493: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
    Jan 23 14:10:12.498: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan 23 14:10:12.499: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan 23 14:10:12.504: INFO: Updating deployment test-recreate-deployment
    Jan 23 14:10:12.504: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 23 14:10:12.556: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-5270  65cdfef3-fb70-439e-9e67-869724ed58fd 130105 2 2024-01-23 14:10:10 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-23 14:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00186b768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-01-23 14:10:12 +0000 UTC,LastTransitionTime:2024-01-23 14:10:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2024-01-23 14:10:12 +0000 UTC,LastTransitionTime:2024-01-23 14:10:10 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 23 14:10:12.557: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-5270  31750653-45ab-48bb-803e-6a93cb2b3f8a 130103 1 2024-01-23 14:10:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 65cdfef3-fb70-439e-9e67-869724ed58fd 0xc00186bc50 0xc00186bc51}] [] [{kube-controller-manager Update apps/v1 2024-01-23 14:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"65cdfef3-fb70-439e-9e67-869724ed58fd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:10:12 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00186bce8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 23 14:10:12.557: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan 23 14:10:12.557: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-5270  4471fdd6-26e0-4e83-9c86-9c2c1e344417 130093 2 2024-01-23 14:10:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 65cdfef3-fb70-439e-9e67-869724ed58fd 0xc00186bb37 0xc00186bb38}] [] [{kube-controller-manager Update apps/v1 2024-01-23 14:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"65cdfef3-fb70-439e-9e67-869724ed58fd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:10:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00186bbe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 23 14:10:12.559: INFO: Pod "test-recreate-deployment-cff6dc657-v98dw" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-v98dw test-recreate-deployment-cff6dc657- deployment-5270  b39f0f47-87ff-46e7-b9ad-694c35ee2a4f 130104 0 2024-01-23 14:10:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 31750653-45ab-48bb-803e-6a93cb2b3f8a 0xc004c707d0 0xc004c707d1}] [] [{kube-controller-manager Update v1 2024-01-23 14:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"31750653-45ab-48bb-803e-6a93cb2b3f8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 14:10:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rpqw4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rpqw4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:10:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:10:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:10:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:10:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:,StartTime:2024-01-23 14:10:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:10:12.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5270" for this suite. 01/23/24 14:10:12.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:10:12.564
Jan 23 14:10:12.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubectl 01/23/24 14:10:12.565
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:10:12.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:10:12.576
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 01/23/24 14:10:12.578
Jan 23 14:10:12.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1383 create -f -'
Jan 23 14:10:14.267: INFO: stderr: ""
Jan 23 14:10:14.267: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/23/24 14:10:14.267
Jan 23 14:10:15.270: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 14:10:15.270: INFO: Found 0 / 1
Jan 23 14:10:16.269: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 14:10:16.269: INFO: Found 1 / 1
Jan 23 14:10:16.269: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/23/24 14:10:16.269
Jan 23 14:10:16.271: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 14:10:16.271: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 23 14:10:16.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1383 patch pod agnhost-primary-wgzz9 -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 23 14:10:16.423: INFO: stderr: ""
Jan 23 14:10:16.423: INFO: stdout: "pod/agnhost-primary-wgzz9 patched\n"
STEP: checking annotations 01/23/24 14:10:16.423
Jan 23 14:10:16.425: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 14:10:16.425: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 23 14:10:16.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1383" for this suite. 01/23/24 14:10:16.428
------------------------------
• [3.868 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:10:12.564
    Jan 23 14:10:12.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubectl 01/23/24 14:10:12.565
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:10:12.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:10:12.576
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 01/23/24 14:10:12.578
    Jan 23 14:10:12.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1383 create -f -'
    Jan 23 14:10:14.267: INFO: stderr: ""
    Jan 23 14:10:14.267: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/23/24 14:10:14.267
    Jan 23 14:10:15.270: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 23 14:10:15.270: INFO: Found 0 / 1
    Jan 23 14:10:16.269: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 23 14:10:16.269: INFO: Found 1 / 1
    Jan 23 14:10:16.269: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/23/24 14:10:16.269
    Jan 23 14:10:16.271: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 23 14:10:16.271: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 23 14:10:16.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1383 patch pod agnhost-primary-wgzz9 -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan 23 14:10:16.423: INFO: stderr: ""
    Jan 23 14:10:16.423: INFO: stdout: "pod/agnhost-primary-wgzz9 patched\n"
    STEP: checking annotations 01/23/24 14:10:16.423
    Jan 23 14:10:16.425: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 23 14:10:16.425: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:10:16.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1383" for this suite. 01/23/24 14:10:16.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:10:16.433
Jan 23 14:10:16.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename services 01/23/24 14:10:16.434
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:10:16.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:10:16.446
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-1768 01/23/24 14:10:16.448
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1768 to expose endpoints map[] 01/23/24 14:10:16.455
Jan 23 14:10:16.457: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan 23 14:10:17.463: INFO: successfully validated that service endpoint-test2 in namespace services-1768 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1768 01/23/24 14:10:17.463
Jan 23 14:10:17.476: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1768" to be "running and ready"
Jan 23 14:10:17.480: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.540249ms
Jan 23 14:10:17.480: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:10:19.482: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006365344s
Jan 23 14:10:19.482: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 23 14:10:19.482: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1768 to expose endpoints map[pod1:[80]] 01/23/24 14:10:19.484
Jan 23 14:10:19.489: INFO: successfully validated that service endpoint-test2 in namespace services-1768 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/23/24 14:10:19.489
Jan 23 14:10:19.490: INFO: Creating new exec pod
Jan 23 14:10:19.498: INFO: Waiting up to 5m0s for pod "execpodz78ms" in namespace "services-1768" to be "running"
Jan 23 14:10:19.500: INFO: Pod "execpodz78ms": Phase="Pending", Reason="", readiness=false. Elapsed: 1.51424ms
Jan 23 14:10:21.509: INFO: Pod "execpodz78ms": Phase="Running", Reason="", readiness=true. Elapsed: 2.010921574s
Jan 23 14:10:21.509: INFO: Pod "execpodz78ms" satisfied condition "running"
Jan 23 14:10:22.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-1768 exec execpodz78ms -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 23 14:10:23.015: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 23 14:10:23.015: INFO: stdout: ""
Jan 23 14:10:23.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-1768 exec execpodz78ms -- /bin/sh -x -c nc -v -z -w 2 10.233.50.241 80'
Jan 23 14:10:23.271: INFO: stderr: "+ nc -v -z -w 2 10.233.50.241 80\nConnection to 10.233.50.241 80 port [tcp/http] succeeded!\n"
Jan 23 14:10:23.271: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-1768 01/23/24 14:10:23.271
Jan 23 14:10:23.283: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1768" to be "running and ready"
Jan 23 14:10:23.285: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.537651ms
Jan 23 14:10:23.285: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:10:25.288: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005342925s
Jan 23 14:10:25.288: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 23 14:10:25.288: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1768 to expose endpoints map[pod1:[80] pod2:[80]] 01/23/24 14:10:25.29
Jan 23 14:10:25.296: INFO: successfully validated that service endpoint-test2 in namespace services-1768 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/23/24 14:10:25.296
Jan 23 14:10:26.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-1768 exec execpodz78ms -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 23 14:10:26.466: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 23 14:10:26.466: INFO: stdout: ""
Jan 23 14:10:26.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-1768 exec execpodz78ms -- /bin/sh -x -c nc -v -z -w 2 10.233.50.241 80'
Jan 23 14:10:26.636: INFO: stderr: "+ nc -v -z -w 2 10.233.50.241 80\nConnection to 10.233.50.241 80 port [tcp/http] succeeded!\n"
Jan 23 14:10:26.636: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-1768 01/23/24 14:10:26.636
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1768 to expose endpoints map[pod2:[80]] 01/23/24 14:10:26.644
Jan 23 14:10:26.649: INFO: successfully validated that service endpoint-test2 in namespace services-1768 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/23/24 14:10:26.649
Jan 23 14:10:27.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-1768 exec execpodz78ms -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 23 14:10:27.826: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 23 14:10:27.826: INFO: stdout: ""
Jan 23 14:10:27.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-1768 exec execpodz78ms -- /bin/sh -x -c nc -v -z -w 2 10.233.50.241 80'
Jan 23 14:10:28.023: INFO: stderr: "+ nc -v -z -w 2 10.233.50.241 80\nConnection to 10.233.50.241 80 port [tcp/http] succeeded!\n"
Jan 23 14:10:28.023: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-1768 01/23/24 14:10:28.023
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1768 to expose endpoints map[] 01/23/24 14:10:28.032
Jan 23 14:10:28.042: INFO: successfully validated that service endpoint-test2 in namespace services-1768 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 23 14:10:28.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1768" for this suite. 01/23/24 14:10:28.059
------------------------------
• [SLOW TEST] [11.629 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:10:16.433
    Jan 23 14:10:16.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename services 01/23/24 14:10:16.434
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:10:16.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:10:16.446
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-1768 01/23/24 14:10:16.448
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1768 to expose endpoints map[] 01/23/24 14:10:16.455
    Jan 23 14:10:16.457: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jan 23 14:10:17.463: INFO: successfully validated that service endpoint-test2 in namespace services-1768 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1768 01/23/24 14:10:17.463
    Jan 23 14:10:17.476: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1768" to be "running and ready"
    Jan 23 14:10:17.480: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.540249ms
    Jan 23 14:10:17.480: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:10:19.482: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006365344s
    Jan 23 14:10:19.482: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 23 14:10:19.482: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1768 to expose endpoints map[pod1:[80]] 01/23/24 14:10:19.484
    Jan 23 14:10:19.489: INFO: successfully validated that service endpoint-test2 in namespace services-1768 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/23/24 14:10:19.489
    Jan 23 14:10:19.490: INFO: Creating new exec pod
    Jan 23 14:10:19.498: INFO: Waiting up to 5m0s for pod "execpodz78ms" in namespace "services-1768" to be "running"
    Jan 23 14:10:19.500: INFO: Pod "execpodz78ms": Phase="Pending", Reason="", readiness=false. Elapsed: 1.51424ms
    Jan 23 14:10:21.509: INFO: Pod "execpodz78ms": Phase="Running", Reason="", readiness=true. Elapsed: 2.010921574s
    Jan 23 14:10:21.509: INFO: Pod "execpodz78ms" satisfied condition "running"
    Jan 23 14:10:22.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-1768 exec execpodz78ms -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 23 14:10:23.015: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 23 14:10:23.015: INFO: stdout: ""
    Jan 23 14:10:23.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-1768 exec execpodz78ms -- /bin/sh -x -c nc -v -z -w 2 10.233.50.241 80'
    Jan 23 14:10:23.271: INFO: stderr: "+ nc -v -z -w 2 10.233.50.241 80\nConnection to 10.233.50.241 80 port [tcp/http] succeeded!\n"
    Jan 23 14:10:23.271: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-1768 01/23/24 14:10:23.271
    Jan 23 14:10:23.283: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1768" to be "running and ready"
    Jan 23 14:10:23.285: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.537651ms
    Jan 23 14:10:23.285: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:10:25.288: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005342925s
    Jan 23 14:10:25.288: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 23 14:10:25.288: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1768 to expose endpoints map[pod1:[80] pod2:[80]] 01/23/24 14:10:25.29
    Jan 23 14:10:25.296: INFO: successfully validated that service endpoint-test2 in namespace services-1768 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/23/24 14:10:25.296
    Jan 23 14:10:26.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-1768 exec execpodz78ms -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 23 14:10:26.466: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 23 14:10:26.466: INFO: stdout: ""
    Jan 23 14:10:26.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-1768 exec execpodz78ms -- /bin/sh -x -c nc -v -z -w 2 10.233.50.241 80'
    Jan 23 14:10:26.636: INFO: stderr: "+ nc -v -z -w 2 10.233.50.241 80\nConnection to 10.233.50.241 80 port [tcp/http] succeeded!\n"
    Jan 23 14:10:26.636: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-1768 01/23/24 14:10:26.636
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1768 to expose endpoints map[pod2:[80]] 01/23/24 14:10:26.644
    Jan 23 14:10:26.649: INFO: successfully validated that service endpoint-test2 in namespace services-1768 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/23/24 14:10:26.649
    Jan 23 14:10:27.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-1768 exec execpodz78ms -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 23 14:10:27.826: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 23 14:10:27.826: INFO: stdout: ""
    Jan 23 14:10:27.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-1768 exec execpodz78ms -- /bin/sh -x -c nc -v -z -w 2 10.233.50.241 80'
    Jan 23 14:10:28.023: INFO: stderr: "+ nc -v -z -w 2 10.233.50.241 80\nConnection to 10.233.50.241 80 port [tcp/http] succeeded!\n"
    Jan 23 14:10:28.023: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-1768 01/23/24 14:10:28.023
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1768 to expose endpoints map[] 01/23/24 14:10:28.032
    Jan 23 14:10:28.042: INFO: successfully validated that service endpoint-test2 in namespace services-1768 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:10:28.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1768" for this suite. 01/23/24 14:10:28.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:10:28.063
Jan 23 14:10:28.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename resourcequota 01/23/24 14:10:28.064
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:10:28.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:10:28.082
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 01/23/24 14:10:28.084
STEP: Creating a ResourceQuota 01/23/24 14:10:33.086
STEP: Ensuring resource quota status is calculated 01/23/24 14:10:33.09
STEP: Creating a ReplicaSet 01/23/24 14:10:35.093
STEP: Ensuring resource quota status captures replicaset creation 01/23/24 14:10:35.1
STEP: Deleting a ReplicaSet 01/23/24 14:10:37.104
STEP: Ensuring resource quota status released usage 01/23/24 14:10:37.107
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 23 14:10:39.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7328" for this suite. 01/23/24 14:10:39.113
------------------------------
• [SLOW TEST] [11.054 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:10:28.063
    Jan 23 14:10:28.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename resourcequota 01/23/24 14:10:28.064
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:10:28.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:10:28.082
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 01/23/24 14:10:28.084
    STEP: Creating a ResourceQuota 01/23/24 14:10:33.086
    STEP: Ensuring resource quota status is calculated 01/23/24 14:10:33.09
    STEP: Creating a ReplicaSet 01/23/24 14:10:35.093
    STEP: Ensuring resource quota status captures replicaset creation 01/23/24 14:10:35.1
    STEP: Deleting a ReplicaSet 01/23/24 14:10:37.104
    STEP: Ensuring resource quota status released usage 01/23/24 14:10:37.107
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:10:39.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7328" for this suite. 01/23/24 14:10:39.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:10:39.117
Jan 23 14:10:39.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename cronjob 01/23/24 14:10:39.118
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:10:39.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:10:39.127
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/23/24 14:10:39.129
STEP: Ensuring no jobs are scheduled 01/23/24 14:10:39.132
STEP: Ensuring no job exists by listing jobs explicitly 01/23/24 14:15:39.138
STEP: Removing cronjob 01/23/24 14:15:39.14
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 23 14:15:39.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3765" for this suite. 01/23/24 14:15:39.145
------------------------------
• [SLOW TEST] [300.032 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:10:39.117
    Jan 23 14:10:39.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename cronjob 01/23/24 14:10:39.118
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:10:39.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:10:39.127
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/23/24 14:10:39.129
    STEP: Ensuring no jobs are scheduled 01/23/24 14:10:39.132
    STEP: Ensuring no job exists by listing jobs explicitly 01/23/24 14:15:39.138
    STEP: Removing cronjob 01/23/24 14:15:39.14
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:15:39.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3765" for this suite. 01/23/24 14:15:39.145
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:15:39.149
Jan 23 14:15:39.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename pods 01/23/24 14:15:39.15
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:15:39.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:15:39.16
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 01/23/24 14:15:39.162
STEP: submitting the pod to kubernetes 01/23/24 14:15:39.162
Jan 23 14:15:39.181: INFO: Waiting up to 5m0s for pod "pod-update-e3365423-864b-4399-a7d2-ac07658f792c" in namespace "pods-8126" to be "running and ready"
Jan 23 14:15:39.182: INFO: Pod "pod-update-e3365423-864b-4399-a7d2-ac07658f792c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.42329ms
Jan 23 14:15:39.182: INFO: The phase of Pod pod-update-e3365423-864b-4399-a7d2-ac07658f792c is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:15:41.186: INFO: Pod "pod-update-e3365423-864b-4399-a7d2-ac07658f792c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004837886s
Jan 23 14:15:41.186: INFO: The phase of Pod pod-update-e3365423-864b-4399-a7d2-ac07658f792c is Running (Ready = true)
Jan 23 14:15:41.186: INFO: Pod "pod-update-e3365423-864b-4399-a7d2-ac07658f792c" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/23/24 14:15:41.187
STEP: updating the pod 01/23/24 14:15:41.189
Jan 23 14:15:41.698: INFO: Successfully updated pod "pod-update-e3365423-864b-4399-a7d2-ac07658f792c"
Jan 23 14:15:41.698: INFO: Waiting up to 5m0s for pod "pod-update-e3365423-864b-4399-a7d2-ac07658f792c" in namespace "pods-8126" to be "running"
Jan 23 14:15:41.701: INFO: Pod "pod-update-e3365423-864b-4399-a7d2-ac07658f792c": Phase="Running", Reason="", readiness=true. Elapsed: 3.074325ms
Jan 23 14:15:41.701: INFO: Pod "pod-update-e3365423-864b-4399-a7d2-ac07658f792c" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/23/24 14:15:41.701
Jan 23 14:15:41.703: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 23 14:15:41.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8126" for this suite. 01/23/24 14:15:41.706
------------------------------
• [2.561 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:15:39.149
    Jan 23 14:15:39.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename pods 01/23/24 14:15:39.15
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:15:39.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:15:39.16
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 01/23/24 14:15:39.162
    STEP: submitting the pod to kubernetes 01/23/24 14:15:39.162
    Jan 23 14:15:39.181: INFO: Waiting up to 5m0s for pod "pod-update-e3365423-864b-4399-a7d2-ac07658f792c" in namespace "pods-8126" to be "running and ready"
    Jan 23 14:15:39.182: INFO: Pod "pod-update-e3365423-864b-4399-a7d2-ac07658f792c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.42329ms
    Jan 23 14:15:39.182: INFO: The phase of Pod pod-update-e3365423-864b-4399-a7d2-ac07658f792c is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:15:41.186: INFO: Pod "pod-update-e3365423-864b-4399-a7d2-ac07658f792c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004837886s
    Jan 23 14:15:41.186: INFO: The phase of Pod pod-update-e3365423-864b-4399-a7d2-ac07658f792c is Running (Ready = true)
    Jan 23 14:15:41.186: INFO: Pod "pod-update-e3365423-864b-4399-a7d2-ac07658f792c" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/23/24 14:15:41.187
    STEP: updating the pod 01/23/24 14:15:41.189
    Jan 23 14:15:41.698: INFO: Successfully updated pod "pod-update-e3365423-864b-4399-a7d2-ac07658f792c"
    Jan 23 14:15:41.698: INFO: Waiting up to 5m0s for pod "pod-update-e3365423-864b-4399-a7d2-ac07658f792c" in namespace "pods-8126" to be "running"
    Jan 23 14:15:41.701: INFO: Pod "pod-update-e3365423-864b-4399-a7d2-ac07658f792c": Phase="Running", Reason="", readiness=true. Elapsed: 3.074325ms
    Jan 23 14:15:41.701: INFO: Pod "pod-update-e3365423-864b-4399-a7d2-ac07658f792c" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/23/24 14:15:41.701
    Jan 23 14:15:41.703: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:15:41.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8126" for this suite. 01/23/24 14:15:41.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:15:41.71
Jan 23 14:15:41.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename webhook 01/23/24 14:15:41.711
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:15:41.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:15:41.722
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/23/24 14:15:41.732
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:15:41.837
STEP: Deploying the webhook pod 01/23/24 14:15:41.846
STEP: Wait for the deployment to be ready 01/23/24 14:15:41.903
Jan 23 14:15:41.950: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/23/24 14:15:43.956
STEP: Verifying the service has paired with the endpoint 01/23/24 14:15:43.962
Jan 23 14:15:44.962: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 01/23/24 14:15:44.993
STEP: Creating a configMap that should be mutated 01/23/24 14:15:45.003
STEP: Deleting the collection of validation webhooks 01/23/24 14:15:45.021
STEP: Creating a configMap that should not be mutated 01/23/24 14:15:45.042
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:15:45.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2211" for this suite. 01/23/24 14:15:45.075
STEP: Destroying namespace "webhook-2211-markers" for this suite. 01/23/24 14:15:45.081
------------------------------
• [3.382 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:15:41.71
    Jan 23 14:15:41.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename webhook 01/23/24 14:15:41.711
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:15:41.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:15:41.722
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/23/24 14:15:41.732
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:15:41.837
    STEP: Deploying the webhook pod 01/23/24 14:15:41.846
    STEP: Wait for the deployment to be ready 01/23/24 14:15:41.903
    Jan 23 14:15:41.950: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/23/24 14:15:43.956
    STEP: Verifying the service has paired with the endpoint 01/23/24 14:15:43.962
    Jan 23 14:15:44.962: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 01/23/24 14:15:44.993
    STEP: Creating a configMap that should be mutated 01/23/24 14:15:45.003
    STEP: Deleting the collection of validation webhooks 01/23/24 14:15:45.021
    STEP: Creating a configMap that should not be mutated 01/23/24 14:15:45.042
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:15:45.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2211" for this suite. 01/23/24 14:15:45.075
    STEP: Destroying namespace "webhook-2211-markers" for this suite. 01/23/24 14:15:45.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:15:45.093
Jan 23 14:15:45.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubectl 01/23/24 14:15:45.094
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:15:45.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:15:45.103
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Jan 23 14:15:45.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-5635 version'
Jan 23 14:15:45.165: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 23 14:15:45.165: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:50:44Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:43:07Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 23 14:15:45.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5635" for this suite. 01/23/24 14:15:45.167
------------------------------
• [0.079 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:15:45.093
    Jan 23 14:15:45.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubectl 01/23/24 14:15:45.094
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:15:45.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:15:45.103
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Jan 23 14:15:45.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-5635 version'
    Jan 23 14:15:45.165: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan 23 14:15:45.165: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:50:44Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:43:07Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:15:45.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5635" for this suite. 01/23/24 14:15:45.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:15:45.172
Jan 23 14:15:45.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename emptydir 01/23/24 14:15:45.173
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:15:45.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:15:45.187
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 01/23/24 14:15:45.189
Jan 23 14:15:45.203: INFO: Waiting up to 5m0s for pod "pod-baf0469b-05b3-4ef0-a0ee-45875e4c75e7" in namespace "emptydir-6624" to be "Succeeded or Failed"
Jan 23 14:15:45.204: INFO: Pod "pod-baf0469b-05b3-4ef0-a0ee-45875e4c75e7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.58645ms
Jan 23 14:15:47.207: INFO: Pod "pod-baf0469b-05b3-4ef0-a0ee-45875e4c75e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.004621099s
Jan 23 14:15:49.209: INFO: Pod "pod-baf0469b-05b3-4ef0-a0ee-45875e4c75e7": Phase="Running", Reason="", readiness=false. Elapsed: 4.005907576s
Jan 23 14:15:51.208: INFO: Pod "pod-baf0469b-05b3-4ef0-a0ee-45875e4c75e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005150399s
STEP: Saw pod success 01/23/24 14:15:51.208
Jan 23 14:15:51.208: INFO: Pod "pod-baf0469b-05b3-4ef0-a0ee-45875e4c75e7" satisfied condition "Succeeded or Failed"
Jan 23 14:15:51.210: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-baf0469b-05b3-4ef0-a0ee-45875e4c75e7 container test-container: <nil>
STEP: delete the pod 01/23/24 14:15:51.222
Jan 23 14:15:51.228: INFO: Waiting for pod pod-baf0469b-05b3-4ef0-a0ee-45875e4c75e7 to disappear
Jan 23 14:15:51.230: INFO: Pod pod-baf0469b-05b3-4ef0-a0ee-45875e4c75e7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 23 14:15:51.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6624" for this suite. 01/23/24 14:15:51.232
------------------------------
• [SLOW TEST] [6.063 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:15:45.172
    Jan 23 14:15:45.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename emptydir 01/23/24 14:15:45.173
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:15:45.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:15:45.187
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/23/24 14:15:45.189
    Jan 23 14:15:45.203: INFO: Waiting up to 5m0s for pod "pod-baf0469b-05b3-4ef0-a0ee-45875e4c75e7" in namespace "emptydir-6624" to be "Succeeded or Failed"
    Jan 23 14:15:45.204: INFO: Pod "pod-baf0469b-05b3-4ef0-a0ee-45875e4c75e7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.58645ms
    Jan 23 14:15:47.207: INFO: Pod "pod-baf0469b-05b3-4ef0-a0ee-45875e4c75e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.004621099s
    Jan 23 14:15:49.209: INFO: Pod "pod-baf0469b-05b3-4ef0-a0ee-45875e4c75e7": Phase="Running", Reason="", readiness=false. Elapsed: 4.005907576s
    Jan 23 14:15:51.208: INFO: Pod "pod-baf0469b-05b3-4ef0-a0ee-45875e4c75e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005150399s
    STEP: Saw pod success 01/23/24 14:15:51.208
    Jan 23 14:15:51.208: INFO: Pod "pod-baf0469b-05b3-4ef0-a0ee-45875e4c75e7" satisfied condition "Succeeded or Failed"
    Jan 23 14:15:51.210: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-baf0469b-05b3-4ef0-a0ee-45875e4c75e7 container test-container: <nil>
    STEP: delete the pod 01/23/24 14:15:51.222
    Jan 23 14:15:51.228: INFO: Waiting for pod pod-baf0469b-05b3-4ef0-a0ee-45875e4c75e7 to disappear
    Jan 23 14:15:51.230: INFO: Pod pod-baf0469b-05b3-4ef0-a0ee-45875e4c75e7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:15:51.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6624" for this suite. 01/23/24 14:15:51.232
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:15:51.235
Jan 23 14:15:51.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename downward-api 01/23/24 14:15:51.236
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:15:51.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:15:51.245
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 01/23/24 14:15:51.247
Jan 23 14:15:51.261: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f7e6481e-d4df-4970-83c1-e156513d6051" in namespace "downward-api-3860" to be "Succeeded or Failed"
Jan 23 14:15:51.263: INFO: Pod "downwardapi-volume-f7e6481e-d4df-4970-83c1-e156513d6051": Phase="Pending", Reason="", readiness=false. Elapsed: 1.822424ms
Jan 23 14:15:53.266: INFO: Pod "downwardapi-volume-f7e6481e-d4df-4970-83c1-e156513d6051": Phase="Running", Reason="", readiness=false. Elapsed: 2.005141304s
Jan 23 14:15:55.266: INFO: Pod "downwardapi-volume-f7e6481e-d4df-4970-83c1-e156513d6051": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005269602s
STEP: Saw pod success 01/23/24 14:15:55.266
Jan 23 14:15:55.266: INFO: Pod "downwardapi-volume-f7e6481e-d4df-4970-83c1-e156513d6051" satisfied condition "Succeeded or Failed"
Jan 23 14:15:55.268: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-f7e6481e-d4df-4970-83c1-e156513d6051 container client-container: <nil>
STEP: delete the pod 01/23/24 14:15:55.271
Jan 23 14:15:55.278: INFO: Waiting for pod downwardapi-volume-f7e6481e-d4df-4970-83c1-e156513d6051 to disappear
Jan 23 14:15:55.280: INFO: Pod downwardapi-volume-f7e6481e-d4df-4970-83c1-e156513d6051 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 23 14:15:55.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3860" for this suite. 01/23/24 14:15:55.282
------------------------------
• [4.049 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:15:51.235
    Jan 23 14:15:51.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename downward-api 01/23/24 14:15:51.236
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:15:51.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:15:51.245
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 01/23/24 14:15:51.247
    Jan 23 14:15:51.261: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f7e6481e-d4df-4970-83c1-e156513d6051" in namespace "downward-api-3860" to be "Succeeded or Failed"
    Jan 23 14:15:51.263: INFO: Pod "downwardapi-volume-f7e6481e-d4df-4970-83c1-e156513d6051": Phase="Pending", Reason="", readiness=false. Elapsed: 1.822424ms
    Jan 23 14:15:53.266: INFO: Pod "downwardapi-volume-f7e6481e-d4df-4970-83c1-e156513d6051": Phase="Running", Reason="", readiness=false. Elapsed: 2.005141304s
    Jan 23 14:15:55.266: INFO: Pod "downwardapi-volume-f7e6481e-d4df-4970-83c1-e156513d6051": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005269602s
    STEP: Saw pod success 01/23/24 14:15:55.266
    Jan 23 14:15:55.266: INFO: Pod "downwardapi-volume-f7e6481e-d4df-4970-83c1-e156513d6051" satisfied condition "Succeeded or Failed"
    Jan 23 14:15:55.268: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-f7e6481e-d4df-4970-83c1-e156513d6051 container client-container: <nil>
    STEP: delete the pod 01/23/24 14:15:55.271
    Jan 23 14:15:55.278: INFO: Waiting for pod downwardapi-volume-f7e6481e-d4df-4970-83c1-e156513d6051 to disappear
    Jan 23 14:15:55.280: INFO: Pod downwardapi-volume-f7e6481e-d4df-4970-83c1-e156513d6051 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:15:55.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3860" for this suite. 01/23/24 14:15:55.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:15:55.286
Jan 23 14:15:55.286: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename secrets 01/23/24 14:15:55.286
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:15:55.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:15:55.296
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-f4355dbf-eb60-453f-8b37-348179432167 01/23/24 14:15:55.298
STEP: Creating a pod to test consume secrets 01/23/24 14:15:55.301
Jan 23 14:15:55.314: INFO: Waiting up to 5m0s for pod "pod-secrets-c5cb19e7-8ec3-45b1-9bb1-6d1e2583e5b7" in namespace "secrets-3457" to be "Succeeded or Failed"
Jan 23 14:15:55.316: INFO: Pod "pod-secrets-c5cb19e7-8ec3-45b1-9bb1-6d1e2583e5b7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.288853ms
Jan 23 14:15:57.319: INFO: Pod "pod-secrets-c5cb19e7-8ec3-45b1-9bb1-6d1e2583e5b7": Phase="Running", Reason="", readiness=false. Elapsed: 2.00436945s
Jan 23 14:15:59.321: INFO: Pod "pod-secrets-c5cb19e7-8ec3-45b1-9bb1-6d1e2583e5b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006186049s
STEP: Saw pod success 01/23/24 14:15:59.321
Jan 23 14:15:59.321: INFO: Pod "pod-secrets-c5cb19e7-8ec3-45b1-9bb1-6d1e2583e5b7" satisfied condition "Succeeded or Failed"
Jan 23 14:15:59.322: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-secrets-c5cb19e7-8ec3-45b1-9bb1-6d1e2583e5b7 container secret-volume-test: <nil>
STEP: delete the pod 01/23/24 14:15:59.326
Jan 23 14:15:59.333: INFO: Waiting for pod pod-secrets-c5cb19e7-8ec3-45b1-9bb1-6d1e2583e5b7 to disappear
Jan 23 14:15:59.334: INFO: Pod pod-secrets-c5cb19e7-8ec3-45b1-9bb1-6d1e2583e5b7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 23 14:15:59.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3457" for this suite. 01/23/24 14:15:59.336
------------------------------
• [4.054 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:15:55.286
    Jan 23 14:15:55.286: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename secrets 01/23/24 14:15:55.286
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:15:55.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:15:55.296
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-f4355dbf-eb60-453f-8b37-348179432167 01/23/24 14:15:55.298
    STEP: Creating a pod to test consume secrets 01/23/24 14:15:55.301
    Jan 23 14:15:55.314: INFO: Waiting up to 5m0s for pod "pod-secrets-c5cb19e7-8ec3-45b1-9bb1-6d1e2583e5b7" in namespace "secrets-3457" to be "Succeeded or Failed"
    Jan 23 14:15:55.316: INFO: Pod "pod-secrets-c5cb19e7-8ec3-45b1-9bb1-6d1e2583e5b7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.288853ms
    Jan 23 14:15:57.319: INFO: Pod "pod-secrets-c5cb19e7-8ec3-45b1-9bb1-6d1e2583e5b7": Phase="Running", Reason="", readiness=false. Elapsed: 2.00436945s
    Jan 23 14:15:59.321: INFO: Pod "pod-secrets-c5cb19e7-8ec3-45b1-9bb1-6d1e2583e5b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006186049s
    STEP: Saw pod success 01/23/24 14:15:59.321
    Jan 23 14:15:59.321: INFO: Pod "pod-secrets-c5cb19e7-8ec3-45b1-9bb1-6d1e2583e5b7" satisfied condition "Succeeded or Failed"
    Jan 23 14:15:59.322: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-secrets-c5cb19e7-8ec3-45b1-9bb1-6d1e2583e5b7 container secret-volume-test: <nil>
    STEP: delete the pod 01/23/24 14:15:59.326
    Jan 23 14:15:59.333: INFO: Waiting for pod pod-secrets-c5cb19e7-8ec3-45b1-9bb1-6d1e2583e5b7 to disappear
    Jan 23 14:15:59.334: INFO: Pod pod-secrets-c5cb19e7-8ec3-45b1-9bb1-6d1e2583e5b7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:15:59.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3457" for this suite. 01/23/24 14:15:59.336
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:15:59.34
Jan 23 14:15:59.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 14:15:59.341
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:15:59.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:15:59.35
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-f794a700-8a28-4df4-bf44-af7f4ec7b89e 01/23/24 14:15:59.352
STEP: Creating a pod to test consume configMaps 01/23/24 14:15:59.356
Jan 23 14:15:59.368: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ac6f6405-4b17-4495-b344-05ab1c43ce6d" in namespace "projected-5669" to be "Succeeded or Failed"
Jan 23 14:15:59.369: INFO: Pod "pod-projected-configmaps-ac6f6405-4b17-4495-b344-05ab1c43ce6d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.407262ms
Jan 23 14:16:01.372: INFO: Pod "pod-projected-configmaps-ac6f6405-4b17-4495-b344-05ab1c43ce6d": Phase="Running", Reason="", readiness=false. Elapsed: 2.004629353s
Jan 23 14:16:03.374: INFO: Pod "pod-projected-configmaps-ac6f6405-4b17-4495-b344-05ab1c43ce6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00607608s
STEP: Saw pod success 01/23/24 14:16:03.374
Jan 23 14:16:03.374: INFO: Pod "pod-projected-configmaps-ac6f6405-4b17-4495-b344-05ab1c43ce6d" satisfied condition "Succeeded or Failed"
Jan 23 14:16:03.375: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-configmaps-ac6f6405-4b17-4495-b344-05ab1c43ce6d container agnhost-container: <nil>
STEP: delete the pod 01/23/24 14:16:03.379
Jan 23 14:16:03.384: INFO: Waiting for pod pod-projected-configmaps-ac6f6405-4b17-4495-b344-05ab1c43ce6d to disappear
Jan 23 14:16:03.386: INFO: Pod pod-projected-configmaps-ac6f6405-4b17-4495-b344-05ab1c43ce6d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 23 14:16:03.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5669" for this suite. 01/23/24 14:16:03.388
------------------------------
• [4.050 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:15:59.34
    Jan 23 14:15:59.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 14:15:59.341
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:15:59.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:15:59.35
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-f794a700-8a28-4df4-bf44-af7f4ec7b89e 01/23/24 14:15:59.352
    STEP: Creating a pod to test consume configMaps 01/23/24 14:15:59.356
    Jan 23 14:15:59.368: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ac6f6405-4b17-4495-b344-05ab1c43ce6d" in namespace "projected-5669" to be "Succeeded or Failed"
    Jan 23 14:15:59.369: INFO: Pod "pod-projected-configmaps-ac6f6405-4b17-4495-b344-05ab1c43ce6d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.407262ms
    Jan 23 14:16:01.372: INFO: Pod "pod-projected-configmaps-ac6f6405-4b17-4495-b344-05ab1c43ce6d": Phase="Running", Reason="", readiness=false. Elapsed: 2.004629353s
    Jan 23 14:16:03.374: INFO: Pod "pod-projected-configmaps-ac6f6405-4b17-4495-b344-05ab1c43ce6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00607608s
    STEP: Saw pod success 01/23/24 14:16:03.374
    Jan 23 14:16:03.374: INFO: Pod "pod-projected-configmaps-ac6f6405-4b17-4495-b344-05ab1c43ce6d" satisfied condition "Succeeded or Failed"
    Jan 23 14:16:03.375: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-configmaps-ac6f6405-4b17-4495-b344-05ab1c43ce6d container agnhost-container: <nil>
    STEP: delete the pod 01/23/24 14:16:03.379
    Jan 23 14:16:03.384: INFO: Waiting for pod pod-projected-configmaps-ac6f6405-4b17-4495-b344-05ab1c43ce6d to disappear
    Jan 23 14:16:03.386: INFO: Pod pod-projected-configmaps-ac6f6405-4b17-4495-b344-05ab1c43ce6d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:16:03.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5669" for this suite. 01/23/24 14:16:03.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:16:03.391
Jan 23 14:16:03.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename container-runtime 01/23/24 14:16:03.391
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:16:03.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:16:03.403
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 01/23/24 14:16:03.404
STEP: wait for the container to reach Succeeded 01/23/24 14:16:03.413
STEP: get the container status 01/23/24 14:16:07.427
STEP: the container should be terminated 01/23/24 14:16:07.429
STEP: the termination message should be set 01/23/24 14:16:07.429
Jan 23 14:16:07.429: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/23/24 14:16:07.429
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 23 14:16:07.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7949" for this suite. 01/23/24 14:16:07.439
------------------------------
• [4.052 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:16:03.391
    Jan 23 14:16:03.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename container-runtime 01/23/24 14:16:03.391
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:16:03.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:16:03.403
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 01/23/24 14:16:03.404
    STEP: wait for the container to reach Succeeded 01/23/24 14:16:03.413
    STEP: get the container status 01/23/24 14:16:07.427
    STEP: the container should be terminated 01/23/24 14:16:07.429
    STEP: the termination message should be set 01/23/24 14:16:07.429
    Jan 23 14:16:07.429: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/23/24 14:16:07.429
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:16:07.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7949" for this suite. 01/23/24 14:16:07.439
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:16:07.442
Jan 23 14:16:07.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename replication-controller 01/23/24 14:16:07.443
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:16:07.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:16:07.452
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 01/23/24 14:16:07.455
STEP: waiting for RC to be added 01/23/24 14:16:07.457
STEP: waiting for available Replicas 01/23/24 14:16:07.458
STEP: patching ReplicationController 01/23/24 14:16:08.211
STEP: waiting for RC to be modified 01/23/24 14:16:08.218
STEP: patching ReplicationController status 01/23/24 14:16:08.218
STEP: waiting for RC to be modified 01/23/24 14:16:08.222
STEP: waiting for available Replicas 01/23/24 14:16:08.222
STEP: fetching ReplicationController status 01/23/24 14:16:08.226
STEP: patching ReplicationController scale 01/23/24 14:16:08.228
STEP: waiting for RC to be modified 01/23/24 14:16:08.231
STEP: waiting for ReplicationController's scale to be the max amount 01/23/24 14:16:08.231
STEP: fetching ReplicationController; ensuring that it's patched 01/23/24 14:16:09.51
STEP: updating ReplicationController status 01/23/24 14:16:09.512
STEP: waiting for RC to be modified 01/23/24 14:16:09.515
STEP: listing all ReplicationControllers 01/23/24 14:16:09.515
STEP: checking that ReplicationController has expected values 01/23/24 14:16:09.517
STEP: deleting ReplicationControllers by collection 01/23/24 14:16:09.517
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/23/24 14:16:09.522
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 23 14:16:09.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8607" for this suite. 01/23/24 14:16:09.584
------------------------------
• [2.145 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:16:07.442
    Jan 23 14:16:07.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename replication-controller 01/23/24 14:16:07.443
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:16:07.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:16:07.452
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 01/23/24 14:16:07.455
    STEP: waiting for RC to be added 01/23/24 14:16:07.457
    STEP: waiting for available Replicas 01/23/24 14:16:07.458
    STEP: patching ReplicationController 01/23/24 14:16:08.211
    STEP: waiting for RC to be modified 01/23/24 14:16:08.218
    STEP: patching ReplicationController status 01/23/24 14:16:08.218
    STEP: waiting for RC to be modified 01/23/24 14:16:08.222
    STEP: waiting for available Replicas 01/23/24 14:16:08.222
    STEP: fetching ReplicationController status 01/23/24 14:16:08.226
    STEP: patching ReplicationController scale 01/23/24 14:16:08.228
    STEP: waiting for RC to be modified 01/23/24 14:16:08.231
    STEP: waiting for ReplicationController's scale to be the max amount 01/23/24 14:16:08.231
    STEP: fetching ReplicationController; ensuring that it's patched 01/23/24 14:16:09.51
    STEP: updating ReplicationController status 01/23/24 14:16:09.512
    STEP: waiting for RC to be modified 01/23/24 14:16:09.515
    STEP: listing all ReplicationControllers 01/23/24 14:16:09.515
    STEP: checking that ReplicationController has expected values 01/23/24 14:16:09.517
    STEP: deleting ReplicationControllers by collection 01/23/24 14:16:09.517
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/23/24 14:16:09.522
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:16:09.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8607" for this suite. 01/23/24 14:16:09.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:16:09.588
Jan 23 14:16:09.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 14:16:09.589
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:16:09.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:16:09.597
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-09fe21b3-efa6-4fd7-adf3-a49bd68916b7 01/23/24 14:16:09.6
STEP: Creating a pod to test consume secrets 01/23/24 14:16:09.604
Jan 23 14:16:09.616: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ed74a444-0a4e-40f8-ab96-d65b37de39ba" in namespace "projected-4553" to be "Succeeded or Failed"
Jan 23 14:16:09.618: INFO: Pod "pod-projected-secrets-ed74a444-0a4e-40f8-ab96-d65b37de39ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1.51318ms
Jan 23 14:16:11.621: INFO: Pod "pod-projected-secrets-ed74a444-0a4e-40f8-ab96-d65b37de39ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.004780582s
Jan 23 14:16:13.620: INFO: Pod "pod-projected-secrets-ed74a444-0a4e-40f8-ab96-d65b37de39ba": Phase="Running", Reason="", readiness=false. Elapsed: 4.00441431s
Jan 23 14:16:15.620: INFO: Pod "pod-projected-secrets-ed74a444-0a4e-40f8-ab96-d65b37de39ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004203909s
STEP: Saw pod success 01/23/24 14:16:15.62
Jan 23 14:16:15.620: INFO: Pod "pod-projected-secrets-ed74a444-0a4e-40f8-ab96-d65b37de39ba" satisfied condition "Succeeded or Failed"
Jan 23 14:16:15.622: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-secrets-ed74a444-0a4e-40f8-ab96-d65b37de39ba container projected-secret-volume-test: <nil>
STEP: delete the pod 01/23/24 14:16:15.625
Jan 23 14:16:15.633: INFO: Waiting for pod pod-projected-secrets-ed74a444-0a4e-40f8-ab96-d65b37de39ba to disappear
Jan 23 14:16:15.635: INFO: Pod pod-projected-secrets-ed74a444-0a4e-40f8-ab96-d65b37de39ba no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 23 14:16:15.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4553" for this suite. 01/23/24 14:16:15.637
------------------------------
• [SLOW TEST] [6.052 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:16:09.588
    Jan 23 14:16:09.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 14:16:09.589
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:16:09.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:16:09.597
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-09fe21b3-efa6-4fd7-adf3-a49bd68916b7 01/23/24 14:16:09.6
    STEP: Creating a pod to test consume secrets 01/23/24 14:16:09.604
    Jan 23 14:16:09.616: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ed74a444-0a4e-40f8-ab96-d65b37de39ba" in namespace "projected-4553" to be "Succeeded or Failed"
    Jan 23 14:16:09.618: INFO: Pod "pod-projected-secrets-ed74a444-0a4e-40f8-ab96-d65b37de39ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1.51318ms
    Jan 23 14:16:11.621: INFO: Pod "pod-projected-secrets-ed74a444-0a4e-40f8-ab96-d65b37de39ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.004780582s
    Jan 23 14:16:13.620: INFO: Pod "pod-projected-secrets-ed74a444-0a4e-40f8-ab96-d65b37de39ba": Phase="Running", Reason="", readiness=false. Elapsed: 4.00441431s
    Jan 23 14:16:15.620: INFO: Pod "pod-projected-secrets-ed74a444-0a4e-40f8-ab96-d65b37de39ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004203909s
    STEP: Saw pod success 01/23/24 14:16:15.62
    Jan 23 14:16:15.620: INFO: Pod "pod-projected-secrets-ed74a444-0a4e-40f8-ab96-d65b37de39ba" satisfied condition "Succeeded or Failed"
    Jan 23 14:16:15.622: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-secrets-ed74a444-0a4e-40f8-ab96-d65b37de39ba container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/23/24 14:16:15.625
    Jan 23 14:16:15.633: INFO: Waiting for pod pod-projected-secrets-ed74a444-0a4e-40f8-ab96-d65b37de39ba to disappear
    Jan 23 14:16:15.635: INFO: Pod pod-projected-secrets-ed74a444-0a4e-40f8-ab96-d65b37de39ba no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:16:15.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4553" for this suite. 01/23/24 14:16:15.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:16:15.641
Jan 23 14:16:15.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename services 01/23/24 14:16:15.642
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:16:15.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:16:15.659
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-8621 01/23/24 14:16:15.66
STEP: creating service affinity-clusterip in namespace services-8621 01/23/24 14:16:15.661
STEP: creating replication controller affinity-clusterip in namespace services-8621 01/23/24 14:16:15.67
I0123 14:16:15.674107      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-8621, replica count: 3
I0123 14:16:18.725662      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 23 14:16:18.729: INFO: Creating new exec pod
Jan 23 14:16:18.738: INFO: Waiting up to 5m0s for pod "execpod-affinitykzck7" in namespace "services-8621" to be "running"
Jan 23 14:16:18.740: INFO: Pod "execpod-affinitykzck7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.923787ms
Jan 23 14:16:20.742: INFO: Pod "execpod-affinitykzck7": Phase="Running", Reason="", readiness=true. Elapsed: 2.003816847s
Jan 23 14:16:20.742: INFO: Pod "execpod-affinitykzck7" satisfied condition "running"
Jan 23 14:16:21.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-8621 exec execpod-affinitykzck7 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jan 23 14:16:21.930: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 23 14:16:21.930: INFO: stdout: ""
Jan 23 14:16:21.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-8621 exec execpod-affinitykzck7 -- /bin/sh -x -c nc -v -z -w 2 10.233.13.66 80'
Jan 23 14:16:22.107: INFO: stderr: "+ nc -v -z -w 2 10.233.13.66 80\nConnection to 10.233.13.66 80 port [tcp/http] succeeded!\n"
Jan 23 14:16:22.107: INFO: stdout: ""
Jan 23 14:16:22.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-8621 exec execpod-affinitykzck7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.13.66:80/ ; done'
Jan 23 14:16:22.363: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n"
Jan 23 14:16:22.363: INFO: stdout: "\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx"
Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
Jan 23 14:16:22.363: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-8621, will wait for the garbage collector to delete the pods 01/23/24 14:16:22.371
Jan 23 14:16:22.427: INFO: Deleting ReplicationController affinity-clusterip took: 3.830627ms
Jan 23 14:16:22.528: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.13621ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 23 14:16:26.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8621" for this suite. 01/23/24 14:16:26.341
------------------------------
• [SLOW TEST] [10.702 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:16:15.641
    Jan 23 14:16:15.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename services 01/23/24 14:16:15.642
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:16:15.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:16:15.659
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-8621 01/23/24 14:16:15.66
    STEP: creating service affinity-clusterip in namespace services-8621 01/23/24 14:16:15.661
    STEP: creating replication controller affinity-clusterip in namespace services-8621 01/23/24 14:16:15.67
    I0123 14:16:15.674107      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-8621, replica count: 3
    I0123 14:16:18.725662      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 23 14:16:18.729: INFO: Creating new exec pod
    Jan 23 14:16:18.738: INFO: Waiting up to 5m0s for pod "execpod-affinitykzck7" in namespace "services-8621" to be "running"
    Jan 23 14:16:18.740: INFO: Pod "execpod-affinitykzck7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.923787ms
    Jan 23 14:16:20.742: INFO: Pod "execpod-affinitykzck7": Phase="Running", Reason="", readiness=true. Elapsed: 2.003816847s
    Jan 23 14:16:20.742: INFO: Pod "execpod-affinitykzck7" satisfied condition "running"
    Jan 23 14:16:21.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-8621 exec execpod-affinitykzck7 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jan 23 14:16:21.930: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan 23 14:16:21.930: INFO: stdout: ""
    Jan 23 14:16:21.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-8621 exec execpod-affinitykzck7 -- /bin/sh -x -c nc -v -z -w 2 10.233.13.66 80'
    Jan 23 14:16:22.107: INFO: stderr: "+ nc -v -z -w 2 10.233.13.66 80\nConnection to 10.233.13.66 80 port [tcp/http] succeeded!\n"
    Jan 23 14:16:22.107: INFO: stdout: ""
    Jan 23 14:16:22.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-8621 exec execpod-affinitykzck7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.13.66:80/ ; done'
    Jan 23 14:16:22.363: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.13.66:80/\n"
    Jan 23 14:16:22.363: INFO: stdout: "\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx\naffinity-clusterip-fz6rx"
    Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
    Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
    Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
    Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
    Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
    Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
    Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
    Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
    Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
    Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
    Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
    Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
    Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
    Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
    Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
    Jan 23 14:16:22.363: INFO: Received response from host: affinity-clusterip-fz6rx
    Jan 23 14:16:22.363: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-8621, will wait for the garbage collector to delete the pods 01/23/24 14:16:22.371
    Jan 23 14:16:22.427: INFO: Deleting ReplicationController affinity-clusterip took: 3.830627ms
    Jan 23 14:16:22.528: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.13621ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:16:26.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8621" for this suite. 01/23/24 14:16:26.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:16:26.344
Jan 23 14:16:26.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename secrets 01/23/24 14:16:26.345
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:16:26.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:16:26.356
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-7e6a7e81-14e8-4f24-88da-73497e9654b6 01/23/24 14:16:26.358
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 23 14:16:26.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4972" for this suite. 01/23/24 14:16:26.363
------------------------------
• [0.021 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:16:26.344
    Jan 23 14:16:26.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename secrets 01/23/24 14:16:26.345
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:16:26.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:16:26.356
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-7e6a7e81-14e8-4f24-88da-73497e9654b6 01/23/24 14:16:26.358
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:16:26.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4972" for this suite. 01/23/24 14:16:26.363
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:16:26.366
Jan 23 14:16:26.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename webhook 01/23/24 14:16:26.367
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:16:26.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:16:26.376
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/23/24 14:16:26.386
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:16:26.606
STEP: Deploying the webhook pod 01/23/24 14:16:26.61
STEP: Wait for the deployment to be ready 01/23/24 14:16:26.62
Jan 23 14:16:26.623: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/23/24 14:16:28.629
STEP: Verifying the service has paired with the endpoint 01/23/24 14:16:28.634
Jan 23 14:16:29.634: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 01/23/24 14:16:29.636
STEP: create a pod that should be denied by the webhook 01/23/24 14:16:29.648
STEP: create a pod that causes the webhook to hang 01/23/24 14:16:29.661
STEP: create a configmap that should be denied by the webhook 01/23/24 14:16:39.67
STEP: create a configmap that should be admitted by the webhook 01/23/24 14:16:39.684
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/23/24 14:16:39.691
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/23/24 14:16:39.696
STEP: create a namespace that bypass the webhook 01/23/24 14:16:39.7
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/23/24 14:16:39.703
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:16:39.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6912" for this suite. 01/23/24 14:16:39.745
STEP: Destroying namespace "webhook-6912-markers" for this suite. 01/23/24 14:16:39.749
------------------------------
• [SLOW TEST] [13.394 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:16:26.366
    Jan 23 14:16:26.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename webhook 01/23/24 14:16:26.367
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:16:26.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:16:26.376
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/23/24 14:16:26.386
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:16:26.606
    STEP: Deploying the webhook pod 01/23/24 14:16:26.61
    STEP: Wait for the deployment to be ready 01/23/24 14:16:26.62
    Jan 23 14:16:26.623: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/23/24 14:16:28.629
    STEP: Verifying the service has paired with the endpoint 01/23/24 14:16:28.634
    Jan 23 14:16:29.634: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 01/23/24 14:16:29.636
    STEP: create a pod that should be denied by the webhook 01/23/24 14:16:29.648
    STEP: create a pod that causes the webhook to hang 01/23/24 14:16:29.661
    STEP: create a configmap that should be denied by the webhook 01/23/24 14:16:39.67
    STEP: create a configmap that should be admitted by the webhook 01/23/24 14:16:39.684
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/23/24 14:16:39.691
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/23/24 14:16:39.696
    STEP: create a namespace that bypass the webhook 01/23/24 14:16:39.7
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/23/24 14:16:39.703
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:16:39.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6912" for this suite. 01/23/24 14:16:39.745
    STEP: Destroying namespace "webhook-6912-markers" for this suite. 01/23/24 14:16:39.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:16:39.761
Jan 23 14:16:39.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename pods 01/23/24 14:16:39.762
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:16:39.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:16:39.772
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Jan 23 14:16:39.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: creating the pod 01/23/24 14:16:39.776
STEP: submitting the pod to kubernetes 01/23/24 14:16:39.776
Jan 23 14:16:39.795: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-c59eaa50-e750-4fbd-b75d-c6ba34accd51" in namespace "pods-5067" to be "running and ready"
Jan 23 14:16:39.797: INFO: Pod "pod-logs-websocket-c59eaa50-e750-4fbd-b75d-c6ba34accd51": Phase="Pending", Reason="", readiness=false. Elapsed: 1.553217ms
Jan 23 14:16:39.797: INFO: The phase of Pod pod-logs-websocket-c59eaa50-e750-4fbd-b75d-c6ba34accd51 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:16:41.799: INFO: Pod "pod-logs-websocket-c59eaa50-e750-4fbd-b75d-c6ba34accd51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0042331s
Jan 23 14:16:41.799: INFO: The phase of Pod pod-logs-websocket-c59eaa50-e750-4fbd-b75d-c6ba34accd51 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:16:43.801: INFO: Pod "pod-logs-websocket-c59eaa50-e750-4fbd-b75d-c6ba34accd51": Phase="Running", Reason="", readiness=true. Elapsed: 4.005572982s
Jan 23 14:16:43.801: INFO: The phase of Pod pod-logs-websocket-c59eaa50-e750-4fbd-b75d-c6ba34accd51 is Running (Ready = true)
Jan 23 14:16:43.801: INFO: Pod "pod-logs-websocket-c59eaa50-e750-4fbd-b75d-c6ba34accd51" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 23 14:16:43.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5067" for this suite. 01/23/24 14:16:43.812
------------------------------
• [4.054 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:16:39.761
    Jan 23 14:16:39.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename pods 01/23/24 14:16:39.762
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:16:39.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:16:39.772
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Jan 23 14:16:39.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: creating the pod 01/23/24 14:16:39.776
    STEP: submitting the pod to kubernetes 01/23/24 14:16:39.776
    Jan 23 14:16:39.795: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-c59eaa50-e750-4fbd-b75d-c6ba34accd51" in namespace "pods-5067" to be "running and ready"
    Jan 23 14:16:39.797: INFO: Pod "pod-logs-websocket-c59eaa50-e750-4fbd-b75d-c6ba34accd51": Phase="Pending", Reason="", readiness=false. Elapsed: 1.553217ms
    Jan 23 14:16:39.797: INFO: The phase of Pod pod-logs-websocket-c59eaa50-e750-4fbd-b75d-c6ba34accd51 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:16:41.799: INFO: Pod "pod-logs-websocket-c59eaa50-e750-4fbd-b75d-c6ba34accd51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0042331s
    Jan 23 14:16:41.799: INFO: The phase of Pod pod-logs-websocket-c59eaa50-e750-4fbd-b75d-c6ba34accd51 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:16:43.801: INFO: Pod "pod-logs-websocket-c59eaa50-e750-4fbd-b75d-c6ba34accd51": Phase="Running", Reason="", readiness=true. Elapsed: 4.005572982s
    Jan 23 14:16:43.801: INFO: The phase of Pod pod-logs-websocket-c59eaa50-e750-4fbd-b75d-c6ba34accd51 is Running (Ready = true)
    Jan 23 14:16:43.801: INFO: Pod "pod-logs-websocket-c59eaa50-e750-4fbd-b75d-c6ba34accd51" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:16:43.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5067" for this suite. 01/23/24 14:16:43.812
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:16:43.815
Jan 23 14:16:43.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename statefulset 01/23/24 14:16:43.816
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:16:43.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:16:43.826
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6550 01/23/24 14:16:43.828
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Jan 23 14:16:43.839: INFO: Found 0 stateful pods, waiting for 1
Jan 23 14:16:53.843: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/23/24 14:16:53.847
W0123 14:16:53.855748      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 23 14:16:53.859: INFO: Found 1 stateful pods, waiting for 2
Jan 23 14:17:03.864: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 14:17:03.864: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/23/24 14:17:03.867
STEP: Delete all of the StatefulSets 01/23/24 14:17:03.869
STEP: Verify that StatefulSets have been deleted 01/23/24 14:17:03.874
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 23 14:17:03.876: INFO: Deleting all statefulset in ns statefulset-6550
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 23 14:17:03.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6550" for this suite. 01/23/24 14:17:03.889
------------------------------
• [SLOW TEST] [20.086 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:16:43.815
    Jan 23 14:16:43.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename statefulset 01/23/24 14:16:43.816
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:16:43.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:16:43.826
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6550 01/23/24 14:16:43.828
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Jan 23 14:16:43.839: INFO: Found 0 stateful pods, waiting for 1
    Jan 23 14:16:53.843: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/23/24 14:16:53.847
    W0123 14:16:53.855748      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 23 14:16:53.859: INFO: Found 1 stateful pods, waiting for 2
    Jan 23 14:17:03.864: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 23 14:17:03.864: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/23/24 14:17:03.867
    STEP: Delete all of the StatefulSets 01/23/24 14:17:03.869
    STEP: Verify that StatefulSets have been deleted 01/23/24 14:17:03.874
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 23 14:17:03.876: INFO: Deleting all statefulset in ns statefulset-6550
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:17:03.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6550" for this suite. 01/23/24 14:17:03.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:17:03.902
Jan 23 14:17:03.902: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename watch 01/23/24 14:17:03.902
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:17:03.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:17:03.914
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/23/24 14:17:03.915
STEP: starting a background goroutine to produce watch events 01/23/24 14:17:03.918
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/23/24 14:17:03.918
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 23 14:17:06.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1733" for this suite. 01/23/24 14:17:06.757
------------------------------
• [2.906 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:17:03.902
    Jan 23 14:17:03.902: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename watch 01/23/24 14:17:03.902
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:17:03.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:17:03.914
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/23/24 14:17:03.915
    STEP: starting a background goroutine to produce watch events 01/23/24 14:17:03.918
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/23/24 14:17:03.918
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:17:06.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1733" for this suite. 01/23/24 14:17:06.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:17:06.809
Jan 23 14:17:06.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 14:17:06.809
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:17:06.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:17:06.819
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-b842165c-bae8-4558-870e-7646ca9e7310 01/23/24 14:17:06.823
STEP: Creating the pod 01/23/24 14:17:06.826
Jan 23 14:17:06.838: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3791ae68-6b80-4165-8f3c-75dd6de74d56" in namespace "projected-563" to be "running and ready"
Jan 23 14:17:06.840: INFO: Pod "pod-projected-configmaps-3791ae68-6b80-4165-8f3c-75dd6de74d56": Phase="Pending", Reason="", readiness=false. Elapsed: 1.346045ms
Jan 23 14:17:06.840: INFO: The phase of Pod pod-projected-configmaps-3791ae68-6b80-4165-8f3c-75dd6de74d56 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:17:08.843: INFO: Pod "pod-projected-configmaps-3791ae68-6b80-4165-8f3c-75dd6de74d56": Phase="Running", Reason="", readiness=true. Elapsed: 2.004746706s
Jan 23 14:17:08.843: INFO: The phase of Pod pod-projected-configmaps-3791ae68-6b80-4165-8f3c-75dd6de74d56 is Running (Ready = true)
Jan 23 14:17:08.843: INFO: Pod "pod-projected-configmaps-3791ae68-6b80-4165-8f3c-75dd6de74d56" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-b842165c-bae8-4558-870e-7646ca9e7310 01/23/24 14:17:08.849
STEP: waiting to observe update in volume 01/23/24 14:17:08.854
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 23 14:17:10.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-563" for this suite. 01/23/24 14:17:10.865
------------------------------
• [4.060 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:17:06.809
    Jan 23 14:17:06.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 14:17:06.809
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:17:06.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:17:06.819
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-b842165c-bae8-4558-870e-7646ca9e7310 01/23/24 14:17:06.823
    STEP: Creating the pod 01/23/24 14:17:06.826
    Jan 23 14:17:06.838: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3791ae68-6b80-4165-8f3c-75dd6de74d56" in namespace "projected-563" to be "running and ready"
    Jan 23 14:17:06.840: INFO: Pod "pod-projected-configmaps-3791ae68-6b80-4165-8f3c-75dd6de74d56": Phase="Pending", Reason="", readiness=false. Elapsed: 1.346045ms
    Jan 23 14:17:06.840: INFO: The phase of Pod pod-projected-configmaps-3791ae68-6b80-4165-8f3c-75dd6de74d56 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:17:08.843: INFO: Pod "pod-projected-configmaps-3791ae68-6b80-4165-8f3c-75dd6de74d56": Phase="Running", Reason="", readiness=true. Elapsed: 2.004746706s
    Jan 23 14:17:08.843: INFO: The phase of Pod pod-projected-configmaps-3791ae68-6b80-4165-8f3c-75dd6de74d56 is Running (Ready = true)
    Jan 23 14:17:08.843: INFO: Pod "pod-projected-configmaps-3791ae68-6b80-4165-8f3c-75dd6de74d56" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-b842165c-bae8-4558-870e-7646ca9e7310 01/23/24 14:17:08.849
    STEP: waiting to observe update in volume 01/23/24 14:17:08.854
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:17:10.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-563" for this suite. 01/23/24 14:17:10.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:17:10.871
Jan 23 14:17:10.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename resourcequota 01/23/24 14:17:10.872
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:17:10.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:17:10.883
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 01/23/24 14:17:10.884
STEP: Getting a ResourceQuota 01/23/24 14:17:10.886
STEP: Updating a ResourceQuota 01/23/24 14:17:10.888
STEP: Verifying a ResourceQuota was modified 01/23/24 14:17:10.891
STEP: Deleting a ResourceQuota 01/23/24 14:17:10.892
STEP: Verifying the deleted ResourceQuota 01/23/24 14:17:10.895
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 23 14:17:10.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9300" for this suite. 01/23/24 14:17:10.899
------------------------------
• [0.031 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:17:10.871
    Jan 23 14:17:10.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename resourcequota 01/23/24 14:17:10.872
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:17:10.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:17:10.883
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 01/23/24 14:17:10.884
    STEP: Getting a ResourceQuota 01/23/24 14:17:10.886
    STEP: Updating a ResourceQuota 01/23/24 14:17:10.888
    STEP: Verifying a ResourceQuota was modified 01/23/24 14:17:10.891
    STEP: Deleting a ResourceQuota 01/23/24 14:17:10.892
    STEP: Verifying the deleted ResourceQuota 01/23/24 14:17:10.895
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:17:10.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9300" for this suite. 01/23/24 14:17:10.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:17:10.903
Jan 23 14:17:10.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 14:17:10.904
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:17:10.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:17:10.914
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 01/23/24 14:17:10.915
Jan 23 14:17:10.930: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fdaa3aff-4a36-442d-b1d7-fa1ff642d23a" in namespace "projected-2020" to be "Succeeded or Failed"
Jan 23 14:17:10.932: INFO: Pod "downwardapi-volume-fdaa3aff-4a36-442d-b1d7-fa1ff642d23a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.446697ms
Jan 23 14:17:12.935: INFO: Pod "downwardapi-volume-fdaa3aff-4a36-442d-b1d7-fa1ff642d23a": Phase="Running", Reason="", readiness=true. Elapsed: 2.004571146s
Jan 23 14:17:14.936: INFO: Pod "downwardapi-volume-fdaa3aff-4a36-442d-b1d7-fa1ff642d23a": Phase="Running", Reason="", readiness=false. Elapsed: 4.005300944s
Jan 23 14:17:16.935: INFO: Pod "downwardapi-volume-fdaa3aff-4a36-442d-b1d7-fa1ff642d23a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005170666s
STEP: Saw pod success 01/23/24 14:17:16.936
Jan 23 14:17:16.936: INFO: Pod "downwardapi-volume-fdaa3aff-4a36-442d-b1d7-fa1ff642d23a" satisfied condition "Succeeded or Failed"
Jan 23 14:17:16.937: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-fdaa3aff-4a36-442d-b1d7-fa1ff642d23a container client-container: <nil>
STEP: delete the pod 01/23/24 14:17:16.941
Jan 23 14:17:16.949: INFO: Waiting for pod downwardapi-volume-fdaa3aff-4a36-442d-b1d7-fa1ff642d23a to disappear
Jan 23 14:17:16.951: INFO: Pod downwardapi-volume-fdaa3aff-4a36-442d-b1d7-fa1ff642d23a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 23 14:17:16.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2020" for this suite. 01/23/24 14:17:16.953
------------------------------
• [SLOW TEST] [6.053 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:17:10.903
    Jan 23 14:17:10.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 14:17:10.904
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:17:10.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:17:10.914
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 01/23/24 14:17:10.915
    Jan 23 14:17:10.930: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fdaa3aff-4a36-442d-b1d7-fa1ff642d23a" in namespace "projected-2020" to be "Succeeded or Failed"
    Jan 23 14:17:10.932: INFO: Pod "downwardapi-volume-fdaa3aff-4a36-442d-b1d7-fa1ff642d23a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.446697ms
    Jan 23 14:17:12.935: INFO: Pod "downwardapi-volume-fdaa3aff-4a36-442d-b1d7-fa1ff642d23a": Phase="Running", Reason="", readiness=true. Elapsed: 2.004571146s
    Jan 23 14:17:14.936: INFO: Pod "downwardapi-volume-fdaa3aff-4a36-442d-b1d7-fa1ff642d23a": Phase="Running", Reason="", readiness=false. Elapsed: 4.005300944s
    Jan 23 14:17:16.935: INFO: Pod "downwardapi-volume-fdaa3aff-4a36-442d-b1d7-fa1ff642d23a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005170666s
    STEP: Saw pod success 01/23/24 14:17:16.936
    Jan 23 14:17:16.936: INFO: Pod "downwardapi-volume-fdaa3aff-4a36-442d-b1d7-fa1ff642d23a" satisfied condition "Succeeded or Failed"
    Jan 23 14:17:16.937: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-fdaa3aff-4a36-442d-b1d7-fa1ff642d23a container client-container: <nil>
    STEP: delete the pod 01/23/24 14:17:16.941
    Jan 23 14:17:16.949: INFO: Waiting for pod downwardapi-volume-fdaa3aff-4a36-442d-b1d7-fa1ff642d23a to disappear
    Jan 23 14:17:16.951: INFO: Pod downwardapi-volume-fdaa3aff-4a36-442d-b1d7-fa1ff642d23a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:17:16.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2020" for this suite. 01/23/24 14:17:16.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:17:16.957
Jan 23 14:17:16.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename statefulset 01/23/24 14:17:16.957
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:17:16.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:17:16.967
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7969 01/23/24 14:17:16.969
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 01/23/24 14:17:16.972
STEP: Creating stateful set ss in namespace statefulset-7969 01/23/24 14:17:16.974
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7969 01/23/24 14:17:16.978
Jan 23 14:17:16.981: INFO: Found 0 stateful pods, waiting for 1
Jan 23 14:17:26.984: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/23/24 14:17:26.984
Jan 23 14:17:26.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-7969 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 14:17:27.161: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 14:17:27.161: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 14:17:27.161: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 23 14:17:27.164: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 23 14:17:37.168: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 14:17:37.168: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 14:17:37.176: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999858s
Jan 23 14:17:38.179: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997769849s
Jan 23 14:17:39.181: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994972446s
Jan 23 14:17:40.184: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.992500599s
Jan 23 14:17:41.188: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.989135396s
Jan 23 14:17:42.191: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.986456767s
Jan 23 14:17:43.193: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.983451099s
Jan 23 14:17:44.196: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.980508787s
Jan 23 14:17:45.199: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.977628933s
Jan 23 14:17:46.203: INFO: Verifying statefulset ss doesn't scale past 1 for another 974.088193ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7969 01/23/24 14:17:47.204
Jan 23 14:17:47.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-7969 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 23 14:17:47.383: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 23 14:17:47.384: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 23 14:17:47.384: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 23 14:17:47.386: INFO: Found 1 stateful pods, waiting for 3
Jan 23 14:17:57.391: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 14:17:57.391: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 14:17:57.391: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/23/24 14:17:57.391
STEP: Scale down will halt with unhealthy stateful pod 01/23/24 14:17:57.391
Jan 23 14:17:57.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-7969 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 14:17:57.560: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 14:17:57.560: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 14:17:57.560: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 23 14:17:57.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-7969 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 14:17:57.736: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 14:17:57.736: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 14:17:57.736: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 23 14:17:57.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-7969 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 14:17:57.907: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 14:17:57.907: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 14:17:57.907: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 23 14:17:57.907: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 14:17:57.909: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 23 14:18:07.915: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 14:18:07.915: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 14:18:07.915: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 14:18:07.922: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999859s
Jan 23 14:18:08.925: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997542464s
Jan 23 14:18:09.928: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994786824s
Jan 23 14:18:10.931: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991287045s
Jan 23 14:18:11.935: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.987619063s
Jan 23 14:18:12.939: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.984849548s
Jan 23 14:18:13.941: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980986564s
Jan 23 14:18:14.944: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.978459421s
Jan 23 14:18:15.948: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.974737618s
Jan 23 14:18:16.952: INFO: Verifying statefulset ss doesn't scale past 3 for another 970.798034ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7969 01/23/24 14:18:17.952
Jan 23 14:18:17.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-7969 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 23 14:18:18.130: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 23 14:18:18.130: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 23 14:18:18.130: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 23 14:18:18.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-7969 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 23 14:18:18.329: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 23 14:18:18.329: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 23 14:18:18.329: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 23 14:18:18.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-7969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 23 14:18:18.520: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 23 14:18:18.520: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 23 14:18:18.520: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 23 14:18:18.520: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/23/24 14:18:28.532
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 23 14:18:28.532: INFO: Deleting all statefulset in ns statefulset-7969
Jan 23 14:18:28.534: INFO: Scaling statefulset ss to 0
Jan 23 14:18:28.540: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 14:18:28.541: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 23 14:18:28.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7969" for this suite. 01/23/24 14:18:28.567
------------------------------
• [SLOW TEST] [71.614 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:17:16.957
    Jan 23 14:17:16.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename statefulset 01/23/24 14:17:16.957
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:17:16.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:17:16.967
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7969 01/23/24 14:17:16.969
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/23/24 14:17:16.972
    STEP: Creating stateful set ss in namespace statefulset-7969 01/23/24 14:17:16.974
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7969 01/23/24 14:17:16.978
    Jan 23 14:17:16.981: INFO: Found 0 stateful pods, waiting for 1
    Jan 23 14:17:26.984: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/23/24 14:17:26.984
    Jan 23 14:17:26.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-7969 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 23 14:17:27.161: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 23 14:17:27.161: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 23 14:17:27.161: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 23 14:17:27.164: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 23 14:17:37.168: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 23 14:17:37.168: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 23 14:17:37.176: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999858s
    Jan 23 14:17:38.179: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997769849s
    Jan 23 14:17:39.181: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994972446s
    Jan 23 14:17:40.184: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.992500599s
    Jan 23 14:17:41.188: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.989135396s
    Jan 23 14:17:42.191: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.986456767s
    Jan 23 14:17:43.193: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.983451099s
    Jan 23 14:17:44.196: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.980508787s
    Jan 23 14:17:45.199: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.977628933s
    Jan 23 14:17:46.203: INFO: Verifying statefulset ss doesn't scale past 1 for another 974.088193ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7969 01/23/24 14:17:47.204
    Jan 23 14:17:47.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-7969 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 23 14:17:47.383: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 23 14:17:47.384: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 23 14:17:47.384: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 23 14:17:47.386: INFO: Found 1 stateful pods, waiting for 3
    Jan 23 14:17:57.391: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 23 14:17:57.391: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 23 14:17:57.391: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/23/24 14:17:57.391
    STEP: Scale down will halt with unhealthy stateful pod 01/23/24 14:17:57.391
    Jan 23 14:17:57.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-7969 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 23 14:17:57.560: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 23 14:17:57.560: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 23 14:17:57.560: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 23 14:17:57.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-7969 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 23 14:17:57.736: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 23 14:17:57.736: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 23 14:17:57.736: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 23 14:17:57.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-7969 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 23 14:17:57.907: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 23 14:17:57.907: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 23 14:17:57.907: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 23 14:17:57.907: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 23 14:17:57.909: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 23 14:18:07.915: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 23 14:18:07.915: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 23 14:18:07.915: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 23 14:18:07.922: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999859s
    Jan 23 14:18:08.925: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997542464s
    Jan 23 14:18:09.928: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994786824s
    Jan 23 14:18:10.931: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991287045s
    Jan 23 14:18:11.935: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.987619063s
    Jan 23 14:18:12.939: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.984849548s
    Jan 23 14:18:13.941: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980986564s
    Jan 23 14:18:14.944: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.978459421s
    Jan 23 14:18:15.948: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.974737618s
    Jan 23 14:18:16.952: INFO: Verifying statefulset ss doesn't scale past 3 for another 970.798034ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7969 01/23/24 14:18:17.952
    Jan 23 14:18:17.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-7969 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 23 14:18:18.130: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 23 14:18:18.130: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 23 14:18:18.130: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 23 14:18:18.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-7969 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 23 14:18:18.329: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 23 14:18:18.329: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 23 14:18:18.329: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 23 14:18:18.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-7969 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 23 14:18:18.520: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 23 14:18:18.520: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 23 14:18:18.520: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 23 14:18:18.520: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/23/24 14:18:28.532
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 23 14:18:28.532: INFO: Deleting all statefulset in ns statefulset-7969
    Jan 23 14:18:28.534: INFO: Scaling statefulset ss to 0
    Jan 23 14:18:28.540: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 23 14:18:28.541: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:18:28.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7969" for this suite. 01/23/24 14:18:28.567
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:18:28.572
Jan 23 14:18:28.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename podtemplate 01/23/24 14:18:28.573
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:18:28.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:18:28.583
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/23/24 14:18:28.589
STEP: Replace a pod template 01/23/24 14:18:28.605
Jan 23 14:18:28.610: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 23 14:18:28.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-6464" for this suite. 01/23/24 14:18:28.615
------------------------------
• [0.047 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:18:28.572
    Jan 23 14:18:28.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename podtemplate 01/23/24 14:18:28.573
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:18:28.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:18:28.583
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/23/24 14:18:28.589
    STEP: Replace a pod template 01/23/24 14:18:28.605
    Jan 23 14:18:28.610: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:18:28.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-6464" for this suite. 01/23/24 14:18:28.615
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:18:28.619
Jan 23 14:18:28.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename emptydir-wrapper 01/23/24 14:18:28.62
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:18:28.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:18:28.629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan 23 14:18:28.655: INFO: Waiting up to 5m0s for pod "pod-secrets-555e1aa4-1c31-4a57-b88c-027d257a8fe4" in namespace "emptydir-wrapper-9824" to be "running and ready"
Jan 23 14:18:28.658: INFO: Pod "pod-secrets-555e1aa4-1c31-4a57-b88c-027d257a8fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.511986ms
Jan 23 14:18:28.658: INFO: The phase of Pod pod-secrets-555e1aa4-1c31-4a57-b88c-027d257a8fe4 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:18:30.661: INFO: Pod "pod-secrets-555e1aa4-1c31-4a57-b88c-027d257a8fe4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005522589s
Jan 23 14:18:30.661: INFO: The phase of Pod pod-secrets-555e1aa4-1c31-4a57-b88c-027d257a8fe4 is Running (Ready = true)
Jan 23 14:18:30.661: INFO: Pod "pod-secrets-555e1aa4-1c31-4a57-b88c-027d257a8fe4" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/23/24 14:18:30.662
STEP: Cleaning up the configmap 01/23/24 14:18:30.665
STEP: Cleaning up the pod 01/23/24 14:18:30.668
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan 23 14:18:30.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-9824" for this suite. 01/23/24 14:18:30.676
------------------------------
• [2.060 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:18:28.619
    Jan 23 14:18:28.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename emptydir-wrapper 01/23/24 14:18:28.62
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:18:28.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:18:28.629
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan 23 14:18:28.655: INFO: Waiting up to 5m0s for pod "pod-secrets-555e1aa4-1c31-4a57-b88c-027d257a8fe4" in namespace "emptydir-wrapper-9824" to be "running and ready"
    Jan 23 14:18:28.658: INFO: Pod "pod-secrets-555e1aa4-1c31-4a57-b88c-027d257a8fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.511986ms
    Jan 23 14:18:28.658: INFO: The phase of Pod pod-secrets-555e1aa4-1c31-4a57-b88c-027d257a8fe4 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:18:30.661: INFO: Pod "pod-secrets-555e1aa4-1c31-4a57-b88c-027d257a8fe4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005522589s
    Jan 23 14:18:30.661: INFO: The phase of Pod pod-secrets-555e1aa4-1c31-4a57-b88c-027d257a8fe4 is Running (Ready = true)
    Jan 23 14:18:30.661: INFO: Pod "pod-secrets-555e1aa4-1c31-4a57-b88c-027d257a8fe4" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/23/24 14:18:30.662
    STEP: Cleaning up the configmap 01/23/24 14:18:30.665
    STEP: Cleaning up the pod 01/23/24 14:18:30.668
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:18:30.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-9824" for this suite. 01/23/24 14:18:30.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:18:30.68
Jan 23 14:18:30.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename gc 01/23/24 14:18:30.681
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:18:30.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:18:30.692
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/23/24 14:18:30.695
STEP: delete the rc 01/23/24 14:18:35.701
STEP: wait for the rc to be deleted 01/23/24 14:18:35.705
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/23/24 14:18:40.708
STEP: Gathering metrics 01/23/24 14:19:10.72
Jan 23 14:19:10.737: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" in namespace "kube-system" to be "running and ready"
Jan 23 14:19:10.740: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local": Phase="Running", Reason="", readiness=true. Elapsed: 3.241095ms
Jan 23 14:19:10.740: INFO: The phase of Pod kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local is Running (Ready = true)
Jan 23 14:19:10.740: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" satisfied condition "running and ready"
Jan 23 14:19:10.786: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 23 14:19:10.786: INFO: Deleting pod "simpletest.rc-24wqt" in namespace "gc-4690"
Jan 23 14:19:10.795: INFO: Deleting pod "simpletest.rc-47rsm" in namespace "gc-4690"
Jan 23 14:19:10.802: INFO: Deleting pod "simpletest.rc-487jp" in namespace "gc-4690"
Jan 23 14:19:10.808: INFO: Deleting pod "simpletest.rc-48z5l" in namespace "gc-4690"
Jan 23 14:19:10.815: INFO: Deleting pod "simpletest.rc-4d79d" in namespace "gc-4690"
Jan 23 14:19:10.820: INFO: Deleting pod "simpletest.rc-4qkpz" in namespace "gc-4690"
Jan 23 14:19:10.828: INFO: Deleting pod "simpletest.rc-4rhdq" in namespace "gc-4690"
Jan 23 14:19:10.834: INFO: Deleting pod "simpletest.rc-4tg7j" in namespace "gc-4690"
Jan 23 14:19:10.839: INFO: Deleting pod "simpletest.rc-54rkb" in namespace "gc-4690"
Jan 23 14:19:10.867: INFO: Deleting pod "simpletest.rc-5gt4c" in namespace "gc-4690"
Jan 23 14:19:10.875: INFO: Deleting pod "simpletest.rc-5tzxm" in namespace "gc-4690"
Jan 23 14:19:10.882: INFO: Deleting pod "simpletest.rc-5z5qt" in namespace "gc-4690"
Jan 23 14:19:10.904: INFO: Deleting pod "simpletest.rc-628tw" in namespace "gc-4690"
Jan 23 14:19:10.913: INFO: Deleting pod "simpletest.rc-6ldpp" in namespace "gc-4690"
Jan 23 14:19:10.931: INFO: Deleting pod "simpletest.rc-6mf95" in namespace "gc-4690"
Jan 23 14:19:10.944: INFO: Deleting pod "simpletest.rc-6np8b" in namespace "gc-4690"
Jan 23 14:19:10.998: INFO: Deleting pod "simpletest.rc-6shzn" in namespace "gc-4690"
Jan 23 14:19:11.010: INFO: Deleting pod "simpletest.rc-6wpjc" in namespace "gc-4690"
Jan 23 14:19:11.028: INFO: Deleting pod "simpletest.rc-75lss" in namespace "gc-4690"
Jan 23 14:19:11.041: INFO: Deleting pod "simpletest.rc-7lvcq" in namespace "gc-4690"
Jan 23 14:19:11.057: INFO: Deleting pod "simpletest.rc-7qrhl" in namespace "gc-4690"
Jan 23 14:19:11.078: INFO: Deleting pod "simpletest.rc-8pvpp" in namespace "gc-4690"
Jan 23 14:19:11.123: INFO: Deleting pod "simpletest.rc-8wc4m" in namespace "gc-4690"
Jan 23 14:19:11.141: INFO: Deleting pod "simpletest.rc-9bmfm" in namespace "gc-4690"
Jan 23 14:19:11.157: INFO: Deleting pod "simpletest.rc-9kbdk" in namespace "gc-4690"
Jan 23 14:19:11.173: INFO: Deleting pod "simpletest.rc-9qjqb" in namespace "gc-4690"
Jan 23 14:19:11.185: INFO: Deleting pod "simpletest.rc-b2xv6" in namespace "gc-4690"
Jan 23 14:19:11.194: INFO: Deleting pod "simpletest.rc-b4hwh" in namespace "gc-4690"
Jan 23 14:19:11.213: INFO: Deleting pod "simpletest.rc-b774s" in namespace "gc-4690"
Jan 23 14:19:11.266: INFO: Deleting pod "simpletest.rc-c8gjv" in namespace "gc-4690"
Jan 23 14:19:11.279: INFO: Deleting pod "simpletest.rc-cmx5t" in namespace "gc-4690"
Jan 23 14:19:11.291: INFO: Deleting pod "simpletest.rc-cwm57" in namespace "gc-4690"
Jan 23 14:19:11.310: INFO: Deleting pod "simpletest.rc-cx8sg" in namespace "gc-4690"
Jan 23 14:19:11.319: INFO: Deleting pod "simpletest.rc-d5mbf" in namespace "gc-4690"
Jan 23 14:19:11.328: INFO: Deleting pod "simpletest.rc-d99v2" in namespace "gc-4690"
Jan 23 14:19:11.338: INFO: Deleting pod "simpletest.rc-dbdmz" in namespace "gc-4690"
Jan 23 14:19:11.351: INFO: Deleting pod "simpletest.rc-dbtm9" in namespace "gc-4690"
Jan 23 14:19:11.360: INFO: Deleting pod "simpletest.rc-dc6fp" in namespace "gc-4690"
Jan 23 14:19:11.397: INFO: Deleting pod "simpletest.rc-dd89m" in namespace "gc-4690"
Jan 23 14:19:11.426: INFO: Deleting pod "simpletest.rc-ddrfk" in namespace "gc-4690"
Jan 23 14:19:11.431: INFO: Deleting pod "simpletest.rc-djk4w" in namespace "gc-4690"
Jan 23 14:19:11.441: INFO: Deleting pod "simpletest.rc-dwg98" in namespace "gc-4690"
Jan 23 14:19:11.453: INFO: Deleting pod "simpletest.rc-f27kh" in namespace "gc-4690"
Jan 23 14:19:11.463: INFO: Deleting pod "simpletest.rc-fmg8q" in namespace "gc-4690"
Jan 23 14:19:11.473: INFO: Deleting pod "simpletest.rc-fskqf" in namespace "gc-4690"
Jan 23 14:19:11.482: INFO: Deleting pod "simpletest.rc-gddpp" in namespace "gc-4690"
Jan 23 14:19:11.491: INFO: Deleting pod "simpletest.rc-ghpxg" in namespace "gc-4690"
Jan 23 14:19:11.532: INFO: Deleting pod "simpletest.rc-h8vjt" in namespace "gc-4690"
Jan 23 14:19:11.544: INFO: Deleting pod "simpletest.rc-hc7p5" in namespace "gc-4690"
Jan 23 14:19:11.555: INFO: Deleting pod "simpletest.rc-hhjrf" in namespace "gc-4690"
Jan 23 14:19:11.562: INFO: Deleting pod "simpletest.rc-hhzlh" in namespace "gc-4690"
Jan 23 14:19:11.576: INFO: Deleting pod "simpletest.rc-hkx8t" in namespace "gc-4690"
Jan 23 14:19:11.591: INFO: Deleting pod "simpletest.rc-hlbvv" in namespace "gc-4690"
Jan 23 14:19:11.603: INFO: Deleting pod "simpletest.rc-hpfxd" in namespace "gc-4690"
Jan 23 14:19:11.622: INFO: Deleting pod "simpletest.rc-hzjcz" in namespace "gc-4690"
Jan 23 14:19:11.660: INFO: Deleting pod "simpletest.rc-jpgpx" in namespace "gc-4690"
Jan 23 14:19:11.670: INFO: Deleting pod "simpletest.rc-k54t2" in namespace "gc-4690"
Jan 23 14:19:11.678: INFO: Deleting pod "simpletest.rc-kghdm" in namespace "gc-4690"
Jan 23 14:19:11.691: INFO: Deleting pod "simpletest.rc-kl6hb" in namespace "gc-4690"
Jan 23 14:19:11.701: INFO: Deleting pod "simpletest.rc-l4z2h" in namespace "gc-4690"
Jan 23 14:19:11.713: INFO: Deleting pod "simpletest.rc-l9vjm" in namespace "gc-4690"
Jan 23 14:19:11.723: INFO: Deleting pod "simpletest.rc-lmx6h" in namespace "gc-4690"
Jan 23 14:19:11.735: INFO: Deleting pod "simpletest.rc-lqc2b" in namespace "gc-4690"
Jan 23 14:19:11.744: INFO: Deleting pod "simpletest.rc-ls26w" in namespace "gc-4690"
Jan 23 14:19:11.794: INFO: Deleting pod "simpletest.rc-ltj7f" in namespace "gc-4690"
Jan 23 14:19:11.809: INFO: Deleting pod "simpletest.rc-ltt94" in namespace "gc-4690"
Jan 23 14:19:11.823: INFO: Deleting pod "simpletest.rc-ltwwq" in namespace "gc-4690"
Jan 23 14:19:11.835: INFO: Deleting pod "simpletest.rc-m4s4k" in namespace "gc-4690"
Jan 23 14:19:11.848: INFO: Deleting pod "simpletest.rc-mkdqh" in namespace "gc-4690"
Jan 23 14:19:11.867: INFO: Deleting pod "simpletest.rc-mz48w" in namespace "gc-4690"
Jan 23 14:19:11.916: INFO: Deleting pod "simpletest.rc-mzzrf" in namespace "gc-4690"
Jan 23 14:19:11.967: INFO: Deleting pod "simpletest.rc-nwnjv" in namespace "gc-4690"
Jan 23 14:19:12.015: INFO: Deleting pod "simpletest.rc-pwfvk" in namespace "gc-4690"
Jan 23 14:19:12.067: INFO: Deleting pod "simpletest.rc-q46x5" in namespace "gc-4690"
Jan 23 14:19:12.117: INFO: Deleting pod "simpletest.rc-q6c89" in namespace "gc-4690"
Jan 23 14:19:12.167: INFO: Deleting pod "simpletest.rc-qkwnt" in namespace "gc-4690"
Jan 23 14:19:12.218: INFO: Deleting pod "simpletest.rc-qv4gn" in namespace "gc-4690"
Jan 23 14:19:12.266: INFO: Deleting pod "simpletest.rc-qwkq7" in namespace "gc-4690"
Jan 23 14:19:12.317: INFO: Deleting pod "simpletest.rc-qzhz8" in namespace "gc-4690"
Jan 23 14:19:12.366: INFO: Deleting pod "simpletest.rc-rz98g" in namespace "gc-4690"
Jan 23 14:19:12.416: INFO: Deleting pod "simpletest.rc-s4rsh" in namespace "gc-4690"
Jan 23 14:19:12.467: INFO: Deleting pod "simpletest.rc-sdmmn" in namespace "gc-4690"
Jan 23 14:19:12.517: INFO: Deleting pod "simpletest.rc-stzd2" in namespace "gc-4690"
Jan 23 14:19:12.566: INFO: Deleting pod "simpletest.rc-t8wwx" in namespace "gc-4690"
Jan 23 14:19:12.616: INFO: Deleting pod "simpletest.rc-t9wcb" in namespace "gc-4690"
Jan 23 14:19:12.668: INFO: Deleting pod "simpletest.rc-vbzrn" in namespace "gc-4690"
Jan 23 14:19:12.716: INFO: Deleting pod "simpletest.rc-vf4jx" in namespace "gc-4690"
Jan 23 14:19:12.770: INFO: Deleting pod "simpletest.rc-vfbvj" in namespace "gc-4690"
Jan 23 14:19:12.814: INFO: Deleting pod "simpletest.rc-vgl6v" in namespace "gc-4690"
Jan 23 14:19:12.867: INFO: Deleting pod "simpletest.rc-vnqs6" in namespace "gc-4690"
Jan 23 14:19:12.917: INFO: Deleting pod "simpletest.rc-vp962" in namespace "gc-4690"
Jan 23 14:19:12.967: INFO: Deleting pod "simpletest.rc-w2gnn" in namespace "gc-4690"
Jan 23 14:19:13.017: INFO: Deleting pod "simpletest.rc-wkmtn" in namespace "gc-4690"
Jan 23 14:19:13.066: INFO: Deleting pod "simpletest.rc-wlt4d" in namespace "gc-4690"
Jan 23 14:19:13.117: INFO: Deleting pod "simpletest.rc-z4cx9" in namespace "gc-4690"
Jan 23 14:19:13.167: INFO: Deleting pod "simpletest.rc-z58zx" in namespace "gc-4690"
Jan 23 14:19:13.218: INFO: Deleting pod "simpletest.rc-zjtwh" in namespace "gc-4690"
Jan 23 14:19:13.267: INFO: Deleting pod "simpletest.rc-zmwp5" in namespace "gc-4690"
Jan 23 14:19:13.318: INFO: Deleting pod "simpletest.rc-zr2jq" in namespace "gc-4690"
Jan 23 14:19:13.367: INFO: Deleting pod "simpletest.rc-zrrxb" in namespace "gc-4690"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 23 14:19:13.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4690" for this suite. 01/23/24 14:19:13.462
------------------------------
• [SLOW TEST] [42.834 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:18:30.68
    Jan 23 14:18:30.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename gc 01/23/24 14:18:30.681
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:18:30.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:18:30.692
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/23/24 14:18:30.695
    STEP: delete the rc 01/23/24 14:18:35.701
    STEP: wait for the rc to be deleted 01/23/24 14:18:35.705
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/23/24 14:18:40.708
    STEP: Gathering metrics 01/23/24 14:19:10.72
    Jan 23 14:19:10.737: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" in namespace "kube-system" to be "running and ready"
    Jan 23 14:19:10.740: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local": Phase="Running", Reason="", readiness=true. Elapsed: 3.241095ms
    Jan 23 14:19:10.740: INFO: The phase of Pod kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local is Running (Ready = true)
    Jan 23 14:19:10.740: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" satisfied condition "running and ready"
    Jan 23 14:19:10.786: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 23 14:19:10.786: INFO: Deleting pod "simpletest.rc-24wqt" in namespace "gc-4690"
    Jan 23 14:19:10.795: INFO: Deleting pod "simpletest.rc-47rsm" in namespace "gc-4690"
    Jan 23 14:19:10.802: INFO: Deleting pod "simpletest.rc-487jp" in namespace "gc-4690"
    Jan 23 14:19:10.808: INFO: Deleting pod "simpletest.rc-48z5l" in namespace "gc-4690"
    Jan 23 14:19:10.815: INFO: Deleting pod "simpletest.rc-4d79d" in namespace "gc-4690"
    Jan 23 14:19:10.820: INFO: Deleting pod "simpletest.rc-4qkpz" in namespace "gc-4690"
    Jan 23 14:19:10.828: INFO: Deleting pod "simpletest.rc-4rhdq" in namespace "gc-4690"
    Jan 23 14:19:10.834: INFO: Deleting pod "simpletest.rc-4tg7j" in namespace "gc-4690"
    Jan 23 14:19:10.839: INFO: Deleting pod "simpletest.rc-54rkb" in namespace "gc-4690"
    Jan 23 14:19:10.867: INFO: Deleting pod "simpletest.rc-5gt4c" in namespace "gc-4690"
    Jan 23 14:19:10.875: INFO: Deleting pod "simpletest.rc-5tzxm" in namespace "gc-4690"
    Jan 23 14:19:10.882: INFO: Deleting pod "simpletest.rc-5z5qt" in namespace "gc-4690"
    Jan 23 14:19:10.904: INFO: Deleting pod "simpletest.rc-628tw" in namespace "gc-4690"
    Jan 23 14:19:10.913: INFO: Deleting pod "simpletest.rc-6ldpp" in namespace "gc-4690"
    Jan 23 14:19:10.931: INFO: Deleting pod "simpletest.rc-6mf95" in namespace "gc-4690"
    Jan 23 14:19:10.944: INFO: Deleting pod "simpletest.rc-6np8b" in namespace "gc-4690"
    Jan 23 14:19:10.998: INFO: Deleting pod "simpletest.rc-6shzn" in namespace "gc-4690"
    Jan 23 14:19:11.010: INFO: Deleting pod "simpletest.rc-6wpjc" in namespace "gc-4690"
    Jan 23 14:19:11.028: INFO: Deleting pod "simpletest.rc-75lss" in namespace "gc-4690"
    Jan 23 14:19:11.041: INFO: Deleting pod "simpletest.rc-7lvcq" in namespace "gc-4690"
    Jan 23 14:19:11.057: INFO: Deleting pod "simpletest.rc-7qrhl" in namespace "gc-4690"
    Jan 23 14:19:11.078: INFO: Deleting pod "simpletest.rc-8pvpp" in namespace "gc-4690"
    Jan 23 14:19:11.123: INFO: Deleting pod "simpletest.rc-8wc4m" in namespace "gc-4690"
    Jan 23 14:19:11.141: INFO: Deleting pod "simpletest.rc-9bmfm" in namespace "gc-4690"
    Jan 23 14:19:11.157: INFO: Deleting pod "simpletest.rc-9kbdk" in namespace "gc-4690"
    Jan 23 14:19:11.173: INFO: Deleting pod "simpletest.rc-9qjqb" in namespace "gc-4690"
    Jan 23 14:19:11.185: INFO: Deleting pod "simpletest.rc-b2xv6" in namespace "gc-4690"
    Jan 23 14:19:11.194: INFO: Deleting pod "simpletest.rc-b4hwh" in namespace "gc-4690"
    Jan 23 14:19:11.213: INFO: Deleting pod "simpletest.rc-b774s" in namespace "gc-4690"
    Jan 23 14:19:11.266: INFO: Deleting pod "simpletest.rc-c8gjv" in namespace "gc-4690"
    Jan 23 14:19:11.279: INFO: Deleting pod "simpletest.rc-cmx5t" in namespace "gc-4690"
    Jan 23 14:19:11.291: INFO: Deleting pod "simpletest.rc-cwm57" in namespace "gc-4690"
    Jan 23 14:19:11.310: INFO: Deleting pod "simpletest.rc-cx8sg" in namespace "gc-4690"
    Jan 23 14:19:11.319: INFO: Deleting pod "simpletest.rc-d5mbf" in namespace "gc-4690"
    Jan 23 14:19:11.328: INFO: Deleting pod "simpletest.rc-d99v2" in namespace "gc-4690"
    Jan 23 14:19:11.338: INFO: Deleting pod "simpletest.rc-dbdmz" in namespace "gc-4690"
    Jan 23 14:19:11.351: INFO: Deleting pod "simpletest.rc-dbtm9" in namespace "gc-4690"
    Jan 23 14:19:11.360: INFO: Deleting pod "simpletest.rc-dc6fp" in namespace "gc-4690"
    Jan 23 14:19:11.397: INFO: Deleting pod "simpletest.rc-dd89m" in namespace "gc-4690"
    Jan 23 14:19:11.426: INFO: Deleting pod "simpletest.rc-ddrfk" in namespace "gc-4690"
    Jan 23 14:19:11.431: INFO: Deleting pod "simpletest.rc-djk4w" in namespace "gc-4690"
    Jan 23 14:19:11.441: INFO: Deleting pod "simpletest.rc-dwg98" in namespace "gc-4690"
    Jan 23 14:19:11.453: INFO: Deleting pod "simpletest.rc-f27kh" in namespace "gc-4690"
    Jan 23 14:19:11.463: INFO: Deleting pod "simpletest.rc-fmg8q" in namespace "gc-4690"
    Jan 23 14:19:11.473: INFO: Deleting pod "simpletest.rc-fskqf" in namespace "gc-4690"
    Jan 23 14:19:11.482: INFO: Deleting pod "simpletest.rc-gddpp" in namespace "gc-4690"
    Jan 23 14:19:11.491: INFO: Deleting pod "simpletest.rc-ghpxg" in namespace "gc-4690"
    Jan 23 14:19:11.532: INFO: Deleting pod "simpletest.rc-h8vjt" in namespace "gc-4690"
    Jan 23 14:19:11.544: INFO: Deleting pod "simpletest.rc-hc7p5" in namespace "gc-4690"
    Jan 23 14:19:11.555: INFO: Deleting pod "simpletest.rc-hhjrf" in namespace "gc-4690"
    Jan 23 14:19:11.562: INFO: Deleting pod "simpletest.rc-hhzlh" in namespace "gc-4690"
    Jan 23 14:19:11.576: INFO: Deleting pod "simpletest.rc-hkx8t" in namespace "gc-4690"
    Jan 23 14:19:11.591: INFO: Deleting pod "simpletest.rc-hlbvv" in namespace "gc-4690"
    Jan 23 14:19:11.603: INFO: Deleting pod "simpletest.rc-hpfxd" in namespace "gc-4690"
    Jan 23 14:19:11.622: INFO: Deleting pod "simpletest.rc-hzjcz" in namespace "gc-4690"
    Jan 23 14:19:11.660: INFO: Deleting pod "simpletest.rc-jpgpx" in namespace "gc-4690"
    Jan 23 14:19:11.670: INFO: Deleting pod "simpletest.rc-k54t2" in namespace "gc-4690"
    Jan 23 14:19:11.678: INFO: Deleting pod "simpletest.rc-kghdm" in namespace "gc-4690"
    Jan 23 14:19:11.691: INFO: Deleting pod "simpletest.rc-kl6hb" in namespace "gc-4690"
    Jan 23 14:19:11.701: INFO: Deleting pod "simpletest.rc-l4z2h" in namespace "gc-4690"
    Jan 23 14:19:11.713: INFO: Deleting pod "simpletest.rc-l9vjm" in namespace "gc-4690"
    Jan 23 14:19:11.723: INFO: Deleting pod "simpletest.rc-lmx6h" in namespace "gc-4690"
    Jan 23 14:19:11.735: INFO: Deleting pod "simpletest.rc-lqc2b" in namespace "gc-4690"
    Jan 23 14:19:11.744: INFO: Deleting pod "simpletest.rc-ls26w" in namespace "gc-4690"
    Jan 23 14:19:11.794: INFO: Deleting pod "simpletest.rc-ltj7f" in namespace "gc-4690"
    Jan 23 14:19:11.809: INFO: Deleting pod "simpletest.rc-ltt94" in namespace "gc-4690"
    Jan 23 14:19:11.823: INFO: Deleting pod "simpletest.rc-ltwwq" in namespace "gc-4690"
    Jan 23 14:19:11.835: INFO: Deleting pod "simpletest.rc-m4s4k" in namespace "gc-4690"
    Jan 23 14:19:11.848: INFO: Deleting pod "simpletest.rc-mkdqh" in namespace "gc-4690"
    Jan 23 14:19:11.867: INFO: Deleting pod "simpletest.rc-mz48w" in namespace "gc-4690"
    Jan 23 14:19:11.916: INFO: Deleting pod "simpletest.rc-mzzrf" in namespace "gc-4690"
    Jan 23 14:19:11.967: INFO: Deleting pod "simpletest.rc-nwnjv" in namespace "gc-4690"
    Jan 23 14:19:12.015: INFO: Deleting pod "simpletest.rc-pwfvk" in namespace "gc-4690"
    Jan 23 14:19:12.067: INFO: Deleting pod "simpletest.rc-q46x5" in namespace "gc-4690"
    Jan 23 14:19:12.117: INFO: Deleting pod "simpletest.rc-q6c89" in namespace "gc-4690"
    Jan 23 14:19:12.167: INFO: Deleting pod "simpletest.rc-qkwnt" in namespace "gc-4690"
    Jan 23 14:19:12.218: INFO: Deleting pod "simpletest.rc-qv4gn" in namespace "gc-4690"
    Jan 23 14:19:12.266: INFO: Deleting pod "simpletest.rc-qwkq7" in namespace "gc-4690"
    Jan 23 14:19:12.317: INFO: Deleting pod "simpletest.rc-qzhz8" in namespace "gc-4690"
    Jan 23 14:19:12.366: INFO: Deleting pod "simpletest.rc-rz98g" in namespace "gc-4690"
    Jan 23 14:19:12.416: INFO: Deleting pod "simpletest.rc-s4rsh" in namespace "gc-4690"
    Jan 23 14:19:12.467: INFO: Deleting pod "simpletest.rc-sdmmn" in namespace "gc-4690"
    Jan 23 14:19:12.517: INFO: Deleting pod "simpletest.rc-stzd2" in namespace "gc-4690"
    Jan 23 14:19:12.566: INFO: Deleting pod "simpletest.rc-t8wwx" in namespace "gc-4690"
    Jan 23 14:19:12.616: INFO: Deleting pod "simpletest.rc-t9wcb" in namespace "gc-4690"
    Jan 23 14:19:12.668: INFO: Deleting pod "simpletest.rc-vbzrn" in namespace "gc-4690"
    Jan 23 14:19:12.716: INFO: Deleting pod "simpletest.rc-vf4jx" in namespace "gc-4690"
    Jan 23 14:19:12.770: INFO: Deleting pod "simpletest.rc-vfbvj" in namespace "gc-4690"
    Jan 23 14:19:12.814: INFO: Deleting pod "simpletest.rc-vgl6v" in namespace "gc-4690"
    Jan 23 14:19:12.867: INFO: Deleting pod "simpletest.rc-vnqs6" in namespace "gc-4690"
    Jan 23 14:19:12.917: INFO: Deleting pod "simpletest.rc-vp962" in namespace "gc-4690"
    Jan 23 14:19:12.967: INFO: Deleting pod "simpletest.rc-w2gnn" in namespace "gc-4690"
    Jan 23 14:19:13.017: INFO: Deleting pod "simpletest.rc-wkmtn" in namespace "gc-4690"
    Jan 23 14:19:13.066: INFO: Deleting pod "simpletest.rc-wlt4d" in namespace "gc-4690"
    Jan 23 14:19:13.117: INFO: Deleting pod "simpletest.rc-z4cx9" in namespace "gc-4690"
    Jan 23 14:19:13.167: INFO: Deleting pod "simpletest.rc-z58zx" in namespace "gc-4690"
    Jan 23 14:19:13.218: INFO: Deleting pod "simpletest.rc-zjtwh" in namespace "gc-4690"
    Jan 23 14:19:13.267: INFO: Deleting pod "simpletest.rc-zmwp5" in namespace "gc-4690"
    Jan 23 14:19:13.318: INFO: Deleting pod "simpletest.rc-zr2jq" in namespace "gc-4690"
    Jan 23 14:19:13.367: INFO: Deleting pod "simpletest.rc-zrrxb" in namespace "gc-4690"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:19:13.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4690" for this suite. 01/23/24 14:19:13.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:19:13.515
Jan 23 14:19:13.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename webhook 01/23/24 14:19:13.516
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:19:13.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:19:13.525
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/23/24 14:19:13.536
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:19:13.888
STEP: Deploying the webhook pod 01/23/24 14:19:13.893
STEP: Wait for the deployment to be ready 01/23/24 14:19:13.902
Jan 23 14:19:13.904: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 23 14:19:15.910: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 14:19:17.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 14:19:19.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 14:19:21.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/23/24 14:19:23.913
STEP: Verifying the service has paired with the endpoint 01/23/24 14:19:23.919
Jan 23 14:19:24.919: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 01/23/24 14:19:24.926
STEP: Creating a configMap that does not comply to the validation webhook rules 01/23/24 14:19:24.938
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/23/24 14:19:24.944
STEP: Creating a configMap that does not comply to the validation webhook rules 01/23/24 14:19:24.95
STEP: Patching a validating webhook configuration's rules to include the create operation 01/23/24 14:19:24.956
STEP: Creating a configMap that does not comply to the validation webhook rules 01/23/24 14:19:24.96
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:19:24.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3212" for this suite. 01/23/24 14:19:24.999
STEP: Destroying namespace "webhook-3212-markers" for this suite. 01/23/24 14:19:25.005
------------------------------
• [SLOW TEST] [11.497 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:19:13.515
    Jan 23 14:19:13.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename webhook 01/23/24 14:19:13.516
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:19:13.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:19:13.525
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/23/24 14:19:13.536
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:19:13.888
    STEP: Deploying the webhook pod 01/23/24 14:19:13.893
    STEP: Wait for the deployment to be ready 01/23/24 14:19:13.902
    Jan 23 14:19:13.904: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 23 14:19:15.910: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 14:19:17.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 14:19:19.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 14:19:21.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 19, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/23/24 14:19:23.913
    STEP: Verifying the service has paired with the endpoint 01/23/24 14:19:23.919
    Jan 23 14:19:24.919: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 01/23/24 14:19:24.926
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/23/24 14:19:24.938
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/23/24 14:19:24.944
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/23/24 14:19:24.95
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/23/24 14:19:24.956
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/23/24 14:19:24.96
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:19:24.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3212" for this suite. 01/23/24 14:19:24.999
    STEP: Destroying namespace "webhook-3212-markers" for this suite. 01/23/24 14:19:25.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:19:25.019
Jan 23 14:19:25.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename namespaces 01/23/24 14:19:25.021
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:19:25.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:19:25.043
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 01/23/24 14:19:25.045
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:19:25.056
STEP: Creating a pod in the namespace 01/23/24 14:19:25.059
STEP: Waiting for the pod to have running status 01/23/24 14:19:25.073
Jan 23 14:19:25.074: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3048" to be "running"
Jan 23 14:19:25.077: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.431104ms
Jan 23 14:19:27.080: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006753235s
Jan 23 14:19:29.080: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006915176s
Jan 23 14:19:31.080: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006757395s
Jan 23 14:19:33.080: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.00636738s
Jan 23 14:19:33.080: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/23/24 14:19:33.08
STEP: Waiting for the namespace to be removed. 01/23/24 14:19:33.083
STEP: Recreating the namespace 01/23/24 14:19:44.086
STEP: Verifying there are no pods in the namespace 01/23/24 14:19:44.095
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:19:44.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2676" for this suite. 01/23/24 14:19:44.099
STEP: Destroying namespace "nsdeletetest-3048" for this suite. 01/23/24 14:19:44.103
Jan 23 14:19:44.104: INFO: Namespace nsdeletetest-3048 was already deleted
STEP: Destroying namespace "nsdeletetest-4702" for this suite. 01/23/24 14:19:44.104
------------------------------
• [SLOW TEST] [19.089 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:19:25.019
    Jan 23 14:19:25.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename namespaces 01/23/24 14:19:25.021
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:19:25.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:19:25.043
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 01/23/24 14:19:25.045
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:19:25.056
    STEP: Creating a pod in the namespace 01/23/24 14:19:25.059
    STEP: Waiting for the pod to have running status 01/23/24 14:19:25.073
    Jan 23 14:19:25.074: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3048" to be "running"
    Jan 23 14:19:25.077: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.431104ms
    Jan 23 14:19:27.080: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006753235s
    Jan 23 14:19:29.080: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006915176s
    Jan 23 14:19:31.080: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006757395s
    Jan 23 14:19:33.080: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.00636738s
    Jan 23 14:19:33.080: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/23/24 14:19:33.08
    STEP: Waiting for the namespace to be removed. 01/23/24 14:19:33.083
    STEP: Recreating the namespace 01/23/24 14:19:44.086
    STEP: Verifying there are no pods in the namespace 01/23/24 14:19:44.095
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:19:44.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2676" for this suite. 01/23/24 14:19:44.099
    STEP: Destroying namespace "nsdeletetest-3048" for this suite. 01/23/24 14:19:44.103
    Jan 23 14:19:44.104: INFO: Namespace nsdeletetest-3048 was already deleted
    STEP: Destroying namespace "nsdeletetest-4702" for this suite. 01/23/24 14:19:44.104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:19:44.108
Jan 23 14:19:44.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename secrets 01/23/24 14:19:44.109
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:19:44.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:19:44.118
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-0ff4cedf-7b88-423e-9e6e-9fdde7fd7d1f 01/23/24 14:19:44.121
STEP: Creating a pod to test consume secrets 01/23/24 14:19:44.125
Jan 23 14:19:44.138: INFO: Waiting up to 5m0s for pod "pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9" in namespace "secrets-4963" to be "Succeeded or Failed"
Jan 23 14:19:44.140: INFO: Pod "pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.437632ms
Jan 23 14:19:46.142: INFO: Pod "pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9": Phase="Running", Reason="", readiness=true. Elapsed: 2.003767516s
Jan 23 14:19:48.143: INFO: Pod "pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9": Phase="Running", Reason="", readiness=true. Elapsed: 4.004599423s
Jan 23 14:19:50.143: INFO: Pod "pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9": Phase="Running", Reason="", readiness=false. Elapsed: 6.004568905s
Jan 23 14:19:52.143: INFO: Pod "pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.004357143s
STEP: Saw pod success 01/23/24 14:19:52.143
Jan 23 14:19:52.143: INFO: Pod "pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9" satisfied condition "Succeeded or Failed"
Jan 23 14:19:52.144: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9 container secret-volume-test: <nil>
STEP: delete the pod 01/23/24 14:19:52.154
Jan 23 14:19:52.160: INFO: Waiting for pod pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9 to disappear
Jan 23 14:19:52.162: INFO: Pod pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 23 14:19:52.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4963" for this suite. 01/23/24 14:19:52.164
------------------------------
• [SLOW TEST] [8.058 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:19:44.108
    Jan 23 14:19:44.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename secrets 01/23/24 14:19:44.109
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:19:44.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:19:44.118
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-0ff4cedf-7b88-423e-9e6e-9fdde7fd7d1f 01/23/24 14:19:44.121
    STEP: Creating a pod to test consume secrets 01/23/24 14:19:44.125
    Jan 23 14:19:44.138: INFO: Waiting up to 5m0s for pod "pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9" in namespace "secrets-4963" to be "Succeeded or Failed"
    Jan 23 14:19:44.140: INFO: Pod "pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.437632ms
    Jan 23 14:19:46.142: INFO: Pod "pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9": Phase="Running", Reason="", readiness=true. Elapsed: 2.003767516s
    Jan 23 14:19:48.143: INFO: Pod "pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9": Phase="Running", Reason="", readiness=true. Elapsed: 4.004599423s
    Jan 23 14:19:50.143: INFO: Pod "pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9": Phase="Running", Reason="", readiness=false. Elapsed: 6.004568905s
    Jan 23 14:19:52.143: INFO: Pod "pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.004357143s
    STEP: Saw pod success 01/23/24 14:19:52.143
    Jan 23 14:19:52.143: INFO: Pod "pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9" satisfied condition "Succeeded or Failed"
    Jan 23 14:19:52.144: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9 container secret-volume-test: <nil>
    STEP: delete the pod 01/23/24 14:19:52.154
    Jan 23 14:19:52.160: INFO: Waiting for pod pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9 to disappear
    Jan 23 14:19:52.162: INFO: Pod pod-secrets-03ea0386-2202-4959-adb8-767f0953ccf9 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:19:52.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4963" for this suite. 01/23/24 14:19:52.164
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:19:52.167
Jan 23 14:19:52.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename conformance-tests 01/23/24 14:19:52.168
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:19:52.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:19:52.177
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/23/24 14:19:52.179
Jan 23 14:19:52.179: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Jan 23 14:19:52.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-9811" for this suite. 01/23/24 14:19:52.184
------------------------------
• [0.021 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:19:52.167
    Jan 23 14:19:52.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename conformance-tests 01/23/24 14:19:52.168
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:19:52.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:19:52.177
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/23/24 14:19:52.179
    Jan 23 14:19:52.179: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:19:52.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-9811" for this suite. 01/23/24 14:19:52.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:19:52.188
Jan 23 14:19:52.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename daemonsets 01/23/24 14:19:52.189
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:19:52.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:19:52.197
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
Jan 23 14:19:52.208: INFO: Create a RollingUpdate DaemonSet
Jan 23 14:19:52.211: INFO: Check that daemon pods launch on every node of the cluster
Jan 23 14:19:52.212: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:19:52.214: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 14:19:52.214: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:19:53.217: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:19:53.219: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 14:19:53.219: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:19:54.217: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:19:54.219: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 23 14:19:54.219: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Jan 23 14:19:54.219: INFO: Update the DaemonSet to trigger a rollout
Jan 23 14:19:54.226: INFO: Updating DaemonSet daemon-set
Jan 23 14:19:59.236: INFO: Roll back the DaemonSet before rollout is complete
Jan 23 14:19:59.241: INFO: Updating DaemonSet daemon-set
Jan 23 14:19:59.241: INFO: Make sure DaemonSet rollback is complete
Jan 23 14:19:59.243: INFO: Wrong image for pod: daemon-set-n98xz. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Jan 23 14:19:59.243: INFO: Pod daemon-set-n98xz is not available
Jan 23 14:19:59.246: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:20:00.251: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:20:01.251: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:20:02.251: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:20:03.250: INFO: Pod daemon-set-4hjxz is not available
Jan 23 14:20:03.253: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 01/23/24 14:20:03.262
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2217, will wait for the garbage collector to delete the pods 01/23/24 14:20:03.262
Jan 23 14:20:03.318: INFO: Deleting DaemonSet.extensions daemon-set took: 4.070803ms
Jan 23 14:20:03.419: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.956811ms
Jan 23 14:20:06.722: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 14:20:06.722: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 23 14:20:06.726: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"137158"},"items":null}

Jan 23 14:20:06.728: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"137158"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:20:06.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2217" for this suite. 01/23/24 14:20:06.736
------------------------------
• [SLOW TEST] [14.550 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:19:52.188
    Jan 23 14:19:52.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename daemonsets 01/23/24 14:19:52.189
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:19:52.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:19:52.197
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:443
    Jan 23 14:19:52.208: INFO: Create a RollingUpdate DaemonSet
    Jan 23 14:19:52.211: INFO: Check that daemon pods launch on every node of the cluster
    Jan 23 14:19:52.212: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:19:52.214: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 14:19:52.214: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:19:53.217: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:19:53.219: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 14:19:53.219: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:19:54.217: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:19:54.219: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 23 14:19:54.219: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Jan 23 14:19:54.219: INFO: Update the DaemonSet to trigger a rollout
    Jan 23 14:19:54.226: INFO: Updating DaemonSet daemon-set
    Jan 23 14:19:59.236: INFO: Roll back the DaemonSet before rollout is complete
    Jan 23 14:19:59.241: INFO: Updating DaemonSet daemon-set
    Jan 23 14:19:59.241: INFO: Make sure DaemonSet rollback is complete
    Jan 23 14:19:59.243: INFO: Wrong image for pod: daemon-set-n98xz. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Jan 23 14:19:59.243: INFO: Pod daemon-set-n98xz is not available
    Jan 23 14:19:59.246: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:20:00.251: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:20:01.251: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:20:02.251: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:20:03.250: INFO: Pod daemon-set-4hjxz is not available
    Jan 23 14:20:03.253: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 01/23/24 14:20:03.262
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2217, will wait for the garbage collector to delete the pods 01/23/24 14:20:03.262
    Jan 23 14:20:03.318: INFO: Deleting DaemonSet.extensions daemon-set took: 4.070803ms
    Jan 23 14:20:03.419: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.956811ms
    Jan 23 14:20:06.722: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 14:20:06.722: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 23 14:20:06.726: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"137158"},"items":null}

    Jan 23 14:20:06.728: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"137158"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:20:06.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2217" for this suite. 01/23/24 14:20:06.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:20:06.74
Jan 23 14:20:06.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename lease-test 01/23/24 14:20:06.74
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:06.749
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:06.751
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Jan 23 14:20:06.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-3296" for this suite. 01/23/24 14:20:06.791
------------------------------
• [0.054 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:20:06.74
    Jan 23 14:20:06.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename lease-test 01/23/24 14:20:06.74
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:06.749
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:06.751
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:20:06.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-3296" for this suite. 01/23/24 14:20:06.791
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:20:06.794
Jan 23 14:20:06.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubectl 01/23/24 14:20:06.795
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:06.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:06.803
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 01/23/24 14:20:06.804
Jan 23 14:20:06.804: INFO: namespace kubectl-8122
Jan 23 14:20:06.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-8122 create -f -'
Jan 23 14:20:07.146: INFO: stderr: ""
Jan 23 14:20:07.146: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/23/24 14:20:07.146
Jan 23 14:20:08.150: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 14:20:08.150: INFO: Found 0 / 1
Jan 23 14:20:09.150: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 14:20:09.150: INFO: Found 1 / 1
Jan 23 14:20:09.150: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 23 14:20:09.152: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 14:20:09.152: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 23 14:20:09.152: INFO: wait on agnhost-primary startup in kubectl-8122 
Jan 23 14:20:09.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-8122 logs agnhost-primary-w7cgj agnhost-primary'
Jan 23 14:20:09.291: INFO: stderr: ""
Jan 23 14:20:09.291: INFO: stdout: "Paused\n"
STEP: exposing RC 01/23/24 14:20:09.291
Jan 23 14:20:09.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-8122 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 23 14:20:09.447: INFO: stderr: ""
Jan 23 14:20:09.447: INFO: stdout: "service/rm2 exposed\n"
Jan 23 14:20:09.450: INFO: Service rm2 in namespace kubectl-8122 found.
STEP: exposing service 01/23/24 14:20:11.453
Jan 23 14:20:11.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-8122 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 23 14:20:11.602: INFO: stderr: ""
Jan 23 14:20:11.602: INFO: stdout: "service/rm3 exposed\n"
Jan 23 14:20:11.604: INFO: Service rm3 in namespace kubectl-8122 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 23 14:20:13.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8122" for this suite. 01/23/24 14:20:13.611
------------------------------
• [SLOW TEST] [6.820 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:20:06.794
    Jan 23 14:20:06.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubectl 01/23/24 14:20:06.795
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:06.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:06.803
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 01/23/24 14:20:06.804
    Jan 23 14:20:06.804: INFO: namespace kubectl-8122
    Jan 23 14:20:06.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-8122 create -f -'
    Jan 23 14:20:07.146: INFO: stderr: ""
    Jan 23 14:20:07.146: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/23/24 14:20:07.146
    Jan 23 14:20:08.150: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 23 14:20:08.150: INFO: Found 0 / 1
    Jan 23 14:20:09.150: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 23 14:20:09.150: INFO: Found 1 / 1
    Jan 23 14:20:09.150: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 23 14:20:09.152: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 23 14:20:09.152: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 23 14:20:09.152: INFO: wait on agnhost-primary startup in kubectl-8122 
    Jan 23 14:20:09.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-8122 logs agnhost-primary-w7cgj agnhost-primary'
    Jan 23 14:20:09.291: INFO: stderr: ""
    Jan 23 14:20:09.291: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/23/24 14:20:09.291
    Jan 23 14:20:09.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-8122 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan 23 14:20:09.447: INFO: stderr: ""
    Jan 23 14:20:09.447: INFO: stdout: "service/rm2 exposed\n"
    Jan 23 14:20:09.450: INFO: Service rm2 in namespace kubectl-8122 found.
    STEP: exposing service 01/23/24 14:20:11.453
    Jan 23 14:20:11.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-8122 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan 23 14:20:11.602: INFO: stderr: ""
    Jan 23 14:20:11.602: INFO: stdout: "service/rm3 exposed\n"
    Jan 23 14:20:11.604: INFO: Service rm3 in namespace kubectl-8122 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:20:13.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8122" for this suite. 01/23/24 14:20:13.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:20:13.614
Jan 23 14:20:13.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename security-context 01/23/24 14:20:13.615
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:13.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:13.624
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/23/24 14:20:13.626
Jan 23 14:20:13.638: INFO: Waiting up to 5m0s for pod "security-context-22f36445-e923-4260-8270-153aacde3965" in namespace "security-context-2394" to be "Succeeded or Failed"
Jan 23 14:20:13.639: INFO: Pod "security-context-22f36445-e923-4260-8270-153aacde3965": Phase="Pending", Reason="", readiness=false. Elapsed: 1.430173ms
Jan 23 14:20:15.642: INFO: Pod "security-context-22f36445-e923-4260-8270-153aacde3965": Phase="Running", Reason="", readiness=false. Elapsed: 2.004383778s
Jan 23 14:20:17.641: INFO: Pod "security-context-22f36445-e923-4260-8270-153aacde3965": Phase="Running", Reason="", readiness=false. Elapsed: 4.003512138s
Jan 23 14:20:19.643: INFO: Pod "security-context-22f36445-e923-4260-8270-153aacde3965": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005826441s
STEP: Saw pod success 01/23/24 14:20:19.644
Jan 23 14:20:19.644: INFO: Pod "security-context-22f36445-e923-4260-8270-153aacde3965" satisfied condition "Succeeded or Failed"
Jan 23 14:20:19.646: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod security-context-22f36445-e923-4260-8270-153aacde3965 container test-container: <nil>
STEP: delete the pod 01/23/24 14:20:19.649
Jan 23 14:20:19.656: INFO: Waiting for pod security-context-22f36445-e923-4260-8270-153aacde3965 to disappear
Jan 23 14:20:19.658: INFO: Pod security-context-22f36445-e923-4260-8270-153aacde3965 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 23 14:20:19.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-2394" for this suite. 01/23/24 14:20:19.66
------------------------------
• [SLOW TEST] [6.049 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:20:13.614
    Jan 23 14:20:13.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename security-context 01/23/24 14:20:13.615
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:13.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:13.624
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/23/24 14:20:13.626
    Jan 23 14:20:13.638: INFO: Waiting up to 5m0s for pod "security-context-22f36445-e923-4260-8270-153aacde3965" in namespace "security-context-2394" to be "Succeeded or Failed"
    Jan 23 14:20:13.639: INFO: Pod "security-context-22f36445-e923-4260-8270-153aacde3965": Phase="Pending", Reason="", readiness=false. Elapsed: 1.430173ms
    Jan 23 14:20:15.642: INFO: Pod "security-context-22f36445-e923-4260-8270-153aacde3965": Phase="Running", Reason="", readiness=false. Elapsed: 2.004383778s
    Jan 23 14:20:17.641: INFO: Pod "security-context-22f36445-e923-4260-8270-153aacde3965": Phase="Running", Reason="", readiness=false. Elapsed: 4.003512138s
    Jan 23 14:20:19.643: INFO: Pod "security-context-22f36445-e923-4260-8270-153aacde3965": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005826441s
    STEP: Saw pod success 01/23/24 14:20:19.644
    Jan 23 14:20:19.644: INFO: Pod "security-context-22f36445-e923-4260-8270-153aacde3965" satisfied condition "Succeeded or Failed"
    Jan 23 14:20:19.646: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod security-context-22f36445-e923-4260-8270-153aacde3965 container test-container: <nil>
    STEP: delete the pod 01/23/24 14:20:19.649
    Jan 23 14:20:19.656: INFO: Waiting for pod security-context-22f36445-e923-4260-8270-153aacde3965 to disappear
    Jan 23 14:20:19.658: INFO: Pod security-context-22f36445-e923-4260-8270-153aacde3965 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:20:19.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-2394" for this suite. 01/23/24 14:20:19.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:20:19.664
Jan 23 14:20:19.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename services 01/23/24 14:20:19.665
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:19.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:19.675
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-932 01/23/24 14:20:19.676
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-932 to expose endpoints map[] 01/23/24 14:20:19.682
Jan 23 14:20:19.685: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 23 14:20:20.690: INFO: successfully validated that service multi-endpoint-test in namespace services-932 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-932 01/23/24 14:20:20.69
Jan 23 14:20:20.701: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-932" to be "running and ready"
Jan 23 14:20:20.703: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.8569ms
Jan 23 14:20:20.703: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:20:22.707: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005654318s
Jan 23 14:20:22.707: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 23 14:20:22.707: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-932 to expose endpoints map[pod1:[100]] 01/23/24 14:20:22.708
Jan 23 14:20:22.713: INFO: successfully validated that service multi-endpoint-test in namespace services-932 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-932 01/23/24 14:20:22.713
Jan 23 14:20:22.726: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-932" to be "running and ready"
Jan 23 14:20:22.727: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.689167ms
Jan 23 14:20:22.727: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:20:24.730: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004514445s
Jan 23 14:20:24.730: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 23 14:20:24.730: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-932 to expose endpoints map[pod1:[100] pod2:[101]] 01/23/24 14:20:24.731
Jan 23 14:20:24.737: INFO: successfully validated that service multi-endpoint-test in namespace services-932 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/23/24 14:20:24.737
Jan 23 14:20:24.737: INFO: Creating new exec pod
Jan 23 14:20:24.746: INFO: Waiting up to 5m0s for pod "execpod2qhdk" in namespace "services-932" to be "running"
Jan 23 14:20:24.748: INFO: Pod "execpod2qhdk": Phase="Pending", Reason="", readiness=false. Elapsed: 1.561031ms
Jan 23 14:20:26.751: INFO: Pod "execpod2qhdk": Phase="Running", Reason="", readiness=true. Elapsed: 2.004770031s
Jan 23 14:20:26.751: INFO: Pod "execpod2qhdk" satisfied condition "running"
Jan 23 14:20:27.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-932 exec execpod2qhdk -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Jan 23 14:20:27.943: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 23 14:20:27.943: INFO: stdout: ""
Jan 23 14:20:27.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-932 exec execpod2qhdk -- /bin/sh -x -c nc -v -z -w 2 10.233.50.70 80'
Jan 23 14:20:28.117: INFO: stderr: "+ nc -v -z -w 2 10.233.50.70 80\nConnection to 10.233.50.70 80 port [tcp/http] succeeded!\n"
Jan 23 14:20:28.117: INFO: stdout: ""
Jan 23 14:20:28.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-932 exec execpod2qhdk -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Jan 23 14:20:28.298: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 23 14:20:28.298: INFO: stdout: ""
Jan 23 14:20:28.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-932 exec execpod2qhdk -- /bin/sh -x -c nc -v -z -w 2 10.233.50.70 81'
Jan 23 14:20:28.475: INFO: stderr: "+ nc -v -z -w 2 10.233.50.70 81\nConnection to 10.233.50.70 81 port [tcp/*] succeeded!\n"
Jan 23 14:20:28.475: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-932 01/23/24 14:20:28.475
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-932 to expose endpoints map[pod2:[101]] 01/23/24 14:20:28.486
Jan 23 14:20:28.494: INFO: successfully validated that service multi-endpoint-test in namespace services-932 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-932 01/23/24 14:20:28.494
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-932 to expose endpoints map[] 01/23/24 14:20:28.501
Jan 23 14:20:28.507: INFO: successfully validated that service multi-endpoint-test in namespace services-932 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 23 14:20:28.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-932" for this suite. 01/23/24 14:20:28.518
------------------------------
• [SLOW TEST] [8.858 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:20:19.664
    Jan 23 14:20:19.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename services 01/23/24 14:20:19.665
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:19.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:19.675
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-932 01/23/24 14:20:19.676
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-932 to expose endpoints map[] 01/23/24 14:20:19.682
    Jan 23 14:20:19.685: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan 23 14:20:20.690: INFO: successfully validated that service multi-endpoint-test in namespace services-932 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-932 01/23/24 14:20:20.69
    Jan 23 14:20:20.701: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-932" to be "running and ready"
    Jan 23 14:20:20.703: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.8569ms
    Jan 23 14:20:20.703: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:20:22.707: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005654318s
    Jan 23 14:20:22.707: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 23 14:20:22.707: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-932 to expose endpoints map[pod1:[100]] 01/23/24 14:20:22.708
    Jan 23 14:20:22.713: INFO: successfully validated that service multi-endpoint-test in namespace services-932 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-932 01/23/24 14:20:22.713
    Jan 23 14:20:22.726: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-932" to be "running and ready"
    Jan 23 14:20:22.727: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.689167ms
    Jan 23 14:20:22.727: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:20:24.730: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004514445s
    Jan 23 14:20:24.730: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 23 14:20:24.730: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-932 to expose endpoints map[pod1:[100] pod2:[101]] 01/23/24 14:20:24.731
    Jan 23 14:20:24.737: INFO: successfully validated that service multi-endpoint-test in namespace services-932 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/23/24 14:20:24.737
    Jan 23 14:20:24.737: INFO: Creating new exec pod
    Jan 23 14:20:24.746: INFO: Waiting up to 5m0s for pod "execpod2qhdk" in namespace "services-932" to be "running"
    Jan 23 14:20:24.748: INFO: Pod "execpod2qhdk": Phase="Pending", Reason="", readiness=false. Elapsed: 1.561031ms
    Jan 23 14:20:26.751: INFO: Pod "execpod2qhdk": Phase="Running", Reason="", readiness=true. Elapsed: 2.004770031s
    Jan 23 14:20:26.751: INFO: Pod "execpod2qhdk" satisfied condition "running"
    Jan 23 14:20:27.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-932 exec execpod2qhdk -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Jan 23 14:20:27.943: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan 23 14:20:27.943: INFO: stdout: ""
    Jan 23 14:20:27.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-932 exec execpod2qhdk -- /bin/sh -x -c nc -v -z -w 2 10.233.50.70 80'
    Jan 23 14:20:28.117: INFO: stderr: "+ nc -v -z -w 2 10.233.50.70 80\nConnection to 10.233.50.70 80 port [tcp/http] succeeded!\n"
    Jan 23 14:20:28.117: INFO: stdout: ""
    Jan 23 14:20:28.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-932 exec execpod2qhdk -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Jan 23 14:20:28.298: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan 23 14:20:28.298: INFO: stdout: ""
    Jan 23 14:20:28.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-932 exec execpod2qhdk -- /bin/sh -x -c nc -v -z -w 2 10.233.50.70 81'
    Jan 23 14:20:28.475: INFO: stderr: "+ nc -v -z -w 2 10.233.50.70 81\nConnection to 10.233.50.70 81 port [tcp/*] succeeded!\n"
    Jan 23 14:20:28.475: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-932 01/23/24 14:20:28.475
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-932 to expose endpoints map[pod2:[101]] 01/23/24 14:20:28.486
    Jan 23 14:20:28.494: INFO: successfully validated that service multi-endpoint-test in namespace services-932 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-932 01/23/24 14:20:28.494
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-932 to expose endpoints map[] 01/23/24 14:20:28.501
    Jan 23 14:20:28.507: INFO: successfully validated that service multi-endpoint-test in namespace services-932 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:20:28.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-932" for this suite. 01/23/24 14:20:28.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:20:28.523
Jan 23 14:20:28.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 14:20:28.523
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:28.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:28.533
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-814a5838-c136-4141-b400-d1a6ad689de7 01/23/24 14:20:28.535
STEP: Creating secret with name secret-projected-all-test-volume-4bb50795-99dc-46ea-8e52-0d76137e6c77 01/23/24 14:20:28.546
STEP: Creating a pod to test Check all projections for projected volume plugin 01/23/24 14:20:28.55
Jan 23 14:20:28.565: INFO: Waiting up to 5m0s for pod "projected-volume-45d64506-0df4-4d69-9091-c286116f2ab0" in namespace "projected-515" to be "Succeeded or Failed"
Jan 23 14:20:28.567: INFO: Pod "projected-volume-45d64506-0df4-4d69-9091-c286116f2ab0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.118767ms
Jan 23 14:20:30.570: INFO: Pod "projected-volume-45d64506-0df4-4d69-9091-c286116f2ab0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004616769s
Jan 23 14:20:32.570: INFO: Pod "projected-volume-45d64506-0df4-4d69-9091-c286116f2ab0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005021343s
Jan 23 14:20:34.576: INFO: Pod "projected-volume-45d64506-0df4-4d69-9091-c286116f2ab0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010938187s
STEP: Saw pod success 01/23/24 14:20:34.576
Jan 23 14:20:34.576: INFO: Pod "projected-volume-45d64506-0df4-4d69-9091-c286116f2ab0" satisfied condition "Succeeded or Failed"
Jan 23 14:20:34.578: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod projected-volume-45d64506-0df4-4d69-9091-c286116f2ab0 container projected-all-volume-test: <nil>
STEP: delete the pod 01/23/24 14:20:34.581
Jan 23 14:20:34.589: INFO: Waiting for pod projected-volume-45d64506-0df4-4d69-9091-c286116f2ab0 to disappear
Jan 23 14:20:34.590: INFO: Pod projected-volume-45d64506-0df4-4d69-9091-c286116f2ab0 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Jan 23 14:20:34.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-515" for this suite. 01/23/24 14:20:34.592
------------------------------
• [SLOW TEST] [6.074 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:20:28.523
    Jan 23 14:20:28.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 14:20:28.523
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:28.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:28.533
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-814a5838-c136-4141-b400-d1a6ad689de7 01/23/24 14:20:28.535
    STEP: Creating secret with name secret-projected-all-test-volume-4bb50795-99dc-46ea-8e52-0d76137e6c77 01/23/24 14:20:28.546
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/23/24 14:20:28.55
    Jan 23 14:20:28.565: INFO: Waiting up to 5m0s for pod "projected-volume-45d64506-0df4-4d69-9091-c286116f2ab0" in namespace "projected-515" to be "Succeeded or Failed"
    Jan 23 14:20:28.567: INFO: Pod "projected-volume-45d64506-0df4-4d69-9091-c286116f2ab0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.118767ms
    Jan 23 14:20:30.570: INFO: Pod "projected-volume-45d64506-0df4-4d69-9091-c286116f2ab0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004616769s
    Jan 23 14:20:32.570: INFO: Pod "projected-volume-45d64506-0df4-4d69-9091-c286116f2ab0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005021343s
    Jan 23 14:20:34.576: INFO: Pod "projected-volume-45d64506-0df4-4d69-9091-c286116f2ab0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010938187s
    STEP: Saw pod success 01/23/24 14:20:34.576
    Jan 23 14:20:34.576: INFO: Pod "projected-volume-45d64506-0df4-4d69-9091-c286116f2ab0" satisfied condition "Succeeded or Failed"
    Jan 23 14:20:34.578: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod projected-volume-45d64506-0df4-4d69-9091-c286116f2ab0 container projected-all-volume-test: <nil>
    STEP: delete the pod 01/23/24 14:20:34.581
    Jan 23 14:20:34.589: INFO: Waiting for pod projected-volume-45d64506-0df4-4d69-9091-c286116f2ab0 to disappear
    Jan 23 14:20:34.590: INFO: Pod projected-volume-45d64506-0df4-4d69-9091-c286116f2ab0 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:20:34.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-515" for this suite. 01/23/24 14:20:34.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:20:34.597
Jan 23 14:20:34.597: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename daemonsets 01/23/24 14:20:34.598
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:34.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:34.607
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
STEP: Creating simple DaemonSet "daemon-set" 01/23/24 14:20:34.617
STEP: Check that daemon pods launch on every node of the cluster. 01/23/24 14:20:34.623
Jan 23 14:20:34.626: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:20:34.630: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 14:20:34.630: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:20:35.633: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:20:35.635: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 14:20:35.635: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:20:36.635: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:20:36.637: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 23 14:20:36.637: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 01/23/24 14:20:36.639
STEP: DeleteCollection of the DaemonSets 01/23/24 14:20:36.641
STEP: Verify that ReplicaSets have been deleted 01/23/24 14:20:36.645
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
Jan 23 14:20:36.658: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"137609"},"items":null}

Jan 23 14:20:36.661: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"137609"},"items":[{"metadata":{"name":"daemon-set-kjthp","generateName":"daemon-set-","namespace":"daemonsets-6873","uid":"db0bce47-6310-44df-a08a-be9dee519313","resourceVersion":"137604","creationTimestamp":"2024-01-23T14:20:34Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d80fc5bb2d3d3a2994514def7617f9a8294e6fcf2b84cc6f4760c4fe4895aabd","cni.projectcalico.org/podIP":"10.233.75.129/32","cni.projectcalico.org/podIPs":"10.233.75.129/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"cc906d55-f657-48d7-86d6-804f8f87deb1","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-01-23T14:20:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc906d55-f657-48d7-86d6-804f8f87deb1\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2024-01-23T14:20:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-01-23T14:20:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.129\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ltzmd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ltzmd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"node-worker-hohyvwot.nova-ht9xu6tk2ptb.local","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["node-worker-hohyvwot.nova-ht9xu6tk2ptb.local"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priorityClassName":"nova-user-critical","priority":1000000000,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-23T14:20:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-23T14:20:35Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-23T14:20:35Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-23T14:20:34Z"}],"hostIP":"172.31.11.40","podIP":"10.233.75.129","podIPs":[{"ip":"10.233.75.129"}],"startTime":"2024-01-23T14:20:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-01-23T14:20:35Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://8cc143015c481322fab3a0985833fdf4eae9b72a81a2c8985f33cd0f5fb27fad","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qzknc","generateName":"daemon-set-","namespace":"daemonsets-6873","uid":"911503bd-7a55-4e6e-b226-ca9cd6a58ed8","resourceVersion":"137602","creationTimestamp":"2024-01-23T14:20:34Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"0d2e7ad404c72075ae01145c2c7b67e6c60bcc5577bae9be49ef56371501455c","cni.projectcalico.org/podIP":"10.233.87.11/32","cni.projectcalico.org/podIPs":"10.233.87.11/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"cc906d55-f657-48d7-86d6-804f8f87deb1","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-01-23T14:20:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc906d55-f657-48d7-86d6-804f8f87deb1\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2024-01-23T14:20:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-01-23T14:20:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-h69rm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-h69rm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priorityClassName":"nova-user-critical","priority":1000000000,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-23T14:20:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-23T14:20:35Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-23T14:20:35Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-23T14:20:34Z"}],"hostIP":"172.31.11.67","podIP":"10.233.87.11","podIPs":[{"ip":"10.233.87.11"}],"startTime":"2024-01-23T14:20:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-01-23T14:20:35Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://1e230814c4347f56ff28c1c7522ce01df556190dd276f61aa5c2cca8b7e038a9","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:20:36.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6873" for this suite. 01/23/24 14:20:36.669
------------------------------
• [2.078 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:20:34.597
    Jan 23 14:20:34.597: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename daemonsets 01/23/24 14:20:34.598
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:34.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:34.607
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:834
    STEP: Creating simple DaemonSet "daemon-set" 01/23/24 14:20:34.617
    STEP: Check that daemon pods launch on every node of the cluster. 01/23/24 14:20:34.623
    Jan 23 14:20:34.626: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:20:34.630: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 14:20:34.630: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:20:35.633: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:20:35.635: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 14:20:35.635: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:20:36.635: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:20:36.637: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 23 14:20:36.637: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 01/23/24 14:20:36.639
    STEP: DeleteCollection of the DaemonSets 01/23/24 14:20:36.641
    STEP: Verify that ReplicaSets have been deleted 01/23/24 14:20:36.645
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    Jan 23 14:20:36.658: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"137609"},"items":null}

    Jan 23 14:20:36.661: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"137609"},"items":[{"metadata":{"name":"daemon-set-kjthp","generateName":"daemon-set-","namespace":"daemonsets-6873","uid":"db0bce47-6310-44df-a08a-be9dee519313","resourceVersion":"137604","creationTimestamp":"2024-01-23T14:20:34Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d80fc5bb2d3d3a2994514def7617f9a8294e6fcf2b84cc6f4760c4fe4895aabd","cni.projectcalico.org/podIP":"10.233.75.129/32","cni.projectcalico.org/podIPs":"10.233.75.129/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"cc906d55-f657-48d7-86d6-804f8f87deb1","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-01-23T14:20:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc906d55-f657-48d7-86d6-804f8f87deb1\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2024-01-23T14:20:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-01-23T14:20:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.129\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ltzmd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ltzmd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"node-worker-hohyvwot.nova-ht9xu6tk2ptb.local","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["node-worker-hohyvwot.nova-ht9xu6tk2ptb.local"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priorityClassName":"nova-user-critical","priority":1000000000,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-23T14:20:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-23T14:20:35Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-23T14:20:35Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-23T14:20:34Z"}],"hostIP":"172.31.11.40","podIP":"10.233.75.129","podIPs":[{"ip":"10.233.75.129"}],"startTime":"2024-01-23T14:20:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-01-23T14:20:35Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://8cc143015c481322fab3a0985833fdf4eae9b72a81a2c8985f33cd0f5fb27fad","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qzknc","generateName":"daemon-set-","namespace":"daemonsets-6873","uid":"911503bd-7a55-4e6e-b226-ca9cd6a58ed8","resourceVersion":"137602","creationTimestamp":"2024-01-23T14:20:34Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"0d2e7ad404c72075ae01145c2c7b67e6c60bcc5577bae9be49ef56371501455c","cni.projectcalico.org/podIP":"10.233.87.11/32","cni.projectcalico.org/podIPs":"10.233.87.11/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"cc906d55-f657-48d7-86d6-804f8f87deb1","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-01-23T14:20:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc906d55-f657-48d7-86d6-804f8f87deb1\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2024-01-23T14:20:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-01-23T14:20:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-h69rm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-h69rm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priorityClassName":"nova-user-critical","priority":1000000000,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-23T14:20:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-23T14:20:35Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-23T14:20:35Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-23T14:20:34Z"}],"hostIP":"172.31.11.67","podIP":"10.233.87.11","podIPs":[{"ip":"10.233.87.11"}],"startTime":"2024-01-23T14:20:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-01-23T14:20:35Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://1e230814c4347f56ff28c1c7522ce01df556190dd276f61aa5c2cca8b7e038a9","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:20:36.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6873" for this suite. 01/23/24 14:20:36.669
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:20:36.675
Jan 23 14:20:36.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename hostport 01/23/24 14:20:36.676
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:36.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:36.706
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/23/24 14:20:36.724
Jan 23 14:20:36.735: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8919" to be "running and ready"
Jan 23 14:20:36.737: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.135627ms
Jan 23 14:20:36.737: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:20:38.791: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.055253247s
Jan 23 14:20:38.791: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 23 14:20:38.791: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.11.67 on the node which pod1 resides and expect scheduled 01/23/24 14:20:38.791
Jan 23 14:20:38.822: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8919" to be "running and ready"
Jan 23 14:20:38.845: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.121219ms
Jan 23 14:20:38.845: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:20:40.848: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.025574749s
Jan 23 14:20:40.848: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 23 14:20:40.848: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.11.67 but use UDP protocol on the node which pod2 resides 01/23/24 14:20:40.848
Jan 23 14:20:40.860: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8919" to be "running and ready"
Jan 23 14:20:40.862: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.980054ms
Jan 23 14:20:40.862: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:20:42.866: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.005575821s
Jan 23 14:20:42.866: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan 23 14:20:42.866: INFO: Pod "pod3" satisfied condition "running and ready"
Jan 23 14:20:42.876: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8919" to be "running and ready"
Jan 23 14:20:42.877: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.721225ms
Jan 23 14:20:42.877: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:20:44.881: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.005419952s
Jan 23 14:20:44.881: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan 23 14:20:44.881: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/23/24 14:20:44.883
Jan 23 14:20:44.883: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.11.67 http://127.0.0.1:54323/hostname] Namespace:hostport-8919 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 14:20:44.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:20:44.883: INFO: ExecWithOptions: Clientset creation
Jan 23 14:20:44.883: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-8919/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.11.67+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.11.67, port: 54323 01/23/24 14:20:44.949
Jan 23 14:20:44.949: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.11.67:54323/hostname] Namespace:hostport-8919 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 14:20:44.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:20:44.950: INFO: ExecWithOptions: Clientset creation
Jan 23 14:20:44.950: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-8919/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.11.67%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.11.67, port: 54323 UDP 01/23/24 14:20:45.009
Jan 23 14:20:45.009: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.11.67 54323] Namespace:hostport-8919 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 14:20:45.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:20:45.010: INFO: ExecWithOptions: Clientset creation
Jan 23 14:20:45.010: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-8919/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.11.67+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Jan 23 14:20:50.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-8919" for this suite. 01/23/24 14:20:50.079
------------------------------
• [SLOW TEST] [13.407 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:20:36.675
    Jan 23 14:20:36.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename hostport 01/23/24 14:20:36.676
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:36.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:36.706
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/23/24 14:20:36.724
    Jan 23 14:20:36.735: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8919" to be "running and ready"
    Jan 23 14:20:36.737: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.135627ms
    Jan 23 14:20:36.737: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:20:38.791: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.055253247s
    Jan 23 14:20:38.791: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 23 14:20:38.791: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.11.67 on the node which pod1 resides and expect scheduled 01/23/24 14:20:38.791
    Jan 23 14:20:38.822: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8919" to be "running and ready"
    Jan 23 14:20:38.845: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.121219ms
    Jan 23 14:20:38.845: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:20:40.848: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.025574749s
    Jan 23 14:20:40.848: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 23 14:20:40.848: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.11.67 but use UDP protocol on the node which pod2 resides 01/23/24 14:20:40.848
    Jan 23 14:20:40.860: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8919" to be "running and ready"
    Jan 23 14:20:40.862: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.980054ms
    Jan 23 14:20:40.862: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:20:42.866: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.005575821s
    Jan 23 14:20:42.866: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan 23 14:20:42.866: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan 23 14:20:42.876: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8919" to be "running and ready"
    Jan 23 14:20:42.877: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.721225ms
    Jan 23 14:20:42.877: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:20:44.881: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.005419952s
    Jan 23 14:20:44.881: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan 23 14:20:44.881: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/23/24 14:20:44.883
    Jan 23 14:20:44.883: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.11.67 http://127.0.0.1:54323/hostname] Namespace:hostport-8919 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 14:20:44.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:20:44.883: INFO: ExecWithOptions: Clientset creation
    Jan 23 14:20:44.883: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-8919/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.11.67+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.11.67, port: 54323 01/23/24 14:20:44.949
    Jan 23 14:20:44.949: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.11.67:54323/hostname] Namespace:hostport-8919 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 14:20:44.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:20:44.950: INFO: ExecWithOptions: Clientset creation
    Jan 23 14:20:44.950: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-8919/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.11.67%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.11.67, port: 54323 UDP 01/23/24 14:20:45.009
    Jan 23 14:20:45.009: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.11.67 54323] Namespace:hostport-8919 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 14:20:45.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:20:45.010: INFO: ExecWithOptions: Clientset creation
    Jan 23 14:20:45.010: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-8919/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.11.67+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:20:50.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-8919" for this suite. 01/23/24 14:20:50.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:20:50.083
Jan 23 14:20:50.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename downward-api 01/23/24 14:20:50.084
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:50.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:50.094
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 01/23/24 14:20:50.096
Jan 23 14:20:50.108: INFO: Waiting up to 5m0s for pod "downwardapi-volume-472859f3-94cd-4166-a3e1-fd9224cdf361" in namespace "downward-api-3083" to be "Succeeded or Failed"
Jan 23 14:20:50.110: INFO: Pod "downwardapi-volume-472859f3-94cd-4166-a3e1-fd9224cdf361": Phase="Pending", Reason="", readiness=false. Elapsed: 2.542259ms
Jan 23 14:20:52.113: INFO: Pod "downwardapi-volume-472859f3-94cd-4166-a3e1-fd9224cdf361": Phase="Running", Reason="", readiness=true. Elapsed: 2.005420197s
Jan 23 14:20:54.115: INFO: Pod "downwardapi-volume-472859f3-94cd-4166-a3e1-fd9224cdf361": Phase="Running", Reason="", readiness=false. Elapsed: 4.006859094s
Jan 23 14:20:56.114: INFO: Pod "downwardapi-volume-472859f3-94cd-4166-a3e1-fd9224cdf361": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005993495s
STEP: Saw pod success 01/23/24 14:20:56.114
Jan 23 14:20:56.114: INFO: Pod "downwardapi-volume-472859f3-94cd-4166-a3e1-fd9224cdf361" satisfied condition "Succeeded or Failed"
Jan 23 14:20:56.116: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-472859f3-94cd-4166-a3e1-fd9224cdf361 container client-container: <nil>
STEP: delete the pod 01/23/24 14:20:56.122
Jan 23 14:20:56.130: INFO: Waiting for pod downwardapi-volume-472859f3-94cd-4166-a3e1-fd9224cdf361 to disappear
Jan 23 14:20:56.131: INFO: Pod downwardapi-volume-472859f3-94cd-4166-a3e1-fd9224cdf361 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 23 14:20:56.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3083" for this suite. 01/23/24 14:20:56.134
------------------------------
• [SLOW TEST] [6.054 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:20:50.083
    Jan 23 14:20:50.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename downward-api 01/23/24 14:20:50.084
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:50.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:50.094
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 01/23/24 14:20:50.096
    Jan 23 14:20:50.108: INFO: Waiting up to 5m0s for pod "downwardapi-volume-472859f3-94cd-4166-a3e1-fd9224cdf361" in namespace "downward-api-3083" to be "Succeeded or Failed"
    Jan 23 14:20:50.110: INFO: Pod "downwardapi-volume-472859f3-94cd-4166-a3e1-fd9224cdf361": Phase="Pending", Reason="", readiness=false. Elapsed: 2.542259ms
    Jan 23 14:20:52.113: INFO: Pod "downwardapi-volume-472859f3-94cd-4166-a3e1-fd9224cdf361": Phase="Running", Reason="", readiness=true. Elapsed: 2.005420197s
    Jan 23 14:20:54.115: INFO: Pod "downwardapi-volume-472859f3-94cd-4166-a3e1-fd9224cdf361": Phase="Running", Reason="", readiness=false. Elapsed: 4.006859094s
    Jan 23 14:20:56.114: INFO: Pod "downwardapi-volume-472859f3-94cd-4166-a3e1-fd9224cdf361": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005993495s
    STEP: Saw pod success 01/23/24 14:20:56.114
    Jan 23 14:20:56.114: INFO: Pod "downwardapi-volume-472859f3-94cd-4166-a3e1-fd9224cdf361" satisfied condition "Succeeded or Failed"
    Jan 23 14:20:56.116: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-472859f3-94cd-4166-a3e1-fd9224cdf361 container client-container: <nil>
    STEP: delete the pod 01/23/24 14:20:56.122
    Jan 23 14:20:56.130: INFO: Waiting for pod downwardapi-volume-472859f3-94cd-4166-a3e1-fd9224cdf361 to disappear
    Jan 23 14:20:56.131: INFO: Pod downwardapi-volume-472859f3-94cd-4166-a3e1-fd9224cdf361 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:20:56.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3083" for this suite. 01/23/24 14:20:56.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:20:56.138
Jan 23 14:20:56.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename replication-controller 01/23/24 14:20:56.139
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:56.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:56.147
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 01/23/24 14:20:56.15
Jan 23 14:20:56.163: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-1582" to be "running and ready"
Jan 23 14:20:56.165: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.168507ms
Jan 23 14:20:56.165: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:20:58.168: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.004923252s
Jan 23 14:20:58.168: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan 23 14:20:58.168: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/23/24 14:20:58.169
STEP: Then the orphan pod is adopted 01/23/24 14:20:58.175
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 23 14:20:59.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1582" for this suite. 01/23/24 14:20:59.181
------------------------------
• [3.046 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:20:56.138
    Jan 23 14:20:56.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename replication-controller 01/23/24 14:20:56.139
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:56.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:56.147
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/23/24 14:20:56.15
    Jan 23 14:20:56.163: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-1582" to be "running and ready"
    Jan 23 14:20:56.165: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.168507ms
    Jan 23 14:20:56.165: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:20:58.168: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.004923252s
    Jan 23 14:20:58.168: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan 23 14:20:58.168: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/23/24 14:20:58.169
    STEP: Then the orphan pod is adopted 01/23/24 14:20:58.175
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:20:59.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1582" for this suite. 01/23/24 14:20:59.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:20:59.184
Jan 23 14:20:59.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename svcaccounts 01/23/24 14:20:59.185
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:59.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:59.193
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  01/23/24 14:20:59.195
Jan 23 14:20:59.210: INFO: Waiting up to 5m0s for pod "test-pod-c234fcd0-9d73-4754-b14e-b4a1666431dd" in namespace "svcaccounts-1678" to be "Succeeded or Failed"
Jan 23 14:20:59.213: INFO: Pod "test-pod-c234fcd0-9d73-4754-b14e-b4a1666431dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.422021ms
Jan 23 14:21:01.215: INFO: Pod "test-pod-c234fcd0-9d73-4754-b14e-b4a1666431dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004764348s
Jan 23 14:21:03.216: INFO: Pod "test-pod-c234fcd0-9d73-4754-b14e-b4a1666431dd": Phase="Running", Reason="", readiness=false. Elapsed: 4.005748533s
Jan 23 14:21:05.215: INFO: Pod "test-pod-c234fcd0-9d73-4754-b14e-b4a1666431dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004934222s
STEP: Saw pod success 01/23/24 14:21:05.215
Jan 23 14:21:05.215: INFO: Pod "test-pod-c234fcd0-9d73-4754-b14e-b4a1666431dd" satisfied condition "Succeeded or Failed"
Jan 23 14:21:05.217: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod test-pod-c234fcd0-9d73-4754-b14e-b4a1666431dd container agnhost-container: <nil>
STEP: delete the pod 01/23/24 14:21:05.22
Jan 23 14:21:05.227: INFO: Waiting for pod test-pod-c234fcd0-9d73-4754-b14e-b4a1666431dd to disappear
Jan 23 14:21:05.228: INFO: Pod test-pod-c234fcd0-9d73-4754-b14e-b4a1666431dd no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 23 14:21:05.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1678" for this suite. 01/23/24 14:21:05.231
------------------------------
• [SLOW TEST] [6.053 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:20:59.184
    Jan 23 14:20:59.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename svcaccounts 01/23/24 14:20:59.185
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:20:59.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:20:59.193
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  01/23/24 14:20:59.195
    Jan 23 14:20:59.210: INFO: Waiting up to 5m0s for pod "test-pod-c234fcd0-9d73-4754-b14e-b4a1666431dd" in namespace "svcaccounts-1678" to be "Succeeded or Failed"
    Jan 23 14:20:59.213: INFO: Pod "test-pod-c234fcd0-9d73-4754-b14e-b4a1666431dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.422021ms
    Jan 23 14:21:01.215: INFO: Pod "test-pod-c234fcd0-9d73-4754-b14e-b4a1666431dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004764348s
    Jan 23 14:21:03.216: INFO: Pod "test-pod-c234fcd0-9d73-4754-b14e-b4a1666431dd": Phase="Running", Reason="", readiness=false. Elapsed: 4.005748533s
    Jan 23 14:21:05.215: INFO: Pod "test-pod-c234fcd0-9d73-4754-b14e-b4a1666431dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004934222s
    STEP: Saw pod success 01/23/24 14:21:05.215
    Jan 23 14:21:05.215: INFO: Pod "test-pod-c234fcd0-9d73-4754-b14e-b4a1666431dd" satisfied condition "Succeeded or Failed"
    Jan 23 14:21:05.217: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod test-pod-c234fcd0-9d73-4754-b14e-b4a1666431dd container agnhost-container: <nil>
    STEP: delete the pod 01/23/24 14:21:05.22
    Jan 23 14:21:05.227: INFO: Waiting for pod test-pod-c234fcd0-9d73-4754-b14e-b4a1666431dd to disappear
    Jan 23 14:21:05.228: INFO: Pod test-pod-c234fcd0-9d73-4754-b14e-b4a1666431dd no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:21:05.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1678" for this suite. 01/23/24 14:21:05.231
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:21:05.237
Jan 23 14:21:05.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename deployment 01/23/24 14:21:05.238
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:21:05.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:21:05.248
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan 23 14:21:05.258: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 23 14:21:10.261: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/23/24 14:21:10.261
Jan 23 14:21:10.261: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 23 14:21:12.265: INFO: Creating deployment "test-rollover-deployment"
Jan 23 14:21:12.270: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 23 14:21:14.274: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 23 14:21:14.278: INFO: Ensure that both replica sets have 1 created replica
Jan 23 14:21:14.281: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 23 14:21:14.286: INFO: Updating deployment test-rollover-deployment
Jan 23 14:21:14.286: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 23 14:21:16.291: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 23 14:21:16.294: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 23 14:21:16.297: INFO: all replica sets need to contain the pod-template-hash label
Jan 23 14:21:16.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 14:21:18.303: INFO: all replica sets need to contain the pod-template-hash label
Jan 23 14:21:18.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 14:21:20.302: INFO: all replica sets need to contain the pod-template-hash label
Jan 23 14:21:20.302: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 14:21:22.302: INFO: all replica sets need to contain the pod-template-hash label
Jan 23 14:21:22.302: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 14:21:24.303: INFO: all replica sets need to contain the pod-template-hash label
Jan 23 14:21:24.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 14:21:26.303: INFO: 
Jan 23 14:21:26.303: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 23 14:21:26.308: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5339  ea8ce98d-38e6-4a04-b3ce-ac05c798ac24 138180 2 2024-01-23 14:21:12 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-23 14:21:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c3739b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-23 14:21:12 +0000 UTC,LastTransitionTime:2024-01-23 14:21:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2024-01-23 14:21:25 +0000 UTC,LastTransitionTime:2024-01-23 14:21:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 23 14:21:26.310: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-5339  c17df7dc-bb30-4e8c-a891-a00a8e08d539 138169 2 2024-01-23 14:21:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment ea8ce98d-38e6-4a04-b3ce-ac05c798ac24 0xc002a92187 0xc002a92188}] [] [{kube-controller-manager Update apps/v1 2024-01-23 14:21:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea8ce98d-38e6-4a04-b3ce-ac05c798ac24\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:21:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a92238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 23 14:21:26.310: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 23 14:21:26.310: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5339  51676b2c-4597-462c-ac6e-886073ec1be4 138179 2 2024-01-23 14:21:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment ea8ce98d-38e6-4a04-b3ce-ac05c798ac24 0xc000ef9607 0xc000ef9608}] [] [{e2e.test Update apps/v1 2024-01-23 14:21:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea8ce98d-38e6-4a04-b3ce-ac05c798ac24\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:21:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002a92118 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 23 14:21:26.310: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-5339  1182d2fa-8355-4165-9545-3df87f809acc 138096 2 2024-01-23 14:21:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment ea8ce98d-38e6-4a04-b3ce-ac05c798ac24 0xc002a922a7 0xc002a922a8}] [] [{kube-controller-manager Update apps/v1 2024-01-23 14:21:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea8ce98d-38e6-4a04-b3ce-ac05c798ac24\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:21:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a925a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 23 14:21:26.312: INFO: Pod "test-rollover-deployment-6c6df9974f-n6hs7" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-n6hs7 test-rollover-deployment-6c6df9974f- deployment-5339  8ab3c5de-18fa-4f52-80bf-81534cc42d6c 138119 0 2024-01-23 14:21:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:d4a64d5b65669307ac773bf61b3982e48cbadea00941d993217502fd9ab0a4fb cni.projectcalico.org/podIP:10.233.87.14/32 cni.projectcalico.org/podIPs:10.233.87.14/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f c17df7dc-bb30-4e8c-a891-a00a8e08d539 0xc00bb749e7 0xc00bb749e8}] [] [{calico Update v1 2024-01-23 14:21:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-23 14:21:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c17df7dc-bb30-4e8c-a891-a00a8e08d539\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 14:21:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wcd7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wcd7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:21:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:21:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:21:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:21:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.14,StartTime:2024-01-23 14:21:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 14:21:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://4e9939826f1a68382b0afcb7a116ddd8d13dee773aae97a5b6797a6d7ae05c4e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 23 14:21:26.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5339" for this suite. 01/23/24 14:21:26.315
------------------------------
• [SLOW TEST] [21.082 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:21:05.237
    Jan 23 14:21:05.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename deployment 01/23/24 14:21:05.238
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:21:05.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:21:05.248
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan 23 14:21:05.258: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan 23 14:21:10.261: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/23/24 14:21:10.261
    Jan 23 14:21:10.261: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan 23 14:21:12.265: INFO: Creating deployment "test-rollover-deployment"
    Jan 23 14:21:12.270: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan 23 14:21:14.274: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan 23 14:21:14.278: INFO: Ensure that both replica sets have 1 created replica
    Jan 23 14:21:14.281: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan 23 14:21:14.286: INFO: Updating deployment test-rollover-deployment
    Jan 23 14:21:14.286: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan 23 14:21:16.291: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan 23 14:21:16.294: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan 23 14:21:16.297: INFO: all replica sets need to contain the pod-template-hash label
    Jan 23 14:21:16.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 14:21:18.303: INFO: all replica sets need to contain the pod-template-hash label
    Jan 23 14:21:18.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 14:21:20.302: INFO: all replica sets need to contain the pod-template-hash label
    Jan 23 14:21:20.302: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 14:21:22.302: INFO: all replica sets need to contain the pod-template-hash label
    Jan 23 14:21:22.302: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 14:21:24.303: INFO: all replica sets need to contain the pod-template-hash label
    Jan 23 14:21:24.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 23, 14, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 23, 14, 21, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 23 14:21:26.303: INFO: 
    Jan 23 14:21:26.303: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 23 14:21:26.308: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-5339  ea8ce98d-38e6-4a04-b3ce-ac05c798ac24 138180 2 2024-01-23 14:21:12 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-23 14:21:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c3739b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-23 14:21:12 +0000 UTC,LastTransitionTime:2024-01-23 14:21:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2024-01-23 14:21:25 +0000 UTC,LastTransitionTime:2024-01-23 14:21:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 23 14:21:26.310: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-5339  c17df7dc-bb30-4e8c-a891-a00a8e08d539 138169 2 2024-01-23 14:21:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment ea8ce98d-38e6-4a04-b3ce-ac05c798ac24 0xc002a92187 0xc002a92188}] [] [{kube-controller-manager Update apps/v1 2024-01-23 14:21:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea8ce98d-38e6-4a04-b3ce-ac05c798ac24\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:21:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a92238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 23 14:21:26.310: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan 23 14:21:26.310: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5339  51676b2c-4597-462c-ac6e-886073ec1be4 138179 2 2024-01-23 14:21:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment ea8ce98d-38e6-4a04-b3ce-ac05c798ac24 0xc000ef9607 0xc000ef9608}] [] [{e2e.test Update apps/v1 2024-01-23 14:21:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea8ce98d-38e6-4a04-b3ce-ac05c798ac24\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:21:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002a92118 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 23 14:21:26.310: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-5339  1182d2fa-8355-4165-9545-3df87f809acc 138096 2 2024-01-23 14:21:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment ea8ce98d-38e6-4a04-b3ce-ac05c798ac24 0xc002a922a7 0xc002a922a8}] [] [{kube-controller-manager Update apps/v1 2024-01-23 14:21:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea8ce98d-38e6-4a04-b3ce-ac05c798ac24\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:21:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a925a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 23 14:21:26.312: INFO: Pod "test-rollover-deployment-6c6df9974f-n6hs7" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-n6hs7 test-rollover-deployment-6c6df9974f- deployment-5339  8ab3c5de-18fa-4f52-80bf-81534cc42d6c 138119 0 2024-01-23 14:21:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:d4a64d5b65669307ac773bf61b3982e48cbadea00941d993217502fd9ab0a4fb cni.projectcalico.org/podIP:10.233.87.14/32 cni.projectcalico.org/podIPs:10.233.87.14/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f c17df7dc-bb30-4e8c-a891-a00a8e08d539 0xc00bb749e7 0xc00bb749e8}] [] [{calico Update v1 2024-01-23 14:21:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-23 14:21:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c17df7dc-bb30-4e8c-a891-a00a8e08d539\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 14:21:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wcd7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wcd7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:21:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:21:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:21:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:21:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.14,StartTime:2024-01-23 14:21:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 14:21:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://4e9939826f1a68382b0afcb7a116ddd8d13dee773aae97a5b6797a6d7ae05c4e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:21:26.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5339" for this suite. 01/23/24 14:21:26.315
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:21:26.32
Jan 23 14:21:26.320: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubelet-test 01/23/24 14:21:26.32
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:21:26.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:21:26.33
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/23/24 14:21:26.345
Jan 23 14:21:26.345: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesa77df502-a4e2-47b5-aa18-5a3bf09d5e05" in namespace "kubelet-test-4707" to be "completed"
Jan 23 14:21:26.346: INFO: Pod "agnhost-host-aliasesa77df502-a4e2-47b5-aa18-5a3bf09d5e05": Phase="Pending", Reason="", readiness=false. Elapsed: 1.341779ms
Jan 23 14:21:28.349: INFO: Pod "agnhost-host-aliasesa77df502-a4e2-47b5-aa18-5a3bf09d5e05": Phase="Running", Reason="", readiness=true. Elapsed: 2.004192193s
Jan 23 14:21:30.351: INFO: Pod "agnhost-host-aliasesa77df502-a4e2-47b5-aa18-5a3bf09d5e05": Phase="Running", Reason="", readiness=false. Elapsed: 4.006183281s
Jan 23 14:21:32.350: INFO: Pod "agnhost-host-aliasesa77df502-a4e2-47b5-aa18-5a3bf09d5e05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005011162s
Jan 23 14:21:32.350: INFO: Pod "agnhost-host-aliasesa77df502-a4e2-47b5-aa18-5a3bf09d5e05" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 23 14:21:32.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4707" for this suite. 01/23/24 14:21:32.356
------------------------------
• [SLOW TEST] [6.040 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:21:26.32
    Jan 23 14:21:26.320: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubelet-test 01/23/24 14:21:26.32
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:21:26.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:21:26.33
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/23/24 14:21:26.345
    Jan 23 14:21:26.345: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesa77df502-a4e2-47b5-aa18-5a3bf09d5e05" in namespace "kubelet-test-4707" to be "completed"
    Jan 23 14:21:26.346: INFO: Pod "agnhost-host-aliasesa77df502-a4e2-47b5-aa18-5a3bf09d5e05": Phase="Pending", Reason="", readiness=false. Elapsed: 1.341779ms
    Jan 23 14:21:28.349: INFO: Pod "agnhost-host-aliasesa77df502-a4e2-47b5-aa18-5a3bf09d5e05": Phase="Running", Reason="", readiness=true. Elapsed: 2.004192193s
    Jan 23 14:21:30.351: INFO: Pod "agnhost-host-aliasesa77df502-a4e2-47b5-aa18-5a3bf09d5e05": Phase="Running", Reason="", readiness=false. Elapsed: 4.006183281s
    Jan 23 14:21:32.350: INFO: Pod "agnhost-host-aliasesa77df502-a4e2-47b5-aa18-5a3bf09d5e05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005011162s
    Jan 23 14:21:32.350: INFO: Pod "agnhost-host-aliasesa77df502-a4e2-47b5-aa18-5a3bf09d5e05" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:21:32.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4707" for this suite. 01/23/24 14:21:32.356
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:21:32.36
Jan 23 14:21:32.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename job 01/23/24 14:21:32.36
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:21:32.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:21:32.369
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 01/23/24 14:21:32.371
STEP: Ensuring active pods == parallelism 01/23/24 14:21:32.375
STEP: Orphaning one of the Job's Pods 01/23/24 14:21:34.378
Jan 23 14:21:34.888: INFO: Successfully updated pod "adopt-release-jrj94"
STEP: Checking that the Job readopts the Pod 01/23/24 14:21:34.888
Jan 23 14:21:34.888: INFO: Waiting up to 15m0s for pod "adopt-release-jrj94" in namespace "job-7768" to be "adopted"
Jan 23 14:21:34.890: INFO: Pod "adopt-release-jrj94": Phase="Running", Reason="", readiness=true. Elapsed: 2.506566ms
Jan 23 14:21:36.894: INFO: Pod "adopt-release-jrj94": Phase="Running", Reason="", readiness=true. Elapsed: 2.006093781s
Jan 23 14:21:36.894: INFO: Pod "adopt-release-jrj94" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/23/24 14:21:36.894
Jan 23 14:21:37.402: INFO: Successfully updated pod "adopt-release-jrj94"
STEP: Checking that the Job releases the Pod 01/23/24 14:21:37.402
Jan 23 14:21:37.402: INFO: Waiting up to 15m0s for pod "adopt-release-jrj94" in namespace "job-7768" to be "released"
Jan 23 14:21:37.403: INFO: Pod "adopt-release-jrj94": Phase="Running", Reason="", readiness=true. Elapsed: 1.449196ms
Jan 23 14:21:39.407: INFO: Pod "adopt-release-jrj94": Phase="Running", Reason="", readiness=true. Elapsed: 2.005161995s
Jan 23 14:21:39.407: INFO: Pod "adopt-release-jrj94" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 23 14:21:39.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7768" for this suite. 01/23/24 14:21:39.409
------------------------------
• [SLOW TEST] [7.053 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:21:32.36
    Jan 23 14:21:32.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename job 01/23/24 14:21:32.36
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:21:32.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:21:32.369
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 01/23/24 14:21:32.371
    STEP: Ensuring active pods == parallelism 01/23/24 14:21:32.375
    STEP: Orphaning one of the Job's Pods 01/23/24 14:21:34.378
    Jan 23 14:21:34.888: INFO: Successfully updated pod "adopt-release-jrj94"
    STEP: Checking that the Job readopts the Pod 01/23/24 14:21:34.888
    Jan 23 14:21:34.888: INFO: Waiting up to 15m0s for pod "adopt-release-jrj94" in namespace "job-7768" to be "adopted"
    Jan 23 14:21:34.890: INFO: Pod "adopt-release-jrj94": Phase="Running", Reason="", readiness=true. Elapsed: 2.506566ms
    Jan 23 14:21:36.894: INFO: Pod "adopt-release-jrj94": Phase="Running", Reason="", readiness=true. Elapsed: 2.006093781s
    Jan 23 14:21:36.894: INFO: Pod "adopt-release-jrj94" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/23/24 14:21:36.894
    Jan 23 14:21:37.402: INFO: Successfully updated pod "adopt-release-jrj94"
    STEP: Checking that the Job releases the Pod 01/23/24 14:21:37.402
    Jan 23 14:21:37.402: INFO: Waiting up to 15m0s for pod "adopt-release-jrj94" in namespace "job-7768" to be "released"
    Jan 23 14:21:37.403: INFO: Pod "adopt-release-jrj94": Phase="Running", Reason="", readiness=true. Elapsed: 1.449196ms
    Jan 23 14:21:39.407: INFO: Pod "adopt-release-jrj94": Phase="Running", Reason="", readiness=true. Elapsed: 2.005161995s
    Jan 23 14:21:39.407: INFO: Pod "adopt-release-jrj94" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:21:39.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7768" for this suite. 01/23/24 14:21:39.409
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:21:39.413
Jan 23 14:21:39.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename prestop 01/23/24 14:21:39.414
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:21:39.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:21:39.423
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-1529 01/23/24 14:21:39.425
STEP: Waiting for pods to come up. 01/23/24 14:21:39.435
Jan 23 14:21:39.436: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1529" to be "running"
Jan 23 14:21:39.438: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.496087ms
Jan 23 14:21:41.441: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.004731416s
Jan 23 14:21:41.441: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-1529 01/23/24 14:21:41.443
Jan 23 14:21:41.453: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1529" to be "running"
Jan 23 14:21:41.454: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.510853ms
Jan 23 14:21:43.458: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.005266015s
Jan 23 14:21:43.458: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/23/24 14:21:43.458
Jan 23 14:21:48.466: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/23/24 14:21:48.466
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Jan 23 14:21:48.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-1529" for this suite. 01/23/24 14:21:48.478
------------------------------
• [SLOW TEST] [9.068 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:21:39.413
    Jan 23 14:21:39.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename prestop 01/23/24 14:21:39.414
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:21:39.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:21:39.423
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-1529 01/23/24 14:21:39.425
    STEP: Waiting for pods to come up. 01/23/24 14:21:39.435
    Jan 23 14:21:39.436: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1529" to be "running"
    Jan 23 14:21:39.438: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.496087ms
    Jan 23 14:21:41.441: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.004731416s
    Jan 23 14:21:41.441: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-1529 01/23/24 14:21:41.443
    Jan 23 14:21:41.453: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1529" to be "running"
    Jan 23 14:21:41.454: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.510853ms
    Jan 23 14:21:43.458: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.005266015s
    Jan 23 14:21:43.458: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/23/24 14:21:43.458
    Jan 23 14:21:48.466: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/23/24 14:21:48.466
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:21:48.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-1529" for this suite. 01/23/24 14:21:48.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:21:48.481
Jan 23 14:21:48.481: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename var-expansion 01/23/24 14:21:48.482
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:21:48.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:21:48.49
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 01/23/24 14:21:48.492
Jan 23 14:21:48.503: INFO: Waiting up to 5m0s for pod "var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53" in namespace "var-expansion-6582" to be "Succeeded or Failed"
Jan 23 14:21:48.505: INFO: Pod "var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53": Phase="Pending", Reason="", readiness=false. Elapsed: 1.675048ms
Jan 23 14:21:50.508: INFO: Pod "var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004876111s
Jan 23 14:21:52.509: INFO: Pod "var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53": Phase="Running", Reason="", readiness=true. Elapsed: 4.005334326s
Jan 23 14:21:54.508: INFO: Pod "var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53": Phase="Running", Reason="", readiness=false. Elapsed: 6.004836877s
Jan 23 14:21:56.508: INFO: Pod "var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.004773363s
STEP: Saw pod success 01/23/24 14:21:56.508
Jan 23 14:21:56.508: INFO: Pod "var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53" satisfied condition "Succeeded or Failed"
Jan 23 14:21:56.510: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53 container dapi-container: <nil>
STEP: delete the pod 01/23/24 14:21:56.513
Jan 23 14:21:56.521: INFO: Waiting for pod var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53 to disappear
Jan 23 14:21:56.522: INFO: Pod var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 23 14:21:56.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6582" for this suite. 01/23/24 14:21:56.524
------------------------------
• [SLOW TEST] [8.046 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:21:48.481
    Jan 23 14:21:48.481: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename var-expansion 01/23/24 14:21:48.482
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:21:48.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:21:48.49
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 01/23/24 14:21:48.492
    Jan 23 14:21:48.503: INFO: Waiting up to 5m0s for pod "var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53" in namespace "var-expansion-6582" to be "Succeeded or Failed"
    Jan 23 14:21:48.505: INFO: Pod "var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53": Phase="Pending", Reason="", readiness=false. Elapsed: 1.675048ms
    Jan 23 14:21:50.508: INFO: Pod "var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004876111s
    Jan 23 14:21:52.509: INFO: Pod "var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53": Phase="Running", Reason="", readiness=true. Elapsed: 4.005334326s
    Jan 23 14:21:54.508: INFO: Pod "var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53": Phase="Running", Reason="", readiness=false. Elapsed: 6.004836877s
    Jan 23 14:21:56.508: INFO: Pod "var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.004773363s
    STEP: Saw pod success 01/23/24 14:21:56.508
    Jan 23 14:21:56.508: INFO: Pod "var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53" satisfied condition "Succeeded or Failed"
    Jan 23 14:21:56.510: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53 container dapi-container: <nil>
    STEP: delete the pod 01/23/24 14:21:56.513
    Jan 23 14:21:56.521: INFO: Waiting for pod var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53 to disappear
    Jan 23 14:21:56.522: INFO: Pod var-expansion-1ce3a5c7-57be-4ee5-a406-793121edfd53 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:21:56.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6582" for this suite. 01/23/24 14:21:56.524
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:21:56.527
Jan 23 14:21:56.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 14:21:56.528
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:21:56.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:21:56.537
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-b93ebaca-447f-491b-9ada-8ecbdd5c8d56 01/23/24 14:21:56.539
STEP: Creating a pod to test consume secrets 01/23/24 14:21:56.542
Jan 23 14:21:56.555: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e8bc6560-4b61-4470-8ab5-98eadd50cf99" in namespace "projected-8337" to be "Succeeded or Failed"
Jan 23 14:21:56.557: INFO: Pod "pod-projected-secrets-e8bc6560-4b61-4470-8ab5-98eadd50cf99": Phase="Pending", Reason="", readiness=false. Elapsed: 1.477672ms
Jan 23 14:21:58.559: INFO: Pod "pod-projected-secrets-e8bc6560-4b61-4470-8ab5-98eadd50cf99": Phase="Running", Reason="", readiness=true. Elapsed: 2.003739328s
Jan 23 14:22:00.559: INFO: Pod "pod-projected-secrets-e8bc6560-4b61-4470-8ab5-98eadd50cf99": Phase="Running", Reason="", readiness=false. Elapsed: 4.004215494s
Jan 23 14:22:02.561: INFO: Pod "pod-projected-secrets-e8bc6560-4b61-4470-8ab5-98eadd50cf99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005465372s
STEP: Saw pod success 01/23/24 14:22:02.561
Jan 23 14:22:02.561: INFO: Pod "pod-projected-secrets-e8bc6560-4b61-4470-8ab5-98eadd50cf99" satisfied condition "Succeeded or Failed"
Jan 23 14:22:02.562: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-secrets-e8bc6560-4b61-4470-8ab5-98eadd50cf99 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/23/24 14:22:02.566
Jan 23 14:22:02.573: INFO: Waiting for pod pod-projected-secrets-e8bc6560-4b61-4470-8ab5-98eadd50cf99 to disappear
Jan 23 14:22:02.575: INFO: Pod pod-projected-secrets-e8bc6560-4b61-4470-8ab5-98eadd50cf99 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 23 14:22:02.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8337" for this suite. 01/23/24 14:22:02.577
------------------------------
• [SLOW TEST] [6.054 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:21:56.527
    Jan 23 14:21:56.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 14:21:56.528
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:21:56.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:21:56.537
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-b93ebaca-447f-491b-9ada-8ecbdd5c8d56 01/23/24 14:21:56.539
    STEP: Creating a pod to test consume secrets 01/23/24 14:21:56.542
    Jan 23 14:21:56.555: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e8bc6560-4b61-4470-8ab5-98eadd50cf99" in namespace "projected-8337" to be "Succeeded or Failed"
    Jan 23 14:21:56.557: INFO: Pod "pod-projected-secrets-e8bc6560-4b61-4470-8ab5-98eadd50cf99": Phase="Pending", Reason="", readiness=false. Elapsed: 1.477672ms
    Jan 23 14:21:58.559: INFO: Pod "pod-projected-secrets-e8bc6560-4b61-4470-8ab5-98eadd50cf99": Phase="Running", Reason="", readiness=true. Elapsed: 2.003739328s
    Jan 23 14:22:00.559: INFO: Pod "pod-projected-secrets-e8bc6560-4b61-4470-8ab5-98eadd50cf99": Phase="Running", Reason="", readiness=false. Elapsed: 4.004215494s
    Jan 23 14:22:02.561: INFO: Pod "pod-projected-secrets-e8bc6560-4b61-4470-8ab5-98eadd50cf99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005465372s
    STEP: Saw pod success 01/23/24 14:22:02.561
    Jan 23 14:22:02.561: INFO: Pod "pod-projected-secrets-e8bc6560-4b61-4470-8ab5-98eadd50cf99" satisfied condition "Succeeded or Failed"
    Jan 23 14:22:02.562: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-secrets-e8bc6560-4b61-4470-8ab5-98eadd50cf99 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/23/24 14:22:02.566
    Jan 23 14:22:02.573: INFO: Waiting for pod pod-projected-secrets-e8bc6560-4b61-4470-8ab5-98eadd50cf99 to disappear
    Jan 23 14:22:02.575: INFO: Pod pod-projected-secrets-e8bc6560-4b61-4470-8ab5-98eadd50cf99 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:22:02.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8337" for this suite. 01/23/24 14:22:02.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:22:02.581
Jan 23 14:22:02.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename replication-controller 01/23/24 14:22:02.582
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:22:02.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:22:02.592
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-xt2dk" 01/23/24 14:22:02.595
Jan 23 14:22:02.599: INFO: Get Replication Controller "e2e-rc-xt2dk" to confirm replicas
Jan 23 14:22:03.600: INFO: Get Replication Controller "e2e-rc-xt2dk" to confirm replicas
Jan 23 14:22:03.603: INFO: Found 1 replicas for "e2e-rc-xt2dk" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-xt2dk" 01/23/24 14:22:03.603
STEP: Updating a scale subresource 01/23/24 14:22:03.604
STEP: Verifying replicas where modified for replication controller "e2e-rc-xt2dk" 01/23/24 14:22:03.608
Jan 23 14:22:03.608: INFO: Get Replication Controller "e2e-rc-xt2dk" to confirm replicas
Jan 23 14:22:04.610: INFO: Get Replication Controller "e2e-rc-xt2dk" to confirm replicas
Jan 23 14:22:04.613: INFO: Found 2 replicas for "e2e-rc-xt2dk" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 23 14:22:04.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2683" for this suite. 01/23/24 14:22:04.615
------------------------------
• [2.038 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:22:02.581
    Jan 23 14:22:02.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename replication-controller 01/23/24 14:22:02.582
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:22:02.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:22:02.592
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-xt2dk" 01/23/24 14:22:02.595
    Jan 23 14:22:02.599: INFO: Get Replication Controller "e2e-rc-xt2dk" to confirm replicas
    Jan 23 14:22:03.600: INFO: Get Replication Controller "e2e-rc-xt2dk" to confirm replicas
    Jan 23 14:22:03.603: INFO: Found 1 replicas for "e2e-rc-xt2dk" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-xt2dk" 01/23/24 14:22:03.603
    STEP: Updating a scale subresource 01/23/24 14:22:03.604
    STEP: Verifying replicas where modified for replication controller "e2e-rc-xt2dk" 01/23/24 14:22:03.608
    Jan 23 14:22:03.608: INFO: Get Replication Controller "e2e-rc-xt2dk" to confirm replicas
    Jan 23 14:22:04.610: INFO: Get Replication Controller "e2e-rc-xt2dk" to confirm replicas
    Jan 23 14:22:04.613: INFO: Found 2 replicas for "e2e-rc-xt2dk" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:22:04.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2683" for this suite. 01/23/24 14:22:04.615
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:22:04.62
Jan 23 14:22:04.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename namespaces 01/23/24 14:22:04.62
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:22:04.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:22:04.629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-v94qn" 01/23/24 14:22:04.631
Jan 23 14:22:04.640: INFO: Namespace "e2e-ns-v94qn-4682" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-v94qn-4682" 01/23/24 14:22:04.64
Jan 23 14:22:04.645: INFO: Namespace "e2e-ns-v94qn-4682" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-v94qn-4682" 01/23/24 14:22:04.645
Jan 23 14:22:04.650: INFO: Namespace "e2e-ns-v94qn-4682" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:22:04.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4365" for this suite. 01/23/24 14:22:04.652
STEP: Destroying namespace "e2e-ns-v94qn-4682" for this suite. 01/23/24 14:22:04.656
------------------------------
• [0.041 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:22:04.62
    Jan 23 14:22:04.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename namespaces 01/23/24 14:22:04.62
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:22:04.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:22:04.629
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-v94qn" 01/23/24 14:22:04.631
    Jan 23 14:22:04.640: INFO: Namespace "e2e-ns-v94qn-4682" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-v94qn-4682" 01/23/24 14:22:04.64
    Jan 23 14:22:04.645: INFO: Namespace "e2e-ns-v94qn-4682" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-v94qn-4682" 01/23/24 14:22:04.645
    Jan 23 14:22:04.650: INFO: Namespace "e2e-ns-v94qn-4682" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:22:04.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4365" for this suite. 01/23/24 14:22:04.652
    STEP: Destroying namespace "e2e-ns-v94qn-4682" for this suite. 01/23/24 14:22:04.656
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:22:04.661
Jan 23 14:22:04.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename runtimeclass 01/23/24 14:22:04.661
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:22:04.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:22:04.669
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/23/24 14:22:04.672
STEP: getting /apis/node.k8s.io 01/23/24 14:22:04.674
STEP: getting /apis/node.k8s.io/v1 01/23/24 14:22:04.674
STEP: creating 01/23/24 14:22:04.675
STEP: watching 01/23/24 14:22:04.681
Jan 23 14:22:04.682: INFO: starting watch
STEP: getting 01/23/24 14:22:04.685
STEP: listing 01/23/24 14:22:04.687
STEP: patching 01/23/24 14:22:04.688
STEP: updating 01/23/24 14:22:04.691
Jan 23 14:22:04.693: INFO: waiting for watch events with expected annotations
STEP: deleting 01/23/24 14:22:04.693
STEP: deleting a collection 01/23/24 14:22:04.698
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 23 14:22:04.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8361" for this suite. 01/23/24 14:22:04.708
------------------------------
• [0.051 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:22:04.661
    Jan 23 14:22:04.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename runtimeclass 01/23/24 14:22:04.661
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:22:04.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:22:04.669
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/23/24 14:22:04.672
    STEP: getting /apis/node.k8s.io 01/23/24 14:22:04.674
    STEP: getting /apis/node.k8s.io/v1 01/23/24 14:22:04.674
    STEP: creating 01/23/24 14:22:04.675
    STEP: watching 01/23/24 14:22:04.681
    Jan 23 14:22:04.682: INFO: starting watch
    STEP: getting 01/23/24 14:22:04.685
    STEP: listing 01/23/24 14:22:04.687
    STEP: patching 01/23/24 14:22:04.688
    STEP: updating 01/23/24 14:22:04.691
    Jan 23 14:22:04.693: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/23/24 14:22:04.693
    STEP: deleting a collection 01/23/24 14:22:04.698
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:22:04.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8361" for this suite. 01/23/24 14:22:04.708
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:22:04.712
Jan 23 14:22:04.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename limitrange 01/23/24 14:22:04.712
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:22:04.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:22:04.72
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-whtqf" in namespace "limitrange-2997" 01/23/24 14:22:04.721
STEP: Creating another limitRange in another namespace 01/23/24 14:22:04.727
Jan 23 14:22:04.734: INFO: Namespace "e2e-limitrange-whtqf-7242" created
Jan 23 14:22:04.734: INFO: Creating LimitRange "e2e-limitrange-whtqf" in namespace "e2e-limitrange-whtqf-7242"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-whtqf" 01/23/24 14:22:04.737
Jan 23 14:22:04.740: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-whtqf" in "limitrange-2997" namespace 01/23/24 14:22:04.74
Jan 23 14:22:04.745: INFO: LimitRange "e2e-limitrange-whtqf" has been patched
STEP: Delete LimitRange "e2e-limitrange-whtqf" by Collection with labelSelector: "e2e-limitrange-whtqf=patched" 01/23/24 14:22:04.745
STEP: Confirm that the limitRange "e2e-limitrange-whtqf" has been deleted 01/23/24 14:22:04.748
Jan 23 14:22:04.749: INFO: Requesting list of LimitRange to confirm quantity
Jan 23 14:22:04.750: INFO: Found 0 LimitRange with label "e2e-limitrange-whtqf=patched"
Jan 23 14:22:04.750: INFO: LimitRange "e2e-limitrange-whtqf" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-whtqf" 01/23/24 14:22:04.75
Jan 23 14:22:04.752: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan 23 14:22:04.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-2997" for this suite. 01/23/24 14:22:04.754
STEP: Destroying namespace "e2e-limitrange-whtqf-7242" for this suite. 01/23/24 14:22:04.756
------------------------------
• [0.073 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:22:04.712
    Jan 23 14:22:04.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename limitrange 01/23/24 14:22:04.712
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:22:04.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:22:04.72
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-whtqf" in namespace "limitrange-2997" 01/23/24 14:22:04.721
    STEP: Creating another limitRange in another namespace 01/23/24 14:22:04.727
    Jan 23 14:22:04.734: INFO: Namespace "e2e-limitrange-whtqf-7242" created
    Jan 23 14:22:04.734: INFO: Creating LimitRange "e2e-limitrange-whtqf" in namespace "e2e-limitrange-whtqf-7242"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-whtqf" 01/23/24 14:22:04.737
    Jan 23 14:22:04.740: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-whtqf" in "limitrange-2997" namespace 01/23/24 14:22:04.74
    Jan 23 14:22:04.745: INFO: LimitRange "e2e-limitrange-whtqf" has been patched
    STEP: Delete LimitRange "e2e-limitrange-whtqf" by Collection with labelSelector: "e2e-limitrange-whtqf=patched" 01/23/24 14:22:04.745
    STEP: Confirm that the limitRange "e2e-limitrange-whtqf" has been deleted 01/23/24 14:22:04.748
    Jan 23 14:22:04.749: INFO: Requesting list of LimitRange to confirm quantity
    Jan 23 14:22:04.750: INFO: Found 0 LimitRange with label "e2e-limitrange-whtqf=patched"
    Jan 23 14:22:04.750: INFO: LimitRange "e2e-limitrange-whtqf" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-whtqf" 01/23/24 14:22:04.75
    Jan 23 14:22:04.752: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:22:04.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-2997" for this suite. 01/23/24 14:22:04.754
    STEP: Destroying namespace "e2e-limitrange-whtqf-7242" for this suite. 01/23/24 14:22:04.756
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:22:04.785
Jan 23 14:22:04.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename ingressclass 01/23/24 14:22:04.786
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:22:04.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:22:04.794
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/23/24 14:22:04.796
STEP: getting /apis/networking.k8s.io 01/23/24 14:22:04.797
STEP: getting /apis/networking.k8s.iov1 01/23/24 14:22:04.798
STEP: creating 01/23/24 14:22:04.799
STEP: getting 01/23/24 14:22:04.816
STEP: listing 01/23/24 14:22:04.818
STEP: watching 01/23/24 14:22:04.821
Jan 23 14:22:04.821: INFO: starting watch
STEP: patching 01/23/24 14:22:04.821
STEP: updating 01/23/24 14:22:04.825
Jan 23 14:22:04.828: INFO: waiting for watch events with expected annotations
Jan 23 14:22:04.828: INFO: saw patched and updated annotations
STEP: deleting 01/23/24 14:22:04.828
STEP: deleting a collection 01/23/24 14:22:04.84
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Jan 23 14:22:04.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-7257" for this suite. 01/23/24 14:22:04.849
------------------------------
• [0.067 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:22:04.785
    Jan 23 14:22:04.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename ingressclass 01/23/24 14:22:04.786
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:22:04.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:22:04.794
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/23/24 14:22:04.796
    STEP: getting /apis/networking.k8s.io 01/23/24 14:22:04.797
    STEP: getting /apis/networking.k8s.iov1 01/23/24 14:22:04.798
    STEP: creating 01/23/24 14:22:04.799
    STEP: getting 01/23/24 14:22:04.816
    STEP: listing 01/23/24 14:22:04.818
    STEP: watching 01/23/24 14:22:04.821
    Jan 23 14:22:04.821: INFO: starting watch
    STEP: patching 01/23/24 14:22:04.821
    STEP: updating 01/23/24 14:22:04.825
    Jan 23 14:22:04.828: INFO: waiting for watch events with expected annotations
    Jan 23 14:22:04.828: INFO: saw patched and updated annotations
    STEP: deleting 01/23/24 14:22:04.828
    STEP: deleting a collection 01/23/24 14:22:04.84
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:22:04.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-7257" for this suite. 01/23/24 14:22:04.849
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:22:04.853
Jan 23 14:22:04.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename custom-resource-definition 01/23/24 14:22:04.853
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:22:04.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:22:04.863
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/23/24 14:22:04.865
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/23/24 14:22:04.865
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/23/24 14:22:04.866
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/23/24 14:22:04.866
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/23/24 14:22:04.866
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/23/24 14:22:04.866
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/23/24 14:22:04.867
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:22:04.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8678" for this suite. 01/23/24 14:22:04.869
------------------------------
• [0.019 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:22:04.853
    Jan 23 14:22:04.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename custom-resource-definition 01/23/24 14:22:04.853
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:22:04.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:22:04.863
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/23/24 14:22:04.865
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/23/24 14:22:04.865
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/23/24 14:22:04.866
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/23/24 14:22:04.866
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/23/24 14:22:04.866
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/23/24 14:22:04.866
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/23/24 14:22:04.867
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:22:04.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8678" for this suite. 01/23/24 14:22:04.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:22:04.872
Jan 23 14:22:04.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename crd-publish-openapi 01/23/24 14:22:04.873
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:22:04.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:22:04.883
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Jan 23 14:22:04.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/23/24 14:22:12.478
Jan 23 14:22:12.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 --namespace=crd-publish-openapi-5717 create -f -'
Jan 23 14:22:13.863: INFO: stderr: ""
Jan 23 14:22:13.863: INFO: stdout: "e2e-test-crd-publish-openapi-5683-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 23 14:22:13.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 --namespace=crd-publish-openapi-5717 delete e2e-test-crd-publish-openapi-5683-crds test-foo'
Jan 23 14:22:14.009: INFO: stderr: ""
Jan 23 14:22:14.009: INFO: stdout: "e2e-test-crd-publish-openapi-5683-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 23 14:22:14.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 --namespace=crd-publish-openapi-5717 apply -f -'
Jan 23 14:22:14.344: INFO: stderr: ""
Jan 23 14:22:14.344: INFO: stdout: "e2e-test-crd-publish-openapi-5683-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 23 14:22:14.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 --namespace=crd-publish-openapi-5717 delete e2e-test-crd-publish-openapi-5683-crds test-foo'
Jan 23 14:22:14.507: INFO: stderr: ""
Jan 23 14:22:14.507: INFO: stdout: "e2e-test-crd-publish-openapi-5683-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/23/24 14:22:14.507
Jan 23 14:22:14.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 --namespace=crd-publish-openapi-5717 create -f -'
Jan 23 14:22:14.847: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/23/24 14:22:14.847
Jan 23 14:22:14.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 --namespace=crd-publish-openapi-5717 create -f -'
Jan 23 14:22:15.196: INFO: rc: 1
Jan 23 14:22:15.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 --namespace=crd-publish-openapi-5717 apply -f -'
Jan 23 14:22:15.537: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/23/24 14:22:15.537
Jan 23 14:22:15.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 --namespace=crd-publish-openapi-5717 create -f -'
Jan 23 14:22:15.884: INFO: rc: 1
Jan 23 14:22:15.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 --namespace=crd-publish-openapi-5717 apply -f -'
Jan 23 14:22:16.255: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/23/24 14:22:16.255
Jan 23 14:22:16.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 explain e2e-test-crd-publish-openapi-5683-crds'
Jan 23 14:22:16.646: INFO: stderr: ""
Jan 23 14:22:16.646: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5683-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/23/24 14:22:16.646
Jan 23 14:22:16.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 explain e2e-test-crd-publish-openapi-5683-crds.metadata'
Jan 23 14:22:17.038: INFO: stderr: ""
Jan 23 14:22:17.038: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5683-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 23 14:22:17.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 explain e2e-test-crd-publish-openapi-5683-crds.spec'
Jan 23 14:22:17.418: INFO: stderr: ""
Jan 23 14:22:17.418: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5683-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 23 14:22:17.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 explain e2e-test-crd-publish-openapi-5683-crds.spec.bars'
Jan 23 14:22:17.811: INFO: stderr: ""
Jan 23 14:22:17.811: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5683-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/23/24 14:22:17.811
Jan 23 14:22:17.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 explain e2e-test-crd-publish-openapi-5683-crds.spec.bars2'
Jan 23 14:22:18.217: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:22:20.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5717" for this suite. 01/23/24 14:22:20.142
------------------------------
• [SLOW TEST] [15.273 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:22:04.872
    Jan 23 14:22:04.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename crd-publish-openapi 01/23/24 14:22:04.873
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:22:04.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:22:04.883
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Jan 23 14:22:04.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/23/24 14:22:12.478
    Jan 23 14:22:12.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 --namespace=crd-publish-openapi-5717 create -f -'
    Jan 23 14:22:13.863: INFO: stderr: ""
    Jan 23 14:22:13.863: INFO: stdout: "e2e-test-crd-publish-openapi-5683-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 23 14:22:13.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 --namespace=crd-publish-openapi-5717 delete e2e-test-crd-publish-openapi-5683-crds test-foo'
    Jan 23 14:22:14.009: INFO: stderr: ""
    Jan 23 14:22:14.009: INFO: stdout: "e2e-test-crd-publish-openapi-5683-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan 23 14:22:14.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 --namespace=crd-publish-openapi-5717 apply -f -'
    Jan 23 14:22:14.344: INFO: stderr: ""
    Jan 23 14:22:14.344: INFO: stdout: "e2e-test-crd-publish-openapi-5683-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 23 14:22:14.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 --namespace=crd-publish-openapi-5717 delete e2e-test-crd-publish-openapi-5683-crds test-foo'
    Jan 23 14:22:14.507: INFO: stderr: ""
    Jan 23 14:22:14.507: INFO: stdout: "e2e-test-crd-publish-openapi-5683-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/23/24 14:22:14.507
    Jan 23 14:22:14.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 --namespace=crd-publish-openapi-5717 create -f -'
    Jan 23 14:22:14.847: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/23/24 14:22:14.847
    Jan 23 14:22:14.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 --namespace=crd-publish-openapi-5717 create -f -'
    Jan 23 14:22:15.196: INFO: rc: 1
    Jan 23 14:22:15.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 --namespace=crd-publish-openapi-5717 apply -f -'
    Jan 23 14:22:15.537: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/23/24 14:22:15.537
    Jan 23 14:22:15.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 --namespace=crd-publish-openapi-5717 create -f -'
    Jan 23 14:22:15.884: INFO: rc: 1
    Jan 23 14:22:15.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 --namespace=crd-publish-openapi-5717 apply -f -'
    Jan 23 14:22:16.255: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/23/24 14:22:16.255
    Jan 23 14:22:16.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 explain e2e-test-crd-publish-openapi-5683-crds'
    Jan 23 14:22:16.646: INFO: stderr: ""
    Jan 23 14:22:16.646: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5683-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/23/24 14:22:16.646
    Jan 23 14:22:16.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 explain e2e-test-crd-publish-openapi-5683-crds.metadata'
    Jan 23 14:22:17.038: INFO: stderr: ""
    Jan 23 14:22:17.038: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5683-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan 23 14:22:17.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 explain e2e-test-crd-publish-openapi-5683-crds.spec'
    Jan 23 14:22:17.418: INFO: stderr: ""
    Jan 23 14:22:17.418: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5683-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan 23 14:22:17.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 explain e2e-test-crd-publish-openapi-5683-crds.spec.bars'
    Jan 23 14:22:17.811: INFO: stderr: ""
    Jan 23 14:22:17.811: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5683-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/23/24 14:22:17.811
    Jan 23 14:22:17.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5717 explain e2e-test-crd-publish-openapi-5683-crds.spec.bars2'
    Jan 23 14:22:18.217: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:22:20.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5717" for this suite. 01/23/24 14:22:20.142
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:22:20.146
Jan 23 14:22:20.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename emptydir-wrapper 01/23/24 14:22:20.147
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:22:20.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:22:20.157
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/23/24 14:22:20.159
STEP: Creating RC which spawns configmap-volume pods 01/23/24 14:22:20.4
Jan 23 14:22:20.500: INFO: Pod name wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780: Found 0 pods out of 5
Jan 23 14:22:25.505: INFO: Pod name wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/23/24 14:22:25.505
Jan 23 14:22:25.505: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-5zxt4" in namespace "emptydir-wrapper-7982" to be "running"
Jan 23 14:22:25.508: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-5zxt4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.206559ms
Jan 23 14:22:27.511: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-5zxt4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006009307s
Jan 23 14:22:29.511: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-5zxt4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006021623s
Jan 23 14:22:31.510: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-5zxt4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004458834s
Jan 23 14:22:33.510: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-5zxt4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004460744s
Jan 23 14:22:35.511: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-5zxt4": Phase="Running", Reason="", readiness=true. Elapsed: 10.006022563s
Jan 23 14:22:35.511: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-5zxt4" satisfied condition "running"
Jan 23 14:22:35.511: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-7q8q5" in namespace "emptydir-wrapper-7982" to be "running"
Jan 23 14:22:35.513: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-7q8q5": Phase="Running", Reason="", readiness=true. Elapsed: 1.849754ms
Jan 23 14:22:35.513: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-7q8q5" satisfied condition "running"
Jan 23 14:22:35.513: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-kkn8q" in namespace "emptydir-wrapper-7982" to be "running"
Jan 23 14:22:35.515: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-kkn8q": Phase="Running", Reason="", readiness=true. Elapsed: 1.461598ms
Jan 23 14:22:35.515: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-kkn8q" satisfied condition "running"
Jan 23 14:22:35.515: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-n2lj4" in namespace "emptydir-wrapper-7982" to be "running"
Jan 23 14:22:35.518: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-n2lj4": Phase="Running", Reason="", readiness=true. Elapsed: 2.738587ms
Jan 23 14:22:35.518: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-n2lj4" satisfied condition "running"
Jan 23 14:22:35.518: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-z4fxb" in namespace "emptydir-wrapper-7982" to be "running"
Jan 23 14:22:35.520: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-z4fxb": Phase="Running", Reason="", readiness=true. Elapsed: 2.450593ms
Jan 23 14:22:35.520: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-z4fxb" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780 in namespace emptydir-wrapper-7982, will wait for the garbage collector to delete the pods 01/23/24 14:22:35.52
Jan 23 14:22:35.597: INFO: Deleting ReplicationController wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780 took: 20.194137ms
Jan 23 14:22:35.798: INFO: Terminating ReplicationController wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780 pods took: 200.46786ms
STEP: Creating RC which spawns configmap-volume pods 01/23/24 14:22:41.401
Jan 23 14:22:41.412: INFO: Pod name wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0: Found 0 pods out of 5
Jan 23 14:22:46.418: INFO: Pod name wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/23/24 14:22:46.418
Jan 23 14:22:46.418: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-2llrp" in namespace "emptydir-wrapper-7982" to be "running"
Jan 23 14:22:46.420: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-2llrp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.3115ms
Jan 23 14:22:48.424: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-2llrp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005703841s
Jan 23 14:22:50.423: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-2llrp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005393919s
Jan 23 14:22:52.424: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-2llrp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005867177s
Jan 23 14:22:54.423: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-2llrp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005419624s
Jan 23 14:22:56.424: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-2llrp": Phase="Running", Reason="", readiness=true. Elapsed: 10.006193429s
Jan 23 14:22:56.424: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-2llrp" satisfied condition "running"
Jan 23 14:22:56.424: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-4gm86" in namespace "emptydir-wrapper-7982" to be "running"
Jan 23 14:22:56.426: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-4gm86": Phase="Running", Reason="", readiness=true. Elapsed: 1.788154ms
Jan 23 14:22:56.426: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-4gm86" satisfied condition "running"
Jan 23 14:22:56.426: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-d9dsg" in namespace "emptydir-wrapper-7982" to be "running"
Jan 23 14:22:56.428: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-d9dsg": Phase="Running", Reason="", readiness=true. Elapsed: 1.809757ms
Jan 23 14:22:56.428: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-d9dsg" satisfied condition "running"
Jan 23 14:22:56.428: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-h7d2k" in namespace "emptydir-wrapper-7982" to be "running"
Jan 23 14:22:56.430: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-h7d2k": Phase="Running", Reason="", readiness=true. Elapsed: 1.708911ms
Jan 23 14:22:56.430: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-h7d2k" satisfied condition "running"
Jan 23 14:22:56.430: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-jhh8x" in namespace "emptydir-wrapper-7982" to be "running"
Jan 23 14:22:56.431: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-jhh8x": Phase="Running", Reason="", readiness=true. Elapsed: 1.579064ms
Jan 23 14:22:56.431: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-jhh8x" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0 in namespace emptydir-wrapper-7982, will wait for the garbage collector to delete the pods 01/23/24 14:22:56.431
Jan 23 14:22:56.490: INFO: Deleting ReplicationController wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0 took: 7.034704ms
Jan 23 14:22:56.591: INFO: Terminating ReplicationController wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0 pods took: 100.358103ms
STEP: Creating RC which spawns configmap-volume pods 01/23/24 14:23:02.394
Jan 23 14:23:02.403: INFO: Pod name wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2: Found 0 pods out of 5
Jan 23 14:23:07.408: INFO: Pod name wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/23/24 14:23:07.408
Jan 23 14:23:07.408: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4l8dj" in namespace "emptydir-wrapper-7982" to be "running"
Jan 23 14:23:07.410: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4l8dj": Phase="Pending", Reason="", readiness=false. Elapsed: 1.780589ms
Jan 23 14:23:09.413: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4l8dj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005008978s
Jan 23 14:23:11.413: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4l8dj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005313651s
Jan 23 14:23:13.413: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4l8dj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005384208s
Jan 23 14:23:15.413: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4l8dj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004489059s
Jan 23 14:23:17.414: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4l8dj": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005583232s
Jan 23 14:23:19.412: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4l8dj": Phase="Running", Reason="", readiness=true. Elapsed: 12.004135039s
Jan 23 14:23:19.412: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4l8dj" satisfied condition "running"
Jan 23 14:23:19.412: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4pg9c" in namespace "emptydir-wrapper-7982" to be "running"
Jan 23 14:23:19.414: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4pg9c": Phase="Running", Reason="", readiness=true. Elapsed: 1.770569ms
Jan 23 14:23:19.414: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4pg9c" satisfied condition "running"
Jan 23 14:23:19.414: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-67b22" in namespace "emptydir-wrapper-7982" to be "running"
Jan 23 14:23:19.416: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-67b22": Phase="Running", Reason="", readiness=true. Elapsed: 1.699217ms
Jan 23 14:23:19.416: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-67b22" satisfied condition "running"
Jan 23 14:23:19.416: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-l6v7l" in namespace "emptydir-wrapper-7982" to be "running"
Jan 23 14:23:19.417: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-l6v7l": Phase="Running", Reason="", readiness=true. Elapsed: 1.702955ms
Jan 23 14:23:19.418: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-l6v7l" satisfied condition "running"
Jan 23 14:23:19.418: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-xfsxg" in namespace "emptydir-wrapper-7982" to be "running"
Jan 23 14:23:19.419: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-xfsxg": Phase="Running", Reason="", readiness=true. Elapsed: 1.482555ms
Jan 23 14:23:19.419: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-xfsxg" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2 in namespace emptydir-wrapper-7982, will wait for the garbage collector to delete the pods 01/23/24 14:23:19.419
Jan 23 14:23:19.483: INFO: Deleting ReplicationController wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2 took: 9.906765ms
Jan 23 14:23:19.583: INFO: Terminating ReplicationController wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2 pods took: 100.655938ms
STEP: Cleaning up the configMaps 01/23/24 14:23:28.284
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan 23 14:23:28.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-7982" for this suite. 01/23/24 14:23:28.405
------------------------------
• [SLOW TEST] [68.263 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:22:20.146
    Jan 23 14:22:20.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename emptydir-wrapper 01/23/24 14:22:20.147
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:22:20.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:22:20.157
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/23/24 14:22:20.159
    STEP: Creating RC which spawns configmap-volume pods 01/23/24 14:22:20.4
    Jan 23 14:22:20.500: INFO: Pod name wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780: Found 0 pods out of 5
    Jan 23 14:22:25.505: INFO: Pod name wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/23/24 14:22:25.505
    Jan 23 14:22:25.505: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-5zxt4" in namespace "emptydir-wrapper-7982" to be "running"
    Jan 23 14:22:25.508: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-5zxt4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.206559ms
    Jan 23 14:22:27.511: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-5zxt4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006009307s
    Jan 23 14:22:29.511: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-5zxt4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006021623s
    Jan 23 14:22:31.510: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-5zxt4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004458834s
    Jan 23 14:22:33.510: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-5zxt4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004460744s
    Jan 23 14:22:35.511: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-5zxt4": Phase="Running", Reason="", readiness=true. Elapsed: 10.006022563s
    Jan 23 14:22:35.511: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-5zxt4" satisfied condition "running"
    Jan 23 14:22:35.511: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-7q8q5" in namespace "emptydir-wrapper-7982" to be "running"
    Jan 23 14:22:35.513: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-7q8q5": Phase="Running", Reason="", readiness=true. Elapsed: 1.849754ms
    Jan 23 14:22:35.513: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-7q8q5" satisfied condition "running"
    Jan 23 14:22:35.513: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-kkn8q" in namespace "emptydir-wrapper-7982" to be "running"
    Jan 23 14:22:35.515: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-kkn8q": Phase="Running", Reason="", readiness=true. Elapsed: 1.461598ms
    Jan 23 14:22:35.515: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-kkn8q" satisfied condition "running"
    Jan 23 14:22:35.515: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-n2lj4" in namespace "emptydir-wrapper-7982" to be "running"
    Jan 23 14:22:35.518: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-n2lj4": Phase="Running", Reason="", readiness=true. Elapsed: 2.738587ms
    Jan 23 14:22:35.518: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-n2lj4" satisfied condition "running"
    Jan 23 14:22:35.518: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-z4fxb" in namespace "emptydir-wrapper-7982" to be "running"
    Jan 23 14:22:35.520: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-z4fxb": Phase="Running", Reason="", readiness=true. Elapsed: 2.450593ms
    Jan 23 14:22:35.520: INFO: Pod "wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780-z4fxb" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780 in namespace emptydir-wrapper-7982, will wait for the garbage collector to delete the pods 01/23/24 14:22:35.52
    Jan 23 14:22:35.597: INFO: Deleting ReplicationController wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780 took: 20.194137ms
    Jan 23 14:22:35.798: INFO: Terminating ReplicationController wrapped-volume-race-ce70e2be-810d-4d74-ab88-64e698ff1780 pods took: 200.46786ms
    STEP: Creating RC which spawns configmap-volume pods 01/23/24 14:22:41.401
    Jan 23 14:22:41.412: INFO: Pod name wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0: Found 0 pods out of 5
    Jan 23 14:22:46.418: INFO: Pod name wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/23/24 14:22:46.418
    Jan 23 14:22:46.418: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-2llrp" in namespace "emptydir-wrapper-7982" to be "running"
    Jan 23 14:22:46.420: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-2llrp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.3115ms
    Jan 23 14:22:48.424: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-2llrp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005703841s
    Jan 23 14:22:50.423: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-2llrp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005393919s
    Jan 23 14:22:52.424: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-2llrp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005867177s
    Jan 23 14:22:54.423: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-2llrp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005419624s
    Jan 23 14:22:56.424: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-2llrp": Phase="Running", Reason="", readiness=true. Elapsed: 10.006193429s
    Jan 23 14:22:56.424: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-2llrp" satisfied condition "running"
    Jan 23 14:22:56.424: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-4gm86" in namespace "emptydir-wrapper-7982" to be "running"
    Jan 23 14:22:56.426: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-4gm86": Phase="Running", Reason="", readiness=true. Elapsed: 1.788154ms
    Jan 23 14:22:56.426: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-4gm86" satisfied condition "running"
    Jan 23 14:22:56.426: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-d9dsg" in namespace "emptydir-wrapper-7982" to be "running"
    Jan 23 14:22:56.428: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-d9dsg": Phase="Running", Reason="", readiness=true. Elapsed: 1.809757ms
    Jan 23 14:22:56.428: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-d9dsg" satisfied condition "running"
    Jan 23 14:22:56.428: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-h7d2k" in namespace "emptydir-wrapper-7982" to be "running"
    Jan 23 14:22:56.430: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-h7d2k": Phase="Running", Reason="", readiness=true. Elapsed: 1.708911ms
    Jan 23 14:22:56.430: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-h7d2k" satisfied condition "running"
    Jan 23 14:22:56.430: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-jhh8x" in namespace "emptydir-wrapper-7982" to be "running"
    Jan 23 14:22:56.431: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-jhh8x": Phase="Running", Reason="", readiness=true. Elapsed: 1.579064ms
    Jan 23 14:22:56.431: INFO: Pod "wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0-jhh8x" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0 in namespace emptydir-wrapper-7982, will wait for the garbage collector to delete the pods 01/23/24 14:22:56.431
    Jan 23 14:22:56.490: INFO: Deleting ReplicationController wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0 took: 7.034704ms
    Jan 23 14:22:56.591: INFO: Terminating ReplicationController wrapped-volume-race-aa85c94c-e920-4d41-9a8f-9067bef87fc0 pods took: 100.358103ms
    STEP: Creating RC which spawns configmap-volume pods 01/23/24 14:23:02.394
    Jan 23 14:23:02.403: INFO: Pod name wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2: Found 0 pods out of 5
    Jan 23 14:23:07.408: INFO: Pod name wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/23/24 14:23:07.408
    Jan 23 14:23:07.408: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4l8dj" in namespace "emptydir-wrapper-7982" to be "running"
    Jan 23 14:23:07.410: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4l8dj": Phase="Pending", Reason="", readiness=false. Elapsed: 1.780589ms
    Jan 23 14:23:09.413: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4l8dj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005008978s
    Jan 23 14:23:11.413: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4l8dj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005313651s
    Jan 23 14:23:13.413: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4l8dj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005384208s
    Jan 23 14:23:15.413: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4l8dj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004489059s
    Jan 23 14:23:17.414: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4l8dj": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005583232s
    Jan 23 14:23:19.412: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4l8dj": Phase="Running", Reason="", readiness=true. Elapsed: 12.004135039s
    Jan 23 14:23:19.412: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4l8dj" satisfied condition "running"
    Jan 23 14:23:19.412: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4pg9c" in namespace "emptydir-wrapper-7982" to be "running"
    Jan 23 14:23:19.414: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4pg9c": Phase="Running", Reason="", readiness=true. Elapsed: 1.770569ms
    Jan 23 14:23:19.414: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-4pg9c" satisfied condition "running"
    Jan 23 14:23:19.414: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-67b22" in namespace "emptydir-wrapper-7982" to be "running"
    Jan 23 14:23:19.416: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-67b22": Phase="Running", Reason="", readiness=true. Elapsed: 1.699217ms
    Jan 23 14:23:19.416: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-67b22" satisfied condition "running"
    Jan 23 14:23:19.416: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-l6v7l" in namespace "emptydir-wrapper-7982" to be "running"
    Jan 23 14:23:19.417: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-l6v7l": Phase="Running", Reason="", readiness=true. Elapsed: 1.702955ms
    Jan 23 14:23:19.418: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-l6v7l" satisfied condition "running"
    Jan 23 14:23:19.418: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-xfsxg" in namespace "emptydir-wrapper-7982" to be "running"
    Jan 23 14:23:19.419: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-xfsxg": Phase="Running", Reason="", readiness=true. Elapsed: 1.482555ms
    Jan 23 14:23:19.419: INFO: Pod "wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2-xfsxg" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2 in namespace emptydir-wrapper-7982, will wait for the garbage collector to delete the pods 01/23/24 14:23:19.419
    Jan 23 14:23:19.483: INFO: Deleting ReplicationController wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2 took: 9.906765ms
    Jan 23 14:23:19.583: INFO: Terminating ReplicationController wrapped-volume-race-45db77e0-2c36-4de0-95b7-195919d58fa2 pods took: 100.655938ms
    STEP: Cleaning up the configMaps 01/23/24 14:23:28.284
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:23:28.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-7982" for this suite. 01/23/24 14:23:28.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:23:28.409
Jan 23 14:23:28.409: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename crd-webhook 01/23/24 14:23:28.41
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:23:28.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:23:28.419
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/23/24 14:23:28.421
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/23/24 14:23:28.618
STEP: Deploying the custom resource conversion webhook pod 01/23/24 14:23:28.623
STEP: Wait for the deployment to be ready 01/23/24 14:23:28.633
Jan 23 14:23:28.635: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/23/24 14:23:30.641
STEP: Verifying the service has paired with the endpoint 01/23/24 14:23:30.646
Jan 23 14:23:31.647: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan 23 14:23:31.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Creating a v1 custom resource 01/23/24 14:23:39.226
STEP: Create a v2 custom resource 01/23/24 14:23:39.239
STEP: List CRs in v1 01/23/24 14:23:39.273
STEP: List CRs in v2 01/23/24 14:23:39.276
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:23:39.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-7485" for this suite. 01/23/24 14:23:39.814
------------------------------
• [SLOW TEST] [11.408 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:23:28.409
    Jan 23 14:23:28.409: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename crd-webhook 01/23/24 14:23:28.41
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:23:28.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:23:28.419
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/23/24 14:23:28.421
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/23/24 14:23:28.618
    STEP: Deploying the custom resource conversion webhook pod 01/23/24 14:23:28.623
    STEP: Wait for the deployment to be ready 01/23/24 14:23:28.633
    Jan 23 14:23:28.635: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/23/24 14:23:30.641
    STEP: Verifying the service has paired with the endpoint 01/23/24 14:23:30.646
    Jan 23 14:23:31.647: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan 23 14:23:31.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Creating a v1 custom resource 01/23/24 14:23:39.226
    STEP: Create a v2 custom resource 01/23/24 14:23:39.239
    STEP: List CRs in v1 01/23/24 14:23:39.273
    STEP: List CRs in v2 01/23/24 14:23:39.276
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:23:39.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-7485" for this suite. 01/23/24 14:23:39.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:23:39.817
Jan 23 14:23:39.817: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename replication-controller 01/23/24 14:23:39.818
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:23:39.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:23:39.834
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 01/23/24 14:23:39.836
STEP: When the matched label of one of its pods change 01/23/24 14:23:39.844
Jan 23 14:23:39.846: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 23 14:23:44.859: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/23/24 14:23:44.865
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 23 14:23:45.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3065" for this suite. 01/23/24 14:23:45.873
------------------------------
• [SLOW TEST] [6.058 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:23:39.817
    Jan 23 14:23:39.817: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename replication-controller 01/23/24 14:23:39.818
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:23:39.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:23:39.834
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 01/23/24 14:23:39.836
    STEP: When the matched label of one of its pods change 01/23/24 14:23:39.844
    Jan 23 14:23:39.846: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan 23 14:23:44.859: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/23/24 14:23:44.865
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:23:45.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3065" for this suite. 01/23/24 14:23:45.873
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:23:45.876
Jan 23 14:23:45.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename dns 01/23/24 14:23:45.877
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:23:45.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:23:45.885
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/23/24 14:23:45.887
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8768.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8768.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8768.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8768.svc.cluster.local;sleep 1; done
 01/23/24 14:23:45.891
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8768.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8768.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8768.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8768.svc.cluster.local;sleep 1; done
 01/23/24 14:23:45.892
STEP: creating a pod to probe DNS 01/23/24 14:23:45.892
STEP: submitting the pod to kubernetes 01/23/24 14:23:45.892
Jan 23 14:23:45.913: INFO: Waiting up to 15m0s for pod "dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3" in namespace "dns-8768" to be "running"
Jan 23 14:23:45.915: INFO: Pod "dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.370664ms
Jan 23 14:23:47.917: INFO: Pod "dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004237015s
Jan 23 14:23:47.918: INFO: Pod "dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3" satisfied condition "running"
STEP: retrieving the pod 01/23/24 14:23:47.918
STEP: looking for the results for each expected name from probers 01/23/24 14:23:47.919
Jan 23 14:23:47.922: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local from pod dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3: the server could not find the requested resource (get pods dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3)
Jan 23 14:23:47.923: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local from pod dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3: the server could not find the requested resource (get pods dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3)
Jan 23 14:23:47.925: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8768.svc.cluster.local from pod dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3: the server could not find the requested resource (get pods dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3)
Jan 23 14:23:47.926: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8768.svc.cluster.local from pod dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3: the server could not find the requested resource (get pods dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3)
Jan 23 14:23:47.928: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local from pod dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3: the server could not find the requested resource (get pods dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3)
Jan 23 14:23:47.930: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local from pod dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3: the server could not find the requested resource (get pods dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3)
Jan 23 14:23:47.931: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8768.svc.cluster.local from pod dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3: the server could not find the requested resource (get pods dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3)
Jan 23 14:23:47.933: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8768.svc.cluster.local from pod dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3: the server could not find the requested resource (get pods dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3)
Jan 23 14:23:47.933: INFO: Lookups using dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8768.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8768.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local jessie_udp@dns-test-service-2.dns-8768.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8768.svc.cluster.local]

Jan 23 14:23:52.948: INFO: DNS probes using dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3 succeeded

STEP: deleting the pod 01/23/24 14:23:52.948
STEP: deleting the test headless service 01/23/24 14:23:52.954
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 23 14:23:52.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8768" for this suite. 01/23/24 14:23:52.964
------------------------------
• [SLOW TEST] [7.091 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:23:45.876
    Jan 23 14:23:45.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename dns 01/23/24 14:23:45.877
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:23:45.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:23:45.885
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/23/24 14:23:45.887
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8768.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8768.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8768.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8768.svc.cluster.local;sleep 1; done
     01/23/24 14:23:45.891
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8768.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8768.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8768.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8768.svc.cluster.local;sleep 1; done
     01/23/24 14:23:45.892
    STEP: creating a pod to probe DNS 01/23/24 14:23:45.892
    STEP: submitting the pod to kubernetes 01/23/24 14:23:45.892
    Jan 23 14:23:45.913: INFO: Waiting up to 15m0s for pod "dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3" in namespace "dns-8768" to be "running"
    Jan 23 14:23:45.915: INFO: Pod "dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.370664ms
    Jan 23 14:23:47.917: INFO: Pod "dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004237015s
    Jan 23 14:23:47.918: INFO: Pod "dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3" satisfied condition "running"
    STEP: retrieving the pod 01/23/24 14:23:47.918
    STEP: looking for the results for each expected name from probers 01/23/24 14:23:47.919
    Jan 23 14:23:47.922: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local from pod dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3: the server could not find the requested resource (get pods dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3)
    Jan 23 14:23:47.923: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local from pod dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3: the server could not find the requested resource (get pods dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3)
    Jan 23 14:23:47.925: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8768.svc.cluster.local from pod dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3: the server could not find the requested resource (get pods dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3)
    Jan 23 14:23:47.926: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8768.svc.cluster.local from pod dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3: the server could not find the requested resource (get pods dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3)
    Jan 23 14:23:47.928: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local from pod dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3: the server could not find the requested resource (get pods dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3)
    Jan 23 14:23:47.930: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local from pod dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3: the server could not find the requested resource (get pods dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3)
    Jan 23 14:23:47.931: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8768.svc.cluster.local from pod dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3: the server could not find the requested resource (get pods dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3)
    Jan 23 14:23:47.933: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8768.svc.cluster.local from pod dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3: the server could not find the requested resource (get pods dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3)
    Jan 23 14:23:47.933: INFO: Lookups using dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8768.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8768.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8768.svc.cluster.local jessie_udp@dns-test-service-2.dns-8768.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8768.svc.cluster.local]

    Jan 23 14:23:52.948: INFO: DNS probes using dns-8768/dns-test-49daaf5b-4768-4d2d-818e-9c9e6cf4ece3 succeeded

    STEP: deleting the pod 01/23/24 14:23:52.948
    STEP: deleting the test headless service 01/23/24 14:23:52.954
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:23:52.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8768" for this suite. 01/23/24 14:23:52.964
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:23:52.967
Jan 23 14:23:52.967: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename subpath 01/23/24 14:23:52.967
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:23:52.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:23:52.976
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/23/24 14:23:52.978
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-mbqk 01/23/24 14:23:52.99
STEP: Creating a pod to test atomic-volume-subpath 01/23/24 14:23:52.99
Jan 23 14:23:53.009: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mbqk" in namespace "subpath-4177" to be "Succeeded or Failed"
Jan 23 14:23:53.011: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Pending", Reason="", readiness=false. Elapsed: 1.806951ms
Jan 23 14:23:55.014: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004389521s
Jan 23 14:23:57.014: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 4.004836742s
Jan 23 14:23:59.015: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 6.005793222s
Jan 23 14:24:01.015: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 8.005475815s
Jan 23 14:24:03.015: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 10.005562238s
Jan 23 14:24:05.015: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 12.005067267s
Jan 23 14:24:07.014: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 14.004518003s
Jan 23 14:24:09.014: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 16.004641912s
Jan 23 14:24:11.014: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 18.004682902s
Jan 23 14:24:13.016: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 20.006571716s
Jan 23 14:24:15.015: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 22.005958747s
Jan 23 14:24:17.015: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 24.005745881s
Jan 23 14:24:19.015: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=false. Elapsed: 26.006003148s
Jan 23 14:24:21.015: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.005544602s
STEP: Saw pod success 01/23/24 14:24:21.015
Jan 23 14:24:21.015: INFO: Pod "pod-subpath-test-configmap-mbqk" satisfied condition "Succeeded or Failed"
Jan 23 14:24:21.017: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-subpath-test-configmap-mbqk container test-container-subpath-configmap-mbqk: <nil>
STEP: delete the pod 01/23/24 14:24:21.026
Jan 23 14:24:21.032: INFO: Waiting for pod pod-subpath-test-configmap-mbqk to disappear
Jan 23 14:24:21.033: INFO: Pod pod-subpath-test-configmap-mbqk no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mbqk 01/23/24 14:24:21.033
Jan 23 14:24:21.033: INFO: Deleting pod "pod-subpath-test-configmap-mbqk" in namespace "subpath-4177"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 23 14:24:21.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4177" for this suite. 01/23/24 14:24:21.036
------------------------------
• [SLOW TEST] [28.073 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:23:52.967
    Jan 23 14:23:52.967: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename subpath 01/23/24 14:23:52.967
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:23:52.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:23:52.976
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/23/24 14:23:52.978
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-mbqk 01/23/24 14:23:52.99
    STEP: Creating a pod to test atomic-volume-subpath 01/23/24 14:23:52.99
    Jan 23 14:23:53.009: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mbqk" in namespace "subpath-4177" to be "Succeeded or Failed"
    Jan 23 14:23:53.011: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Pending", Reason="", readiness=false. Elapsed: 1.806951ms
    Jan 23 14:23:55.014: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004389521s
    Jan 23 14:23:57.014: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 4.004836742s
    Jan 23 14:23:59.015: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 6.005793222s
    Jan 23 14:24:01.015: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 8.005475815s
    Jan 23 14:24:03.015: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 10.005562238s
    Jan 23 14:24:05.015: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 12.005067267s
    Jan 23 14:24:07.014: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 14.004518003s
    Jan 23 14:24:09.014: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 16.004641912s
    Jan 23 14:24:11.014: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 18.004682902s
    Jan 23 14:24:13.016: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 20.006571716s
    Jan 23 14:24:15.015: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 22.005958747s
    Jan 23 14:24:17.015: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=true. Elapsed: 24.005745881s
    Jan 23 14:24:19.015: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Running", Reason="", readiness=false. Elapsed: 26.006003148s
    Jan 23 14:24:21.015: INFO: Pod "pod-subpath-test-configmap-mbqk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.005544602s
    STEP: Saw pod success 01/23/24 14:24:21.015
    Jan 23 14:24:21.015: INFO: Pod "pod-subpath-test-configmap-mbqk" satisfied condition "Succeeded or Failed"
    Jan 23 14:24:21.017: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-subpath-test-configmap-mbqk container test-container-subpath-configmap-mbqk: <nil>
    STEP: delete the pod 01/23/24 14:24:21.026
    Jan 23 14:24:21.032: INFO: Waiting for pod pod-subpath-test-configmap-mbqk to disappear
    Jan 23 14:24:21.033: INFO: Pod pod-subpath-test-configmap-mbqk no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-mbqk 01/23/24 14:24:21.033
    Jan 23 14:24:21.033: INFO: Deleting pod "pod-subpath-test-configmap-mbqk" in namespace "subpath-4177"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:24:21.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4177" for this suite. 01/23/24 14:24:21.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:24:21.04
Jan 23 14:24:21.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubectl 01/23/24 14:24:21.041
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:24:21.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:24:21.051
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 01/23/24 14:24:21.053
Jan 23 14:24:21.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3984 cluster-info'
Jan 23 14:24:21.199: INFO: stderr: ""
Jan 23 14:24:21.199: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 23 14:24:21.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3984" for this suite. 01/23/24 14:24:21.202
------------------------------
• [0.165 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:24:21.04
    Jan 23 14:24:21.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubectl 01/23/24 14:24:21.041
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:24:21.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:24:21.051
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 01/23/24 14:24:21.053
    Jan 23 14:24:21.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3984 cluster-info'
    Jan 23 14:24:21.199: INFO: stderr: ""
    Jan 23 14:24:21.199: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:24:21.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3984" for this suite. 01/23/24 14:24:21.202
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:24:21.205
Jan 23 14:24:21.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename webhook 01/23/24 14:24:21.206
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:24:21.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:24:21.215
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/23/24 14:24:21.224
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:24:21.643
STEP: Deploying the webhook pod 01/23/24 14:24:21.647
STEP: Wait for the deployment to be ready 01/23/24 14:24:21.655
Jan 23 14:24:21.659: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/23/24 14:24:23.667
STEP: Verifying the service has paired with the endpoint 01/23/24 14:24:23.672
Jan 23 14:24:24.673: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 01/23/24 14:24:24.675
STEP: create a pod 01/23/24 14:24:24.687
Jan 23 14:24:24.695: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6430" to be "running"
Jan 23 14:24:24.696: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.564736ms
Jan 23 14:24:26.699: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004451744s
Jan 23 14:24:26.699: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/23/24 14:24:26.699
Jan 23 14:24:26.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=webhook-6430 attach --namespace=webhook-6430 to-be-attached-pod -i -c=container1'
Jan 23 14:24:26.822: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:24:26.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6430" for this suite. 01/23/24 14:24:26.857
STEP: Destroying namespace "webhook-6430-markers" for this suite. 01/23/24 14:24:26.864
------------------------------
• [SLOW TEST] [5.661 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:24:21.205
    Jan 23 14:24:21.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename webhook 01/23/24 14:24:21.206
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:24:21.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:24:21.215
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/23/24 14:24:21.224
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:24:21.643
    STEP: Deploying the webhook pod 01/23/24 14:24:21.647
    STEP: Wait for the deployment to be ready 01/23/24 14:24:21.655
    Jan 23 14:24:21.659: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/23/24 14:24:23.667
    STEP: Verifying the service has paired with the endpoint 01/23/24 14:24:23.672
    Jan 23 14:24:24.673: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 01/23/24 14:24:24.675
    STEP: create a pod 01/23/24 14:24:24.687
    Jan 23 14:24:24.695: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6430" to be "running"
    Jan 23 14:24:24.696: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.564736ms
    Jan 23 14:24:26.699: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004451744s
    Jan 23 14:24:26.699: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/23/24 14:24:26.699
    Jan 23 14:24:26.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=webhook-6430 attach --namespace=webhook-6430 to-be-attached-pod -i -c=container1'
    Jan 23 14:24:26.822: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:24:26.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6430" for this suite. 01/23/24 14:24:26.857
    STEP: Destroying namespace "webhook-6430-markers" for this suite. 01/23/24 14:24:26.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:24:26.868
Jan 23 14:24:26.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename endpointslicemirroring 01/23/24 14:24:26.869
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:24:26.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:24:26.88
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/23/24 14:24:26.889
Jan 23 14:24:26.897: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/23/24 14:24:28.899
Jan 23 14:24:28.904: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/23/24 14:24:30.907
Jan 23 14:24:30.911: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Jan 23 14:24:32.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-6556" for this suite. 01/23/24 14:24:32.917
------------------------------
• [SLOW TEST] [6.051 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:24:26.868
    Jan 23 14:24:26.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename endpointslicemirroring 01/23/24 14:24:26.869
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:24:26.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:24:26.88
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/23/24 14:24:26.889
    Jan 23 14:24:26.897: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/23/24 14:24:28.899
    Jan 23 14:24:28.904: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/23/24 14:24:30.907
    Jan 23 14:24:30.911: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:24:32.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-6556" for this suite. 01/23/24 14:24:32.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:24:32.92
Jan 23 14:24:32.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename configmap 01/23/24 14:24:32.921
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:24:32.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:24:32.931
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-413d73ee-7911-490b-b1f8-eab9704c8b45 01/23/24 14:24:32.933
STEP: Creating a pod to test consume configMaps 01/23/24 14:24:32.937
Jan 23 14:24:32.949: INFO: Waiting up to 5m0s for pod "pod-configmaps-aac2f946-2984-49b9-9851-a05f61b4fe5b" in namespace "configmap-4397" to be "Succeeded or Failed"
Jan 23 14:24:32.951: INFO: Pod "pod-configmaps-aac2f946-2984-49b9-9851-a05f61b4fe5b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.636708ms
Jan 23 14:24:34.954: INFO: Pod "pod-configmaps-aac2f946-2984-49b9-9851-a05f61b4fe5b": Phase="Running", Reason="", readiness=false. Elapsed: 2.004799716s
Jan 23 14:24:36.954: INFO: Pod "pod-configmaps-aac2f946-2984-49b9-9851-a05f61b4fe5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004558399s
STEP: Saw pod success 01/23/24 14:24:36.954
Jan 23 14:24:36.954: INFO: Pod "pod-configmaps-aac2f946-2984-49b9-9851-a05f61b4fe5b" satisfied condition "Succeeded or Failed"
Jan 23 14:24:36.956: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-aac2f946-2984-49b9-9851-a05f61b4fe5b container agnhost-container: <nil>
STEP: delete the pod 01/23/24 14:24:36.96
Jan 23 14:24:36.966: INFO: Waiting for pod pod-configmaps-aac2f946-2984-49b9-9851-a05f61b4fe5b to disappear
Jan 23 14:24:36.967: INFO: Pod pod-configmaps-aac2f946-2984-49b9-9851-a05f61b4fe5b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 23 14:24:36.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4397" for this suite. 01/23/24 14:24:36.969
------------------------------
• [4.052 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:24:32.92
    Jan 23 14:24:32.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename configmap 01/23/24 14:24:32.921
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:24:32.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:24:32.931
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-413d73ee-7911-490b-b1f8-eab9704c8b45 01/23/24 14:24:32.933
    STEP: Creating a pod to test consume configMaps 01/23/24 14:24:32.937
    Jan 23 14:24:32.949: INFO: Waiting up to 5m0s for pod "pod-configmaps-aac2f946-2984-49b9-9851-a05f61b4fe5b" in namespace "configmap-4397" to be "Succeeded or Failed"
    Jan 23 14:24:32.951: INFO: Pod "pod-configmaps-aac2f946-2984-49b9-9851-a05f61b4fe5b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.636708ms
    Jan 23 14:24:34.954: INFO: Pod "pod-configmaps-aac2f946-2984-49b9-9851-a05f61b4fe5b": Phase="Running", Reason="", readiness=false. Elapsed: 2.004799716s
    Jan 23 14:24:36.954: INFO: Pod "pod-configmaps-aac2f946-2984-49b9-9851-a05f61b4fe5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004558399s
    STEP: Saw pod success 01/23/24 14:24:36.954
    Jan 23 14:24:36.954: INFO: Pod "pod-configmaps-aac2f946-2984-49b9-9851-a05f61b4fe5b" satisfied condition "Succeeded or Failed"
    Jan 23 14:24:36.956: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-aac2f946-2984-49b9-9851-a05f61b4fe5b container agnhost-container: <nil>
    STEP: delete the pod 01/23/24 14:24:36.96
    Jan 23 14:24:36.966: INFO: Waiting for pod pod-configmaps-aac2f946-2984-49b9-9851-a05f61b4fe5b to disappear
    Jan 23 14:24:36.967: INFO: Pod pod-configmaps-aac2f946-2984-49b9-9851-a05f61b4fe5b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:24:36.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4397" for this suite. 01/23/24 14:24:36.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:24:36.973
Jan 23 14:24:36.973: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename container-lifecycle-hook 01/23/24 14:24:36.973
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:24:36.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:24:36.983
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/23/24 14:24:36.986
Jan 23 14:24:37.002: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8448" to be "running and ready"
Jan 23 14:24:37.004: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038616ms
Jan 23 14:24:37.005: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:24:39.010: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.00758569s
Jan 23 14:24:39.010: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 23 14:24:39.010: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 01/23/24 14:24:39.014
Jan 23 14:24:39.033: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8448" to be "running and ready"
Jan 23 14:24:39.037: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.809266ms
Jan 23 14:24:39.037: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:24:41.040: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006493134s
Jan 23 14:24:41.040: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan 23 14:24:41.040: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/23/24 14:24:41.041
STEP: delete the pod with lifecycle hook 01/23/24 14:24:41.045
Jan 23 14:24:41.049: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 23 14:24:41.051: INFO: Pod pod-with-poststart-http-hook still exists
Jan 23 14:24:43.052: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 23 14:24:43.054: INFO: Pod pod-with-poststart-http-hook still exists
Jan 23 14:24:45.052: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 23 14:24:45.056: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 23 14:24:45.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-8448" for this suite. 01/23/24 14:24:45.058
------------------------------
• [SLOW TEST] [8.088 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:24:36.973
    Jan 23 14:24:36.973: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/23/24 14:24:36.973
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:24:36.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:24:36.983
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/23/24 14:24:36.986
    Jan 23 14:24:37.002: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8448" to be "running and ready"
    Jan 23 14:24:37.004: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038616ms
    Jan 23 14:24:37.005: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:24:39.010: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.00758569s
    Jan 23 14:24:39.010: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 23 14:24:39.010: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 01/23/24 14:24:39.014
    Jan 23 14:24:39.033: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8448" to be "running and ready"
    Jan 23 14:24:39.037: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.809266ms
    Jan 23 14:24:39.037: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:24:41.040: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006493134s
    Jan 23 14:24:41.040: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan 23 14:24:41.040: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/23/24 14:24:41.041
    STEP: delete the pod with lifecycle hook 01/23/24 14:24:41.045
    Jan 23 14:24:41.049: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 23 14:24:41.051: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 23 14:24:43.052: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 23 14:24:43.054: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 23 14:24:45.052: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 23 14:24:45.056: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:24:45.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-8448" for this suite. 01/23/24 14:24:45.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:24:45.063
Jan 23 14:24:45.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubectl 01/23/24 14:24:45.063
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:24:45.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:24:45.072
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 01/23/24 14:24:45.074
Jan 23 14:24:45.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-935 create -f -'
Jan 23 14:24:46.855: INFO: stderr: ""
Jan 23 14:24:46.855: INFO: stdout: "pod/pause created\n"
Jan 23 14:24:46.855: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 23 14:24:46.855: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-935" to be "running and ready"
Jan 23 14:24:46.857: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.956861ms
Jan 23 14:24:46.857: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local' to be 'Running' but was 'Pending'
Jan 23 14:24:48.860: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.004744056s
Jan 23 14:24:48.860: INFO: Pod "pause" satisfied condition "running and ready"
Jan 23 14:24:48.860: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 01/23/24 14:24:48.86
Jan 23 14:24:48.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-935 label pods pause testing-label=testing-label-value'
Jan 23 14:24:49.006: INFO: stderr: ""
Jan 23 14:24:49.006: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/23/24 14:24:49.006
Jan 23 14:24:49.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-935 get pod pause -L testing-label'
Jan 23 14:24:49.148: INFO: stderr: ""
Jan 23 14:24:49.148: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/23/24 14:24:49.148
Jan 23 14:24:49.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-935 label pods pause testing-label-'
Jan 23 14:24:49.309: INFO: stderr: ""
Jan 23 14:24:49.309: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/23/24 14:24:49.309
Jan 23 14:24:49.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-935 get pod pause -L testing-label'
Jan 23 14:24:49.447: INFO: stderr: ""
Jan 23 14:24:49.447: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 01/23/24 14:24:49.447
Jan 23 14:24:49.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-935 delete --grace-period=0 --force -f -'
Jan 23 14:24:49.533: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 14:24:49.533: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 23 14:24:49.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-935 get rc,svc -l name=pause --no-headers'
Jan 23 14:24:49.742: INFO: stderr: "No resources found in kubectl-935 namespace.\n"
Jan 23 14:24:49.742: INFO: stdout: ""
Jan 23 14:24:49.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-935 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 23 14:24:49.889: INFO: stderr: ""
Jan 23 14:24:49.889: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 23 14:24:49.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-935" for this suite. 01/23/24 14:24:49.892
------------------------------
• [4.832 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:24:45.063
    Jan 23 14:24:45.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubectl 01/23/24 14:24:45.063
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:24:45.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:24:45.072
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 01/23/24 14:24:45.074
    Jan 23 14:24:45.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-935 create -f -'
    Jan 23 14:24:46.855: INFO: stderr: ""
    Jan 23 14:24:46.855: INFO: stdout: "pod/pause created\n"
    Jan 23 14:24:46.855: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan 23 14:24:46.855: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-935" to be "running and ready"
    Jan 23 14:24:46.857: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.956861ms
    Jan 23 14:24:46.857: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local' to be 'Running' but was 'Pending'
    Jan 23 14:24:48.860: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.004744056s
    Jan 23 14:24:48.860: INFO: Pod "pause" satisfied condition "running and ready"
    Jan 23 14:24:48.860: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 01/23/24 14:24:48.86
    Jan 23 14:24:48.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-935 label pods pause testing-label=testing-label-value'
    Jan 23 14:24:49.006: INFO: stderr: ""
    Jan 23 14:24:49.006: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/23/24 14:24:49.006
    Jan 23 14:24:49.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-935 get pod pause -L testing-label'
    Jan 23 14:24:49.148: INFO: stderr: ""
    Jan 23 14:24:49.148: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/23/24 14:24:49.148
    Jan 23 14:24:49.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-935 label pods pause testing-label-'
    Jan 23 14:24:49.309: INFO: stderr: ""
    Jan 23 14:24:49.309: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/23/24 14:24:49.309
    Jan 23 14:24:49.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-935 get pod pause -L testing-label'
    Jan 23 14:24:49.447: INFO: stderr: ""
    Jan 23 14:24:49.447: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 01/23/24 14:24:49.447
    Jan 23 14:24:49.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-935 delete --grace-period=0 --force -f -'
    Jan 23 14:24:49.533: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 23 14:24:49.533: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan 23 14:24:49.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-935 get rc,svc -l name=pause --no-headers'
    Jan 23 14:24:49.742: INFO: stderr: "No resources found in kubectl-935 namespace.\n"
    Jan 23 14:24:49.742: INFO: stdout: ""
    Jan 23 14:24:49.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-935 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 23 14:24:49.889: INFO: stderr: ""
    Jan 23 14:24:49.889: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:24:49.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-935" for this suite. 01/23/24 14:24:49.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:24:49.896
Jan 23 14:24:49.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename resourcequota 01/23/24 14:24:49.897
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:24:49.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:24:49.907
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 01/23/24 14:24:49.913
STEP: Ensuring ResourceQuota status is calculated 01/23/24 14:24:49.916
STEP: Creating a ResourceQuota with not best effort scope 01/23/24 14:24:51.92
STEP: Ensuring ResourceQuota status is calculated 01/23/24 14:24:51.923
STEP: Creating a best-effort pod 01/23/24 14:24:53.926
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/23/24 14:24:53.937
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/23/24 14:24:55.941
STEP: Deleting the pod 01/23/24 14:24:57.943
STEP: Ensuring resource quota status released the pod usage 01/23/24 14:24:57.954
STEP: Creating a not best-effort pod 01/23/24 14:24:59.956
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/23/24 14:24:59.969
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/23/24 14:25:01.971
STEP: Deleting the pod 01/23/24 14:25:03.975
STEP: Ensuring resource quota status released the pod usage 01/23/24 14:25:03.982
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 23 14:25:05.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5810" for this suite. 01/23/24 14:25:05.988
------------------------------
• [SLOW TEST] [16.095 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:24:49.896
    Jan 23 14:24:49.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename resourcequota 01/23/24 14:24:49.897
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:24:49.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:24:49.907
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 01/23/24 14:24:49.913
    STEP: Ensuring ResourceQuota status is calculated 01/23/24 14:24:49.916
    STEP: Creating a ResourceQuota with not best effort scope 01/23/24 14:24:51.92
    STEP: Ensuring ResourceQuota status is calculated 01/23/24 14:24:51.923
    STEP: Creating a best-effort pod 01/23/24 14:24:53.926
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/23/24 14:24:53.937
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/23/24 14:24:55.941
    STEP: Deleting the pod 01/23/24 14:24:57.943
    STEP: Ensuring resource quota status released the pod usage 01/23/24 14:24:57.954
    STEP: Creating a not best-effort pod 01/23/24 14:24:59.956
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/23/24 14:24:59.969
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/23/24 14:25:01.971
    STEP: Deleting the pod 01/23/24 14:25:03.975
    STEP: Ensuring resource quota status released the pod usage 01/23/24 14:25:03.982
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:25:05.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5810" for this suite. 01/23/24 14:25:05.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:25:05.992
Jan 23 14:25:05.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename taint-single-pod 01/23/24 14:25:05.993
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:25:06.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:25:06.005
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Jan 23 14:25:06.007: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 23 14:26:06.049: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Jan 23 14:26:06.052: INFO: Starting informer...
STEP: Starting pod... 01/23/24 14:26:06.052
Jan 23 14:26:06.267: INFO: Pod is running on node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local. Tainting Node
STEP: Trying to apply a taint on the Node 01/23/24 14:26:06.267
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/23/24 14:26:06.275
STEP: Waiting short time to make sure Pod is queued for deletion 01/23/24 14:26:06.279
Jan 23 14:26:06.279: INFO: Pod wasn't evicted. Proceeding
Jan 23 14:26:06.279: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/23/24 14:26:06.291
STEP: Waiting some time to make sure that toleration time passed. 01/23/24 14:26:06.301
Jan 23 14:27:21.301: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:27:21.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-9184" for this suite. 01/23/24 14:27:21.306
------------------------------
• [SLOW TEST] [135.317 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:25:05.992
    Jan 23 14:25:05.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename taint-single-pod 01/23/24 14:25:05.993
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:25:06.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:25:06.005
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Jan 23 14:25:06.007: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 23 14:26:06.049: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Jan 23 14:26:06.052: INFO: Starting informer...
    STEP: Starting pod... 01/23/24 14:26:06.052
    Jan 23 14:26:06.267: INFO: Pod is running on node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local. Tainting Node
    STEP: Trying to apply a taint on the Node 01/23/24 14:26:06.267
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/23/24 14:26:06.275
    STEP: Waiting short time to make sure Pod is queued for deletion 01/23/24 14:26:06.279
    Jan 23 14:26:06.279: INFO: Pod wasn't evicted. Proceeding
    Jan 23 14:26:06.279: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/23/24 14:26:06.291
    STEP: Waiting some time to make sure that toleration time passed. 01/23/24 14:26:06.301
    Jan 23 14:27:21.301: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:27:21.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-9184" for this suite. 01/23/24 14:27:21.306
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:27:21.309
Jan 23 14:27:21.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename dns 01/23/24 14:27:21.31
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:27:21.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:27:21.32
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/23/24 14:27:21.322
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9668.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9668.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 76.39.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.39.76_udp@PTR;check="$$(dig +tcp +noall +answer +search 76.39.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.39.76_tcp@PTR;sleep 1; done
 01/23/24 14:27:21.331
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9668.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9668.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 76.39.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.39.76_udp@PTR;check="$$(dig +tcp +noall +answer +search 76.39.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.39.76_tcp@PTR;sleep 1; done
 01/23/24 14:27:21.332
STEP: creating a pod to probe DNS 01/23/24 14:27:21.332
STEP: submitting the pod to kubernetes 01/23/24 14:27:21.332
Jan 23 14:27:21.358: INFO: Waiting up to 15m0s for pod "dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548" in namespace "dns-9668" to be "running"
Jan 23 14:27:21.362: INFO: Pod "dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548": Phase="Pending", Reason="", readiness=false. Elapsed: 3.802488ms
Jan 23 14:27:23.365: INFO: Pod "dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548": Phase="Running", Reason="", readiness=true. Elapsed: 2.00727384s
Jan 23 14:27:23.365: INFO: Pod "dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548" satisfied condition "running"
STEP: retrieving the pod 01/23/24 14:27:23.365
STEP: looking for the results for each expected name from probers 01/23/24 14:27:23.367
Jan 23 14:27:23.370: INFO: Unable to read wheezy_udp@dns-test-service.dns-9668.svc.cluster.local from pod dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548: the server could not find the requested resource (get pods dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548)
Jan 23 14:27:23.371: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9668.svc.cluster.local from pod dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548: the server could not find the requested resource (get pods dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548)
Jan 23 14:27:23.373: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local from pod dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548: the server could not find the requested resource (get pods dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548)
Jan 23 14:27:23.375: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local from pod dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548: the server could not find the requested resource (get pods dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548)
Jan 23 14:27:23.382: INFO: Unable to read jessie_udp@dns-test-service.dns-9668.svc.cluster.local from pod dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548: the server could not find the requested resource (get pods dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548)
Jan 23 14:27:23.384: INFO: Unable to read jessie_tcp@dns-test-service.dns-9668.svc.cluster.local from pod dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548: the server could not find the requested resource (get pods dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548)
Jan 23 14:27:23.386: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local from pod dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548: the server could not find the requested resource (get pods dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548)
Jan 23 14:27:23.387: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local from pod dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548: the server could not find the requested resource (get pods dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548)
Jan 23 14:27:23.394: INFO: Lookups using dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548 failed for: [wheezy_udp@dns-test-service.dns-9668.svc.cluster.local wheezy_tcp@dns-test-service.dns-9668.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local jessie_udp@dns-test-service.dns-9668.svc.cluster.local jessie_tcp@dns-test-service.dns-9668.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local]

Jan 23 14:27:28.421: INFO: DNS probes using dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548 succeeded

STEP: deleting the pod 01/23/24 14:27:28.421
STEP: deleting the test service 01/23/24 14:27:28.428
STEP: deleting the test headless service 01/23/24 14:27:28.446
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 23 14:27:28.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9668" for this suite. 01/23/24 14:27:28.456
------------------------------
• [SLOW TEST] [7.157 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:27:21.309
    Jan 23 14:27:21.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename dns 01/23/24 14:27:21.31
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:27:21.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:27:21.32
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/23/24 14:27:21.322
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9668.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9668.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 76.39.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.39.76_udp@PTR;check="$$(dig +tcp +noall +answer +search 76.39.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.39.76_tcp@PTR;sleep 1; done
     01/23/24 14:27:21.331
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9668.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9668.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 76.39.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.39.76_udp@PTR;check="$$(dig +tcp +noall +answer +search 76.39.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.39.76_tcp@PTR;sleep 1; done
     01/23/24 14:27:21.332
    STEP: creating a pod to probe DNS 01/23/24 14:27:21.332
    STEP: submitting the pod to kubernetes 01/23/24 14:27:21.332
    Jan 23 14:27:21.358: INFO: Waiting up to 15m0s for pod "dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548" in namespace "dns-9668" to be "running"
    Jan 23 14:27:21.362: INFO: Pod "dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548": Phase="Pending", Reason="", readiness=false. Elapsed: 3.802488ms
    Jan 23 14:27:23.365: INFO: Pod "dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548": Phase="Running", Reason="", readiness=true. Elapsed: 2.00727384s
    Jan 23 14:27:23.365: INFO: Pod "dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548" satisfied condition "running"
    STEP: retrieving the pod 01/23/24 14:27:23.365
    STEP: looking for the results for each expected name from probers 01/23/24 14:27:23.367
    Jan 23 14:27:23.370: INFO: Unable to read wheezy_udp@dns-test-service.dns-9668.svc.cluster.local from pod dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548: the server could not find the requested resource (get pods dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548)
    Jan 23 14:27:23.371: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9668.svc.cluster.local from pod dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548: the server could not find the requested resource (get pods dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548)
    Jan 23 14:27:23.373: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local from pod dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548: the server could not find the requested resource (get pods dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548)
    Jan 23 14:27:23.375: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local from pod dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548: the server could not find the requested resource (get pods dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548)
    Jan 23 14:27:23.382: INFO: Unable to read jessie_udp@dns-test-service.dns-9668.svc.cluster.local from pod dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548: the server could not find the requested resource (get pods dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548)
    Jan 23 14:27:23.384: INFO: Unable to read jessie_tcp@dns-test-service.dns-9668.svc.cluster.local from pod dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548: the server could not find the requested resource (get pods dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548)
    Jan 23 14:27:23.386: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local from pod dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548: the server could not find the requested resource (get pods dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548)
    Jan 23 14:27:23.387: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local from pod dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548: the server could not find the requested resource (get pods dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548)
    Jan 23 14:27:23.394: INFO: Lookups using dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548 failed for: [wheezy_udp@dns-test-service.dns-9668.svc.cluster.local wheezy_tcp@dns-test-service.dns-9668.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local jessie_udp@dns-test-service.dns-9668.svc.cluster.local jessie_tcp@dns-test-service.dns-9668.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9668.svc.cluster.local]

    Jan 23 14:27:28.421: INFO: DNS probes using dns-9668/dns-test-5f71a7a1-da15-42ac-820a-967a10bcb548 succeeded

    STEP: deleting the pod 01/23/24 14:27:28.421
    STEP: deleting the test service 01/23/24 14:27:28.428
    STEP: deleting the test headless service 01/23/24 14:27:28.446
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:27:28.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9668" for this suite. 01/23/24 14:27:28.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:27:28.468
Jan 23 14:27:28.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename job 01/23/24 14:27:28.47
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:27:28.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:27:28.48
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 01/23/24 14:27:28.482
STEP: Ensuring active pods == parallelism 01/23/24 14:27:28.485
STEP: delete a job 01/23/24 14:27:32.487
STEP: deleting Job.batch foo in namespace job-1811, will wait for the garbage collector to delete the pods 01/23/24 14:27:32.487
Jan 23 14:27:32.544: INFO: Deleting Job.batch foo took: 4.147485ms
Jan 23 14:27:32.644: INFO: Terminating Job.batch foo pods took: 100.102288ms
STEP: Ensuring job was deleted 01/23/24 14:28:05.445
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 23 14:28:05.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1811" for this suite. 01/23/24 14:28:05.45
------------------------------
• [SLOW TEST] [36.987 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:27:28.468
    Jan 23 14:27:28.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename job 01/23/24 14:27:28.47
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:27:28.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:27:28.48
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 01/23/24 14:27:28.482
    STEP: Ensuring active pods == parallelism 01/23/24 14:27:28.485
    STEP: delete a job 01/23/24 14:27:32.487
    STEP: deleting Job.batch foo in namespace job-1811, will wait for the garbage collector to delete the pods 01/23/24 14:27:32.487
    Jan 23 14:27:32.544: INFO: Deleting Job.batch foo took: 4.147485ms
    Jan 23 14:27:32.644: INFO: Terminating Job.batch foo pods took: 100.102288ms
    STEP: Ensuring job was deleted 01/23/24 14:28:05.445
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:28:05.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1811" for this suite. 01/23/24 14:28:05.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:28:05.455
Jan 23 14:28:05.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename resourcequota 01/23/24 14:28:05.456
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:28:05.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:28:05.467
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 01/23/24 14:28:22.472
STEP: Creating a ResourceQuota 01/23/24 14:28:27.475
STEP: Ensuring resource quota status is calculated 01/23/24 14:28:27.478
STEP: Creating a ConfigMap 01/23/24 14:28:29.482
STEP: Ensuring resource quota status captures configMap creation 01/23/24 14:28:29.49
STEP: Deleting a ConfigMap 01/23/24 14:28:31.494
STEP: Ensuring resource quota status released usage 01/23/24 14:28:31.497
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 23 14:28:33.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6762" for this suite. 01/23/24 14:28:33.503
------------------------------
• [SLOW TEST] [28.053 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:28:05.455
    Jan 23 14:28:05.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename resourcequota 01/23/24 14:28:05.456
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:28:05.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:28:05.467
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 01/23/24 14:28:22.472
    STEP: Creating a ResourceQuota 01/23/24 14:28:27.475
    STEP: Ensuring resource quota status is calculated 01/23/24 14:28:27.478
    STEP: Creating a ConfigMap 01/23/24 14:28:29.482
    STEP: Ensuring resource quota status captures configMap creation 01/23/24 14:28:29.49
    STEP: Deleting a ConfigMap 01/23/24 14:28:31.494
    STEP: Ensuring resource quota status released usage 01/23/24 14:28:31.497
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:28:33.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6762" for this suite. 01/23/24 14:28:33.503
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:28:33.508
Jan 23 14:28:33.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename custom-resource-definition 01/23/24 14:28:33.509
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:28:33.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:28:33.519
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan 23 14:28:33.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:28:39.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6483" for this suite. 01/23/24 14:28:39.537
------------------------------
• [SLOW TEST] [6.032 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:28:33.508
    Jan 23 14:28:33.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename custom-resource-definition 01/23/24 14:28:33.509
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:28:33.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:28:33.519
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan 23 14:28:33.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:28:39.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6483" for this suite. 01/23/24 14:28:39.537
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:28:39.54
Jan 23 14:28:39.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename container-probe 01/23/24 14:28:39.541
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:28:39.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:28:39.551
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Jan 23 14:28:39.565: INFO: Waiting up to 5m0s for pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae" in namespace "container-probe-8713" to be "running and ready"
Jan 23 14:28:39.566: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1.137912ms
Jan 23 14:28:39.566: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:28:41.568: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 2.003113477s
Jan 23 14:28:41.568: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
Jan 23 14:28:43.569: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 4.004632424s
Jan 23 14:28:43.569: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
Jan 23 14:28:45.569: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 6.004012651s
Jan 23 14:28:45.569: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
Jan 23 14:28:47.570: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 8.004941095s
Jan 23 14:28:47.570: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
Jan 23 14:28:49.589: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 10.023893624s
Jan 23 14:28:49.589: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
Jan 23 14:28:51.569: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 12.004135677s
Jan 23 14:28:51.569: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
Jan 23 14:28:53.570: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 14.005427455s
Jan 23 14:28:53.570: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
Jan 23 14:28:55.569: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 16.004263507s
Jan 23 14:28:55.569: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
Jan 23 14:28:57.571: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 18.0057934s
Jan 23 14:28:57.571: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
Jan 23 14:28:59.570: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 20.004886582s
Jan 23 14:28:59.570: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
Jan 23 14:29:01.569: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=true. Elapsed: 22.004411158s
Jan 23 14:29:01.569: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = true)
Jan 23 14:29:01.569: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae" satisfied condition "running and ready"
Jan 23 14:29:01.571: INFO: Container started at 2024-01-23 14:28:40 +0000 UTC, pod became ready at 2024-01-23 14:28:59 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 23 14:29:01.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8713" for this suite. 01/23/24 14:29:01.573
------------------------------
• [SLOW TEST] [22.035 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:28:39.54
    Jan 23 14:28:39.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename container-probe 01/23/24 14:28:39.541
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:28:39.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:28:39.551
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Jan 23 14:28:39.565: INFO: Waiting up to 5m0s for pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae" in namespace "container-probe-8713" to be "running and ready"
    Jan 23 14:28:39.566: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Pending", Reason="", readiness=false. Elapsed: 1.137912ms
    Jan 23 14:28:39.566: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:28:41.568: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 2.003113477s
    Jan 23 14:28:41.568: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
    Jan 23 14:28:43.569: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 4.004632424s
    Jan 23 14:28:43.569: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
    Jan 23 14:28:45.569: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 6.004012651s
    Jan 23 14:28:45.569: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
    Jan 23 14:28:47.570: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 8.004941095s
    Jan 23 14:28:47.570: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
    Jan 23 14:28:49.589: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 10.023893624s
    Jan 23 14:28:49.589: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
    Jan 23 14:28:51.569: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 12.004135677s
    Jan 23 14:28:51.569: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
    Jan 23 14:28:53.570: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 14.005427455s
    Jan 23 14:28:53.570: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
    Jan 23 14:28:55.569: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 16.004263507s
    Jan 23 14:28:55.569: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
    Jan 23 14:28:57.571: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 18.0057934s
    Jan 23 14:28:57.571: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
    Jan 23 14:28:59.570: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=false. Elapsed: 20.004886582s
    Jan 23 14:28:59.570: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = false)
    Jan 23 14:29:01.569: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae": Phase="Running", Reason="", readiness=true. Elapsed: 22.004411158s
    Jan 23 14:29:01.569: INFO: The phase of Pod test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae is Running (Ready = true)
    Jan 23 14:29:01.569: INFO: Pod "test-webserver-603a1954-c389-4a63-9a32-7dfc103b79ae" satisfied condition "running and ready"
    Jan 23 14:29:01.571: INFO: Container started at 2024-01-23 14:28:40 +0000 UTC, pod became ready at 2024-01-23 14:28:59 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:29:01.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8713" for this suite. 01/23/24 14:29:01.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:29:01.576
Jan 23 14:29:01.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 14:29:01.577
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:01.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:01.586
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-e1e8356a-d38b-4ae6-b15f-59b677996313 01/23/24 14:29:01.591
STEP: Creating configMap with name cm-test-opt-upd-d37001b2-4f89-4202-a1df-1c3f527793f2 01/23/24 14:29:01.595
STEP: Creating the pod 01/23/24 14:29:01.598
Jan 23 14:29:01.624: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e95d9f71-18c5-42ac-b386-4580a4b5b4c6" in namespace "projected-4404" to be "running and ready"
Jan 23 14:29:01.626: INFO: Pod "pod-projected-configmaps-e95d9f71-18c5-42ac-b386-4580a4b5b4c6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.750105ms
Jan 23 14:29:01.626: INFO: The phase of Pod pod-projected-configmaps-e95d9f71-18c5-42ac-b386-4580a4b5b4c6 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:29:03.629: INFO: Pod "pod-projected-configmaps-e95d9f71-18c5-42ac-b386-4580a4b5b4c6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004711621s
Jan 23 14:29:03.629: INFO: The phase of Pod pod-projected-configmaps-e95d9f71-18c5-42ac-b386-4580a4b5b4c6 is Running (Ready = true)
Jan 23 14:29:03.629: INFO: Pod "pod-projected-configmaps-e95d9f71-18c5-42ac-b386-4580a4b5b4c6" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-e1e8356a-d38b-4ae6-b15f-59b677996313 01/23/24 14:29:03.646
STEP: Updating configmap cm-test-opt-upd-d37001b2-4f89-4202-a1df-1c3f527793f2 01/23/24 14:29:03.649
STEP: Creating configMap with name cm-test-opt-create-b4abb30b-217e-475b-b111-5cfd45718388 01/23/24 14:29:03.653
STEP: waiting to observe update in volume 01/23/24 14:29:03.657
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 23 14:29:05.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4404" for this suite. 01/23/24 14:29:05.674
------------------------------
• [4.101 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:29:01.576
    Jan 23 14:29:01.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 14:29:01.577
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:01.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:01.586
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-e1e8356a-d38b-4ae6-b15f-59b677996313 01/23/24 14:29:01.591
    STEP: Creating configMap with name cm-test-opt-upd-d37001b2-4f89-4202-a1df-1c3f527793f2 01/23/24 14:29:01.595
    STEP: Creating the pod 01/23/24 14:29:01.598
    Jan 23 14:29:01.624: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e95d9f71-18c5-42ac-b386-4580a4b5b4c6" in namespace "projected-4404" to be "running and ready"
    Jan 23 14:29:01.626: INFO: Pod "pod-projected-configmaps-e95d9f71-18c5-42ac-b386-4580a4b5b4c6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.750105ms
    Jan 23 14:29:01.626: INFO: The phase of Pod pod-projected-configmaps-e95d9f71-18c5-42ac-b386-4580a4b5b4c6 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:29:03.629: INFO: Pod "pod-projected-configmaps-e95d9f71-18c5-42ac-b386-4580a4b5b4c6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004711621s
    Jan 23 14:29:03.629: INFO: The phase of Pod pod-projected-configmaps-e95d9f71-18c5-42ac-b386-4580a4b5b4c6 is Running (Ready = true)
    Jan 23 14:29:03.629: INFO: Pod "pod-projected-configmaps-e95d9f71-18c5-42ac-b386-4580a4b5b4c6" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-e1e8356a-d38b-4ae6-b15f-59b677996313 01/23/24 14:29:03.646
    STEP: Updating configmap cm-test-opt-upd-d37001b2-4f89-4202-a1df-1c3f527793f2 01/23/24 14:29:03.649
    STEP: Creating configMap with name cm-test-opt-create-b4abb30b-217e-475b-b111-5cfd45718388 01/23/24 14:29:03.653
    STEP: waiting to observe update in volume 01/23/24 14:29:03.657
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:29:05.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4404" for this suite. 01/23/24 14:29:05.674
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:29:05.677
Jan 23 14:29:05.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename emptydir 01/23/24 14:29:05.678
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:05.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:05.69
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 01/23/24 14:29:05.692
Jan 23 14:29:05.704: INFO: Waiting up to 5m0s for pod "pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719" in namespace "emptydir-9491" to be "Succeeded or Failed"
Jan 23 14:29:05.707: INFO: Pod "pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719": Phase="Pending", Reason="", readiness=false. Elapsed: 3.156645ms
Jan 23 14:29:07.711: INFO: Pod "pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006929699s
Jan 23 14:29:09.711: INFO: Pod "pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719": Phase="Running", Reason="", readiness=false. Elapsed: 4.007050264s
Jan 23 14:29:11.711: INFO: Pod "pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719": Phase="Running", Reason="", readiness=false. Elapsed: 6.006424822s
Jan 23 14:29:13.710: INFO: Pod "pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.005267814s
STEP: Saw pod success 01/23/24 14:29:13.71
Jan 23 14:29:13.710: INFO: Pod "pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719" satisfied condition "Succeeded or Failed"
Jan 23 14:29:13.711: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719 container test-container: <nil>
STEP: delete the pod 01/23/24 14:29:13.716
Jan 23 14:29:13.722: INFO: Waiting for pod pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719 to disappear
Jan 23 14:29:13.723: INFO: Pod pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 23 14:29:13.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9491" for this suite. 01/23/24 14:29:13.725
------------------------------
• [SLOW TEST] [8.051 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:29:05.677
    Jan 23 14:29:05.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename emptydir 01/23/24 14:29:05.678
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:05.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:05.69
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/23/24 14:29:05.692
    Jan 23 14:29:05.704: INFO: Waiting up to 5m0s for pod "pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719" in namespace "emptydir-9491" to be "Succeeded or Failed"
    Jan 23 14:29:05.707: INFO: Pod "pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719": Phase="Pending", Reason="", readiness=false. Elapsed: 3.156645ms
    Jan 23 14:29:07.711: INFO: Pod "pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006929699s
    Jan 23 14:29:09.711: INFO: Pod "pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719": Phase="Running", Reason="", readiness=false. Elapsed: 4.007050264s
    Jan 23 14:29:11.711: INFO: Pod "pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719": Phase="Running", Reason="", readiness=false. Elapsed: 6.006424822s
    Jan 23 14:29:13.710: INFO: Pod "pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.005267814s
    STEP: Saw pod success 01/23/24 14:29:13.71
    Jan 23 14:29:13.710: INFO: Pod "pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719" satisfied condition "Succeeded or Failed"
    Jan 23 14:29:13.711: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719 container test-container: <nil>
    STEP: delete the pod 01/23/24 14:29:13.716
    Jan 23 14:29:13.722: INFO: Waiting for pod pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719 to disappear
    Jan 23 14:29:13.723: INFO: Pod pod-bbe9f0ed-6048-4018-bee8-4f329bd4a719 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:29:13.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9491" for this suite. 01/23/24 14:29:13.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:29:13.729
Jan 23 14:29:13.729: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 14:29:13.73
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:13.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:13.738
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 01/23/24 14:29:13.741
Jan 23 14:29:13.753: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8bada35f-204a-4457-9149-f9e45c1e7078" in namespace "projected-7495" to be "Succeeded or Failed"
Jan 23 14:29:13.755: INFO: Pod "downwardapi-volume-8bada35f-204a-4457-9149-f9e45c1e7078": Phase="Pending", Reason="", readiness=false. Elapsed: 1.607597ms
Jan 23 14:29:15.758: INFO: Pod "downwardapi-volume-8bada35f-204a-4457-9149-f9e45c1e7078": Phase="Running", Reason="", readiness=false. Elapsed: 2.004610704s
Jan 23 14:29:17.760: INFO: Pod "downwardapi-volume-8bada35f-204a-4457-9149-f9e45c1e7078": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006750458s
STEP: Saw pod success 01/23/24 14:29:17.76
Jan 23 14:29:17.760: INFO: Pod "downwardapi-volume-8bada35f-204a-4457-9149-f9e45c1e7078" satisfied condition "Succeeded or Failed"
Jan 23 14:29:17.762: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-8bada35f-204a-4457-9149-f9e45c1e7078 container client-container: <nil>
STEP: delete the pod 01/23/24 14:29:17.765
Jan 23 14:29:17.772: INFO: Waiting for pod downwardapi-volume-8bada35f-204a-4457-9149-f9e45c1e7078 to disappear
Jan 23 14:29:17.774: INFO: Pod downwardapi-volume-8bada35f-204a-4457-9149-f9e45c1e7078 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 23 14:29:17.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7495" for this suite. 01/23/24 14:29:17.776
------------------------------
• [4.050 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:29:13.729
    Jan 23 14:29:13.729: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 14:29:13.73
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:13.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:13.738
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 01/23/24 14:29:13.741
    Jan 23 14:29:13.753: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8bada35f-204a-4457-9149-f9e45c1e7078" in namespace "projected-7495" to be "Succeeded or Failed"
    Jan 23 14:29:13.755: INFO: Pod "downwardapi-volume-8bada35f-204a-4457-9149-f9e45c1e7078": Phase="Pending", Reason="", readiness=false. Elapsed: 1.607597ms
    Jan 23 14:29:15.758: INFO: Pod "downwardapi-volume-8bada35f-204a-4457-9149-f9e45c1e7078": Phase="Running", Reason="", readiness=false. Elapsed: 2.004610704s
    Jan 23 14:29:17.760: INFO: Pod "downwardapi-volume-8bada35f-204a-4457-9149-f9e45c1e7078": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006750458s
    STEP: Saw pod success 01/23/24 14:29:17.76
    Jan 23 14:29:17.760: INFO: Pod "downwardapi-volume-8bada35f-204a-4457-9149-f9e45c1e7078" satisfied condition "Succeeded or Failed"
    Jan 23 14:29:17.762: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-8bada35f-204a-4457-9149-f9e45c1e7078 container client-container: <nil>
    STEP: delete the pod 01/23/24 14:29:17.765
    Jan 23 14:29:17.772: INFO: Waiting for pod downwardapi-volume-8bada35f-204a-4457-9149-f9e45c1e7078 to disappear
    Jan 23 14:29:17.774: INFO: Pod downwardapi-volume-8bada35f-204a-4457-9149-f9e45c1e7078 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:29:17.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7495" for this suite. 01/23/24 14:29:17.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:29:17.779
Jan 23 14:29:17.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 14:29:17.78
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:17.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:17.789
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-f3eed2a4-751b-4f8d-b10c-3473181247ab 01/23/24 14:29:17.792
STEP: Creating a pod to test consume secrets 01/23/24 14:29:17.796
Jan 23 14:29:17.809: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4736fe66-23c7-4f03-a43c-44e774099e22" in namespace "projected-8015" to be "Succeeded or Failed"
Jan 23 14:29:17.811: INFO: Pod "pod-projected-secrets-4736fe66-23c7-4f03-a43c-44e774099e22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134373ms
Jan 23 14:29:19.814: INFO: Pod "pod-projected-secrets-4736fe66-23c7-4f03-a43c-44e774099e22": Phase="Running", Reason="", readiness=false. Elapsed: 2.005009979s
Jan 23 14:29:21.814: INFO: Pod "pod-projected-secrets-4736fe66-23c7-4f03-a43c-44e774099e22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004967204s
STEP: Saw pod success 01/23/24 14:29:21.814
Jan 23 14:29:21.814: INFO: Pod "pod-projected-secrets-4736fe66-23c7-4f03-a43c-44e774099e22" satisfied condition "Succeeded or Failed"
Jan 23 14:29:21.816: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-secrets-4736fe66-23c7-4f03-a43c-44e774099e22 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/23/24 14:29:21.819
Jan 23 14:29:21.825: INFO: Waiting for pod pod-projected-secrets-4736fe66-23c7-4f03-a43c-44e774099e22 to disappear
Jan 23 14:29:21.826: INFO: Pod pod-projected-secrets-4736fe66-23c7-4f03-a43c-44e774099e22 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 23 14:29:21.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8015" for this suite. 01/23/24 14:29:21.829
------------------------------
• [4.052 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:29:17.779
    Jan 23 14:29:17.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 14:29:17.78
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:17.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:17.789
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-f3eed2a4-751b-4f8d-b10c-3473181247ab 01/23/24 14:29:17.792
    STEP: Creating a pod to test consume secrets 01/23/24 14:29:17.796
    Jan 23 14:29:17.809: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4736fe66-23c7-4f03-a43c-44e774099e22" in namespace "projected-8015" to be "Succeeded or Failed"
    Jan 23 14:29:17.811: INFO: Pod "pod-projected-secrets-4736fe66-23c7-4f03-a43c-44e774099e22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134373ms
    Jan 23 14:29:19.814: INFO: Pod "pod-projected-secrets-4736fe66-23c7-4f03-a43c-44e774099e22": Phase="Running", Reason="", readiness=false. Elapsed: 2.005009979s
    Jan 23 14:29:21.814: INFO: Pod "pod-projected-secrets-4736fe66-23c7-4f03-a43c-44e774099e22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004967204s
    STEP: Saw pod success 01/23/24 14:29:21.814
    Jan 23 14:29:21.814: INFO: Pod "pod-projected-secrets-4736fe66-23c7-4f03-a43c-44e774099e22" satisfied condition "Succeeded or Failed"
    Jan 23 14:29:21.816: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-secrets-4736fe66-23c7-4f03-a43c-44e774099e22 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/23/24 14:29:21.819
    Jan 23 14:29:21.825: INFO: Waiting for pod pod-projected-secrets-4736fe66-23c7-4f03-a43c-44e774099e22 to disappear
    Jan 23 14:29:21.826: INFO: Pod pod-projected-secrets-4736fe66-23c7-4f03-a43c-44e774099e22 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:29:21.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8015" for this suite. 01/23/24 14:29:21.829
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:29:21.832
Jan 23 14:29:21.832: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename disruption 01/23/24 14:29:21.833
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:21.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:21.841
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 01/23/24 14:29:21.845
STEP: Waiting for all pods to be running 01/23/24 14:29:23.877
Jan 23 14:29:23.880: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 23 14:29:25.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1236" for this suite. 01/23/24 14:29:25.886
------------------------------
• [4.057 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:29:21.832
    Jan 23 14:29:21.832: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename disruption 01/23/24 14:29:21.833
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:21.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:21.841
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 01/23/24 14:29:21.845
    STEP: Waiting for all pods to be running 01/23/24 14:29:23.877
    Jan 23 14:29:23.880: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:29:25.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1236" for this suite. 01/23/24 14:29:25.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:29:25.891
Jan 23 14:29:25.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename svcaccounts 01/23/24 14:29:25.892
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:25.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:25.9
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Jan 23 14:29:25.913: INFO: Waiting up to 5m0s for pod "pod-service-account-e4510408-dcb7-4041-b5d7-d1ad3ca40590" in namespace "svcaccounts-4539" to be "running"
Jan 23 14:29:25.916: INFO: Pod "pod-service-account-e4510408-dcb7-4041-b5d7-d1ad3ca40590": Phase="Pending", Reason="", readiness=false. Elapsed: 2.804601ms
Jan 23 14:29:27.919: INFO: Pod "pod-service-account-e4510408-dcb7-4041-b5d7-d1ad3ca40590": Phase="Running", Reason="", readiness=true. Elapsed: 2.006240066s
Jan 23 14:29:27.919: INFO: Pod "pod-service-account-e4510408-dcb7-4041-b5d7-d1ad3ca40590" satisfied condition "running"
STEP: reading a file in the container 01/23/24 14:29:27.919
Jan 23 14:29:27.919: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4539 pod-service-account-e4510408-dcb7-4041-b5d7-d1ad3ca40590 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/23/24 14:29:28.091
Jan 23 14:29:28.092: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4539 pod-service-account-e4510408-dcb7-4041-b5d7-d1ad3ca40590 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/23/24 14:29:28.255
Jan 23 14:29:28.256: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4539 pod-service-account-e4510408-dcb7-4041-b5d7-d1ad3ca40590 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 23 14:29:28.426: INFO: Got root ca configmap in namespace "svcaccounts-4539"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 23 14:29:28.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4539" for this suite. 01/23/24 14:29:28.431
------------------------------
• [2.543 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:29:25.891
    Jan 23 14:29:25.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename svcaccounts 01/23/24 14:29:25.892
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:25.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:25.9
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Jan 23 14:29:25.913: INFO: Waiting up to 5m0s for pod "pod-service-account-e4510408-dcb7-4041-b5d7-d1ad3ca40590" in namespace "svcaccounts-4539" to be "running"
    Jan 23 14:29:25.916: INFO: Pod "pod-service-account-e4510408-dcb7-4041-b5d7-d1ad3ca40590": Phase="Pending", Reason="", readiness=false. Elapsed: 2.804601ms
    Jan 23 14:29:27.919: INFO: Pod "pod-service-account-e4510408-dcb7-4041-b5d7-d1ad3ca40590": Phase="Running", Reason="", readiness=true. Elapsed: 2.006240066s
    Jan 23 14:29:27.919: INFO: Pod "pod-service-account-e4510408-dcb7-4041-b5d7-d1ad3ca40590" satisfied condition "running"
    STEP: reading a file in the container 01/23/24 14:29:27.919
    Jan 23 14:29:27.919: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4539 pod-service-account-e4510408-dcb7-4041-b5d7-d1ad3ca40590 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/23/24 14:29:28.091
    Jan 23 14:29:28.092: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4539 pod-service-account-e4510408-dcb7-4041-b5d7-d1ad3ca40590 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/23/24 14:29:28.255
    Jan 23 14:29:28.256: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4539 pod-service-account-e4510408-dcb7-4041-b5d7-d1ad3ca40590 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan 23 14:29:28.426: INFO: Got root ca configmap in namespace "svcaccounts-4539"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:29:28.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4539" for this suite. 01/23/24 14:29:28.431
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:29:28.434
Jan 23 14:29:28.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename container-runtime 01/23/24 14:29:28.435
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:28.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:28.443
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 01/23/24 14:29:28.445
STEP: wait for the container to reach Failed 01/23/24 14:29:28.458
STEP: get the container status 01/23/24 14:29:32.47
STEP: the container should be terminated 01/23/24 14:29:32.473
STEP: the termination message should be set 01/23/24 14:29:32.473
Jan 23 14:29:32.473: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/23/24 14:29:32.473
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 23 14:29:32.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-709" for this suite. 01/23/24 14:29:32.486
------------------------------
• [4.054 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:29:28.434
    Jan 23 14:29:28.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename container-runtime 01/23/24 14:29:28.435
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:28.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:28.443
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 01/23/24 14:29:28.445
    STEP: wait for the container to reach Failed 01/23/24 14:29:28.458
    STEP: get the container status 01/23/24 14:29:32.47
    STEP: the container should be terminated 01/23/24 14:29:32.473
    STEP: the termination message should be set 01/23/24 14:29:32.473
    Jan 23 14:29:32.473: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/23/24 14:29:32.473
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:29:32.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-709" for this suite. 01/23/24 14:29:32.486
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:29:32.488
Jan 23 14:29:32.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename secrets 01/23/24 14:29:32.489
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:32.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:32.499
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-7190168a-7de7-4161-b5a0-3d3e0ef84daa 01/23/24 14:29:32.5
STEP: Creating a pod to test consume secrets 01/23/24 14:29:32.503
Jan 23 14:29:32.515: INFO: Waiting up to 5m0s for pod "pod-secrets-efc97122-0488-4d57-b72b-3771aae99634" in namespace "secrets-9707" to be "Succeeded or Failed"
Jan 23 14:29:32.516: INFO: Pod "pod-secrets-efc97122-0488-4d57-b72b-3771aae99634": Phase="Pending", Reason="", readiness=false. Elapsed: 1.349057ms
Jan 23 14:29:34.519: INFO: Pod "pod-secrets-efc97122-0488-4d57-b72b-3771aae99634": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003817807s
Jan 23 14:29:36.519: INFO: Pod "pod-secrets-efc97122-0488-4d57-b72b-3771aae99634": Phase="Running", Reason="", readiness=false. Elapsed: 4.004431628s
Jan 23 14:29:38.519: INFO: Pod "pod-secrets-efc97122-0488-4d57-b72b-3771aae99634": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00407669s
STEP: Saw pod success 01/23/24 14:29:38.519
Jan 23 14:29:38.519: INFO: Pod "pod-secrets-efc97122-0488-4d57-b72b-3771aae99634" satisfied condition "Succeeded or Failed"
Jan 23 14:29:38.521: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-secrets-efc97122-0488-4d57-b72b-3771aae99634 container secret-volume-test: <nil>
STEP: delete the pod 01/23/24 14:29:38.525
Jan 23 14:29:38.531: INFO: Waiting for pod pod-secrets-efc97122-0488-4d57-b72b-3771aae99634 to disappear
Jan 23 14:29:38.532: INFO: Pod pod-secrets-efc97122-0488-4d57-b72b-3771aae99634 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 23 14:29:38.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9707" for this suite. 01/23/24 14:29:38.534
------------------------------
• [SLOW TEST] [6.049 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:29:32.488
    Jan 23 14:29:32.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename secrets 01/23/24 14:29:32.489
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:32.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:32.499
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-7190168a-7de7-4161-b5a0-3d3e0ef84daa 01/23/24 14:29:32.5
    STEP: Creating a pod to test consume secrets 01/23/24 14:29:32.503
    Jan 23 14:29:32.515: INFO: Waiting up to 5m0s for pod "pod-secrets-efc97122-0488-4d57-b72b-3771aae99634" in namespace "secrets-9707" to be "Succeeded or Failed"
    Jan 23 14:29:32.516: INFO: Pod "pod-secrets-efc97122-0488-4d57-b72b-3771aae99634": Phase="Pending", Reason="", readiness=false. Elapsed: 1.349057ms
    Jan 23 14:29:34.519: INFO: Pod "pod-secrets-efc97122-0488-4d57-b72b-3771aae99634": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003817807s
    Jan 23 14:29:36.519: INFO: Pod "pod-secrets-efc97122-0488-4d57-b72b-3771aae99634": Phase="Running", Reason="", readiness=false. Elapsed: 4.004431628s
    Jan 23 14:29:38.519: INFO: Pod "pod-secrets-efc97122-0488-4d57-b72b-3771aae99634": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00407669s
    STEP: Saw pod success 01/23/24 14:29:38.519
    Jan 23 14:29:38.519: INFO: Pod "pod-secrets-efc97122-0488-4d57-b72b-3771aae99634" satisfied condition "Succeeded or Failed"
    Jan 23 14:29:38.521: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-secrets-efc97122-0488-4d57-b72b-3771aae99634 container secret-volume-test: <nil>
    STEP: delete the pod 01/23/24 14:29:38.525
    Jan 23 14:29:38.531: INFO: Waiting for pod pod-secrets-efc97122-0488-4d57-b72b-3771aae99634 to disappear
    Jan 23 14:29:38.532: INFO: Pod pod-secrets-efc97122-0488-4d57-b72b-3771aae99634 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:29:38.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9707" for this suite. 01/23/24 14:29:38.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:29:38.539
Jan 23 14:29:38.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename proxy 01/23/24 14:29:38.539
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:38.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:38.549
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan 23 14:29:38.550: INFO: Creating pod...
Jan 23 14:29:38.562: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4892" to be "running"
Jan 23 14:29:38.563: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.313844ms
Jan 23 14:29:40.566: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.00409948s
Jan 23 14:29:40.566: INFO: Pod "agnhost" satisfied condition "running"
Jan 23 14:29:40.566: INFO: Creating service...
Jan 23 14:29:40.571: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/pods/agnhost/proxy/some/path/with/DELETE
Jan 23 14:29:40.574: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 23 14:29:40.574: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/pods/agnhost/proxy/some/path/with/GET
Jan 23 14:29:40.577: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 23 14:29:40.577: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/pods/agnhost/proxy/some/path/with/HEAD
Jan 23 14:29:40.578: INFO: http.Client request:HEAD | StatusCode:200
Jan 23 14:29:40.578: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 23 14:29:40.580: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 23 14:29:40.580: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/pods/agnhost/proxy/some/path/with/PATCH
Jan 23 14:29:40.582: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 23 14:29:40.582: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/pods/agnhost/proxy/some/path/with/POST
Jan 23 14:29:40.583: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 23 14:29:40.583: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/pods/agnhost/proxy/some/path/with/PUT
Jan 23 14:29:40.584: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 23 14:29:40.584: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/services/test-service/proxy/some/path/with/DELETE
Jan 23 14:29:40.587: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 23 14:29:40.587: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/services/test-service/proxy/some/path/with/GET
Jan 23 14:29:40.589: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 23 14:29:40.589: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/services/test-service/proxy/some/path/with/HEAD
Jan 23 14:29:40.591: INFO: http.Client request:HEAD | StatusCode:200
Jan 23 14:29:40.591: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/services/test-service/proxy/some/path/with/OPTIONS
Jan 23 14:29:40.592: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 23 14:29:40.592: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/services/test-service/proxy/some/path/with/PATCH
Jan 23 14:29:40.595: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 23 14:29:40.595: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/services/test-service/proxy/some/path/with/POST
Jan 23 14:29:40.597: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 23 14:29:40.597: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/services/test-service/proxy/some/path/with/PUT
Jan 23 14:29:40.599: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 23 14:29:40.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-4892" for this suite. 01/23/24 14:29:40.601
------------------------------
• [2.068 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:29:38.539
    Jan 23 14:29:38.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename proxy 01/23/24 14:29:38.539
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:38.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:38.549
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan 23 14:29:38.550: INFO: Creating pod...
    Jan 23 14:29:38.562: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4892" to be "running"
    Jan 23 14:29:38.563: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.313844ms
    Jan 23 14:29:40.566: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.00409948s
    Jan 23 14:29:40.566: INFO: Pod "agnhost" satisfied condition "running"
    Jan 23 14:29:40.566: INFO: Creating service...
    Jan 23 14:29:40.571: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/pods/agnhost/proxy/some/path/with/DELETE
    Jan 23 14:29:40.574: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 23 14:29:40.574: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/pods/agnhost/proxy/some/path/with/GET
    Jan 23 14:29:40.577: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 23 14:29:40.577: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/pods/agnhost/proxy/some/path/with/HEAD
    Jan 23 14:29:40.578: INFO: http.Client request:HEAD | StatusCode:200
    Jan 23 14:29:40.578: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan 23 14:29:40.580: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 23 14:29:40.580: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/pods/agnhost/proxy/some/path/with/PATCH
    Jan 23 14:29:40.582: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 23 14:29:40.582: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/pods/agnhost/proxy/some/path/with/POST
    Jan 23 14:29:40.583: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 23 14:29:40.583: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/pods/agnhost/proxy/some/path/with/PUT
    Jan 23 14:29:40.584: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 23 14:29:40.584: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/services/test-service/proxy/some/path/with/DELETE
    Jan 23 14:29:40.587: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 23 14:29:40.587: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/services/test-service/proxy/some/path/with/GET
    Jan 23 14:29:40.589: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 23 14:29:40.589: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/services/test-service/proxy/some/path/with/HEAD
    Jan 23 14:29:40.591: INFO: http.Client request:HEAD | StatusCode:200
    Jan 23 14:29:40.591: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/services/test-service/proxy/some/path/with/OPTIONS
    Jan 23 14:29:40.592: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 23 14:29:40.592: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/services/test-service/proxy/some/path/with/PATCH
    Jan 23 14:29:40.595: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 23 14:29:40.595: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/services/test-service/proxy/some/path/with/POST
    Jan 23 14:29:40.597: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 23 14:29:40.597: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4892/services/test-service/proxy/some/path/with/PUT
    Jan 23 14:29:40.599: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:29:40.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-4892" for this suite. 01/23/24 14:29:40.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:29:40.607
Jan 23 14:29:40.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename endpointslice 01/23/24 14:29:40.608
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:40.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:40.617
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Jan 23 14:29:40.623: INFO: Endpoints addresses: [172.31.11.120] , ports: [6443]
Jan 23 14:29:40.623: INFO: EndpointSlices addresses: [172.31.11.120] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 23 14:29:40.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1717" for this suite. 01/23/24 14:29:40.626
------------------------------
• [0.026 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:29:40.607
    Jan 23 14:29:40.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename endpointslice 01/23/24 14:29:40.608
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:40.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:40.617
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Jan 23 14:29:40.623: INFO: Endpoints addresses: [172.31.11.120] , ports: [6443]
    Jan 23 14:29:40.623: INFO: EndpointSlices addresses: [172.31.11.120] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:29:40.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1717" for this suite. 01/23/24 14:29:40.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:29:40.633
Jan 23 14:29:40.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename container-runtime 01/23/24 14:29:40.634
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:40.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:40.654
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 01/23/24 14:29:40.656
STEP: wait for the container to reach Succeeded 01/23/24 14:29:40.673
STEP: get the container status 01/23/24 14:29:44.689
STEP: the container should be terminated 01/23/24 14:29:44.691
STEP: the termination message should be set 01/23/24 14:29:44.691
Jan 23 14:29:44.691: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/23/24 14:29:44.691
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 23 14:29:44.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6091" for this suite. 01/23/24 14:29:44.701
------------------------------
• [4.071 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:29:40.633
    Jan 23 14:29:40.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename container-runtime 01/23/24 14:29:40.634
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:40.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:40.654
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 01/23/24 14:29:40.656
    STEP: wait for the container to reach Succeeded 01/23/24 14:29:40.673
    STEP: get the container status 01/23/24 14:29:44.689
    STEP: the container should be terminated 01/23/24 14:29:44.691
    STEP: the termination message should be set 01/23/24 14:29:44.691
    Jan 23 14:29:44.691: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/23/24 14:29:44.691
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:29:44.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6091" for this suite. 01/23/24 14:29:44.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:29:44.705
Jan 23 14:29:44.705: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename watch 01/23/24 14:29:44.706
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:44.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:44.714
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/23/24 14:29:44.716
STEP: creating a new configmap 01/23/24 14:29:44.717
STEP: modifying the configmap once 01/23/24 14:29:44.721
STEP: changing the label value of the configmap 01/23/24 14:29:44.727
STEP: Expecting to observe a delete notification for the watched object 01/23/24 14:29:44.734
Jan 23 14:29:44.734: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7435  24b122a0-eaa9-40b9-91c2-a97b446652b5 143469 0 2024-01-23 14:29:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-23 14:29:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 23 14:29:44.734: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7435  24b122a0-eaa9-40b9-91c2-a97b446652b5 143471 0 2024-01-23 14:29:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-23 14:29:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 23 14:29:44.734: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7435  24b122a0-eaa9-40b9-91c2-a97b446652b5 143473 0 2024-01-23 14:29:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-23 14:29:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/23/24 14:29:44.734
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/23/24 14:29:44.74
STEP: changing the label value of the configmap back 01/23/24 14:29:54.741
STEP: modifying the configmap a third time 01/23/24 14:29:54.748
STEP: deleting the configmap 01/23/24 14:29:54.753
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/23/24 14:29:54.757
Jan 23 14:29:54.757: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7435  24b122a0-eaa9-40b9-91c2-a97b446652b5 143562 0 2024-01-23 14:29:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-23 14:29:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 23 14:29:54.757: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7435  24b122a0-eaa9-40b9-91c2-a97b446652b5 143563 0 2024-01-23 14:29:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-23 14:29:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 23 14:29:54.757: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7435  24b122a0-eaa9-40b9-91c2-a97b446652b5 143564 0 2024-01-23 14:29:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-23 14:29:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 23 14:29:54.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7435" for this suite. 01/23/24 14:29:54.759
------------------------------
• [SLOW TEST] [10.057 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:29:44.705
    Jan 23 14:29:44.705: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename watch 01/23/24 14:29:44.706
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:44.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:44.714
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/23/24 14:29:44.716
    STEP: creating a new configmap 01/23/24 14:29:44.717
    STEP: modifying the configmap once 01/23/24 14:29:44.721
    STEP: changing the label value of the configmap 01/23/24 14:29:44.727
    STEP: Expecting to observe a delete notification for the watched object 01/23/24 14:29:44.734
    Jan 23 14:29:44.734: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7435  24b122a0-eaa9-40b9-91c2-a97b446652b5 143469 0 2024-01-23 14:29:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-23 14:29:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 23 14:29:44.734: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7435  24b122a0-eaa9-40b9-91c2-a97b446652b5 143471 0 2024-01-23 14:29:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-23 14:29:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 23 14:29:44.734: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7435  24b122a0-eaa9-40b9-91c2-a97b446652b5 143473 0 2024-01-23 14:29:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-23 14:29:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/23/24 14:29:44.734
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/23/24 14:29:44.74
    STEP: changing the label value of the configmap back 01/23/24 14:29:54.741
    STEP: modifying the configmap a third time 01/23/24 14:29:54.748
    STEP: deleting the configmap 01/23/24 14:29:54.753
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/23/24 14:29:54.757
    Jan 23 14:29:54.757: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7435  24b122a0-eaa9-40b9-91c2-a97b446652b5 143562 0 2024-01-23 14:29:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-23 14:29:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 23 14:29:54.757: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7435  24b122a0-eaa9-40b9-91c2-a97b446652b5 143563 0 2024-01-23 14:29:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-23 14:29:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 23 14:29:54.757: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7435  24b122a0-eaa9-40b9-91c2-a97b446652b5 143564 0 2024-01-23 14:29:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-23 14:29:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:29:54.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7435" for this suite. 01/23/24 14:29:54.759
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:29:54.762
Jan 23 14:29:54.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename resourcequota 01/23/24 14:29:54.763
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:54.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:54.772
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 01/23/24 14:29:54.774
STEP: Creating a ResourceQuota 01/23/24 14:29:59.776
STEP: Ensuring resource quota status is calculated 01/23/24 14:29:59.778
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 23 14:30:01.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3276" for this suite. 01/23/24 14:30:01.785
------------------------------
• [SLOW TEST] [7.025 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:29:54.762
    Jan 23 14:29:54.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename resourcequota 01/23/24 14:29:54.763
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:29:54.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:29:54.772
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 01/23/24 14:29:54.774
    STEP: Creating a ResourceQuota 01/23/24 14:29:59.776
    STEP: Ensuring resource quota status is calculated 01/23/24 14:29:59.778
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:30:01.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3276" for this suite. 01/23/24 14:30:01.785
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:30:01.789
Jan 23 14:30:01.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename secrets 01/23/24 14:30:01.789
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:30:01.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:30:01.798
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-8281538d-02b2-4fc7-b959-a4074947dcbc 01/23/24 14:30:01.811
STEP: Creating a pod to test consume secrets 01/23/24 14:30:01.815
Jan 23 14:30:01.828: INFO: Waiting up to 5m0s for pod "pod-secrets-96a064ca-26b5-4ef3-9b6e-5901cb1dba33" in namespace "secrets-8801" to be "Succeeded or Failed"
Jan 23 14:30:01.830: INFO: Pod "pod-secrets-96a064ca-26b5-4ef3-9b6e-5901cb1dba33": Phase="Pending", Reason="", readiness=false. Elapsed: 1.830068ms
Jan 23 14:30:03.833: INFO: Pod "pod-secrets-96a064ca-26b5-4ef3-9b6e-5901cb1dba33": Phase="Running", Reason="", readiness=false. Elapsed: 2.005215134s
Jan 23 14:30:05.834: INFO: Pod "pod-secrets-96a064ca-26b5-4ef3-9b6e-5901cb1dba33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005553532s
STEP: Saw pod success 01/23/24 14:30:05.834
Jan 23 14:30:05.834: INFO: Pod "pod-secrets-96a064ca-26b5-4ef3-9b6e-5901cb1dba33" satisfied condition "Succeeded or Failed"
Jan 23 14:30:05.836: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-secrets-96a064ca-26b5-4ef3-9b6e-5901cb1dba33 container secret-volume-test: <nil>
STEP: delete the pod 01/23/24 14:30:05.839
Jan 23 14:30:05.848: INFO: Waiting for pod pod-secrets-96a064ca-26b5-4ef3-9b6e-5901cb1dba33 to disappear
Jan 23 14:30:05.849: INFO: Pod pod-secrets-96a064ca-26b5-4ef3-9b6e-5901cb1dba33 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 23 14:30:05.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8801" for this suite. 01/23/24 14:30:05.852
STEP: Destroying namespace "secret-namespace-2963" for this suite. 01/23/24 14:30:05.856
------------------------------
• [4.070 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:30:01.789
    Jan 23 14:30:01.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename secrets 01/23/24 14:30:01.789
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:30:01.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:30:01.798
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-8281538d-02b2-4fc7-b959-a4074947dcbc 01/23/24 14:30:01.811
    STEP: Creating a pod to test consume secrets 01/23/24 14:30:01.815
    Jan 23 14:30:01.828: INFO: Waiting up to 5m0s for pod "pod-secrets-96a064ca-26b5-4ef3-9b6e-5901cb1dba33" in namespace "secrets-8801" to be "Succeeded or Failed"
    Jan 23 14:30:01.830: INFO: Pod "pod-secrets-96a064ca-26b5-4ef3-9b6e-5901cb1dba33": Phase="Pending", Reason="", readiness=false. Elapsed: 1.830068ms
    Jan 23 14:30:03.833: INFO: Pod "pod-secrets-96a064ca-26b5-4ef3-9b6e-5901cb1dba33": Phase="Running", Reason="", readiness=false. Elapsed: 2.005215134s
    Jan 23 14:30:05.834: INFO: Pod "pod-secrets-96a064ca-26b5-4ef3-9b6e-5901cb1dba33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005553532s
    STEP: Saw pod success 01/23/24 14:30:05.834
    Jan 23 14:30:05.834: INFO: Pod "pod-secrets-96a064ca-26b5-4ef3-9b6e-5901cb1dba33" satisfied condition "Succeeded or Failed"
    Jan 23 14:30:05.836: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-secrets-96a064ca-26b5-4ef3-9b6e-5901cb1dba33 container secret-volume-test: <nil>
    STEP: delete the pod 01/23/24 14:30:05.839
    Jan 23 14:30:05.848: INFO: Waiting for pod pod-secrets-96a064ca-26b5-4ef3-9b6e-5901cb1dba33 to disappear
    Jan 23 14:30:05.849: INFO: Pod pod-secrets-96a064ca-26b5-4ef3-9b6e-5901cb1dba33 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:30:05.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8801" for this suite. 01/23/24 14:30:05.852
    STEP: Destroying namespace "secret-namespace-2963" for this suite. 01/23/24 14:30:05.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:30:05.859
Jan 23 14:30:05.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename emptydir 01/23/24 14:30:05.859
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:30:05.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:30:05.869
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/23/24 14:30:05.871
Jan 23 14:30:05.883: INFO: Waiting up to 5m0s for pod "pod-f692a5ba-d1d6-4191-acd0-dbe6e6bbbfc3" in namespace "emptydir-9113" to be "Succeeded or Failed"
Jan 23 14:30:05.884: INFO: Pod "pod-f692a5ba-d1d6-4191-acd0-dbe6e6bbbfc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.310002ms
Jan 23 14:30:07.887: INFO: Pod "pod-f692a5ba-d1d6-4191-acd0-dbe6e6bbbfc3": Phase="Running", Reason="", readiness=false. Elapsed: 2.004324584s
Jan 23 14:30:09.887: INFO: Pod "pod-f692a5ba-d1d6-4191-acd0-dbe6e6bbbfc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004581854s
STEP: Saw pod success 01/23/24 14:30:09.887
Jan 23 14:30:09.887: INFO: Pod "pod-f692a5ba-d1d6-4191-acd0-dbe6e6bbbfc3" satisfied condition "Succeeded or Failed"
Jan 23 14:30:09.889: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-f692a5ba-d1d6-4191-acd0-dbe6e6bbbfc3 container test-container: <nil>
STEP: delete the pod 01/23/24 14:30:09.893
Jan 23 14:30:09.899: INFO: Waiting for pod pod-f692a5ba-d1d6-4191-acd0-dbe6e6bbbfc3 to disappear
Jan 23 14:30:09.901: INFO: Pod pod-f692a5ba-d1d6-4191-acd0-dbe6e6bbbfc3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 23 14:30:09.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9113" for this suite. 01/23/24 14:30:09.903
------------------------------
• [4.047 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:30:05.859
    Jan 23 14:30:05.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename emptydir 01/23/24 14:30:05.859
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:30:05.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:30:05.869
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/23/24 14:30:05.871
    Jan 23 14:30:05.883: INFO: Waiting up to 5m0s for pod "pod-f692a5ba-d1d6-4191-acd0-dbe6e6bbbfc3" in namespace "emptydir-9113" to be "Succeeded or Failed"
    Jan 23 14:30:05.884: INFO: Pod "pod-f692a5ba-d1d6-4191-acd0-dbe6e6bbbfc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.310002ms
    Jan 23 14:30:07.887: INFO: Pod "pod-f692a5ba-d1d6-4191-acd0-dbe6e6bbbfc3": Phase="Running", Reason="", readiness=false. Elapsed: 2.004324584s
    Jan 23 14:30:09.887: INFO: Pod "pod-f692a5ba-d1d6-4191-acd0-dbe6e6bbbfc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004581854s
    STEP: Saw pod success 01/23/24 14:30:09.887
    Jan 23 14:30:09.887: INFO: Pod "pod-f692a5ba-d1d6-4191-acd0-dbe6e6bbbfc3" satisfied condition "Succeeded or Failed"
    Jan 23 14:30:09.889: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-f692a5ba-d1d6-4191-acd0-dbe6e6bbbfc3 container test-container: <nil>
    STEP: delete the pod 01/23/24 14:30:09.893
    Jan 23 14:30:09.899: INFO: Waiting for pod pod-f692a5ba-d1d6-4191-acd0-dbe6e6bbbfc3 to disappear
    Jan 23 14:30:09.901: INFO: Pod pod-f692a5ba-d1d6-4191-acd0-dbe6e6bbbfc3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:30:09.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9113" for this suite. 01/23/24 14:30:09.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:30:09.906
Jan 23 14:30:09.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename cronjob 01/23/24 14:30:09.907
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:30:09.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:30:09.914
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/23/24 14:30:09.916
STEP: Ensuring more than one job is running at a time 01/23/24 14:30:09.919
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/23/24 14:32:01.922
STEP: Removing cronjob 01/23/24 14:32:01.923
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 23 14:32:01.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1632" for this suite. 01/23/24 14:32:01.929
------------------------------
• [SLOW TEST] [112.035 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:30:09.906
    Jan 23 14:30:09.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename cronjob 01/23/24 14:30:09.907
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:30:09.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:30:09.914
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/23/24 14:30:09.916
    STEP: Ensuring more than one job is running at a time 01/23/24 14:30:09.919
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/23/24 14:32:01.922
    STEP: Removing cronjob 01/23/24 14:32:01.923
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:32:01.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1632" for this suite. 01/23/24 14:32:01.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:32:01.942
Jan 23 14:32:01.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename emptydir 01/23/24 14:32:01.943
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:32:01.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:32:01.953
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 01/23/24 14:32:01.957
Jan 23 14:32:01.970: INFO: Waiting up to 5m0s for pod "pod-931ecce5-1735-4e05-b687-77ce2b1e14b7" in namespace "emptydir-4486" to be "Succeeded or Failed"
Jan 23 14:32:01.975: INFO: Pod "pod-931ecce5-1735-4e05-b687-77ce2b1e14b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.164426ms
Jan 23 14:32:03.978: INFO: Pod "pod-931ecce5-1735-4e05-b687-77ce2b1e14b7": Phase="Running", Reason="", readiness=false. Elapsed: 2.008355143s
Jan 23 14:32:05.980: INFO: Pod "pod-931ecce5-1735-4e05-b687-77ce2b1e14b7": Phase="Running", Reason="", readiness=false. Elapsed: 4.009610798s
Jan 23 14:32:07.980: INFO: Pod "pod-931ecce5-1735-4e05-b687-77ce2b1e14b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010096933s
STEP: Saw pod success 01/23/24 14:32:07.98
Jan 23 14:32:07.980: INFO: Pod "pod-931ecce5-1735-4e05-b687-77ce2b1e14b7" satisfied condition "Succeeded or Failed"
Jan 23 14:32:07.982: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-931ecce5-1735-4e05-b687-77ce2b1e14b7 container test-container: <nil>
STEP: delete the pod 01/23/24 14:32:07.991
Jan 23 14:32:07.998: INFO: Waiting for pod pod-931ecce5-1735-4e05-b687-77ce2b1e14b7 to disappear
Jan 23 14:32:08.000: INFO: Pod pod-931ecce5-1735-4e05-b687-77ce2b1e14b7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 23 14:32:08.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4486" for this suite. 01/23/24 14:32:08.003
------------------------------
• [SLOW TEST] [6.064 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:32:01.942
    Jan 23 14:32:01.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename emptydir 01/23/24 14:32:01.943
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:32:01.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:32:01.953
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/23/24 14:32:01.957
    Jan 23 14:32:01.970: INFO: Waiting up to 5m0s for pod "pod-931ecce5-1735-4e05-b687-77ce2b1e14b7" in namespace "emptydir-4486" to be "Succeeded or Failed"
    Jan 23 14:32:01.975: INFO: Pod "pod-931ecce5-1735-4e05-b687-77ce2b1e14b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.164426ms
    Jan 23 14:32:03.978: INFO: Pod "pod-931ecce5-1735-4e05-b687-77ce2b1e14b7": Phase="Running", Reason="", readiness=false. Elapsed: 2.008355143s
    Jan 23 14:32:05.980: INFO: Pod "pod-931ecce5-1735-4e05-b687-77ce2b1e14b7": Phase="Running", Reason="", readiness=false. Elapsed: 4.009610798s
    Jan 23 14:32:07.980: INFO: Pod "pod-931ecce5-1735-4e05-b687-77ce2b1e14b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010096933s
    STEP: Saw pod success 01/23/24 14:32:07.98
    Jan 23 14:32:07.980: INFO: Pod "pod-931ecce5-1735-4e05-b687-77ce2b1e14b7" satisfied condition "Succeeded or Failed"
    Jan 23 14:32:07.982: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-931ecce5-1735-4e05-b687-77ce2b1e14b7 container test-container: <nil>
    STEP: delete the pod 01/23/24 14:32:07.991
    Jan 23 14:32:07.998: INFO: Waiting for pod pod-931ecce5-1735-4e05-b687-77ce2b1e14b7 to disappear
    Jan 23 14:32:08.000: INFO: Pod pod-931ecce5-1735-4e05-b687-77ce2b1e14b7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:32:08.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4486" for this suite. 01/23/24 14:32:08.003
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:32:08.006
Jan 23 14:32:08.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename namespaces 01/23/24 14:32:08.007
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:32:08.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:32:08.016
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 01/23/24 14:32:08.019
Jan 23 14:32:08.021: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/23/24 14:32:08.021
Jan 23 14:32:08.024: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/23/24 14:32:08.024
Jan 23 14:32:08.028: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:32:08.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-821" for this suite. 01/23/24 14:32:08.031
------------------------------
• [0.029 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:32:08.006
    Jan 23 14:32:08.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename namespaces 01/23/24 14:32:08.007
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:32:08.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:32:08.016
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 01/23/24 14:32:08.019
    Jan 23 14:32:08.021: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/23/24 14:32:08.021
    Jan 23 14:32:08.024: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/23/24 14:32:08.024
    Jan 23 14:32:08.028: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:32:08.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-821" for this suite. 01/23/24 14:32:08.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:32:08.036
Jan 23 14:32:08.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 14:32:08.037
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:32:08.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:32:08.045
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 01/23/24 14:32:08.047
Jan 23 14:32:08.063: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ea686278-c7c3-4fa3-9353-51d6f4aef7fb" in namespace "projected-4230" to be "Succeeded or Failed"
Jan 23 14:32:08.064: INFO: Pod "downwardapi-volume-ea686278-c7c3-4fa3-9353-51d6f4aef7fb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.396448ms
Jan 23 14:32:10.067: INFO: Pod "downwardapi-volume-ea686278-c7c3-4fa3-9353-51d6f4aef7fb": Phase="Running", Reason="", readiness=false. Elapsed: 2.003804241s
Jan 23 14:32:12.067: INFO: Pod "downwardapi-volume-ea686278-c7c3-4fa3-9353-51d6f4aef7fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004344151s
STEP: Saw pod success 01/23/24 14:32:12.067
Jan 23 14:32:12.067: INFO: Pod "downwardapi-volume-ea686278-c7c3-4fa3-9353-51d6f4aef7fb" satisfied condition "Succeeded or Failed"
Jan 23 14:32:12.069: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-ea686278-c7c3-4fa3-9353-51d6f4aef7fb container client-container: <nil>
STEP: delete the pod 01/23/24 14:32:12.073
Jan 23 14:32:12.081: INFO: Waiting for pod downwardapi-volume-ea686278-c7c3-4fa3-9353-51d6f4aef7fb to disappear
Jan 23 14:32:12.082: INFO: Pod downwardapi-volume-ea686278-c7c3-4fa3-9353-51d6f4aef7fb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 23 14:32:12.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4230" for this suite. 01/23/24 14:32:12.085
------------------------------
• [4.052 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:32:08.036
    Jan 23 14:32:08.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 14:32:08.037
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:32:08.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:32:08.045
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 01/23/24 14:32:08.047
    Jan 23 14:32:08.063: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ea686278-c7c3-4fa3-9353-51d6f4aef7fb" in namespace "projected-4230" to be "Succeeded or Failed"
    Jan 23 14:32:08.064: INFO: Pod "downwardapi-volume-ea686278-c7c3-4fa3-9353-51d6f4aef7fb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.396448ms
    Jan 23 14:32:10.067: INFO: Pod "downwardapi-volume-ea686278-c7c3-4fa3-9353-51d6f4aef7fb": Phase="Running", Reason="", readiness=false. Elapsed: 2.003804241s
    Jan 23 14:32:12.067: INFO: Pod "downwardapi-volume-ea686278-c7c3-4fa3-9353-51d6f4aef7fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004344151s
    STEP: Saw pod success 01/23/24 14:32:12.067
    Jan 23 14:32:12.067: INFO: Pod "downwardapi-volume-ea686278-c7c3-4fa3-9353-51d6f4aef7fb" satisfied condition "Succeeded or Failed"
    Jan 23 14:32:12.069: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-ea686278-c7c3-4fa3-9353-51d6f4aef7fb container client-container: <nil>
    STEP: delete the pod 01/23/24 14:32:12.073
    Jan 23 14:32:12.081: INFO: Waiting for pod downwardapi-volume-ea686278-c7c3-4fa3-9353-51d6f4aef7fb to disappear
    Jan 23 14:32:12.082: INFO: Pod downwardapi-volume-ea686278-c7c3-4fa3-9353-51d6f4aef7fb no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:32:12.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4230" for this suite. 01/23/24 14:32:12.085
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:32:12.089
Jan 23 14:32:12.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename container-probe 01/23/24 14:32:12.089
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:32:12.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:32:12.098
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d in namespace container-probe-8690 01/23/24 14:32:12.099
Jan 23 14:32:12.112: INFO: Waiting up to 5m0s for pod "liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d" in namespace "container-probe-8690" to be "not pending"
Jan 23 14:32:12.114: INFO: Pod "liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.25437ms
Jan 23 14:32:14.117: INFO: Pod "liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005068196s
Jan 23 14:32:14.117: INFO: Pod "liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d" satisfied condition "not pending"
Jan 23 14:32:14.117: INFO: Started pod liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d in namespace container-probe-8690
STEP: checking the pod's current state and verifying that restartCount is present 01/23/24 14:32:14.117
Jan 23 14:32:14.119: INFO: Initial restart count of pod liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d is 0
Jan 23 14:32:34.156: INFO: Restart count of pod container-probe-8690/liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d is now 1 (20.036873532s elapsed)
Jan 23 14:32:54.188: INFO: Restart count of pod container-probe-8690/liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d is now 2 (40.069757732s elapsed)
Jan 23 14:33:14.222: INFO: Restart count of pod container-probe-8690/liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d is now 3 (1m0.103691634s elapsed)
Jan 23 14:33:34.257: INFO: Restart count of pod container-probe-8690/liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d is now 4 (1m20.138662758s elapsed)
Jan 23 14:34:36.357: INFO: Restart count of pod container-probe-8690/liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d is now 5 (2m22.238575521s elapsed)
STEP: deleting the pod 01/23/24 14:34:36.357
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 23 14:34:36.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8690" for this suite. 01/23/24 14:34:36.365
------------------------------
• [SLOW TEST] [144.279 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:32:12.089
    Jan 23 14:32:12.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename container-probe 01/23/24 14:32:12.089
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:32:12.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:32:12.098
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d in namespace container-probe-8690 01/23/24 14:32:12.099
    Jan 23 14:32:12.112: INFO: Waiting up to 5m0s for pod "liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d" in namespace "container-probe-8690" to be "not pending"
    Jan 23 14:32:12.114: INFO: Pod "liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.25437ms
    Jan 23 14:32:14.117: INFO: Pod "liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005068196s
    Jan 23 14:32:14.117: INFO: Pod "liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d" satisfied condition "not pending"
    Jan 23 14:32:14.117: INFO: Started pod liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d in namespace container-probe-8690
    STEP: checking the pod's current state and verifying that restartCount is present 01/23/24 14:32:14.117
    Jan 23 14:32:14.119: INFO: Initial restart count of pod liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d is 0
    Jan 23 14:32:34.156: INFO: Restart count of pod container-probe-8690/liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d is now 1 (20.036873532s elapsed)
    Jan 23 14:32:54.188: INFO: Restart count of pod container-probe-8690/liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d is now 2 (40.069757732s elapsed)
    Jan 23 14:33:14.222: INFO: Restart count of pod container-probe-8690/liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d is now 3 (1m0.103691634s elapsed)
    Jan 23 14:33:34.257: INFO: Restart count of pod container-probe-8690/liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d is now 4 (1m20.138662758s elapsed)
    Jan 23 14:34:36.357: INFO: Restart count of pod container-probe-8690/liveness-3493b106-f47b-42e3-80b3-2cb5f3582e0d is now 5 (2m22.238575521s elapsed)
    STEP: deleting the pod 01/23/24 14:34:36.357
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:34:36.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8690" for this suite. 01/23/24 14:34:36.365
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:34:36.367
Jan 23 14:34:36.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 14:34:36.368
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:34:36.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:34:36.377
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 01/23/24 14:34:36.379
Jan 23 14:34:36.400: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20a8818f-93d1-4dcc-8fd4-4d492b005777" in namespace "projected-2427" to be "Succeeded or Failed"
Jan 23 14:34:36.402: INFO: Pod "downwardapi-volume-20a8818f-93d1-4dcc-8fd4-4d492b005777": Phase="Pending", Reason="", readiness=false. Elapsed: 1.298918ms
Jan 23 14:34:38.405: INFO: Pod "downwardapi-volume-20a8818f-93d1-4dcc-8fd4-4d492b005777": Phase="Running", Reason="", readiness=false. Elapsed: 2.004223588s
Jan 23 14:34:40.406: INFO: Pod "downwardapi-volume-20a8818f-93d1-4dcc-8fd4-4d492b005777": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005281022s
STEP: Saw pod success 01/23/24 14:34:40.406
Jan 23 14:34:40.406: INFO: Pod "downwardapi-volume-20a8818f-93d1-4dcc-8fd4-4d492b005777" satisfied condition "Succeeded or Failed"
Jan 23 14:34:40.407: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-20a8818f-93d1-4dcc-8fd4-4d492b005777 container client-container: <nil>
STEP: delete the pod 01/23/24 14:34:40.418
Jan 23 14:34:40.427: INFO: Waiting for pod downwardapi-volume-20a8818f-93d1-4dcc-8fd4-4d492b005777 to disappear
Jan 23 14:34:40.428: INFO: Pod downwardapi-volume-20a8818f-93d1-4dcc-8fd4-4d492b005777 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 23 14:34:40.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2427" for this suite. 01/23/24 14:34:40.431
------------------------------
• [4.069 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:34:36.367
    Jan 23 14:34:36.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 14:34:36.368
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:34:36.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:34:36.377
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 01/23/24 14:34:36.379
    Jan 23 14:34:36.400: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20a8818f-93d1-4dcc-8fd4-4d492b005777" in namespace "projected-2427" to be "Succeeded or Failed"
    Jan 23 14:34:36.402: INFO: Pod "downwardapi-volume-20a8818f-93d1-4dcc-8fd4-4d492b005777": Phase="Pending", Reason="", readiness=false. Elapsed: 1.298918ms
    Jan 23 14:34:38.405: INFO: Pod "downwardapi-volume-20a8818f-93d1-4dcc-8fd4-4d492b005777": Phase="Running", Reason="", readiness=false. Elapsed: 2.004223588s
    Jan 23 14:34:40.406: INFO: Pod "downwardapi-volume-20a8818f-93d1-4dcc-8fd4-4d492b005777": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005281022s
    STEP: Saw pod success 01/23/24 14:34:40.406
    Jan 23 14:34:40.406: INFO: Pod "downwardapi-volume-20a8818f-93d1-4dcc-8fd4-4d492b005777" satisfied condition "Succeeded or Failed"
    Jan 23 14:34:40.407: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-20a8818f-93d1-4dcc-8fd4-4d492b005777 container client-container: <nil>
    STEP: delete the pod 01/23/24 14:34:40.418
    Jan 23 14:34:40.427: INFO: Waiting for pod downwardapi-volume-20a8818f-93d1-4dcc-8fd4-4d492b005777 to disappear
    Jan 23 14:34:40.428: INFO: Pod downwardapi-volume-20a8818f-93d1-4dcc-8fd4-4d492b005777 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:34:40.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2427" for this suite. 01/23/24 14:34:40.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:34:40.438
Jan 23 14:34:40.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename gc 01/23/24 14:34:40.438
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:34:40.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:34:40.448
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/23/24 14:34:40.454
STEP: delete the rc 01/23/24 14:34:45.47
STEP: wait for the rc to be deleted 01/23/24 14:34:45.484
Jan 23 14:34:46.493: INFO: 80 pods remaining
Jan 23 14:34:46.493: INFO: 80 pods has nil DeletionTimestamp
Jan 23 14:34:46.493: INFO: 
Jan 23 14:34:47.491: INFO: 72 pods remaining
Jan 23 14:34:47.491: INFO: 72 pods has nil DeletionTimestamp
Jan 23 14:34:47.491: INFO: 
Jan 23 14:34:48.491: INFO: 60 pods remaining
Jan 23 14:34:48.491: INFO: 60 pods has nil DeletionTimestamp
Jan 23 14:34:48.491: INFO: 
Jan 23 14:34:49.490: INFO: 40 pods remaining
Jan 23 14:34:49.490: INFO: 40 pods has nil DeletionTimestamp
Jan 23 14:34:49.490: INFO: 
Jan 23 14:34:50.490: INFO: 32 pods remaining
Jan 23 14:34:50.490: INFO: 32 pods has nil DeletionTimestamp
Jan 23 14:34:50.490: INFO: 
Jan 23 14:34:51.489: INFO: 20 pods remaining
Jan 23 14:34:51.489: INFO: 20 pods has nil DeletionTimestamp
Jan 23 14:34:51.489: INFO: 
STEP: Gathering metrics 01/23/24 14:34:52.488
Jan 23 14:34:52.508: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" in namespace "kube-system" to be "running and ready"
Jan 23 14:34:52.510: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local": Phase="Running", Reason="", readiness=true. Elapsed: 1.717446ms
Jan 23 14:34:52.510: INFO: The phase of Pod kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local is Running (Ready = true)
Jan 23 14:34:52.510: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" satisfied condition "running and ready"
Jan 23 14:34:52.555: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 23 14:34:52.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9057" for this suite. 01/23/24 14:34:52.557
------------------------------
• [SLOW TEST] [12.123 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:34:40.438
    Jan 23 14:34:40.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename gc 01/23/24 14:34:40.438
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:34:40.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:34:40.448
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/23/24 14:34:40.454
    STEP: delete the rc 01/23/24 14:34:45.47
    STEP: wait for the rc to be deleted 01/23/24 14:34:45.484
    Jan 23 14:34:46.493: INFO: 80 pods remaining
    Jan 23 14:34:46.493: INFO: 80 pods has nil DeletionTimestamp
    Jan 23 14:34:46.493: INFO: 
    Jan 23 14:34:47.491: INFO: 72 pods remaining
    Jan 23 14:34:47.491: INFO: 72 pods has nil DeletionTimestamp
    Jan 23 14:34:47.491: INFO: 
    Jan 23 14:34:48.491: INFO: 60 pods remaining
    Jan 23 14:34:48.491: INFO: 60 pods has nil DeletionTimestamp
    Jan 23 14:34:48.491: INFO: 
    Jan 23 14:34:49.490: INFO: 40 pods remaining
    Jan 23 14:34:49.490: INFO: 40 pods has nil DeletionTimestamp
    Jan 23 14:34:49.490: INFO: 
    Jan 23 14:34:50.490: INFO: 32 pods remaining
    Jan 23 14:34:50.490: INFO: 32 pods has nil DeletionTimestamp
    Jan 23 14:34:50.490: INFO: 
    Jan 23 14:34:51.489: INFO: 20 pods remaining
    Jan 23 14:34:51.489: INFO: 20 pods has nil DeletionTimestamp
    Jan 23 14:34:51.489: INFO: 
    STEP: Gathering metrics 01/23/24 14:34:52.488
    Jan 23 14:34:52.508: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" in namespace "kube-system" to be "running and ready"
    Jan 23 14:34:52.510: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local": Phase="Running", Reason="", readiness=true. Elapsed: 1.717446ms
    Jan 23 14:34:52.510: INFO: The phase of Pod kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local is Running (Ready = true)
    Jan 23 14:34:52.510: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" satisfied condition "running and ready"
    Jan 23 14:34:52.555: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:34:52.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9057" for this suite. 01/23/24 14:34:52.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:34:52.568
Jan 23 14:34:52.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename watch 01/23/24 14:34:52.569
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:34:52.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:34:52.578
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/23/24 14:34:52.579
STEP: modifying the configmap once 01/23/24 14:34:52.583
STEP: modifying the configmap a second time 01/23/24 14:34:52.589
STEP: deleting the configmap 01/23/24 14:34:52.596
STEP: creating a watch on configmaps from the resource version returned by the first update 01/23/24 14:34:52.599
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/23/24 14:34:52.6
Jan 23 14:34:52.600: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5657  03e654cc-c732-41aa-8eaf-e95e7a2b972a 146168 0 2024-01-23 14:34:52 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2024-01-23 14:34:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 23 14:34:52.600: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5657  03e654cc-c732-41aa-8eaf-e95e7a2b972a 146169 0 2024-01-23 14:34:52 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2024-01-23 14:34:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 23 14:34:52.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5657" for this suite. 01/23/24 14:34:52.602
------------------------------
• [0.037 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:34:52.568
    Jan 23 14:34:52.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename watch 01/23/24 14:34:52.569
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:34:52.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:34:52.578
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/23/24 14:34:52.579
    STEP: modifying the configmap once 01/23/24 14:34:52.583
    STEP: modifying the configmap a second time 01/23/24 14:34:52.589
    STEP: deleting the configmap 01/23/24 14:34:52.596
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/23/24 14:34:52.599
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/23/24 14:34:52.6
    Jan 23 14:34:52.600: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5657  03e654cc-c732-41aa-8eaf-e95e7a2b972a 146168 0 2024-01-23 14:34:52 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2024-01-23 14:34:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 23 14:34:52.600: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5657  03e654cc-c732-41aa-8eaf-e95e7a2b972a 146169 0 2024-01-23 14:34:52 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2024-01-23 14:34:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:34:52.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5657" for this suite. 01/23/24 14:34:52.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:34:52.606
Jan 23 14:34:52.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename replicaset 01/23/24 14:34:52.607
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:34:52.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:34:52.614
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan 23 14:34:52.616: INFO: Creating ReplicaSet my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11
Jan 23 14:34:52.620: INFO: Pod name my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11: Found 0 pods out of 1
Jan 23 14:34:57.622: INFO: Pod name my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11: Found 1 pods out of 1
Jan 23 14:34:57.623: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11" is running
Jan 23 14:34:57.623: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm" in namespace "replicaset-4571" to be "running"
Jan 23 14:34:57.625: INFO: Pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.237934ms
Jan 23 14:34:59.627: INFO: Pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004186691s
Jan 23 14:35:01.627: INFO: Pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004397934s
Jan 23 14:35:03.628: INFO: Pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005590851s
Jan 23 14:35:05.627: INFO: Pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004638566s
Jan 23 14:35:07.629: INFO: Pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006465684s
Jan 23 14:35:09.627: INFO: Pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm": Phase="Running", Reason="", readiness=true. Elapsed: 12.004122971s
Jan 23 14:35:09.627: INFO: Pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm" satisfied condition "running"
Jan 23 14:35:09.627: INFO: Pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-23 14:34:52 +0000 UTC Reason: Message:}])
Jan 23 14:35:09.627: INFO: Trying to dial the pod
Jan 23 14:35:14.635: INFO: Controller my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11: Got expected result from replica 1 [my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm]: "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 23 14:35:14.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4571" for this suite. 01/23/24 14:35:14.637
------------------------------
• [SLOW TEST] [22.035 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:34:52.606
    Jan 23 14:34:52.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename replicaset 01/23/24 14:34:52.607
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:34:52.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:34:52.614
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan 23 14:34:52.616: INFO: Creating ReplicaSet my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11
    Jan 23 14:34:52.620: INFO: Pod name my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11: Found 0 pods out of 1
    Jan 23 14:34:57.622: INFO: Pod name my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11: Found 1 pods out of 1
    Jan 23 14:34:57.623: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11" is running
    Jan 23 14:34:57.623: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm" in namespace "replicaset-4571" to be "running"
    Jan 23 14:34:57.625: INFO: Pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.237934ms
    Jan 23 14:34:59.627: INFO: Pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004186691s
    Jan 23 14:35:01.627: INFO: Pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004397934s
    Jan 23 14:35:03.628: INFO: Pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005590851s
    Jan 23 14:35:05.627: INFO: Pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004638566s
    Jan 23 14:35:07.629: INFO: Pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006465684s
    Jan 23 14:35:09.627: INFO: Pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm": Phase="Running", Reason="", readiness=true. Elapsed: 12.004122971s
    Jan 23 14:35:09.627: INFO: Pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm" satisfied condition "running"
    Jan 23 14:35:09.627: INFO: Pod "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-23 14:34:52 +0000 UTC Reason: Message:}])
    Jan 23 14:35:09.627: INFO: Trying to dial the pod
    Jan 23 14:35:14.635: INFO: Controller my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11: Got expected result from replica 1 [my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm]: "my-hostname-basic-a0d60d71-99bb-4d03-8329-12865995dd11-sq5mm", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:35:14.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4571" for this suite. 01/23/24 14:35:14.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:35:14.641
Jan 23 14:35:14.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename configmap 01/23/24 14:35:14.642
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:35:14.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:35:14.651
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-7ecad0c7-a226-47d8-bc51-c1d34674e6e7 01/23/24 14:35:14.653
STEP: Creating a pod to test consume configMaps 01/23/24 14:35:14.656
Jan 23 14:35:14.667: INFO: Waiting up to 5m0s for pod "pod-configmaps-7661a3d0-6871-4509-b594-0d3c69e57188" in namespace "configmap-9479" to be "Succeeded or Failed"
Jan 23 14:35:14.669: INFO: Pod "pod-configmaps-7661a3d0-6871-4509-b594-0d3c69e57188": Phase="Pending", Reason="", readiness=false. Elapsed: 1.943739ms
Jan 23 14:35:16.672: INFO: Pod "pod-configmaps-7661a3d0-6871-4509-b594-0d3c69e57188": Phase="Running", Reason="", readiness=true. Elapsed: 2.004403584s
Jan 23 14:35:18.673: INFO: Pod "pod-configmaps-7661a3d0-6871-4509-b594-0d3c69e57188": Phase="Running", Reason="", readiness=false. Elapsed: 4.005986598s
Jan 23 14:35:20.672: INFO: Pod "pod-configmaps-7661a3d0-6871-4509-b594-0d3c69e57188": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004922353s
STEP: Saw pod success 01/23/24 14:35:20.672
Jan 23 14:35:20.673: INFO: Pod "pod-configmaps-7661a3d0-6871-4509-b594-0d3c69e57188" satisfied condition "Succeeded or Failed"
Jan 23 14:35:20.674: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-7661a3d0-6871-4509-b594-0d3c69e57188 container agnhost-container: <nil>
STEP: delete the pod 01/23/24 14:35:20.678
Jan 23 14:35:20.685: INFO: Waiting for pod pod-configmaps-7661a3d0-6871-4509-b594-0d3c69e57188 to disappear
Jan 23 14:35:20.686: INFO: Pod pod-configmaps-7661a3d0-6871-4509-b594-0d3c69e57188 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 23 14:35:20.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9479" for this suite. 01/23/24 14:35:20.689
------------------------------
• [SLOW TEST] [6.050 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:35:14.641
    Jan 23 14:35:14.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename configmap 01/23/24 14:35:14.642
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:35:14.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:35:14.651
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-7ecad0c7-a226-47d8-bc51-c1d34674e6e7 01/23/24 14:35:14.653
    STEP: Creating a pod to test consume configMaps 01/23/24 14:35:14.656
    Jan 23 14:35:14.667: INFO: Waiting up to 5m0s for pod "pod-configmaps-7661a3d0-6871-4509-b594-0d3c69e57188" in namespace "configmap-9479" to be "Succeeded or Failed"
    Jan 23 14:35:14.669: INFO: Pod "pod-configmaps-7661a3d0-6871-4509-b594-0d3c69e57188": Phase="Pending", Reason="", readiness=false. Elapsed: 1.943739ms
    Jan 23 14:35:16.672: INFO: Pod "pod-configmaps-7661a3d0-6871-4509-b594-0d3c69e57188": Phase="Running", Reason="", readiness=true. Elapsed: 2.004403584s
    Jan 23 14:35:18.673: INFO: Pod "pod-configmaps-7661a3d0-6871-4509-b594-0d3c69e57188": Phase="Running", Reason="", readiness=false. Elapsed: 4.005986598s
    Jan 23 14:35:20.672: INFO: Pod "pod-configmaps-7661a3d0-6871-4509-b594-0d3c69e57188": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004922353s
    STEP: Saw pod success 01/23/24 14:35:20.672
    Jan 23 14:35:20.673: INFO: Pod "pod-configmaps-7661a3d0-6871-4509-b594-0d3c69e57188" satisfied condition "Succeeded or Failed"
    Jan 23 14:35:20.674: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-7661a3d0-6871-4509-b594-0d3c69e57188 container agnhost-container: <nil>
    STEP: delete the pod 01/23/24 14:35:20.678
    Jan 23 14:35:20.685: INFO: Waiting for pod pod-configmaps-7661a3d0-6871-4509-b594-0d3c69e57188 to disappear
    Jan 23 14:35:20.686: INFO: Pod pod-configmaps-7661a3d0-6871-4509-b594-0d3c69e57188 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:35:20.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9479" for this suite. 01/23/24 14:35:20.689
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:35:20.691
Jan 23 14:35:20.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubectl 01/23/24 14:35:20.692
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:35:20.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:35:20.7
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Jan 23 14:35:20.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4565 create -f -'
Jan 23 14:35:22.470: INFO: stderr: ""
Jan 23 14:35:22.470: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 23 14:35:22.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4565 create -f -'
Jan 23 14:35:22.809: INFO: stderr: ""
Jan 23 14:35:22.809: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/23/24 14:35:22.809
Jan 23 14:35:23.812: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 14:35:23.812: INFO: Found 0 / 1
Jan 23 14:35:24.812: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 14:35:24.812: INFO: Found 1 / 1
Jan 23 14:35:24.812: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 23 14:35:24.813: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 14:35:24.813: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 23 14:35:24.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4565 describe pod agnhost-primary-gsxhd'
Jan 23 14:35:24.976: INFO: stderr: ""
Jan 23 14:35:24.976: INFO: stdout: "Name:                 agnhost-primary-gsxhd\nNamespace:            kubectl-4565\nPriority:             1000000000\nPriority Class Name:  nova-user-critical\nService Account:      default\nNode:                 node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local/172.31.11.67\nStart Time:           Tue, 23 Jan 2024 14:35:22 +0000\nLabels:               app=agnhost\n                      role=primary\nAnnotations:          cni.projectcalico.org/containerID: 76c508cb851b8798d6a5bcf3ac914e56e71812214db13dec313409cad681739a\n                      cni.projectcalico.org/podIP: 10.233.87.100/32\n                      cni.projectcalico.org/podIPs: 10.233.87.100/32\nStatus:               Running\nIP:                   10.233.87.100\nIPs:\n  IP:           10.233.87.100\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://2950d60ced1d25f4325749c93221d950148032a3cc22708e50f9090d1d0e96bc\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 23 Jan 2024 14:35:23 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rxt4b (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-rxt4b:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-4565/agnhost-primary-gsxhd to node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jan 23 14:35:24.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4565 describe rc agnhost-primary'
Jan 23 14:35:25.123: INFO: stderr: ""
Jan 23 14:35:25.123: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4565\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-gsxhd\n"
Jan 23 14:35:25.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4565 describe service agnhost-primary'
Jan 23 14:35:25.267: INFO: stderr: ""
Jan 23 14:35:25.267: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4565\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.37.165\nIPs:               10.233.37.165\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.87.100:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 23 14:35:25.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4565 describe node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local'
Jan 23 14:35:25.439: INFO: stderr: ""
Jan 23 14:35:25.439: INFO: stdout: "Name:               node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"secrets-store.csi.k8s.io\":\"node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.11.120/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.233.122.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 23 Jan 2024 09:01:54 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 23 Jan 2024 14:35:24 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 23 Jan 2024 13:15:56 +0000   Tue, 23 Jan 2024 13:15:56 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 23 Jan 2024 14:33:07 +0000   Tue, 23 Jan 2024 09:01:53 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 23 Jan 2024 14:33:07 +0000   Tue, 23 Jan 2024 09:01:53 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 23 Jan 2024 14:33:07 +0000   Tue, 23 Jan 2024 09:01:53 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 23 Jan 2024 14:33:07 +0000   Tue, 23 Jan 2024 09:02:25 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.31.11.120\n  Hostname:    node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local\nCapacity:\n  cpu:                6\n  ephemeral-storage:  33543148Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             24665748Ki\n  pods:               110\nAllocatable:\n  cpu:                6\n  ephemeral-storage:  30913365146\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             24563348Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 62c09258e0c94f6190d3fe62db836233\n  System UUID:                a09f5d73-2f15-4565-9207-e0c354535913\n  Boot ID:                    619befcc-8069-464a-8553-fb6e1fa132c3\n  Kernel Version:             4.18.0-348.20.1.el8_5.x86_64\n  OS Image:                   AlmaLinux 8.5 (Arctic Sphynx)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.18\n  Kubelet Version:            v1.26.8\n  Kube-Proxy Version:         v1.26.8\nPodCIDR:                      10.233.65.0/24\nPodCIDRs:                     10.233.65.0/24\nNon-terminated Pods:          (18 in total)\n  Namespace                   Name                                                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                                    ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-hg74l                                                       250m (4%)     0 (0%)      0 (0%)           0 (0%)         5h33m\n  kube-system                 kube-apiserver-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local             250m (4%)     0 (0%)      0 (0%)           0 (0%)         5h33m\n  kube-system                 kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local    200m (3%)     0 (0%)      0 (0%)           0 (0%)         5h33m\n  kube-system                 kube-proxy-kzcgv                                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h33m\n  kube-system                 kube-scheduler-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local             100m (1%)     0 (0%)      0 (0%)           0 (0%)         5h33m\n  kube-system                 nginx-proxy-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local                25m (0%)      0 (0%)      32M (0%)         0 (0%)         5h32m\n  kube-system                 nova-vpa-admission-controller-7f5d794944-pf4h7                          50m (0%)      0 (0%)      100Mi (0%)       0 (0%)         5h28m\n  kube-system                 nova-vpa-recommender-7c476fb969-k7h7p                                   50m (0%)      0 (0%)      100Mi (0%)       0 (0%)         5h28m\n  kube-system                 nova-vpa-updater-f7d87cc-4p8rs                                          50m (0%)      0 (0%)      100Mi (0%)       0 (0%)         5h28m\n  kube-system                 snapshot-controller-fdfdc7f78-446jn                                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         5h27m\n  kube-system                 snapshot-validation-deployment-5f6d968b9d-jb5lw                         10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         5h27m\n  nova-csi-drivers            nova-oauth-csi-provider-6l87z                                           50m (0%)      50m (0%)    128Mi (0%)       128Mi (0%)     5h31m\n  nova-csi-drivers            nova-secrets-store-csi-driver-8rbbk                                     70m (1%)      400m (6%)   140Mi (0%)       400Mi (1%)     5h31m\n  nova-monitoring             nova-cadvisor-zcbcq                                                     100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         5h25m\n  nova-monitoring             nova-prometheus-node-exporter-rsnnk                                     10m (0%)      0 (0%)      32Mi (0%)        0 (0%)         5h26m\n  nova-secrets-webhook        nova-oauth-secrets-webhook-l6n5x                                        50m (0%)      0 (0%)      64Mi (0%)        0 (0%)         5h31m\n  sonobuoy                    sonobuoy-e2e-job-2130fd0f13cf43d3                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         74m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-tdl7n                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         74m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                1275m (21%)     450m (7%)\n  memory             906512384 (3%)  528Mi (2%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-1Gi      0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:              <none>\n"
Jan 23 14:35:25.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4565 describe namespace kubectl-4565'
Jan 23 14:35:25.586: INFO: stderr: ""
Jan 23 14:35:25.586: INFO: stdout: "Name:         kubectl-4565\nLabels:       e2e-framework=kubectl\n              e2e-run=13937306-003f-4b12-8d30-d166651e4e5b\n              kubernetes.io/metadata.name=kubectl-4565\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 23 14:35:25.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4565" for this suite. 01/23/24 14:35:25.588
------------------------------
• [4.900 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:35:20.691
    Jan 23 14:35:20.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubectl 01/23/24 14:35:20.692
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:35:20.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:35:20.7
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Jan 23 14:35:20.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4565 create -f -'
    Jan 23 14:35:22.470: INFO: stderr: ""
    Jan 23 14:35:22.470: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan 23 14:35:22.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4565 create -f -'
    Jan 23 14:35:22.809: INFO: stderr: ""
    Jan 23 14:35:22.809: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/23/24 14:35:22.809
    Jan 23 14:35:23.812: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 23 14:35:23.812: INFO: Found 0 / 1
    Jan 23 14:35:24.812: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 23 14:35:24.812: INFO: Found 1 / 1
    Jan 23 14:35:24.812: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 23 14:35:24.813: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 23 14:35:24.813: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 23 14:35:24.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4565 describe pod agnhost-primary-gsxhd'
    Jan 23 14:35:24.976: INFO: stderr: ""
    Jan 23 14:35:24.976: INFO: stdout: "Name:                 agnhost-primary-gsxhd\nNamespace:            kubectl-4565\nPriority:             1000000000\nPriority Class Name:  nova-user-critical\nService Account:      default\nNode:                 node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local/172.31.11.67\nStart Time:           Tue, 23 Jan 2024 14:35:22 +0000\nLabels:               app=agnhost\n                      role=primary\nAnnotations:          cni.projectcalico.org/containerID: 76c508cb851b8798d6a5bcf3ac914e56e71812214db13dec313409cad681739a\n                      cni.projectcalico.org/podIP: 10.233.87.100/32\n                      cni.projectcalico.org/podIPs: 10.233.87.100/32\nStatus:               Running\nIP:                   10.233.87.100\nIPs:\n  IP:           10.233.87.100\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://2950d60ced1d25f4325749c93221d950148032a3cc22708e50f9090d1d0e96bc\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 23 Jan 2024 14:35:23 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rxt4b (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-rxt4b:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-4565/agnhost-primary-gsxhd to node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Jan 23 14:35:24.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4565 describe rc agnhost-primary'
    Jan 23 14:35:25.123: INFO: stderr: ""
    Jan 23 14:35:25.123: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4565\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-gsxhd\n"
    Jan 23 14:35:25.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4565 describe service agnhost-primary'
    Jan 23 14:35:25.267: INFO: stderr: ""
    Jan 23 14:35:25.267: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4565\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.37.165\nIPs:               10.233.37.165\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.87.100:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan 23 14:35:25.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4565 describe node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local'
    Jan 23 14:35:25.439: INFO: stderr: ""
    Jan 23 14:35:25.439: INFO: stdout: "Name:               node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"secrets-store.csi.k8s.io\":\"node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.11.120/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.233.122.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 23 Jan 2024 09:01:54 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 23 Jan 2024 14:35:24 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 23 Jan 2024 13:15:56 +0000   Tue, 23 Jan 2024 13:15:56 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 23 Jan 2024 14:33:07 +0000   Tue, 23 Jan 2024 09:01:53 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 23 Jan 2024 14:33:07 +0000   Tue, 23 Jan 2024 09:01:53 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 23 Jan 2024 14:33:07 +0000   Tue, 23 Jan 2024 09:01:53 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 23 Jan 2024 14:33:07 +0000   Tue, 23 Jan 2024 09:02:25 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.31.11.120\n  Hostname:    node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local\nCapacity:\n  cpu:                6\n  ephemeral-storage:  33543148Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             24665748Ki\n  pods:               110\nAllocatable:\n  cpu:                6\n  ephemeral-storage:  30913365146\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             24563348Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 62c09258e0c94f6190d3fe62db836233\n  System UUID:                a09f5d73-2f15-4565-9207-e0c354535913\n  Boot ID:                    619befcc-8069-464a-8553-fb6e1fa132c3\n  Kernel Version:             4.18.0-348.20.1.el8_5.x86_64\n  OS Image:                   AlmaLinux 8.5 (Arctic Sphynx)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.18\n  Kubelet Version:            v1.26.8\n  Kube-Proxy Version:         v1.26.8\nPodCIDR:                      10.233.65.0/24\nPodCIDRs:                     10.233.65.0/24\nNon-terminated Pods:          (18 in total)\n  Namespace                   Name                                                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                                    ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-hg74l                                                       250m (4%)     0 (0%)      0 (0%)           0 (0%)         5h33m\n  kube-system                 kube-apiserver-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local             250m (4%)     0 (0%)      0 (0%)           0 (0%)         5h33m\n  kube-system                 kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local    200m (3%)     0 (0%)      0 (0%)           0 (0%)         5h33m\n  kube-system                 kube-proxy-kzcgv                                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h33m\n  kube-system                 kube-scheduler-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local             100m (1%)     0 (0%)      0 (0%)           0 (0%)         5h33m\n  kube-system                 nginx-proxy-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local                25m (0%)      0 (0%)      32M (0%)         0 (0%)         5h32m\n  kube-system                 nova-vpa-admission-controller-7f5d794944-pf4h7                          50m (0%)      0 (0%)      100Mi (0%)       0 (0%)         5h28m\n  kube-system                 nova-vpa-recommender-7c476fb969-k7h7p                                   50m (0%)      0 (0%)      100Mi (0%)       0 (0%)         5h28m\n  kube-system                 nova-vpa-updater-f7d87cc-4p8rs                                          50m (0%)      0 (0%)      100Mi (0%)       0 (0%)         5h28m\n  kube-system                 snapshot-controller-fdfdc7f78-446jn                                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         5h27m\n  kube-system                 snapshot-validation-deployment-5f6d968b9d-jb5lw                         10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         5h27m\n  nova-csi-drivers            nova-oauth-csi-provider-6l87z                                           50m (0%)      50m (0%)    128Mi (0%)       128Mi (0%)     5h31m\n  nova-csi-drivers            nova-secrets-store-csi-driver-8rbbk                                     70m (1%)      400m (6%)   140Mi (0%)       400Mi (1%)     5h31m\n  nova-monitoring             nova-cadvisor-zcbcq                                                     100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         5h25m\n  nova-monitoring             nova-prometheus-node-exporter-rsnnk                                     10m (0%)      0 (0%)      32Mi (0%)        0 (0%)         5h26m\n  nova-secrets-webhook        nova-oauth-secrets-webhook-l6n5x                                        50m (0%)      0 (0%)      64Mi (0%)        0 (0%)         5h31m\n  sonobuoy                    sonobuoy-e2e-job-2130fd0f13cf43d3                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         74m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-tdl7n                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         74m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                1275m (21%)     450m (7%)\n  memory             906512384 (3%)  528Mi (2%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-1Gi      0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:              <none>\n"
    Jan 23 14:35:25.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-4565 describe namespace kubectl-4565'
    Jan 23 14:35:25.586: INFO: stderr: ""
    Jan 23 14:35:25.586: INFO: stdout: "Name:         kubectl-4565\nLabels:       e2e-framework=kubectl\n              e2e-run=13937306-003f-4b12-8d30-d166651e4e5b\n              kubernetes.io/metadata.name=kubectl-4565\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:35:25.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4565" for this suite. 01/23/24 14:35:25.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:35:25.592
Jan 23 14:35:25.592: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubectl 01/23/24 14:35:25.592
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:35:25.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:35:25.601
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 01/23/24 14:35:25.602
Jan 23 14:35:25.602: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 23 14:35:25.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 create -f -'
Jan 23 14:35:25.950: INFO: stderr: ""
Jan 23 14:35:25.950: INFO: stdout: "service/agnhost-replica created\n"
Jan 23 14:35:25.950: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 23 14:35:25.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 create -f -'
Jan 23 14:35:26.303: INFO: stderr: ""
Jan 23 14:35:26.303: INFO: stdout: "service/agnhost-primary created\n"
Jan 23 14:35:26.303: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 23 14:35:26.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 create -f -'
Jan 23 14:35:26.653: INFO: stderr: ""
Jan 23 14:35:26.654: INFO: stdout: "service/frontend created\n"
Jan 23 14:35:26.654: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 23 14:35:26.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 create -f -'
Jan 23 14:35:26.988: INFO: stderr: ""
Jan 23 14:35:26.988: INFO: stdout: "deployment.apps/frontend created\n"
Jan 23 14:35:26.988: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 23 14:35:26.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 create -f -'
Jan 23 14:35:27.351: INFO: stderr: ""
Jan 23 14:35:27.351: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 23 14:35:27.351: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 23 14:35:27.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 create -f -'
Jan 23 14:35:27.698: INFO: stderr: ""
Jan 23 14:35:27.698: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/23/24 14:35:27.698
Jan 23 14:35:27.698: INFO: Waiting for all frontend pods to be Running.
Jan 23 14:35:32.749: INFO: Waiting for frontend to serve content.
Jan 23 14:35:32.755: INFO: Trying to add a new entry to the guestbook.
Jan 23 14:35:32.761: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 01/23/24 14:35:32.766
Jan 23 14:35:32.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 delete --grace-period=0 --force -f -'
Jan 23 14:35:32.857: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 14:35:32.857: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/23/24 14:35:32.857
Jan 23 14:35:32.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 delete --grace-period=0 --force -f -'
Jan 23 14:35:32.956: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 14:35:32.956: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/23/24 14:35:32.956
Jan 23 14:35:32.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 delete --grace-period=0 --force -f -'
Jan 23 14:35:33.050: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 14:35:33.050: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/23/24 14:35:33.051
Jan 23 14:35:33.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 delete --grace-period=0 --force -f -'
Jan 23 14:35:33.139: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 14:35:33.139: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/23/24 14:35:33.139
Jan 23 14:35:33.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 delete --grace-period=0 --force -f -'
Jan 23 14:35:33.237: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 14:35:33.237: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/23/24 14:35:33.237
Jan 23 14:35:33.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 delete --grace-period=0 --force -f -'
Jan 23 14:35:33.327: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 14:35:33.327: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 23 14:35:33.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3667" for this suite. 01/23/24 14:35:33.33
------------------------------
• [SLOW TEST] [7.743 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:35:25.592
    Jan 23 14:35:25.592: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubectl 01/23/24 14:35:25.592
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:35:25.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:35:25.601
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 01/23/24 14:35:25.602
    Jan 23 14:35:25.602: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan 23 14:35:25.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 create -f -'
    Jan 23 14:35:25.950: INFO: stderr: ""
    Jan 23 14:35:25.950: INFO: stdout: "service/agnhost-replica created\n"
    Jan 23 14:35:25.950: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan 23 14:35:25.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 create -f -'
    Jan 23 14:35:26.303: INFO: stderr: ""
    Jan 23 14:35:26.303: INFO: stdout: "service/agnhost-primary created\n"
    Jan 23 14:35:26.303: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan 23 14:35:26.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 create -f -'
    Jan 23 14:35:26.653: INFO: stderr: ""
    Jan 23 14:35:26.654: INFO: stdout: "service/frontend created\n"
    Jan 23 14:35:26.654: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan 23 14:35:26.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 create -f -'
    Jan 23 14:35:26.988: INFO: stderr: ""
    Jan 23 14:35:26.988: INFO: stdout: "deployment.apps/frontend created\n"
    Jan 23 14:35:26.988: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 23 14:35:26.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 create -f -'
    Jan 23 14:35:27.351: INFO: stderr: ""
    Jan 23 14:35:27.351: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan 23 14:35:27.351: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 23 14:35:27.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 create -f -'
    Jan 23 14:35:27.698: INFO: stderr: ""
    Jan 23 14:35:27.698: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/23/24 14:35:27.698
    Jan 23 14:35:27.698: INFO: Waiting for all frontend pods to be Running.
    Jan 23 14:35:32.749: INFO: Waiting for frontend to serve content.
    Jan 23 14:35:32.755: INFO: Trying to add a new entry to the guestbook.
    Jan 23 14:35:32.761: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 01/23/24 14:35:32.766
    Jan 23 14:35:32.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 delete --grace-period=0 --force -f -'
    Jan 23 14:35:32.857: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 23 14:35:32.857: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/23/24 14:35:32.857
    Jan 23 14:35:32.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 delete --grace-period=0 --force -f -'
    Jan 23 14:35:32.956: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 23 14:35:32.956: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/23/24 14:35:32.956
    Jan 23 14:35:32.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 delete --grace-period=0 --force -f -'
    Jan 23 14:35:33.050: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 23 14:35:33.050: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/23/24 14:35:33.051
    Jan 23 14:35:33.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 delete --grace-period=0 --force -f -'
    Jan 23 14:35:33.139: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 23 14:35:33.139: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/23/24 14:35:33.139
    Jan 23 14:35:33.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 delete --grace-period=0 --force -f -'
    Jan 23 14:35:33.237: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 23 14:35:33.237: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/23/24 14:35:33.237
    Jan 23 14:35:33.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-3667 delete --grace-period=0 --force -f -'
    Jan 23 14:35:33.327: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 23 14:35:33.327: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:35:33.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3667" for this suite. 01/23/24 14:35:33.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:35:33.336
Jan 23 14:35:33.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubectl 01/23/24 14:35:33.337
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:35:33.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:35:33.346
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 01/23/24 14:35:33.347
Jan 23 14:35:33.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-8114 api-versions'
Jan 23 14:35:33.465: INFO: stderr: ""
Jan 23 14:35:33.465: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling.k8s.io/v1\nautoscaling.k8s.io/v1beta2\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncert-manager.io/v1\ncertificates.k8s.io/v1\nconfig.nova-platform.io/v1alpha1\nconfig.nova-platform.io/v1alpha2\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ncustom.metrics.k8s.io/v1beta1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.toolkit.fluxcd.io/v2beta1\nimage.toolkit.fluxcd.io/v1beta1\nimage.toolkit.fluxcd.io/v1beta2\nkustomize.toolkit.fluxcd.io/v1\nkustomize.toolkit.fluxcd.io/v1beta1\nkustomize.toolkit.fluxcd.io/v1beta2\nlogging-extensions.banzaicloud.io/v1alpha1\nlogging.banzaicloud.io/v1alpha1\nlogging.banzaicloud.io/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nneuvector.com/v1\nnode.k8s.io/v1\nnotification.toolkit.fluxcd.io/v1\nnotification.toolkit.fluxcd.io/v1beta1\nnotification.toolkit.fluxcd.io/v1beta2\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsecrets-store.csi.x-k8s.io/v1\nsecrets-store.csi.x-k8s.io/v1alpha1\nsnapshot.storage.k8s.io/v1\nsource.toolkit.fluxcd.io/v1\nsource.toolkit.fluxcd.io/v1beta1\nsource.toolkit.fluxcd.io/v1beta2\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nsusecloud.net/v1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 23 14:35:33.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8114" for this suite. 01/23/24 14:35:33.468
------------------------------
• [0.138 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:35:33.336
    Jan 23 14:35:33.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubectl 01/23/24 14:35:33.337
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:35:33.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:35:33.346
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 01/23/24 14:35:33.347
    Jan 23 14:35:33.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-8114 api-versions'
    Jan 23 14:35:33.465: INFO: stderr: ""
    Jan 23 14:35:33.465: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling.k8s.io/v1\nautoscaling.k8s.io/v1beta2\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncert-manager.io/v1\ncertificates.k8s.io/v1\nconfig.nova-platform.io/v1alpha1\nconfig.nova-platform.io/v1alpha2\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ncustom.metrics.k8s.io/v1beta1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.toolkit.fluxcd.io/v2beta1\nimage.toolkit.fluxcd.io/v1beta1\nimage.toolkit.fluxcd.io/v1beta2\nkustomize.toolkit.fluxcd.io/v1\nkustomize.toolkit.fluxcd.io/v1beta1\nkustomize.toolkit.fluxcd.io/v1beta2\nlogging-extensions.banzaicloud.io/v1alpha1\nlogging.banzaicloud.io/v1alpha1\nlogging.banzaicloud.io/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nneuvector.com/v1\nnode.k8s.io/v1\nnotification.toolkit.fluxcd.io/v1\nnotification.toolkit.fluxcd.io/v1beta1\nnotification.toolkit.fluxcd.io/v1beta2\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsecrets-store.csi.x-k8s.io/v1\nsecrets-store.csi.x-k8s.io/v1alpha1\nsnapshot.storage.k8s.io/v1\nsource.toolkit.fluxcd.io/v1\nsource.toolkit.fluxcd.io/v1beta1\nsource.toolkit.fluxcd.io/v1beta2\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nsusecloud.net/v1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:35:33.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8114" for this suite. 01/23/24 14:35:33.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:35:33.474
Jan 23 14:35:33.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename sched-pred 01/23/24 14:35:33.475
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:35:33.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:35:33.482
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 23 14:35:33.484: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 23 14:35:33.488: INFO: Waiting for terminating namespaces to be deleted...
Jan 23 14:35:33.490: INFO: 
Logging pods the apiserver thinks is on node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local before test
Jan 23 14:35:33.503: INFO: calico-kube-controllers-7d4c856855-qrf8w from kube-system started at 2024-01-23 09:02:23 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container calico-kube-controllers ready: true, restart count 4
Jan 23 14:35:33.503: INFO: calico-node-hx9hg from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container calico-node ready: true, restart count 1
Jan 23 14:35:33.503: INFO: coredns-8446d7bc66-zglp7 from kube-system started at 2024-01-23 09:02:28 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container coredns ready: true, restart count 1
Jan 23 14:35:33.503: INFO: kube-proxy-5p4pt from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container kube-proxy ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nginx-proxy-node-worker-hohyvwot.nova-ht9xu6tk2ptb.local from kube-system started at 2024-01-23 09:02:32 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container nginx-proxy ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nova-dns-667b6f9dd9-f4wkr from kube-system started at 2024-01-23 09:02:53 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container nova-dns ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nova-dns-667b6f9dd9-fc8vq from kube-system started at 2024-01-23 09:02:53 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container nova-dns ready: true, restart count 1
Jan 23 14:35:33.503: INFO: agnhost-replica-74c659fd6f-vjdfb from kubectl-3667 started at 2024-01-23 14:35:27 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container replica ready: true, restart count 0
Jan 23 14:35:33.503: INFO: frontend-78cd9dfff5-4dq7n from kubectl-3667 started at 2024-01-23 14:35:27 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container guestbook-frontend ready: true, restart count 0
Jan 23 14:35:33.503: INFO: nova-reflector-7bc9b5d4dd-vgml8 from nova-automation started at 2024-01-23 09:06:13 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container reflector ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nova-release-git-main-0 from nova-automation started at 2024-01-23 09:08:44 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container gitea ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nova-reloader-744fcf7b8f-ztn6c from nova-automation started at 2024-01-23 09:06:13 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container nova-reloader ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nova-cert-manager-74fb9fd7f9-q6q9f from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container cert-manager-controller ready: true, restart count 2
Jan 23 14:35:33.503: INFO: nova-cert-manager-cainjector-74465474c6-pjhbz from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container cert-manager-cainjector ready: true, restart count 4
Jan 23 14:35:33.503: INFO: nova-cert-manager-webhook-74b6ccdf8-f5bt2 from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container cert-manager-webhook ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nova-console-675844f8c4-p92th from nova-console started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container nova-console ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nova-local-path-provisioner-59754bbcb5-nm6ps from nova-csi-drivers started at 2024-01-23 09:06:07 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container local-path-provisioner ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nova-oauth-csi-provider-msq2s from nova-csi-drivers started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container vault-csi-provider ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nova-secrets-store-csi-driver-s4f8p from nova-csi-drivers started at 2024-01-23 09:03:57 +0000 UTC (3 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container liveness-probe ready: true, restart count 1
Jan 23 14:35:33.503: INFO: 	Container node-driver-registrar ready: true, restart count 1
Jan 23 14:35:33.503: INFO: 	Container secrets-store ready: true, restart count 1
Jan 23 14:35:33.503: INFO: secrets-store-csi-driver-upgrade-crds-jvnt9 from nova-csi-drivers started at 2024-01-23 09:03:57 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container crds-upgrade ready: false, restart count 0
Jan 23 14:35:33.503: INFO: nova-descheduler-575f46487d-wcml5 from nova-descheduler started at 2024-01-23 09:06:07 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container descheduler ready: true, restart count 4
Jan 23 14:35:33.503: INFO: helm-controller-6ffdd7974c-ndrg6 from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container manager ready: true, restart count 5
Jan 23 14:35:33.503: INFO: image-automation-controller-79bb688dbd-sq8gm from nova-gitops started at 2024-01-23 09:09:27 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container manager ready: true, restart count 4
Jan 23 14:35:33.503: INFO: image-reflector-controller-6b744758c7-nwppg from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container manager ready: true, restart count 4
Jan 23 14:35:33.503: INFO: kustomize-controller-5d5bb4d48-99q8z from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container manager ready: true, restart count 4
Jan 23 14:35:33.503: INFO: notification-controller-5974fbb84-8wcdc from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container manager ready: true, restart count 4
Jan 23 14:35:33.503: INFO: source-controller-7f8d6bc9d7-dbb88 from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container manager ready: true, restart count 4
Jan 23 14:35:33.503: INFO: nova-ingress-internal-controller-g8rl8 from nova-ingress-internal started at 2024-01-23 09:08:09 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container controller ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nova-logging-operator-7587849584-mq59r from nova-logging-operator started at 2024-01-23 09:06:18 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container logging-operator ready: true, restart count 5
Jan 23 14:35:33.503: INFO: alertmanager-main-0 from nova-monitoring started at 2024-01-23 09:09:10 +0000 UTC (3 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container alertmanager ready: true, restart count 1
Jan 23 14:35:33.503: INFO: 	Container config-reloader ready: true, restart count 1
Jan 23 14:35:33.503: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 23 14:35:33.503: INFO: monitoring-plugin-6dcd875fb-6sn96 from nova-monitoring started at 2024-01-23 09:08:38 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container monitoring-plugin ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nova-cadvisor-jvkj7 from nova-monitoring started at 2024-01-23 09:09:34 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container nova-cadvisor ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nova-grafana-5fcb766f99-xlrkr from nova-monitoring started at 2024-01-23 09:09:09 +0000 UTC (3 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container grafana ready: true, restart count 1
Jan 23 14:35:33.503: INFO: 	Container nova-release-grafana-sc-dashboard ready: true, restart count 1
Jan 23 14:35:33.503: INFO: 	Container nova-release-grafana-sc-datasources ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nova-kube-state-metrics-6c99956449-vwb5x from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container nova-release-kube-state-metrics ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nova-metrics-server-6fc9cb6c86-6dfwd from nova-monitoring started at 2024-01-23 09:06:11 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container metrics-server ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nova-prometheus-adapter-55b7c8779-b9ffb from nova-monitoring started at 2024-01-23 09:09:27 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container prometheus-adapter ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nova-prometheus-main-operator-fcb966c79-2wnc9 from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container main ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nova-prometheus-node-exporter-cc7w5 from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container node-exporter ready: true, restart count 1
Jan 23 14:35:33.503: INFO: prometheus-main-0 from nova-monitoring started at 2024-01-23 09:09:42 +0000 UTC (5 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container config-reloader ready: true, restart count 1
Jan 23 14:35:33.503: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 23 14:35:33.503: INFO: 	Container prometheus ready: true, restart count 1
Jan 23 14:35:33.503: INFO: 	Container thanos-sidecar ready: true, restart count 1
Jan 23 14:35:33.503: INFO: 	Container vault-agent-auth ready: true, restart count 1
Jan 23 14:35:33.503: INFO: nova-oauth-secrets-webhook-gmmxd from nova-secrets-webhook started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container vault-secrets-webhook ready: true, restart count 1
Jan 23 14:35:33.503: INFO: sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-4htxf from sonobuoy started at 2024-01-23 13:21:23 +0000 UTC (2 container statuses recorded)
Jan 23 14:35:33.503: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 14:35:33.503: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 23 14:35:33.503: INFO: 
Logging pods the apiserver thinks is on node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local before test
Jan 23 14:35:33.511: INFO: calico-node-44dpv from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.511: INFO: 	Container calico-node ready: true, restart count 1
Jan 23 14:35:33.511: INFO: coredns-8446d7bc66-rpjs8 from kube-system started at 2024-01-23 09:02:28 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.511: INFO: 	Container coredns ready: true, restart count 1
Jan 23 14:35:33.511: INFO: kube-proxy-dvptq from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.511: INFO: 	Container kube-proxy ready: true, restart count 1
Jan 23 14:35:33.511: INFO: nginx-proxy-node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local from kube-system started at 2024-01-23 09:02:45 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.511: INFO: 	Container nginx-proxy ready: true, restart count 1
Jan 23 14:35:33.511: INFO: agnhost-primary-75659d4b45-5wcrs from kubectl-3667 started at 2024-01-23 14:35:27 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.511: INFO: 	Container primary ready: true, restart count 0
Jan 23 14:35:33.511: INFO: agnhost-replica-74c659fd6f-stk6p from kubectl-3667 started at 2024-01-23 14:35:27 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.511: INFO: 	Container replica ready: true, restart count 0
Jan 23 14:35:33.511: INFO: frontend-78cd9dfff5-6wsql from kubectl-3667 started at 2024-01-23 14:35:27 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.511: INFO: 	Container guestbook-frontend ready: true, restart count 0
Jan 23 14:35:33.511: INFO: frontend-78cd9dfff5-hjbl9 from kubectl-3667 started at 2024-01-23 14:35:27 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.511: INFO: 	Container guestbook-frontend ready: true, restart count 0
Jan 23 14:35:33.511: INFO: nova-oauth-csi-provider-pj45z from nova-csi-drivers started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.511: INFO: 	Container vault-csi-provider ready: true, restart count 1
Jan 23 14:35:33.511: INFO: nova-secrets-store-csi-driver-ft87l from nova-csi-drivers started at 2024-01-23 09:03:58 +0000 UTC (3 container statuses recorded)
Jan 23 14:35:33.511: INFO: 	Container liveness-probe ready: true, restart count 1
Jan 23 14:35:33.511: INFO: 	Container node-driver-registrar ready: true, restart count 1
Jan 23 14:35:33.511: INFO: 	Container secrets-store ready: true, restart count 1
Jan 23 14:35:33.511: INFO: nova-ingress-public-controller-l6fx2 from nova-ingress-public started at 2024-01-23 14:26:19 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.511: INFO: 	Container controller ready: true, restart count 0
Jan 23 14:35:33.511: INFO: nova-cadvisor-tq2rp from nova-monitoring started at 2024-01-23 09:09:34 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.511: INFO: 	Container nova-cadvisor ready: true, restart count 1
Jan 23 14:35:33.511: INFO: nova-prometheus-node-exporter-xj6bh from nova-monitoring started at 2024-01-23 14:26:08 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.511: INFO: 	Container node-exporter ready: true, restart count 0
Jan 23 14:35:33.511: INFO: nova-oauth-secrets-webhook-h4wz9 from nova-secrets-webhook started at 2024-01-23 14:26:08 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.511: INFO: 	Container vault-secrets-webhook ready: true, restart count 0
Jan 23 14:35:33.511: INFO: sonobuoy from sonobuoy started at 2024-01-23 13:21:17 +0000 UTC (1 container statuses recorded)
Jan 23 14:35:33.511: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 23 14:35:33.511: INFO: sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-7csgd from sonobuoy started at 2024-01-23 13:21:23 +0000 UTC (2 container statuses recorded)
Jan 23 14:35:33.511: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 14:35:33.511: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/23/24 14:35:33.511
Jan 23 14:35:33.524: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5615" to be "running"
Jan 23 14:35:33.526: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.203103ms
Jan 23 14:35:35.529: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004465453s
Jan 23 14:35:37.528: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.003962364s
Jan 23 14:35:37.528: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/23/24 14:35:37.53
STEP: Trying to apply a random label on the found node. 01/23/24 14:35:37.538
STEP: verifying the node has the label kubernetes.io/e2e-0c721983-7fc7-4b68-9c58-e26c11d9aa38 95 01/23/24 14:35:37.544
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/23/24 14:35:37.55
Jan 23 14:35:37.562: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5615" to be "not pending"
Jan 23 14:35:37.563: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.541147ms
Jan 23 14:35:39.567: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004818078s
Jan 23 14:35:41.566: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.00399528s
Jan 23 14:35:41.566: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.11.67 on the node which pod4 resides and expect not scheduled 01/23/24 14:35:41.566
Jan 23 14:35:41.583: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5615" to be "not pending"
Jan 23 14:35:41.585: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.655496ms
Jan 23 14:35:43.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004322286s
Jan 23 14:35:45.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004463602s
Jan 23 14:35:47.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004517861s
Jan 23 14:35:49.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005861284s
Jan 23 14:35:51.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004590007s
Jan 23 14:35:53.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004432738s
Jan 23 14:35:55.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004935299s
Jan 23 14:35:57.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004928149s
Jan 23 14:35:59.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.004625729s
Jan 23 14:36:01.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005777214s
Jan 23 14:36:03.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004288103s
Jan 23 14:36:05.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004750819s
Jan 23 14:36:07.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004761054s
Jan 23 14:36:09.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004749573s
Jan 23 14:36:11.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005288717s
Jan 23 14:36:13.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005560583s
Jan 23 14:36:15.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.00484182s
Jan 23 14:36:17.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.004711946s
Jan 23 14:36:19.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.005009769s
Jan 23 14:36:21.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004844276s
Jan 23 14:36:23.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004398319s
Jan 23 14:36:25.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005076958s
Jan 23 14:36:27.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004758825s
Jan 23 14:36:29.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006068275s
Jan 23 14:36:31.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.004506698s
Jan 23 14:36:33.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004431113s
Jan 23 14:36:35.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.004681079s
Jan 23 14:36:37.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.004828362s
Jan 23 14:36:39.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.00459619s
Jan 23 14:36:41.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005875425s
Jan 23 14:36:43.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005383753s
Jan 23 14:36:45.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004311454s
Jan 23 14:36:47.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004597976s
Jan 23 14:36:49.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.00489885s
Jan 23 14:36:51.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004165908s
Jan 23 14:36:53.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004118474s
Jan 23 14:36:55.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004546803s
Jan 23 14:36:57.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.005615067s
Jan 23 14:36:59.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.00502839s
Jan 23 14:37:01.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004394742s
Jan 23 14:37:03.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.005172609s
Jan 23 14:37:05.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.004976926s
Jan 23 14:37:07.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005418031s
Jan 23 14:37:09.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.00646805s
Jan 23 14:37:11.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004905124s
Jan 23 14:37:13.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.005109965s
Jan 23 14:37:15.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.004427436s
Jan 23 14:37:17.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005155946s
Jan 23 14:37:19.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.005547144s
Jan 23 14:37:21.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004820281s
Jan 23 14:37:23.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.004921865s
Jan 23 14:37:25.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.00468004s
Jan 23 14:37:27.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.006008779s
Jan 23 14:37:29.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005044662s
Jan 23 14:37:31.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.00419625s
Jan 23 14:37:33.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004610892s
Jan 23 14:37:35.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.004422162s
Jan 23 14:37:37.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.005199295s
Jan 23 14:37:39.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.005119992s
Jan 23 14:37:41.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004902595s
Jan 23 14:37:43.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.004738845s
Jan 23 14:37:45.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.004279338s
Jan 23 14:37:47.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.006651057s
Jan 23 14:37:49.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.006360358s
Jan 23 14:37:51.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.005690889s
Jan 23 14:37:53.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.004177114s
Jan 23 14:37:55.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.004533761s
Jan 23 14:37:57.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.004902996s
Jan 23 14:37:59.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.00536126s
Jan 23 14:38:01.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.004567269s
Jan 23 14:38:03.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.005726677s
Jan 23 14:38:05.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.004313808s
Jan 23 14:38:07.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.004214806s
Jan 23 14:38:09.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.003983216s
Jan 23 14:38:11.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.004558222s
Jan 23 14:38:13.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.003514099s
Jan 23 14:38:15.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.005274208s
Jan 23 14:38:17.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.005328931s
Jan 23 14:38:19.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.005242208s
Jan 23 14:38:21.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.004928914s
Jan 23 14:38:23.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.00504813s
Jan 23 14:38:25.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.004826074s
Jan 23 14:38:27.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.004931431s
Jan 23 14:38:29.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005398487s
Jan 23 14:38:31.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.005123591s
Jan 23 14:38:33.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.004909579s
Jan 23 14:38:35.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.004493851s
Jan 23 14:38:37.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.005742826s
Jan 23 14:38:39.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.005387194s
Jan 23 14:38:41.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.005126342s
Jan 23 14:38:43.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.004959518s
Jan 23 14:38:45.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.004840022s
Jan 23 14:38:47.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.005149205s
Jan 23 14:38:49.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.006081488s
Jan 23 14:38:51.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.004830932s
Jan 23 14:38:53.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.004113268s
Jan 23 14:38:55.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.004806072s
Jan 23 14:38:57.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.003687896s
Jan 23 14:38:59.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.005239321s
Jan 23 14:39:01.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.0039001s
Jan 23 14:39:03.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.005293499s
Jan 23 14:39:05.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.004208943s
Jan 23 14:39:07.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.005493914s
Jan 23 14:39:09.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.006787703s
Jan 23 14:39:11.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.00474209s
Jan 23 14:39:13.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.004321484s
Jan 23 14:39:15.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.004498843s
Jan 23 14:39:17.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.00562372s
Jan 23 14:39:19.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.005954895s
Jan 23 14:39:21.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.004400007s
Jan 23 14:39:23.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.004281068s
Jan 23 14:39:25.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.004400079s
Jan 23 14:39:27.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.004740833s
Jan 23 14:39:29.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.006075842s
Jan 23 14:39:31.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.005127717s
Jan 23 14:39:33.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.006448804s
Jan 23 14:39:35.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.004214815s
Jan 23 14:39:37.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.005564405s
Jan 23 14:39:39.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.005237712s
Jan 23 14:39:41.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.005393178s
Jan 23 14:39:43.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.004401253s
Jan 23 14:39:45.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.005258403s
Jan 23 14:39:47.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.00499441s
Jan 23 14:39:49.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.004839545s
Jan 23 14:39:51.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.003919759s
Jan 23 14:39:53.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.005064159s
Jan 23 14:39:55.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.004941282s
Jan 23 14:39:57.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.005454167s
Jan 23 14:39:59.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.005617853s
Jan 23 14:40:01.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.005003673s
Jan 23 14:40:03.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.005546152s
Jan 23 14:40:05.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.003868797s
Jan 23 14:40:07.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.00460329s
Jan 23 14:40:09.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.003914824s
Jan 23 14:40:11.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.004837865s
Jan 23 14:40:13.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.004807217s
Jan 23 14:40:15.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.004493437s
Jan 23 14:40:17.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.005800715s
Jan 23 14:40:19.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.005350436s
Jan 23 14:40:21.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.004702998s
Jan 23 14:40:23.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.004695273s
Jan 23 14:40:25.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.004881895s
Jan 23 14:40:27.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.005022408s
Jan 23 14:40:29.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.005847093s
Jan 23 14:40:31.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.004411646s
Jan 23 14:40:33.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.005872323s
Jan 23 14:40:35.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.00421023s
Jan 23 14:40:37.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.003642324s
Jan 23 14:40:39.591: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.007901699s
Jan 23 14:40:41.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.005074555s
Jan 23 14:40:41.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.006625416s
STEP: removing the label kubernetes.io/e2e-0c721983-7fc7-4b68-9c58-e26c11d9aa38 off the node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local 01/23/24 14:40:41.59
STEP: verifying the node doesn't have the label kubernetes.io/e2e-0c721983-7fc7-4b68-9c58-e26c11d9aa38 01/23/24 14:40:41.6
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:40:41.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5615" for this suite. 01/23/24 14:40:41.604
------------------------------
• [SLOW TEST] [308.133 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:35:33.474
    Jan 23 14:35:33.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename sched-pred 01/23/24 14:35:33.475
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:35:33.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:35:33.482
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 23 14:35:33.484: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 23 14:35:33.488: INFO: Waiting for terminating namespaces to be deleted...
    Jan 23 14:35:33.490: INFO: 
    Logging pods the apiserver thinks is on node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local before test
    Jan 23 14:35:33.503: INFO: calico-kube-controllers-7d4c856855-qrf8w from kube-system started at 2024-01-23 09:02:23 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container calico-kube-controllers ready: true, restart count 4
    Jan 23 14:35:33.503: INFO: calico-node-hx9hg from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container calico-node ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: coredns-8446d7bc66-zglp7 from kube-system started at 2024-01-23 09:02:28 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container coredns ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: kube-proxy-5p4pt from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container kube-proxy ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nginx-proxy-node-worker-hohyvwot.nova-ht9xu6tk2ptb.local from kube-system started at 2024-01-23 09:02:32 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container nginx-proxy ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nova-dns-667b6f9dd9-f4wkr from kube-system started at 2024-01-23 09:02:53 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container nova-dns ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nova-dns-667b6f9dd9-fc8vq from kube-system started at 2024-01-23 09:02:53 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container nova-dns ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: agnhost-replica-74c659fd6f-vjdfb from kubectl-3667 started at 2024-01-23 14:35:27 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container replica ready: true, restart count 0
    Jan 23 14:35:33.503: INFO: frontend-78cd9dfff5-4dq7n from kubectl-3667 started at 2024-01-23 14:35:27 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container guestbook-frontend ready: true, restart count 0
    Jan 23 14:35:33.503: INFO: nova-reflector-7bc9b5d4dd-vgml8 from nova-automation started at 2024-01-23 09:06:13 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container reflector ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nova-release-git-main-0 from nova-automation started at 2024-01-23 09:08:44 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container gitea ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nova-reloader-744fcf7b8f-ztn6c from nova-automation started at 2024-01-23 09:06:13 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container nova-reloader ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nova-cert-manager-74fb9fd7f9-q6q9f from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container cert-manager-controller ready: true, restart count 2
    Jan 23 14:35:33.503: INFO: nova-cert-manager-cainjector-74465474c6-pjhbz from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container cert-manager-cainjector ready: true, restart count 4
    Jan 23 14:35:33.503: INFO: nova-cert-manager-webhook-74b6ccdf8-f5bt2 from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container cert-manager-webhook ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nova-console-675844f8c4-p92th from nova-console started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container nova-console ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nova-local-path-provisioner-59754bbcb5-nm6ps from nova-csi-drivers started at 2024-01-23 09:06:07 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container local-path-provisioner ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nova-oauth-csi-provider-msq2s from nova-csi-drivers started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container vault-csi-provider ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nova-secrets-store-csi-driver-s4f8p from nova-csi-drivers started at 2024-01-23 09:03:57 +0000 UTC (3 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container liveness-probe ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: 	Container node-driver-registrar ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: 	Container secrets-store ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: secrets-store-csi-driver-upgrade-crds-jvnt9 from nova-csi-drivers started at 2024-01-23 09:03:57 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container crds-upgrade ready: false, restart count 0
    Jan 23 14:35:33.503: INFO: nova-descheduler-575f46487d-wcml5 from nova-descheduler started at 2024-01-23 09:06:07 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container descheduler ready: true, restart count 4
    Jan 23 14:35:33.503: INFO: helm-controller-6ffdd7974c-ndrg6 from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container manager ready: true, restart count 5
    Jan 23 14:35:33.503: INFO: image-automation-controller-79bb688dbd-sq8gm from nova-gitops started at 2024-01-23 09:09:27 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container manager ready: true, restart count 4
    Jan 23 14:35:33.503: INFO: image-reflector-controller-6b744758c7-nwppg from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container manager ready: true, restart count 4
    Jan 23 14:35:33.503: INFO: kustomize-controller-5d5bb4d48-99q8z from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container manager ready: true, restart count 4
    Jan 23 14:35:33.503: INFO: notification-controller-5974fbb84-8wcdc from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container manager ready: true, restart count 4
    Jan 23 14:35:33.503: INFO: source-controller-7f8d6bc9d7-dbb88 from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container manager ready: true, restart count 4
    Jan 23 14:35:33.503: INFO: nova-ingress-internal-controller-g8rl8 from nova-ingress-internal started at 2024-01-23 09:08:09 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container controller ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nova-logging-operator-7587849584-mq59r from nova-logging-operator started at 2024-01-23 09:06:18 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container logging-operator ready: true, restart count 5
    Jan 23 14:35:33.503: INFO: alertmanager-main-0 from nova-monitoring started at 2024-01-23 09:09:10 +0000 UTC (3 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: 	Container config-reloader ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: monitoring-plugin-6dcd875fb-6sn96 from nova-monitoring started at 2024-01-23 09:08:38 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container monitoring-plugin ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nova-cadvisor-jvkj7 from nova-monitoring started at 2024-01-23 09:09:34 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container nova-cadvisor ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nova-grafana-5fcb766f99-xlrkr from nova-monitoring started at 2024-01-23 09:09:09 +0000 UTC (3 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container grafana ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: 	Container nova-release-grafana-sc-dashboard ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: 	Container nova-release-grafana-sc-datasources ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nova-kube-state-metrics-6c99956449-vwb5x from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container nova-release-kube-state-metrics ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nova-metrics-server-6fc9cb6c86-6dfwd from nova-monitoring started at 2024-01-23 09:06:11 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container metrics-server ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nova-prometheus-adapter-55b7c8779-b9ffb from nova-monitoring started at 2024-01-23 09:09:27 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container prometheus-adapter ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nova-prometheus-main-operator-fcb966c79-2wnc9 from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container main ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nova-prometheus-node-exporter-cc7w5 from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: prometheus-main-0 from nova-monitoring started at 2024-01-23 09:09:42 +0000 UTC (5 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container config-reloader ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: 	Container prometheus ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: 	Container thanos-sidecar ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: 	Container vault-agent-auth ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: nova-oauth-secrets-webhook-gmmxd from nova-secrets-webhook started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container vault-secrets-webhook ready: true, restart count 1
    Jan 23 14:35:33.503: INFO: sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-4htxf from sonobuoy started at 2024-01-23 13:21:23 +0000 UTC (2 container statuses recorded)
    Jan 23 14:35:33.503: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 23 14:35:33.503: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 23 14:35:33.503: INFO: 
    Logging pods the apiserver thinks is on node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local before test
    Jan 23 14:35:33.511: INFO: calico-node-44dpv from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.511: INFO: 	Container calico-node ready: true, restart count 1
    Jan 23 14:35:33.511: INFO: coredns-8446d7bc66-rpjs8 from kube-system started at 2024-01-23 09:02:28 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.511: INFO: 	Container coredns ready: true, restart count 1
    Jan 23 14:35:33.511: INFO: kube-proxy-dvptq from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.511: INFO: 	Container kube-proxy ready: true, restart count 1
    Jan 23 14:35:33.511: INFO: nginx-proxy-node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local from kube-system started at 2024-01-23 09:02:45 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.511: INFO: 	Container nginx-proxy ready: true, restart count 1
    Jan 23 14:35:33.511: INFO: agnhost-primary-75659d4b45-5wcrs from kubectl-3667 started at 2024-01-23 14:35:27 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.511: INFO: 	Container primary ready: true, restart count 0
    Jan 23 14:35:33.511: INFO: agnhost-replica-74c659fd6f-stk6p from kubectl-3667 started at 2024-01-23 14:35:27 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.511: INFO: 	Container replica ready: true, restart count 0
    Jan 23 14:35:33.511: INFO: frontend-78cd9dfff5-6wsql from kubectl-3667 started at 2024-01-23 14:35:27 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.511: INFO: 	Container guestbook-frontend ready: true, restart count 0
    Jan 23 14:35:33.511: INFO: frontend-78cd9dfff5-hjbl9 from kubectl-3667 started at 2024-01-23 14:35:27 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.511: INFO: 	Container guestbook-frontend ready: true, restart count 0
    Jan 23 14:35:33.511: INFO: nova-oauth-csi-provider-pj45z from nova-csi-drivers started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.511: INFO: 	Container vault-csi-provider ready: true, restart count 1
    Jan 23 14:35:33.511: INFO: nova-secrets-store-csi-driver-ft87l from nova-csi-drivers started at 2024-01-23 09:03:58 +0000 UTC (3 container statuses recorded)
    Jan 23 14:35:33.511: INFO: 	Container liveness-probe ready: true, restart count 1
    Jan 23 14:35:33.511: INFO: 	Container node-driver-registrar ready: true, restart count 1
    Jan 23 14:35:33.511: INFO: 	Container secrets-store ready: true, restart count 1
    Jan 23 14:35:33.511: INFO: nova-ingress-public-controller-l6fx2 from nova-ingress-public started at 2024-01-23 14:26:19 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.511: INFO: 	Container controller ready: true, restart count 0
    Jan 23 14:35:33.511: INFO: nova-cadvisor-tq2rp from nova-monitoring started at 2024-01-23 09:09:34 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.511: INFO: 	Container nova-cadvisor ready: true, restart count 1
    Jan 23 14:35:33.511: INFO: nova-prometheus-node-exporter-xj6bh from nova-monitoring started at 2024-01-23 14:26:08 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.511: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 23 14:35:33.511: INFO: nova-oauth-secrets-webhook-h4wz9 from nova-secrets-webhook started at 2024-01-23 14:26:08 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.511: INFO: 	Container vault-secrets-webhook ready: true, restart count 0
    Jan 23 14:35:33.511: INFO: sonobuoy from sonobuoy started at 2024-01-23 13:21:17 +0000 UTC (1 container statuses recorded)
    Jan 23 14:35:33.511: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 23 14:35:33.511: INFO: sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-7csgd from sonobuoy started at 2024-01-23 13:21:23 +0000 UTC (2 container statuses recorded)
    Jan 23 14:35:33.511: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 23 14:35:33.511: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/23/24 14:35:33.511
    Jan 23 14:35:33.524: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5615" to be "running"
    Jan 23 14:35:33.526: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.203103ms
    Jan 23 14:35:35.529: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004465453s
    Jan 23 14:35:37.528: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.003962364s
    Jan 23 14:35:37.528: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/23/24 14:35:37.53
    STEP: Trying to apply a random label on the found node. 01/23/24 14:35:37.538
    STEP: verifying the node has the label kubernetes.io/e2e-0c721983-7fc7-4b68-9c58-e26c11d9aa38 95 01/23/24 14:35:37.544
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/23/24 14:35:37.55
    Jan 23 14:35:37.562: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5615" to be "not pending"
    Jan 23 14:35:37.563: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.541147ms
    Jan 23 14:35:39.567: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004818078s
    Jan 23 14:35:41.566: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.00399528s
    Jan 23 14:35:41.566: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.11.67 on the node which pod4 resides and expect not scheduled 01/23/24 14:35:41.566
    Jan 23 14:35:41.583: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5615" to be "not pending"
    Jan 23 14:35:41.585: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.655496ms
    Jan 23 14:35:43.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004322286s
    Jan 23 14:35:45.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004463602s
    Jan 23 14:35:47.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004517861s
    Jan 23 14:35:49.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005861284s
    Jan 23 14:35:51.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004590007s
    Jan 23 14:35:53.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004432738s
    Jan 23 14:35:55.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004935299s
    Jan 23 14:35:57.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004928149s
    Jan 23 14:35:59.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.004625729s
    Jan 23 14:36:01.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005777214s
    Jan 23 14:36:03.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004288103s
    Jan 23 14:36:05.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004750819s
    Jan 23 14:36:07.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004761054s
    Jan 23 14:36:09.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004749573s
    Jan 23 14:36:11.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005288717s
    Jan 23 14:36:13.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005560583s
    Jan 23 14:36:15.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.00484182s
    Jan 23 14:36:17.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.004711946s
    Jan 23 14:36:19.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.005009769s
    Jan 23 14:36:21.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004844276s
    Jan 23 14:36:23.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004398319s
    Jan 23 14:36:25.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005076958s
    Jan 23 14:36:27.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004758825s
    Jan 23 14:36:29.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006068275s
    Jan 23 14:36:31.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.004506698s
    Jan 23 14:36:33.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004431113s
    Jan 23 14:36:35.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.004681079s
    Jan 23 14:36:37.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.004828362s
    Jan 23 14:36:39.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.00459619s
    Jan 23 14:36:41.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005875425s
    Jan 23 14:36:43.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005383753s
    Jan 23 14:36:45.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004311454s
    Jan 23 14:36:47.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004597976s
    Jan 23 14:36:49.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.00489885s
    Jan 23 14:36:51.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004165908s
    Jan 23 14:36:53.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004118474s
    Jan 23 14:36:55.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004546803s
    Jan 23 14:36:57.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.005615067s
    Jan 23 14:36:59.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.00502839s
    Jan 23 14:37:01.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004394742s
    Jan 23 14:37:03.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.005172609s
    Jan 23 14:37:05.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.004976926s
    Jan 23 14:37:07.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005418031s
    Jan 23 14:37:09.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.00646805s
    Jan 23 14:37:11.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004905124s
    Jan 23 14:37:13.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.005109965s
    Jan 23 14:37:15.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.004427436s
    Jan 23 14:37:17.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005155946s
    Jan 23 14:37:19.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.005547144s
    Jan 23 14:37:21.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004820281s
    Jan 23 14:37:23.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.004921865s
    Jan 23 14:37:25.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.00468004s
    Jan 23 14:37:27.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.006008779s
    Jan 23 14:37:29.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005044662s
    Jan 23 14:37:31.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.00419625s
    Jan 23 14:37:33.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004610892s
    Jan 23 14:37:35.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.004422162s
    Jan 23 14:37:37.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.005199295s
    Jan 23 14:37:39.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.005119992s
    Jan 23 14:37:41.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004902595s
    Jan 23 14:37:43.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.004738845s
    Jan 23 14:37:45.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.004279338s
    Jan 23 14:37:47.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.006651057s
    Jan 23 14:37:49.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.006360358s
    Jan 23 14:37:51.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.005690889s
    Jan 23 14:37:53.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.004177114s
    Jan 23 14:37:55.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.004533761s
    Jan 23 14:37:57.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.004902996s
    Jan 23 14:37:59.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.00536126s
    Jan 23 14:38:01.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.004567269s
    Jan 23 14:38:03.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.005726677s
    Jan 23 14:38:05.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.004313808s
    Jan 23 14:38:07.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.004214806s
    Jan 23 14:38:09.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.003983216s
    Jan 23 14:38:11.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.004558222s
    Jan 23 14:38:13.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.003514099s
    Jan 23 14:38:15.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.005274208s
    Jan 23 14:38:17.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.005328931s
    Jan 23 14:38:19.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.005242208s
    Jan 23 14:38:21.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.004928914s
    Jan 23 14:38:23.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.00504813s
    Jan 23 14:38:25.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.004826074s
    Jan 23 14:38:27.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.004931431s
    Jan 23 14:38:29.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005398487s
    Jan 23 14:38:31.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.005123591s
    Jan 23 14:38:33.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.004909579s
    Jan 23 14:38:35.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.004493851s
    Jan 23 14:38:37.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.005742826s
    Jan 23 14:38:39.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.005387194s
    Jan 23 14:38:41.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.005126342s
    Jan 23 14:38:43.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.004959518s
    Jan 23 14:38:45.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.004840022s
    Jan 23 14:38:47.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.005149205s
    Jan 23 14:38:49.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.006081488s
    Jan 23 14:38:51.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.004830932s
    Jan 23 14:38:53.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.004113268s
    Jan 23 14:38:55.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.004806072s
    Jan 23 14:38:57.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.003687896s
    Jan 23 14:38:59.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.005239321s
    Jan 23 14:39:01.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.0039001s
    Jan 23 14:39:03.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.005293499s
    Jan 23 14:39:05.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.004208943s
    Jan 23 14:39:07.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.005493914s
    Jan 23 14:39:09.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.006787703s
    Jan 23 14:39:11.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.00474209s
    Jan 23 14:39:13.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.004321484s
    Jan 23 14:39:15.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.004498843s
    Jan 23 14:39:17.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.00562372s
    Jan 23 14:39:19.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.005954895s
    Jan 23 14:39:21.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.004400007s
    Jan 23 14:39:23.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.004281068s
    Jan 23 14:39:25.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.004400079s
    Jan 23 14:39:27.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.004740833s
    Jan 23 14:39:29.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.006075842s
    Jan 23 14:39:31.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.005127717s
    Jan 23 14:39:33.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.006448804s
    Jan 23 14:39:35.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.004214815s
    Jan 23 14:39:37.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.005564405s
    Jan 23 14:39:39.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.005237712s
    Jan 23 14:39:41.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.005393178s
    Jan 23 14:39:43.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.004401253s
    Jan 23 14:39:45.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.005258403s
    Jan 23 14:39:47.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.00499441s
    Jan 23 14:39:49.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.004839545s
    Jan 23 14:39:51.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.003919759s
    Jan 23 14:39:53.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.005064159s
    Jan 23 14:39:55.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.004941282s
    Jan 23 14:39:57.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.005454167s
    Jan 23 14:39:59.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.005617853s
    Jan 23 14:40:01.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.005003673s
    Jan 23 14:40:03.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.005546152s
    Jan 23 14:40:05.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.003868797s
    Jan 23 14:40:07.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.00460329s
    Jan 23 14:40:09.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.003914824s
    Jan 23 14:40:11.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.004837865s
    Jan 23 14:40:13.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.004807217s
    Jan 23 14:40:15.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.004493437s
    Jan 23 14:40:17.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.005800715s
    Jan 23 14:40:19.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.005350436s
    Jan 23 14:40:21.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.004702998s
    Jan 23 14:40:23.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.004695273s
    Jan 23 14:40:25.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.004881895s
    Jan 23 14:40:27.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.005022408s
    Jan 23 14:40:29.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.005847093s
    Jan 23 14:40:31.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.004411646s
    Jan 23 14:40:33.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.005872323s
    Jan 23 14:40:35.588: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.00421023s
    Jan 23 14:40:37.587: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.003642324s
    Jan 23 14:40:39.591: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.007901699s
    Jan 23 14:40:41.589: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.005074555s
    Jan 23 14:40:41.590: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.006625416s
    STEP: removing the label kubernetes.io/e2e-0c721983-7fc7-4b68-9c58-e26c11d9aa38 off the node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local 01/23/24 14:40:41.59
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-0c721983-7fc7-4b68-9c58-e26c11d9aa38 01/23/24 14:40:41.6
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:40:41.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5615" for this suite. 01/23/24 14:40:41.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:40:41.608
Jan 23 14:40:41.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename services 01/23/24 14:40:41.609
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:40:41.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:40:41.622
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-371 01/23/24 14:40:41.624
STEP: creating service affinity-nodeport in namespace services-371 01/23/24 14:40:41.624
STEP: creating replication controller affinity-nodeport in namespace services-371 01/23/24 14:40:41.641
I0123 14:40:41.644778      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-371, replica count: 3
I0123 14:40:44.697074      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 23 14:40:44.703: INFO: Creating new exec pod
Jan 23 14:40:44.714: INFO: Waiting up to 5m0s for pod "execpod-affinity4pkwp" in namespace "services-371" to be "running"
Jan 23 14:40:44.716: INFO: Pod "execpod-affinity4pkwp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.611685ms
Jan 23 14:40:46.719: INFO: Pod "execpod-affinity4pkwp": Phase="Running", Reason="", readiness=true. Elapsed: 2.005278125s
Jan 23 14:40:46.719: INFO: Pod "execpod-affinity4pkwp" satisfied condition "running"
Jan 23 14:40:47.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-371 exec execpod-affinity4pkwp -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Jan 23 14:40:47.897: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 23 14:40:47.897: INFO: stdout: ""
Jan 23 14:40:47.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-371 exec execpod-affinity4pkwp -- /bin/sh -x -c nc -v -z -w 2 10.233.13.210 80'
Jan 23 14:40:48.065: INFO: stderr: "+ nc -v -z -w 2 10.233.13.210 80\nConnection to 10.233.13.210 80 port [tcp/http] succeeded!\n"
Jan 23 14:40:48.065: INFO: stdout: ""
Jan 23 14:40:48.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-371 exec execpod-affinity4pkwp -- /bin/sh -x -c nc -v -z -w 2 172.31.11.40 32034'
Jan 23 14:40:48.234: INFO: stderr: "+ nc -v -z -w 2 172.31.11.40 32034\nConnection to 172.31.11.40 32034 port [tcp/*] succeeded!\n"
Jan 23 14:40:48.234: INFO: stdout: ""
Jan 23 14:40:48.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-371 exec execpod-affinity4pkwp -- /bin/sh -x -c nc -v -z -w 2 172.31.11.67 32034'
Jan 23 14:40:48.402: INFO: stderr: "+ nc -v -z -w 2 172.31.11.67 32034\nConnection to 172.31.11.67 32034 port [tcp/*] succeeded!\n"
Jan 23 14:40:48.402: INFO: stdout: ""
Jan 23 14:40:48.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-371 exec execpod-affinity4pkwp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.40:32034/ ; done'
Jan 23 14:40:48.628: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n"
Jan 23 14:40:48.628: INFO: stdout: "\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26"
Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
Jan 23 14:40:48.628: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-371, will wait for the garbage collector to delete the pods 01/23/24 14:40:48.636
Jan 23 14:40:48.692: INFO: Deleting ReplicationController affinity-nodeport took: 4.03022ms
Jan 23 14:40:48.793: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.31604ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 23 14:40:51.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-371" for this suite. 01/23/24 14:40:51.506
------------------------------
• [SLOW TEST] [9.901 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:40:41.608
    Jan 23 14:40:41.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename services 01/23/24 14:40:41.609
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:40:41.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:40:41.622
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-371 01/23/24 14:40:41.624
    STEP: creating service affinity-nodeport in namespace services-371 01/23/24 14:40:41.624
    STEP: creating replication controller affinity-nodeport in namespace services-371 01/23/24 14:40:41.641
    I0123 14:40:41.644778      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-371, replica count: 3
    I0123 14:40:44.697074      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 23 14:40:44.703: INFO: Creating new exec pod
    Jan 23 14:40:44.714: INFO: Waiting up to 5m0s for pod "execpod-affinity4pkwp" in namespace "services-371" to be "running"
    Jan 23 14:40:44.716: INFO: Pod "execpod-affinity4pkwp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.611685ms
    Jan 23 14:40:46.719: INFO: Pod "execpod-affinity4pkwp": Phase="Running", Reason="", readiness=true. Elapsed: 2.005278125s
    Jan 23 14:40:46.719: INFO: Pod "execpod-affinity4pkwp" satisfied condition "running"
    Jan 23 14:40:47.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-371 exec execpod-affinity4pkwp -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Jan 23 14:40:47.897: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan 23 14:40:47.897: INFO: stdout: ""
    Jan 23 14:40:47.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-371 exec execpod-affinity4pkwp -- /bin/sh -x -c nc -v -z -w 2 10.233.13.210 80'
    Jan 23 14:40:48.065: INFO: stderr: "+ nc -v -z -w 2 10.233.13.210 80\nConnection to 10.233.13.210 80 port [tcp/http] succeeded!\n"
    Jan 23 14:40:48.065: INFO: stdout: ""
    Jan 23 14:40:48.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-371 exec execpod-affinity4pkwp -- /bin/sh -x -c nc -v -z -w 2 172.31.11.40 32034'
    Jan 23 14:40:48.234: INFO: stderr: "+ nc -v -z -w 2 172.31.11.40 32034\nConnection to 172.31.11.40 32034 port [tcp/*] succeeded!\n"
    Jan 23 14:40:48.234: INFO: stdout: ""
    Jan 23 14:40:48.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-371 exec execpod-affinity4pkwp -- /bin/sh -x -c nc -v -z -w 2 172.31.11.67 32034'
    Jan 23 14:40:48.402: INFO: stderr: "+ nc -v -z -w 2 172.31.11.67 32034\nConnection to 172.31.11.67 32034 port [tcp/*] succeeded!\n"
    Jan 23 14:40:48.402: INFO: stdout: ""
    Jan 23 14:40:48.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-371 exec execpod-affinity4pkwp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.40:32034/ ; done'
    Jan 23 14:40:48.628: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.40:32034/\n"
    Jan 23 14:40:48.628: INFO: stdout: "\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26\naffinity-nodeport-rqs26"
    Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
    Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
    Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
    Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
    Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
    Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
    Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
    Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
    Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
    Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
    Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
    Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
    Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
    Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
    Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
    Jan 23 14:40:48.628: INFO: Received response from host: affinity-nodeport-rqs26
    Jan 23 14:40:48.628: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-371, will wait for the garbage collector to delete the pods 01/23/24 14:40:48.636
    Jan 23 14:40:48.692: INFO: Deleting ReplicationController affinity-nodeport took: 4.03022ms
    Jan 23 14:40:48.793: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.31604ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:40:51.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-371" for this suite. 01/23/24 14:40:51.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:40:51.509
Jan 23 14:40:51.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename ephemeral-containers-test 01/23/24 14:40:51.51
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:40:51.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:40:51.519
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/23/24 14:40:51.52
Jan 23 14:40:51.533: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3862" to be "running and ready"
Jan 23 14:40:51.536: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.052956ms
Jan 23 14:40:51.536: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:40:53.539: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006023603s
Jan 23 14:40:53.539: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan 23 14:40:53.539: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/23/24 14:40:53.541
Jan 23 14:40:53.550: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3862" to be "container debugger running"
Jan 23 14:40:53.551: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.505018ms
Jan 23 14:40:55.555: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004640071s
Jan 23 14:40:57.554: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.004496683s
Jan 23 14:40:57.555: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/23/24 14:40:57.555
Jan 23 14:40:57.555: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3862 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 14:40:57.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:40:57.555: INFO: ExecWithOptions: Clientset creation
Jan 23 14:40:57.555: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-3862/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan 23 14:40:57.616: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:40:57.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-3862" for this suite. 01/23/24 14:40:57.629
------------------------------
• [SLOW TEST] [6.124 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:40:51.509
    Jan 23 14:40:51.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/23/24 14:40:51.51
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:40:51.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:40:51.519
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/23/24 14:40:51.52
    Jan 23 14:40:51.533: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3862" to be "running and ready"
    Jan 23 14:40:51.536: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.052956ms
    Jan 23 14:40:51.536: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:40:53.539: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006023603s
    Jan 23 14:40:53.539: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan 23 14:40:53.539: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/23/24 14:40:53.541
    Jan 23 14:40:53.550: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3862" to be "container debugger running"
    Jan 23 14:40:53.551: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.505018ms
    Jan 23 14:40:55.555: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004640071s
    Jan 23 14:40:57.554: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.004496683s
    Jan 23 14:40:57.555: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/23/24 14:40:57.555
    Jan 23 14:40:57.555: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3862 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 14:40:57.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:40:57.555: INFO: ExecWithOptions: Clientset creation
    Jan 23 14:40:57.555: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-3862/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan 23 14:40:57.616: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:40:57.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-3862" for this suite. 01/23/24 14:40:57.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:40:57.634
Jan 23 14:40:57.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename svcaccounts 01/23/24 14:40:57.635
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:40:57.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:40:57.644
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Jan 23 14:40:57.647: INFO: Got root ca configmap in namespace "svcaccounts-233"
Jan 23 14:40:57.650: INFO: Deleted root ca configmap in namespace "svcaccounts-233"
STEP: waiting for a new root ca configmap created 01/23/24 14:40:58.15
Jan 23 14:40:58.153: INFO: Recreated root ca configmap in namespace "svcaccounts-233"
Jan 23 14:40:58.157: INFO: Updated root ca configmap in namespace "svcaccounts-233"
STEP: waiting for the root ca configmap reconciled 01/23/24 14:40:58.658
Jan 23 14:40:58.660: INFO: Reconciled root ca configmap in namespace "svcaccounts-233"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 23 14:40:58.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-233" for this suite. 01/23/24 14:40:58.663
------------------------------
• [1.032 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:40:57.634
    Jan 23 14:40:57.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename svcaccounts 01/23/24 14:40:57.635
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:40:57.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:40:57.644
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Jan 23 14:40:57.647: INFO: Got root ca configmap in namespace "svcaccounts-233"
    Jan 23 14:40:57.650: INFO: Deleted root ca configmap in namespace "svcaccounts-233"
    STEP: waiting for a new root ca configmap created 01/23/24 14:40:58.15
    Jan 23 14:40:58.153: INFO: Recreated root ca configmap in namespace "svcaccounts-233"
    Jan 23 14:40:58.157: INFO: Updated root ca configmap in namespace "svcaccounts-233"
    STEP: waiting for the root ca configmap reconciled 01/23/24 14:40:58.658
    Jan 23 14:40:58.660: INFO: Reconciled root ca configmap in namespace "svcaccounts-233"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:40:58.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-233" for this suite. 01/23/24 14:40:58.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:40:58.666
Jan 23 14:40:58.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename replicaset 01/23/24 14:40:58.667
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:40:58.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:40:58.675
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/23/24 14:40:58.676
Jan 23 14:40:58.789: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 23 14:41:03.792: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/23/24 14:41:03.792
STEP: getting scale subresource 01/23/24 14:41:03.792
STEP: updating a scale subresource 01/23/24 14:41:03.793
STEP: verifying the replicaset Spec.Replicas was modified 01/23/24 14:41:03.797
STEP: Patch a scale subresource 01/23/24 14:41:03.798
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 23 14:41:03.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7941" for this suite. 01/23/24 14:41:03.808
------------------------------
• [SLOW TEST] [5.145 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:40:58.666
    Jan 23 14:40:58.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename replicaset 01/23/24 14:40:58.667
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:40:58.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:40:58.675
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/23/24 14:40:58.676
    Jan 23 14:40:58.789: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 23 14:41:03.792: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/23/24 14:41:03.792
    STEP: getting scale subresource 01/23/24 14:41:03.792
    STEP: updating a scale subresource 01/23/24 14:41:03.793
    STEP: verifying the replicaset Spec.Replicas was modified 01/23/24 14:41:03.797
    STEP: Patch a scale subresource 01/23/24 14:41:03.798
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:41:03.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7941" for this suite. 01/23/24 14:41:03.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:41:03.813
Jan 23 14:41:03.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename security-context 01/23/24 14:41:03.813
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:41:03.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:41:03.825
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/23/24 14:41:03.827
Jan 23 14:41:03.840: INFO: Waiting up to 5m0s for pod "security-context-b89c9a22-42a1-4786-aa8a-094a800ec553" in namespace "security-context-7289" to be "Succeeded or Failed"
Jan 23 14:41:03.847: INFO: Pod "security-context-b89c9a22-42a1-4786-aa8a-094a800ec553": Phase="Pending", Reason="", readiness=false. Elapsed: 7.484173ms
Jan 23 14:41:05.851: INFO: Pod "security-context-b89c9a22-42a1-4786-aa8a-094a800ec553": Phase="Running", Reason="", readiness=false. Elapsed: 2.011029376s
Jan 23 14:41:07.850: INFO: Pod "security-context-b89c9a22-42a1-4786-aa8a-094a800ec553": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010217978s
STEP: Saw pod success 01/23/24 14:41:07.85
Jan 23 14:41:07.850: INFO: Pod "security-context-b89c9a22-42a1-4786-aa8a-094a800ec553" satisfied condition "Succeeded or Failed"
Jan 23 14:41:07.851: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod security-context-b89c9a22-42a1-4786-aa8a-094a800ec553 container test-container: <nil>
STEP: delete the pod 01/23/24 14:41:07.854
Jan 23 14:41:07.861: INFO: Waiting for pod security-context-b89c9a22-42a1-4786-aa8a-094a800ec553 to disappear
Jan 23 14:41:07.864: INFO: Pod security-context-b89c9a22-42a1-4786-aa8a-094a800ec553 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 23 14:41:07.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-7289" for this suite. 01/23/24 14:41:07.866
------------------------------
• [4.057 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:41:03.813
    Jan 23 14:41:03.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename security-context 01/23/24 14:41:03.813
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:41:03.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:41:03.825
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/23/24 14:41:03.827
    Jan 23 14:41:03.840: INFO: Waiting up to 5m0s for pod "security-context-b89c9a22-42a1-4786-aa8a-094a800ec553" in namespace "security-context-7289" to be "Succeeded or Failed"
    Jan 23 14:41:03.847: INFO: Pod "security-context-b89c9a22-42a1-4786-aa8a-094a800ec553": Phase="Pending", Reason="", readiness=false. Elapsed: 7.484173ms
    Jan 23 14:41:05.851: INFO: Pod "security-context-b89c9a22-42a1-4786-aa8a-094a800ec553": Phase="Running", Reason="", readiness=false. Elapsed: 2.011029376s
    Jan 23 14:41:07.850: INFO: Pod "security-context-b89c9a22-42a1-4786-aa8a-094a800ec553": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010217978s
    STEP: Saw pod success 01/23/24 14:41:07.85
    Jan 23 14:41:07.850: INFO: Pod "security-context-b89c9a22-42a1-4786-aa8a-094a800ec553" satisfied condition "Succeeded or Failed"
    Jan 23 14:41:07.851: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod security-context-b89c9a22-42a1-4786-aa8a-094a800ec553 container test-container: <nil>
    STEP: delete the pod 01/23/24 14:41:07.854
    Jan 23 14:41:07.861: INFO: Waiting for pod security-context-b89c9a22-42a1-4786-aa8a-094a800ec553 to disappear
    Jan 23 14:41:07.864: INFO: Pod security-context-b89c9a22-42a1-4786-aa8a-094a800ec553 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:41:07.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-7289" for this suite. 01/23/24 14:41:07.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:41:07.87
Jan 23 14:41:07.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename webhook 01/23/24 14:41:07.871
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:41:07.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:41:07.884
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/23/24 14:41:07.893
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:41:08.219
STEP: Deploying the webhook pod 01/23/24 14:41:08.224
STEP: Wait for the deployment to be ready 01/23/24 14:41:08.232
Jan 23 14:41:08.234: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/23/24 14:41:10.24
STEP: Verifying the service has paired with the endpoint 01/23/24 14:41:10.245
Jan 23 14:41:11.246: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 01/23/24 14:41:11.248
STEP: Creating a custom resource definition that should be denied by the webhook 01/23/24 14:41:11.261
Jan 23 14:41:11.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:41:11.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6922" for this suite. 01/23/24 14:41:11.292
STEP: Destroying namespace "webhook-6922-markers" for this suite. 01/23/24 14:41:11.295
------------------------------
• [3.431 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:41:07.87
    Jan 23 14:41:07.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename webhook 01/23/24 14:41:07.871
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:41:07.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:41:07.884
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/23/24 14:41:07.893
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:41:08.219
    STEP: Deploying the webhook pod 01/23/24 14:41:08.224
    STEP: Wait for the deployment to be ready 01/23/24 14:41:08.232
    Jan 23 14:41:08.234: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/23/24 14:41:10.24
    STEP: Verifying the service has paired with the endpoint 01/23/24 14:41:10.245
    Jan 23 14:41:11.246: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/23/24 14:41:11.248
    STEP: Creating a custom resource definition that should be denied by the webhook 01/23/24 14:41:11.261
    Jan 23 14:41:11.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:41:11.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6922" for this suite. 01/23/24 14:41:11.292
    STEP: Destroying namespace "webhook-6922-markers" for this suite. 01/23/24 14:41:11.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:41:11.302
Jan 23 14:41:11.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubelet-test 01/23/24 14:41:11.302
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:41:11.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:41:11.315
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 23 14:41:11.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5387" for this suite. 01/23/24 14:41:11.34
------------------------------
• [0.044 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:41:11.302
    Jan 23 14:41:11.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubelet-test 01/23/24 14:41:11.302
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:41:11.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:41:11.315
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:41:11.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5387" for this suite. 01/23/24 14:41:11.34
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:41:11.346
Jan 23 14:41:11.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename replicaset 01/23/24 14:41:11.347
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:41:11.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:41:11.362
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/23/24 14:41:11.364
Jan 23 14:41:11.376: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-8722" to be "running and ready"
Jan 23 14:41:11.381: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.500338ms
Jan 23 14:41:11.381: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:41:13.384: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.007794604s
Jan 23 14:41:13.384: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan 23 14:41:13.384: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/23/24 14:41:13.386
STEP: Then the orphan pod is adopted 01/23/24 14:41:13.39
STEP: When the matched label of one of its pods change 01/23/24 14:41:14.397
Jan 23 14:41:14.399: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/23/24 14:41:14.406
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 23 14:41:15.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8722" for this suite. 01/23/24 14:41:15.415
------------------------------
• [4.073 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:41:11.346
    Jan 23 14:41:11.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename replicaset 01/23/24 14:41:11.347
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:41:11.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:41:11.362
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/23/24 14:41:11.364
    Jan 23 14:41:11.376: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-8722" to be "running and ready"
    Jan 23 14:41:11.381: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.500338ms
    Jan 23 14:41:11.381: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:41:13.384: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.007794604s
    Jan 23 14:41:13.384: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan 23 14:41:13.384: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/23/24 14:41:13.386
    STEP: Then the orphan pod is adopted 01/23/24 14:41:13.39
    STEP: When the matched label of one of its pods change 01/23/24 14:41:14.397
    Jan 23 14:41:14.399: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/23/24 14:41:14.406
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:41:15.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8722" for this suite. 01/23/24 14:41:15.415
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:41:15.419
Jan 23 14:41:15.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename daemonsets 01/23/24 14:41:15.42
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:41:15.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:41:15.43
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
STEP: Creating simple DaemonSet "daemon-set" 01/23/24 14:41:15.444
STEP: Check that daemon pods launch on every node of the cluster. 01/23/24 14:41:15.453
Jan 23 14:41:15.455: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:15.458: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 14:41:15.458: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:41:16.465: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:16.467: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 14:41:16.467: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:41:17.461: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:17.462: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 23 14:41:17.462: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/23/24 14:41:17.464
Jan 23 14:41:17.472: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:17.474: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 23 14:41:17.474: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:41:18.477: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:18.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 23 14:41:18.479: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:41:19.477: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:19.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 23 14:41:19.479: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:41:20.477: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:20.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 23 14:41:20.479: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:41:21.478: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:21.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 23 14:41:21.480: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:41:22.478: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:22.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 23 14:41:22.480: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 01/23/24 14:41:22.481
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8125, will wait for the garbage collector to delete the pods 01/23/24 14:41:22.481
Jan 23 14:41:22.538: INFO: Deleting DaemonSet.extensions daemon-set took: 4.148939ms
Jan 23 14:41:22.639: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.945924ms
Jan 23 14:41:25.641: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 14:41:25.641: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 23 14:41:25.642: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"149403"},"items":null}

Jan 23 14:41:25.644: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"149403"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:41:25.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8125" for this suite. 01/23/24 14:41:25.65
------------------------------
• [SLOW TEST] [10.235 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:41:15.419
    Jan 23 14:41:15.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename daemonsets 01/23/24 14:41:15.42
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:41:15.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:41:15.43
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:177
    STEP: Creating simple DaemonSet "daemon-set" 01/23/24 14:41:15.444
    STEP: Check that daemon pods launch on every node of the cluster. 01/23/24 14:41:15.453
    Jan 23 14:41:15.455: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:15.458: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 14:41:15.458: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:41:16.465: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:16.467: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 14:41:16.467: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:41:17.461: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:17.462: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 23 14:41:17.462: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/23/24 14:41:17.464
    Jan 23 14:41:17.472: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:17.474: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 23 14:41:17.474: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:41:18.477: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:18.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 23 14:41:18.479: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:41:19.477: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:19.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 23 14:41:19.479: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:41:20.477: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:20.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 23 14:41:20.479: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:41:21.478: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:21.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 23 14:41:21.480: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:41:22.478: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:22.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 23 14:41:22.480: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 01/23/24 14:41:22.481
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8125, will wait for the garbage collector to delete the pods 01/23/24 14:41:22.481
    Jan 23 14:41:22.538: INFO: Deleting DaemonSet.extensions daemon-set took: 4.148939ms
    Jan 23 14:41:22.639: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.945924ms
    Jan 23 14:41:25.641: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 14:41:25.641: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 23 14:41:25.642: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"149403"},"items":null}

    Jan 23 14:41:25.644: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"149403"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:41:25.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8125" for this suite. 01/23/24 14:41:25.65
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:41:25.655
Jan 23 14:41:25.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename daemonsets 01/23/24 14:41:25.656
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:41:25.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:41:25.664
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
Jan 23 14:41:25.677: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/23/24 14:41:25.681
Jan 23 14:41:25.688: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:25.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 14:41:25.690: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:41:26.694: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:26.700: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 23 14:41:26.700: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 01/23/24 14:41:26.706
STEP: Check that daemon pods images are updated. 01/23/24 14:41:26.711
Jan 23 14:41:26.714: INFO: Wrong image for pod: daemon-set-lzdfw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 23 14:41:26.714: INFO: Wrong image for pod: daemon-set-q5hg2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 23 14:41:26.716: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:27.718: INFO: Wrong image for pod: daemon-set-lzdfw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 23 14:41:27.720: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:28.719: INFO: Wrong image for pod: daemon-set-lzdfw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 23 14:41:28.721: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:29.718: INFO: Wrong image for pod: daemon-set-lzdfw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 23 14:41:29.720: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:30.719: INFO: Pod daemon-set-4hgx8 is not available
Jan 23 14:41:30.719: INFO: Wrong image for pod: daemon-set-lzdfw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 23 14:41:30.721: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:31.721: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:32.721: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:33.718: INFO: Pod daemon-set-gdd69 is not available
Jan 23 14:41:33.720: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 01/23/24 14:41:33.72
Jan 23 14:41:33.722: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:33.723: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 23 14:41:33.723: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:41:34.728: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:34.730: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 23 14:41:34.730: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
Jan 23 14:41:35.728: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 14:41:35.731: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 23 14:41:35.731: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 01/23/24 14:41:35.74
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6869, will wait for the garbage collector to delete the pods 01/23/24 14:41:35.74
Jan 23 14:41:35.796: INFO: Deleting DaemonSet.extensions daemon-set took: 3.446985ms
Jan 23 14:41:35.896: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.299885ms
Jan 23 14:41:37.798: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 23 14:41:37.798: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 23 14:41:37.799: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"149603"},"items":null}

Jan 23 14:41:37.800: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"149603"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:41:37.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6869" for this suite. 01/23/24 14:41:37.807
------------------------------
• [SLOW TEST] [12.154 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:41:25.655
    Jan 23 14:41:25.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename daemonsets 01/23/24 14:41:25.656
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:41:25.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:41:25.664
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:385
    Jan 23 14:41:25.677: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/23/24 14:41:25.681
    Jan 23 14:41:25.688: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:25.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 14:41:25.690: INFO: Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:41:26.694: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:26.700: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 23 14:41:26.700: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 01/23/24 14:41:26.706
    STEP: Check that daemon pods images are updated. 01/23/24 14:41:26.711
    Jan 23 14:41:26.714: INFO: Wrong image for pod: daemon-set-lzdfw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 23 14:41:26.714: INFO: Wrong image for pod: daemon-set-q5hg2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 23 14:41:26.716: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:27.718: INFO: Wrong image for pod: daemon-set-lzdfw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 23 14:41:27.720: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:28.719: INFO: Wrong image for pod: daemon-set-lzdfw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 23 14:41:28.721: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:29.718: INFO: Wrong image for pod: daemon-set-lzdfw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 23 14:41:29.720: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:30.719: INFO: Pod daemon-set-4hgx8 is not available
    Jan 23 14:41:30.719: INFO: Wrong image for pod: daemon-set-lzdfw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 23 14:41:30.721: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:31.721: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:32.721: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:33.718: INFO: Pod daemon-set-gdd69 is not available
    Jan 23 14:41:33.720: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 01/23/24 14:41:33.72
    Jan 23 14:41:33.722: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:33.723: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 23 14:41:33.723: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:41:34.728: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:34.730: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 23 14:41:34.730: INFO: Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local is running 0 daemon pod, expected 1
    Jan 23 14:41:35.728: INFO: DaemonSet pods can't tolerate node node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 23 14:41:35.731: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 23 14:41:35.731: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 01/23/24 14:41:35.74
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6869, will wait for the garbage collector to delete the pods 01/23/24 14:41:35.74
    Jan 23 14:41:35.796: INFO: Deleting DaemonSet.extensions daemon-set took: 3.446985ms
    Jan 23 14:41:35.896: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.299885ms
    Jan 23 14:41:37.798: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 23 14:41:37.798: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 23 14:41:37.799: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"149603"},"items":null}

    Jan 23 14:41:37.800: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"149603"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:41:37.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6869" for this suite. 01/23/24 14:41:37.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:41:37.81
Jan 23 14:41:37.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename subpath 01/23/24 14:41:37.811
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:41:37.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:41:37.819
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/23/24 14:41:37.822
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-rqkv 01/23/24 14:41:37.831
STEP: Creating a pod to test atomic-volume-subpath 01/23/24 14:41:37.831
Jan 23 14:41:37.842: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rqkv" in namespace "subpath-6474" to be "Succeeded or Failed"
Jan 23 14:41:37.844: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Pending", Reason="", readiness=false. Elapsed: 1.384694ms
Jan 23 14:41:39.847: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 2.004353554s
Jan 23 14:41:41.847: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 4.004688557s
Jan 23 14:41:43.846: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 6.004189921s
Jan 23 14:41:45.847: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 8.004681103s
Jan 23 14:41:47.846: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 10.00406457s
Jan 23 14:41:49.847: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 12.005089497s
Jan 23 14:41:51.846: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 14.004176035s
Jan 23 14:41:53.848: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 16.005355055s
Jan 23 14:41:55.846: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 18.003418321s
Jan 23 14:41:57.848: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 20.005345512s
Jan 23 14:41:59.847: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=false. Elapsed: 22.004391884s
Jan 23 14:42:01.847: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004650913s
STEP: Saw pod success 01/23/24 14:42:01.847
Jan 23 14:42:01.847: INFO: Pod "pod-subpath-test-configmap-rqkv" satisfied condition "Succeeded or Failed"
Jan 23 14:42:01.849: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-subpath-test-configmap-rqkv container test-container-subpath-configmap-rqkv: <nil>
STEP: delete the pod 01/23/24 14:42:01.852
Jan 23 14:42:01.859: INFO: Waiting for pod pod-subpath-test-configmap-rqkv to disappear
Jan 23 14:42:01.860: INFO: Pod pod-subpath-test-configmap-rqkv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-rqkv 01/23/24 14:42:01.86
Jan 23 14:42:01.860: INFO: Deleting pod "pod-subpath-test-configmap-rqkv" in namespace "subpath-6474"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 23 14:42:01.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6474" for this suite. 01/23/24 14:42:01.865
------------------------------
• [SLOW TEST] [24.057 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:41:37.81
    Jan 23 14:41:37.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename subpath 01/23/24 14:41:37.811
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:41:37.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:41:37.819
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/23/24 14:41:37.822
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-rqkv 01/23/24 14:41:37.831
    STEP: Creating a pod to test atomic-volume-subpath 01/23/24 14:41:37.831
    Jan 23 14:41:37.842: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rqkv" in namespace "subpath-6474" to be "Succeeded or Failed"
    Jan 23 14:41:37.844: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Pending", Reason="", readiness=false. Elapsed: 1.384694ms
    Jan 23 14:41:39.847: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 2.004353554s
    Jan 23 14:41:41.847: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 4.004688557s
    Jan 23 14:41:43.846: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 6.004189921s
    Jan 23 14:41:45.847: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 8.004681103s
    Jan 23 14:41:47.846: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 10.00406457s
    Jan 23 14:41:49.847: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 12.005089497s
    Jan 23 14:41:51.846: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 14.004176035s
    Jan 23 14:41:53.848: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 16.005355055s
    Jan 23 14:41:55.846: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 18.003418321s
    Jan 23 14:41:57.848: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=true. Elapsed: 20.005345512s
    Jan 23 14:41:59.847: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Running", Reason="", readiness=false. Elapsed: 22.004391884s
    Jan 23 14:42:01.847: INFO: Pod "pod-subpath-test-configmap-rqkv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004650913s
    STEP: Saw pod success 01/23/24 14:42:01.847
    Jan 23 14:42:01.847: INFO: Pod "pod-subpath-test-configmap-rqkv" satisfied condition "Succeeded or Failed"
    Jan 23 14:42:01.849: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-subpath-test-configmap-rqkv container test-container-subpath-configmap-rqkv: <nil>
    STEP: delete the pod 01/23/24 14:42:01.852
    Jan 23 14:42:01.859: INFO: Waiting for pod pod-subpath-test-configmap-rqkv to disappear
    Jan 23 14:42:01.860: INFO: Pod pod-subpath-test-configmap-rqkv no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-rqkv 01/23/24 14:42:01.86
    Jan 23 14:42:01.860: INFO: Deleting pod "pod-subpath-test-configmap-rqkv" in namespace "subpath-6474"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:42:01.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6474" for this suite. 01/23/24 14:42:01.865
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:42:01.868
Jan 23 14:42:01.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename statefulset 01/23/24 14:42:01.868
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:42:01.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:42:01.876
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5361 01/23/24 14:42:01.879
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 01/23/24 14:42:01.881
STEP: Creating pod with conflicting port in namespace statefulset-5361 01/23/24 14:42:01.883
STEP: Waiting until pod test-pod will start running in namespace statefulset-5361 01/23/24 14:42:01.895
Jan 23 14:42:01.895: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5361" to be "running"
Jan 23 14:42:01.897: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.452156ms
Jan 23 14:42:03.899: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003786614s
Jan 23 14:42:03.899: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-5361 01/23/24 14:42:03.899
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5361 01/23/24 14:42:03.903
Jan 23 14:42:03.920: INFO: Observed stateful pod in namespace: statefulset-5361, name: ss-0, uid: 6f197250-e59e-429a-a62f-749f11150f39, status phase: Pending. Waiting for statefulset controller to delete.
Jan 23 14:42:03.928: INFO: Observed stateful pod in namespace: statefulset-5361, name: ss-0, uid: 6f197250-e59e-429a-a62f-749f11150f39, status phase: Failed. Waiting for statefulset controller to delete.
Jan 23 14:42:03.934: INFO: Observed stateful pod in namespace: statefulset-5361, name: ss-0, uid: 6f197250-e59e-429a-a62f-749f11150f39, status phase: Failed. Waiting for statefulset controller to delete.
Jan 23 14:42:03.937: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5361
STEP: Removing pod with conflicting port in namespace statefulset-5361 01/23/24 14:42:03.937
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5361 and will be in running state 01/23/24 14:42:03.942
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 23 14:42:05.945: INFO: Deleting all statefulset in ns statefulset-5361
Jan 23 14:42:05.947: INFO: Scaling statefulset ss to 0
Jan 23 14:42:15.956: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 14:42:15.958: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 23 14:42:15.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5361" for this suite. 01/23/24 14:42:15.971
------------------------------
• [SLOW TEST] [14.107 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:42:01.868
    Jan 23 14:42:01.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename statefulset 01/23/24 14:42:01.868
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:42:01.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:42:01.876
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5361 01/23/24 14:42:01.879
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 01/23/24 14:42:01.881
    STEP: Creating pod with conflicting port in namespace statefulset-5361 01/23/24 14:42:01.883
    STEP: Waiting until pod test-pod will start running in namespace statefulset-5361 01/23/24 14:42:01.895
    Jan 23 14:42:01.895: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5361" to be "running"
    Jan 23 14:42:01.897: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.452156ms
    Jan 23 14:42:03.899: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003786614s
    Jan 23 14:42:03.899: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-5361 01/23/24 14:42:03.899
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5361 01/23/24 14:42:03.903
    Jan 23 14:42:03.920: INFO: Observed stateful pod in namespace: statefulset-5361, name: ss-0, uid: 6f197250-e59e-429a-a62f-749f11150f39, status phase: Pending. Waiting for statefulset controller to delete.
    Jan 23 14:42:03.928: INFO: Observed stateful pod in namespace: statefulset-5361, name: ss-0, uid: 6f197250-e59e-429a-a62f-749f11150f39, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 23 14:42:03.934: INFO: Observed stateful pod in namespace: statefulset-5361, name: ss-0, uid: 6f197250-e59e-429a-a62f-749f11150f39, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 23 14:42:03.937: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5361
    STEP: Removing pod with conflicting port in namespace statefulset-5361 01/23/24 14:42:03.937
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5361 and will be in running state 01/23/24 14:42:03.942
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 23 14:42:05.945: INFO: Deleting all statefulset in ns statefulset-5361
    Jan 23 14:42:05.947: INFO: Scaling statefulset ss to 0
    Jan 23 14:42:15.956: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 23 14:42:15.958: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:42:15.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5361" for this suite. 01/23/24 14:42:15.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:42:15.975
Jan 23 14:42:15.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename crd-publish-openapi 01/23/24 14:42:15.976
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:42:15.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:42:15.99
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 01/23/24 14:42:15.991
Jan 23 14:42:15.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: rename a version 01/23/24 14:42:26.139
STEP: check the new version name is served 01/23/24 14:42:26.149
STEP: check the old version name is removed 01/23/24 14:42:28.083
STEP: check the other version is not changed 01/23/24 14:42:28.947
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:42:32.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7553" for this suite. 01/23/24 14:42:32.691
------------------------------
• [SLOW TEST] [16.719 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:42:15.975
    Jan 23 14:42:15.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename crd-publish-openapi 01/23/24 14:42:15.976
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:42:15.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:42:15.99
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 01/23/24 14:42:15.991
    Jan 23 14:42:15.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: rename a version 01/23/24 14:42:26.139
    STEP: check the new version name is served 01/23/24 14:42:26.149
    STEP: check the old version name is removed 01/23/24 14:42:28.083
    STEP: check the other version is not changed 01/23/24 14:42:28.947
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:42:32.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7553" for this suite. 01/23/24 14:42:32.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:42:32.696
Jan 23 14:42:32.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename cronjob 01/23/24 14:42:32.697
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:42:32.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:42:32.706
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/23/24 14:42:32.707
STEP: Ensuring a job is scheduled 01/23/24 14:42:32.711
STEP: Ensuring exactly one is scheduled 01/23/24 14:43:00.715
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/23/24 14:43:00.716
STEP: Ensuring no more jobs are scheduled 01/23/24 14:43:00.718
STEP: Removing cronjob 01/23/24 14:48:00.722
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 23 14:48:00.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2950" for this suite. 01/23/24 14:48:00.727
------------------------------
• [SLOW TEST] [328.040 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:42:32.696
    Jan 23 14:42:32.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename cronjob 01/23/24 14:42:32.697
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:42:32.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:42:32.706
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/23/24 14:42:32.707
    STEP: Ensuring a job is scheduled 01/23/24 14:42:32.711
    STEP: Ensuring exactly one is scheduled 01/23/24 14:43:00.715
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/23/24 14:43:00.716
    STEP: Ensuring no more jobs are scheduled 01/23/24 14:43:00.718
    STEP: Removing cronjob 01/23/24 14:48:00.722
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:48:00.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2950" for this suite. 01/23/24 14:48:00.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:48:00.736
Jan 23 14:48:00.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename init-container 01/23/24 14:48:00.737
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:00.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:00.751
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 01/23/24 14:48:00.753
Jan 23 14:48:00.754: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:48:06.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4819" for this suite. 01/23/24 14:48:06.571
------------------------------
• [SLOW TEST] [5.837 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:48:00.736
    Jan 23 14:48:00.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename init-container 01/23/24 14:48:00.737
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:00.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:00.751
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 01/23/24 14:48:00.753
    Jan 23 14:48:00.754: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:48:06.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4819" for this suite. 01/23/24 14:48:06.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:48:06.575
Jan 23 14:48:06.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename runtimeclass 01/23/24 14:48:06.575
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:06.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:06.584
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan 23 14:48:06.598: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8871 to be scheduled
Jan 23 14:48:06.599: INFO: 1 pods are not scheduled: [runtimeclass-8871/test-runtimeclass-runtimeclass-8871-preconfigured-handler-bffp2(8a1cf2e9-bbd3-49fc-8d11-50b53ab574f5)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 23 14:48:08.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8871" for this suite. 01/23/24 14:48:08.608
------------------------------
• [2.036 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:48:06.575
    Jan 23 14:48:06.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename runtimeclass 01/23/24 14:48:06.575
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:06.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:06.584
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan 23 14:48:06.598: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8871 to be scheduled
    Jan 23 14:48:06.599: INFO: 1 pods are not scheduled: [runtimeclass-8871/test-runtimeclass-runtimeclass-8871-preconfigured-handler-bffp2(8a1cf2e9-bbd3-49fc-8d11-50b53ab574f5)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:48:08.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8871" for this suite. 01/23/24 14:48:08.608
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:48:08.611
Jan 23 14:48:08.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename events 01/23/24 14:48:08.612
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:08.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:08.619
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/23/24 14:48:08.623
STEP: listing all events in all namespaces 01/23/24 14:48:08.628
STEP: patching the test event 01/23/24 14:48:08.632
STEP: fetching the test event 01/23/24 14:48:08.636
STEP: updating the test event 01/23/24 14:48:08.637
STEP: getting the test event 01/23/24 14:48:08.642
STEP: deleting the test event 01/23/24 14:48:08.643
STEP: listing all events in all namespaces 01/23/24 14:48:08.647
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan 23 14:48:08.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-2491" for this suite. 01/23/24 14:48:08.655
------------------------------
• [0.047 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:48:08.611
    Jan 23 14:48:08.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename events 01/23/24 14:48:08.612
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:08.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:08.619
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/23/24 14:48:08.623
    STEP: listing all events in all namespaces 01/23/24 14:48:08.628
    STEP: patching the test event 01/23/24 14:48:08.632
    STEP: fetching the test event 01/23/24 14:48:08.636
    STEP: updating the test event 01/23/24 14:48:08.637
    STEP: getting the test event 01/23/24 14:48:08.642
    STEP: deleting the test event 01/23/24 14:48:08.643
    STEP: listing all events in all namespaces 01/23/24 14:48:08.647
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:48:08.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-2491" for this suite. 01/23/24 14:48:08.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:48:08.658
Jan 23 14:48:08.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename pods 01/23/24 14:48:08.66
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:08.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:08.681
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Jan 23 14:48:08.701: INFO: Waiting up to 5m0s for pod "server-envvars-b4ad99dd-715d-4d06-a35f-0880987db26c" in namespace "pods-3984" to be "running and ready"
Jan 23 14:48:08.703: INFO: Pod "server-envvars-b4ad99dd-715d-4d06-a35f-0880987db26c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075856ms
Jan 23 14:48:08.703: INFO: The phase of Pod server-envvars-b4ad99dd-715d-4d06-a35f-0880987db26c is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:48:10.707: INFO: Pod "server-envvars-b4ad99dd-715d-4d06-a35f-0880987db26c": Phase="Running", Reason="", readiness=true. Elapsed: 2.005378682s
Jan 23 14:48:10.707: INFO: The phase of Pod server-envvars-b4ad99dd-715d-4d06-a35f-0880987db26c is Running (Ready = true)
Jan 23 14:48:10.707: INFO: Pod "server-envvars-b4ad99dd-715d-4d06-a35f-0880987db26c" satisfied condition "running and ready"
Jan 23 14:48:10.724: INFO: Waiting up to 5m0s for pod "client-envvars-32d88bde-daee-42bd-a332-345d1e6fe348" in namespace "pods-3984" to be "Succeeded or Failed"
Jan 23 14:48:10.725: INFO: Pod "client-envvars-32d88bde-daee-42bd-a332-345d1e6fe348": Phase="Pending", Reason="", readiness=false. Elapsed: 1.472851ms
Jan 23 14:48:12.728: INFO: Pod "client-envvars-32d88bde-daee-42bd-a332-345d1e6fe348": Phase="Running", Reason="", readiness=false. Elapsed: 2.004582192s
Jan 23 14:48:14.729: INFO: Pod "client-envvars-32d88bde-daee-42bd-a332-345d1e6fe348": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00479184s
STEP: Saw pod success 01/23/24 14:48:14.729
Jan 23 14:48:14.729: INFO: Pod "client-envvars-32d88bde-daee-42bd-a332-345d1e6fe348" satisfied condition "Succeeded or Failed"
Jan 23 14:48:14.730: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod client-envvars-32d88bde-daee-42bd-a332-345d1e6fe348 container env3cont: <nil>
STEP: delete the pod 01/23/24 14:48:14.741
Jan 23 14:48:14.747: INFO: Waiting for pod client-envvars-32d88bde-daee-42bd-a332-345d1e6fe348 to disappear
Jan 23 14:48:14.749: INFO: Pod client-envvars-32d88bde-daee-42bd-a332-345d1e6fe348 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 23 14:48:14.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3984" for this suite. 01/23/24 14:48:14.751
------------------------------
• [SLOW TEST] [6.096 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:48:08.658
    Jan 23 14:48:08.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename pods 01/23/24 14:48:08.66
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:08.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:08.681
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Jan 23 14:48:08.701: INFO: Waiting up to 5m0s for pod "server-envvars-b4ad99dd-715d-4d06-a35f-0880987db26c" in namespace "pods-3984" to be "running and ready"
    Jan 23 14:48:08.703: INFO: Pod "server-envvars-b4ad99dd-715d-4d06-a35f-0880987db26c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075856ms
    Jan 23 14:48:08.703: INFO: The phase of Pod server-envvars-b4ad99dd-715d-4d06-a35f-0880987db26c is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:48:10.707: INFO: Pod "server-envvars-b4ad99dd-715d-4d06-a35f-0880987db26c": Phase="Running", Reason="", readiness=true. Elapsed: 2.005378682s
    Jan 23 14:48:10.707: INFO: The phase of Pod server-envvars-b4ad99dd-715d-4d06-a35f-0880987db26c is Running (Ready = true)
    Jan 23 14:48:10.707: INFO: Pod "server-envvars-b4ad99dd-715d-4d06-a35f-0880987db26c" satisfied condition "running and ready"
    Jan 23 14:48:10.724: INFO: Waiting up to 5m0s for pod "client-envvars-32d88bde-daee-42bd-a332-345d1e6fe348" in namespace "pods-3984" to be "Succeeded or Failed"
    Jan 23 14:48:10.725: INFO: Pod "client-envvars-32d88bde-daee-42bd-a332-345d1e6fe348": Phase="Pending", Reason="", readiness=false. Elapsed: 1.472851ms
    Jan 23 14:48:12.728: INFO: Pod "client-envvars-32d88bde-daee-42bd-a332-345d1e6fe348": Phase="Running", Reason="", readiness=false. Elapsed: 2.004582192s
    Jan 23 14:48:14.729: INFO: Pod "client-envvars-32d88bde-daee-42bd-a332-345d1e6fe348": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00479184s
    STEP: Saw pod success 01/23/24 14:48:14.729
    Jan 23 14:48:14.729: INFO: Pod "client-envvars-32d88bde-daee-42bd-a332-345d1e6fe348" satisfied condition "Succeeded or Failed"
    Jan 23 14:48:14.730: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod client-envvars-32d88bde-daee-42bd-a332-345d1e6fe348 container env3cont: <nil>
    STEP: delete the pod 01/23/24 14:48:14.741
    Jan 23 14:48:14.747: INFO: Waiting for pod client-envvars-32d88bde-daee-42bd-a332-345d1e6fe348 to disappear
    Jan 23 14:48:14.749: INFO: Pod client-envvars-32d88bde-daee-42bd-a332-345d1e6fe348 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:48:14.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3984" for this suite. 01/23/24 14:48:14.751
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:48:14.754
Jan 23 14:48:14.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename containers 01/23/24 14:48:14.755
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:14.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:14.764
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Jan 23 14:48:14.778: INFO: Waiting up to 5m0s for pod "client-containers-f7379b2c-2014-4eeb-b29c-a78f522e0685" in namespace "containers-9996" to be "running"
Jan 23 14:48:14.779: INFO: Pod "client-containers-f7379b2c-2014-4eeb-b29c-a78f522e0685": Phase="Pending", Reason="", readiness=false. Elapsed: 1.134062ms
Jan 23 14:48:16.789: INFO: Pod "client-containers-f7379b2c-2014-4eeb-b29c-a78f522e0685": Phase="Running", Reason="", readiness=true. Elapsed: 2.010928471s
Jan 23 14:48:16.789: INFO: Pod "client-containers-f7379b2c-2014-4eeb-b29c-a78f522e0685" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 23 14:48:16.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9996" for this suite. 01/23/24 14:48:16.795
------------------------------
• [2.044 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:48:14.754
    Jan 23 14:48:14.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename containers 01/23/24 14:48:14.755
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:14.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:14.764
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Jan 23 14:48:14.778: INFO: Waiting up to 5m0s for pod "client-containers-f7379b2c-2014-4eeb-b29c-a78f522e0685" in namespace "containers-9996" to be "running"
    Jan 23 14:48:14.779: INFO: Pod "client-containers-f7379b2c-2014-4eeb-b29c-a78f522e0685": Phase="Pending", Reason="", readiness=false. Elapsed: 1.134062ms
    Jan 23 14:48:16.789: INFO: Pod "client-containers-f7379b2c-2014-4eeb-b29c-a78f522e0685": Phase="Running", Reason="", readiness=true. Elapsed: 2.010928471s
    Jan 23 14:48:16.789: INFO: Pod "client-containers-f7379b2c-2014-4eeb-b29c-a78f522e0685" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:48:16.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9996" for this suite. 01/23/24 14:48:16.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:48:16.799
Jan 23 14:48:16.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename webhook 01/23/24 14:48:16.8
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:16.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:16.808
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/23/24 14:48:16.823
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:48:17.187
STEP: Deploying the webhook pod 01/23/24 14:48:17.193
STEP: Wait for the deployment to be ready 01/23/24 14:48:17.205
Jan 23 14:48:17.210: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/23/24 14:48:19.216
STEP: Verifying the service has paired with the endpoint 01/23/24 14:48:19.221
Jan 23 14:48:20.222: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/23/24 14:48:20.225
STEP: create a namespace for the webhook 01/23/24 14:48:20.235
STEP: create a configmap should be unconditionally rejected by the webhook 01/23/24 14:48:20.24
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:48:20.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6790" for this suite. 01/23/24 14:48:20.276
STEP: Destroying namespace "webhook-6790-markers" for this suite. 01/23/24 14:48:20.284
------------------------------
• [3.492 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:48:16.799
    Jan 23 14:48:16.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename webhook 01/23/24 14:48:16.8
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:16.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:16.808
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/23/24 14:48:16.823
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:48:17.187
    STEP: Deploying the webhook pod 01/23/24 14:48:17.193
    STEP: Wait for the deployment to be ready 01/23/24 14:48:17.205
    Jan 23 14:48:17.210: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/23/24 14:48:19.216
    STEP: Verifying the service has paired with the endpoint 01/23/24 14:48:19.221
    Jan 23 14:48:20.222: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/23/24 14:48:20.225
    STEP: create a namespace for the webhook 01/23/24 14:48:20.235
    STEP: create a configmap should be unconditionally rejected by the webhook 01/23/24 14:48:20.24
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:48:20.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6790" for this suite. 01/23/24 14:48:20.276
    STEP: Destroying namespace "webhook-6790-markers" for this suite. 01/23/24 14:48:20.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:48:20.292
Jan 23 14:48:20.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename downward-api 01/23/24 14:48:20.293
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:20.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:20.303
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 01/23/24 14:48:20.307
Jan 23 14:48:20.329: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9499184d-55da-4652-8757-e5a70010c463" in namespace "downward-api-9618" to be "Succeeded or Failed"
Jan 23 14:48:20.331: INFO: Pod "downwardapi-volume-9499184d-55da-4652-8757-e5a70010c463": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039081ms
Jan 23 14:48:22.334: INFO: Pod "downwardapi-volume-9499184d-55da-4652-8757-e5a70010c463": Phase="Running", Reason="", readiness=true. Elapsed: 2.004859148s
Jan 23 14:48:24.335: INFO: Pod "downwardapi-volume-9499184d-55da-4652-8757-e5a70010c463": Phase="Running", Reason="", readiness=false. Elapsed: 4.006042702s
Jan 23 14:48:26.334: INFO: Pod "downwardapi-volume-9499184d-55da-4652-8757-e5a70010c463": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005348983s
STEP: Saw pod success 01/23/24 14:48:26.334
Jan 23 14:48:26.334: INFO: Pod "downwardapi-volume-9499184d-55da-4652-8757-e5a70010c463" satisfied condition "Succeeded or Failed"
Jan 23 14:48:26.336: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-9499184d-55da-4652-8757-e5a70010c463 container client-container: <nil>
STEP: delete the pod 01/23/24 14:48:26.34
Jan 23 14:48:26.348: INFO: Waiting for pod downwardapi-volume-9499184d-55da-4652-8757-e5a70010c463 to disappear
Jan 23 14:48:26.349: INFO: Pod downwardapi-volume-9499184d-55da-4652-8757-e5a70010c463 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 23 14:48:26.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9618" for this suite. 01/23/24 14:48:26.352
------------------------------
• [SLOW TEST] [6.063 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:48:20.292
    Jan 23 14:48:20.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename downward-api 01/23/24 14:48:20.293
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:20.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:20.303
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 01/23/24 14:48:20.307
    Jan 23 14:48:20.329: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9499184d-55da-4652-8757-e5a70010c463" in namespace "downward-api-9618" to be "Succeeded or Failed"
    Jan 23 14:48:20.331: INFO: Pod "downwardapi-volume-9499184d-55da-4652-8757-e5a70010c463": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039081ms
    Jan 23 14:48:22.334: INFO: Pod "downwardapi-volume-9499184d-55da-4652-8757-e5a70010c463": Phase="Running", Reason="", readiness=true. Elapsed: 2.004859148s
    Jan 23 14:48:24.335: INFO: Pod "downwardapi-volume-9499184d-55da-4652-8757-e5a70010c463": Phase="Running", Reason="", readiness=false. Elapsed: 4.006042702s
    Jan 23 14:48:26.334: INFO: Pod "downwardapi-volume-9499184d-55da-4652-8757-e5a70010c463": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005348983s
    STEP: Saw pod success 01/23/24 14:48:26.334
    Jan 23 14:48:26.334: INFO: Pod "downwardapi-volume-9499184d-55da-4652-8757-e5a70010c463" satisfied condition "Succeeded or Failed"
    Jan 23 14:48:26.336: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-9499184d-55da-4652-8757-e5a70010c463 container client-container: <nil>
    STEP: delete the pod 01/23/24 14:48:26.34
    Jan 23 14:48:26.348: INFO: Waiting for pod downwardapi-volume-9499184d-55da-4652-8757-e5a70010c463 to disappear
    Jan 23 14:48:26.349: INFO: Pod downwardapi-volume-9499184d-55da-4652-8757-e5a70010c463 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:48:26.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9618" for this suite. 01/23/24 14:48:26.352
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:48:26.355
Jan 23 14:48:26.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename configmap 01/23/24 14:48:26.356
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:26.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:26.364
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-4287f865-6d96-41c2-bac2-4fa892f2686e 01/23/24 14:48:26.368
STEP: Creating the pod 01/23/24 14:48:26.372
Jan 23 14:48:26.388: INFO: Waiting up to 5m0s for pod "pod-configmaps-9de68fe5-5994-48ee-8a0b-011176b6efce" in namespace "configmap-5714" to be "running"
Jan 23 14:48:26.390: INFO: Pod "pod-configmaps-9de68fe5-5994-48ee-8a0b-011176b6efce": Phase="Pending", Reason="", readiness=false. Elapsed: 1.762227ms
Jan 23 14:48:28.393: INFO: Pod "pod-configmaps-9de68fe5-5994-48ee-8a0b-011176b6efce": Phase="Running", Reason="", readiness=true. Elapsed: 2.004977651s
Jan 23 14:48:28.393: INFO: Pod "pod-configmaps-9de68fe5-5994-48ee-8a0b-011176b6efce" satisfied condition "running"
STEP: Waiting for pod with text data 01/23/24 14:48:28.393
STEP: Waiting for pod with binary data 01/23/24 14:48:28.397
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 23 14:48:28.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5714" for this suite. 01/23/24 14:48:28.403
------------------------------
• [2.052 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:48:26.355
    Jan 23 14:48:26.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename configmap 01/23/24 14:48:26.356
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:26.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:26.364
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-4287f865-6d96-41c2-bac2-4fa892f2686e 01/23/24 14:48:26.368
    STEP: Creating the pod 01/23/24 14:48:26.372
    Jan 23 14:48:26.388: INFO: Waiting up to 5m0s for pod "pod-configmaps-9de68fe5-5994-48ee-8a0b-011176b6efce" in namespace "configmap-5714" to be "running"
    Jan 23 14:48:26.390: INFO: Pod "pod-configmaps-9de68fe5-5994-48ee-8a0b-011176b6efce": Phase="Pending", Reason="", readiness=false. Elapsed: 1.762227ms
    Jan 23 14:48:28.393: INFO: Pod "pod-configmaps-9de68fe5-5994-48ee-8a0b-011176b6efce": Phase="Running", Reason="", readiness=true. Elapsed: 2.004977651s
    Jan 23 14:48:28.393: INFO: Pod "pod-configmaps-9de68fe5-5994-48ee-8a0b-011176b6efce" satisfied condition "running"
    STEP: Waiting for pod with text data 01/23/24 14:48:28.393
    STEP: Waiting for pod with binary data 01/23/24 14:48:28.397
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:48:28.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5714" for this suite. 01/23/24 14:48:28.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:48:28.407
Jan 23 14:48:28.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename services 01/23/24 14:48:28.408
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:28.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:28.416
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7406 01/23/24 14:48:28.418
STEP: changing the ExternalName service to type=NodePort 01/23/24 14:48:28.42
STEP: creating replication controller externalname-service in namespace services-7406 01/23/24 14:48:28.431
I0123 14:48:28.436463      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7406, replica count: 2
I0123 14:48:31.487838      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 23 14:48:31.487: INFO: Creating new exec pod
Jan 23 14:48:31.496: INFO: Waiting up to 5m0s for pod "execpod9dmxn" in namespace "services-7406" to be "running"
Jan 23 14:48:31.498: INFO: Pod "execpod9dmxn": Phase="Pending", Reason="", readiness=false. Elapsed: 1.378313ms
Jan 23 14:48:33.500: INFO: Pod "execpod9dmxn": Phase="Running", Reason="", readiness=true. Elapsed: 2.003514993s
Jan 23 14:48:33.500: INFO: Pod "execpod9dmxn" satisfied condition "running"
Jan 23 14:48:34.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-7406 exec execpod9dmxn -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan 23 14:48:34.676: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 23 14:48:34.676: INFO: stdout: ""
Jan 23 14:48:34.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-7406 exec execpod9dmxn -- /bin/sh -x -c nc -v -z -w 2 10.233.32.226 80'
Jan 23 14:48:34.849: INFO: stderr: "+ nc -v -z -w 2 10.233.32.226 80\nConnection to 10.233.32.226 80 port [tcp/http] succeeded!\n"
Jan 23 14:48:34.849: INFO: stdout: ""
Jan 23 14:48:34.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-7406 exec execpod9dmxn -- /bin/sh -x -c nc -v -z -w 2 172.31.11.40 31539'
Jan 23 14:48:35.024: INFO: stderr: "+ nc -v -z -w 2 172.31.11.40 31539\nConnection to 172.31.11.40 31539 port [tcp/*] succeeded!\n"
Jan 23 14:48:35.024: INFO: stdout: ""
Jan 23 14:48:35.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-7406 exec execpod9dmxn -- /bin/sh -x -c nc -v -z -w 2 172.31.11.67 31539'
Jan 23 14:48:35.206: INFO: stderr: "+ nc -v -z -w 2 172.31.11.67 31539\nConnection to 172.31.11.67 31539 port [tcp/*] succeeded!\n"
Jan 23 14:48:35.206: INFO: stdout: ""
Jan 23 14:48:35.206: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 23 14:48:35.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7406" for this suite. 01/23/24 14:48:35.241
------------------------------
• [SLOW TEST] [6.837 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:48:28.407
    Jan 23 14:48:28.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename services 01/23/24 14:48:28.408
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:28.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:28.416
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7406 01/23/24 14:48:28.418
    STEP: changing the ExternalName service to type=NodePort 01/23/24 14:48:28.42
    STEP: creating replication controller externalname-service in namespace services-7406 01/23/24 14:48:28.431
    I0123 14:48:28.436463      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7406, replica count: 2
    I0123 14:48:31.487838      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 23 14:48:31.487: INFO: Creating new exec pod
    Jan 23 14:48:31.496: INFO: Waiting up to 5m0s for pod "execpod9dmxn" in namespace "services-7406" to be "running"
    Jan 23 14:48:31.498: INFO: Pod "execpod9dmxn": Phase="Pending", Reason="", readiness=false. Elapsed: 1.378313ms
    Jan 23 14:48:33.500: INFO: Pod "execpod9dmxn": Phase="Running", Reason="", readiness=true. Elapsed: 2.003514993s
    Jan 23 14:48:33.500: INFO: Pod "execpod9dmxn" satisfied condition "running"
    Jan 23 14:48:34.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-7406 exec execpod9dmxn -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan 23 14:48:34.676: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 23 14:48:34.676: INFO: stdout: ""
    Jan 23 14:48:34.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-7406 exec execpod9dmxn -- /bin/sh -x -c nc -v -z -w 2 10.233.32.226 80'
    Jan 23 14:48:34.849: INFO: stderr: "+ nc -v -z -w 2 10.233.32.226 80\nConnection to 10.233.32.226 80 port [tcp/http] succeeded!\n"
    Jan 23 14:48:34.849: INFO: stdout: ""
    Jan 23 14:48:34.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-7406 exec execpod9dmxn -- /bin/sh -x -c nc -v -z -w 2 172.31.11.40 31539'
    Jan 23 14:48:35.024: INFO: stderr: "+ nc -v -z -w 2 172.31.11.40 31539\nConnection to 172.31.11.40 31539 port [tcp/*] succeeded!\n"
    Jan 23 14:48:35.024: INFO: stdout: ""
    Jan 23 14:48:35.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-7406 exec execpod9dmxn -- /bin/sh -x -c nc -v -z -w 2 172.31.11.67 31539'
    Jan 23 14:48:35.206: INFO: stderr: "+ nc -v -z -w 2 172.31.11.67 31539\nConnection to 172.31.11.67 31539 port [tcp/*] succeeded!\n"
    Jan 23 14:48:35.206: INFO: stdout: ""
    Jan 23 14:48:35.206: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:48:35.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7406" for this suite. 01/23/24 14:48:35.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:48:35.246
Jan 23 14:48:35.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename gc 01/23/24 14:48:35.247
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:35.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:35.257
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/23/24 14:48:35.26
STEP: Wait for the Deployment to create new ReplicaSet 01/23/24 14:48:35.264
STEP: delete the deployment 01/23/24 14:48:35.775
STEP: wait for all rs to be garbage collected 01/23/24 14:48:35.779
STEP: expected 0 rs, got 1 rs 01/23/24 14:48:35.781
STEP: expected 0 pods, got 2 pods 01/23/24 14:48:35.784
STEP: Gathering metrics 01/23/24 14:48:36.289
Jan 23 14:48:36.306: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" in namespace "kube-system" to be "running and ready"
Jan 23 14:48:36.308: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local": Phase="Running", Reason="", readiness=true. Elapsed: 1.801014ms
Jan 23 14:48:36.308: INFO: The phase of Pod kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local is Running (Ready = true)
Jan 23 14:48:36.308: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" satisfied condition "running and ready"
Jan 23 14:48:36.354: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 23 14:48:36.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7253" for this suite. 01/23/24 14:48:36.357
------------------------------
• [1.116 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:48:35.246
    Jan 23 14:48:35.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename gc 01/23/24 14:48:35.247
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:35.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:35.257
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/23/24 14:48:35.26
    STEP: Wait for the Deployment to create new ReplicaSet 01/23/24 14:48:35.264
    STEP: delete the deployment 01/23/24 14:48:35.775
    STEP: wait for all rs to be garbage collected 01/23/24 14:48:35.779
    STEP: expected 0 rs, got 1 rs 01/23/24 14:48:35.781
    STEP: expected 0 pods, got 2 pods 01/23/24 14:48:35.784
    STEP: Gathering metrics 01/23/24 14:48:36.289
    Jan 23 14:48:36.306: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" in namespace "kube-system" to be "running and ready"
    Jan 23 14:48:36.308: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local": Phase="Running", Reason="", readiness=true. Elapsed: 1.801014ms
    Jan 23 14:48:36.308: INFO: The phase of Pod kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local is Running (Ready = true)
    Jan 23 14:48:36.308: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" satisfied condition "running and ready"
    Jan 23 14:48:36.354: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:48:36.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7253" for this suite. 01/23/24 14:48:36.357
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:48:36.362
Jan 23 14:48:36.362: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename deployment 01/23/24 14:48:36.363
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:36.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:36.373
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan 23 14:48:36.379: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 23 14:48:41.382: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/23/24 14:48:41.382
Jan 23 14:48:41.382: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/23/24 14:48:41.394
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 23 14:48:41.401: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7899  82f30388-148a-4772-9581-56a1e8994126 152420 1 2024-01-23 14:48:41 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2024-01-23 14:48:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036ebf18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 23 14:48:41.403: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 23 14:48:41.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7899" for this suite. 01/23/24 14:48:41.407
------------------------------
• [SLOW TEST] [5.054 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:48:36.362
    Jan 23 14:48:36.362: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename deployment 01/23/24 14:48:36.363
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:36.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:36.373
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan 23 14:48:36.379: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan 23 14:48:41.382: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/23/24 14:48:41.382
    Jan 23 14:48:41.382: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/23/24 14:48:41.394
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 23 14:48:41.401: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7899  82f30388-148a-4772-9581-56a1e8994126 152420 1 2024-01-23 14:48:41 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2024-01-23 14:48:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036ebf18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 23 14:48:41.403: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:48:41.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7899" for this suite. 01/23/24 14:48:41.407
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:48:41.417
Jan 23 14:48:41.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubectl 01/23/24 14:48:41.417
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:41.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:41.434
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 01/23/24 14:48:41.435
Jan 23 14:48:41.435: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-9550 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/23/24 14:48:41.503
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 23 14:48:41.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9550" for this suite. 01/23/24 14:48:41.513
------------------------------
• [0.099 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:48:41.417
    Jan 23 14:48:41.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubectl 01/23/24 14:48:41.417
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:41.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:41.434
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 01/23/24 14:48:41.435
    Jan 23 14:48:41.435: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-9550 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/23/24 14:48:41.503
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:48:41.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9550" for this suite. 01/23/24 14:48:41.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:48:41.517
Jan 23 14:48:41.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename downward-api 01/23/24 14:48:41.518
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:41.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:41.527
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 01/23/24 14:48:41.53
Jan 23 14:48:41.543: INFO: Waiting up to 5m0s for pod "labelsupdate372a0d21-14d6-46e6-b9b0-122cf6ec1cc0" in namespace "downward-api-2255" to be "running and ready"
Jan 23 14:48:41.545: INFO: Pod "labelsupdate372a0d21-14d6-46e6-b9b0-122cf6ec1cc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.467551ms
Jan 23 14:48:41.545: INFO: The phase of Pod labelsupdate372a0d21-14d6-46e6-b9b0-122cf6ec1cc0 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:48:43.548: INFO: Pod "labelsupdate372a0d21-14d6-46e6-b9b0-122cf6ec1cc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.005540721s
Jan 23 14:48:43.548: INFO: The phase of Pod labelsupdate372a0d21-14d6-46e6-b9b0-122cf6ec1cc0 is Running (Ready = true)
Jan 23 14:48:43.548: INFO: Pod "labelsupdate372a0d21-14d6-46e6-b9b0-122cf6ec1cc0" satisfied condition "running and ready"
Jan 23 14:48:44.062: INFO: Successfully updated pod "labelsupdate372a0d21-14d6-46e6-b9b0-122cf6ec1cc0"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 23 14:48:48.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2255" for this suite. 01/23/24 14:48:48.081
------------------------------
• [SLOW TEST] [6.567 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:48:41.517
    Jan 23 14:48:41.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename downward-api 01/23/24 14:48:41.518
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:41.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:41.527
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 01/23/24 14:48:41.53
    Jan 23 14:48:41.543: INFO: Waiting up to 5m0s for pod "labelsupdate372a0d21-14d6-46e6-b9b0-122cf6ec1cc0" in namespace "downward-api-2255" to be "running and ready"
    Jan 23 14:48:41.545: INFO: Pod "labelsupdate372a0d21-14d6-46e6-b9b0-122cf6ec1cc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.467551ms
    Jan 23 14:48:41.545: INFO: The phase of Pod labelsupdate372a0d21-14d6-46e6-b9b0-122cf6ec1cc0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:48:43.548: INFO: Pod "labelsupdate372a0d21-14d6-46e6-b9b0-122cf6ec1cc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.005540721s
    Jan 23 14:48:43.548: INFO: The phase of Pod labelsupdate372a0d21-14d6-46e6-b9b0-122cf6ec1cc0 is Running (Ready = true)
    Jan 23 14:48:43.548: INFO: Pod "labelsupdate372a0d21-14d6-46e6-b9b0-122cf6ec1cc0" satisfied condition "running and ready"
    Jan 23 14:48:44.062: INFO: Successfully updated pod "labelsupdate372a0d21-14d6-46e6-b9b0-122cf6ec1cc0"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:48:48.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2255" for this suite. 01/23/24 14:48:48.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:48:48.085
Jan 23 14:48:48.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename security-context-test 01/23/24 14:48:48.085
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:48.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:48.094
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Jan 23 14:48:48.106: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-062c9547-e11b-40a5-8f46-3e7bbbc33906" in namespace "security-context-test-8235" to be "Succeeded or Failed"
Jan 23 14:48:48.107: INFO: Pod "busybox-privileged-false-062c9547-e11b-40a5-8f46-3e7bbbc33906": Phase="Pending", Reason="", readiness=false. Elapsed: 1.385028ms
Jan 23 14:48:50.111: INFO: Pod "busybox-privileged-false-062c9547-e11b-40a5-8f46-3e7bbbc33906": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00466059s
Jan 23 14:48:52.111: INFO: Pod "busybox-privileged-false-062c9547-e11b-40a5-8f46-3e7bbbc33906": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004611396s
Jan 23 14:48:52.111: INFO: Pod "busybox-privileged-false-062c9547-e11b-40a5-8f46-3e7bbbc33906" satisfied condition "Succeeded or Failed"
Jan 23 14:48:52.116: INFO: Got logs for pod "busybox-privileged-false-062c9547-e11b-40a5-8f46-3e7bbbc33906": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 23 14:48:52.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8235" for this suite. 01/23/24 14:48:52.119
------------------------------
• [4.038 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:48:48.085
    Jan 23 14:48:48.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename security-context-test 01/23/24 14:48:48.085
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:48.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:48.094
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Jan 23 14:48:48.106: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-062c9547-e11b-40a5-8f46-3e7bbbc33906" in namespace "security-context-test-8235" to be "Succeeded or Failed"
    Jan 23 14:48:48.107: INFO: Pod "busybox-privileged-false-062c9547-e11b-40a5-8f46-3e7bbbc33906": Phase="Pending", Reason="", readiness=false. Elapsed: 1.385028ms
    Jan 23 14:48:50.111: INFO: Pod "busybox-privileged-false-062c9547-e11b-40a5-8f46-3e7bbbc33906": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00466059s
    Jan 23 14:48:52.111: INFO: Pod "busybox-privileged-false-062c9547-e11b-40a5-8f46-3e7bbbc33906": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004611396s
    Jan 23 14:48:52.111: INFO: Pod "busybox-privileged-false-062c9547-e11b-40a5-8f46-3e7bbbc33906" satisfied condition "Succeeded or Failed"
    Jan 23 14:48:52.116: INFO: Got logs for pod "busybox-privileged-false-062c9547-e11b-40a5-8f46-3e7bbbc33906": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:48:52.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8235" for this suite. 01/23/24 14:48:52.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:48:52.123
Jan 23 14:48:52.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 14:48:52.124
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:52.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:52.133
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 01/23/24 14:48:52.135
Jan 23 14:48:52.155: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78234413-3590-4059-9037-029627dd5c74" in namespace "projected-6254" to be "Succeeded or Failed"
Jan 23 14:48:52.162: INFO: Pod "downwardapi-volume-78234413-3590-4059-9037-029627dd5c74": Phase="Pending", Reason="", readiness=false. Elapsed: 6.858207ms
Jan 23 14:48:54.165: INFO: Pod "downwardapi-volume-78234413-3590-4059-9037-029627dd5c74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010093905s
Jan 23 14:48:56.165: INFO: Pod "downwardapi-volume-78234413-3590-4059-9037-029627dd5c74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010259146s
STEP: Saw pod success 01/23/24 14:48:56.165
Jan 23 14:48:56.165: INFO: Pod "downwardapi-volume-78234413-3590-4059-9037-029627dd5c74" satisfied condition "Succeeded or Failed"
Jan 23 14:48:56.167: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-78234413-3590-4059-9037-029627dd5c74 container client-container: <nil>
STEP: delete the pod 01/23/24 14:48:56.17
Jan 23 14:48:56.176: INFO: Waiting for pod downwardapi-volume-78234413-3590-4059-9037-029627dd5c74 to disappear
Jan 23 14:48:56.178: INFO: Pod downwardapi-volume-78234413-3590-4059-9037-029627dd5c74 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 23 14:48:56.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6254" for this suite. 01/23/24 14:48:56.179
------------------------------
• [4.059 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:48:52.123
    Jan 23 14:48:52.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 14:48:52.124
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:52.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:52.133
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 01/23/24 14:48:52.135
    Jan 23 14:48:52.155: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78234413-3590-4059-9037-029627dd5c74" in namespace "projected-6254" to be "Succeeded or Failed"
    Jan 23 14:48:52.162: INFO: Pod "downwardapi-volume-78234413-3590-4059-9037-029627dd5c74": Phase="Pending", Reason="", readiness=false. Elapsed: 6.858207ms
    Jan 23 14:48:54.165: INFO: Pod "downwardapi-volume-78234413-3590-4059-9037-029627dd5c74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010093905s
    Jan 23 14:48:56.165: INFO: Pod "downwardapi-volume-78234413-3590-4059-9037-029627dd5c74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010259146s
    STEP: Saw pod success 01/23/24 14:48:56.165
    Jan 23 14:48:56.165: INFO: Pod "downwardapi-volume-78234413-3590-4059-9037-029627dd5c74" satisfied condition "Succeeded or Failed"
    Jan 23 14:48:56.167: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-78234413-3590-4059-9037-029627dd5c74 container client-container: <nil>
    STEP: delete the pod 01/23/24 14:48:56.17
    Jan 23 14:48:56.176: INFO: Waiting for pod downwardapi-volume-78234413-3590-4059-9037-029627dd5c74 to disappear
    Jan 23 14:48:56.178: INFO: Pod downwardapi-volume-78234413-3590-4059-9037-029627dd5c74 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:48:56.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6254" for this suite. 01/23/24 14:48:56.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:48:56.183
Jan 23 14:48:56.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubelet-test 01/23/24 14:48:56.184
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:56.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:56.191
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 23 14:49:00.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6748" for this suite. 01/23/24 14:49:00.212
------------------------------
• [4.032 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:48:56.183
    Jan 23 14:48:56.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubelet-test 01/23/24 14:48:56.184
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:48:56.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:48:56.191
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:49:00.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6748" for this suite. 01/23/24 14:49:00.212
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:49:00.216
Jan 23 14:49:00.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename crd-publish-openapi 01/23/24 14:49:00.217
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:49:00.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:49:00.227
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Jan 23 14:49:00.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/23/24 14:49:08.108
Jan 23 14:49:08.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-3954 --namespace=crd-publish-openapi-3954 create -f -'
Jan 23 14:49:09.369: INFO: stderr: ""
Jan 23 14:49:09.369: INFO: stdout: "e2e-test-crd-publish-openapi-582-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 23 14:49:09.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-3954 --namespace=crd-publish-openapi-3954 delete e2e-test-crd-publish-openapi-582-crds test-cr'
Jan 23 14:49:09.509: INFO: stderr: ""
Jan 23 14:49:09.509: INFO: stdout: "e2e-test-crd-publish-openapi-582-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 23 14:49:09.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-3954 --namespace=crd-publish-openapi-3954 apply -f -'
Jan 23 14:49:09.832: INFO: stderr: ""
Jan 23 14:49:09.832: INFO: stdout: "e2e-test-crd-publish-openapi-582-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 23 14:49:09.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-3954 --namespace=crd-publish-openapi-3954 delete e2e-test-crd-publish-openapi-582-crds test-cr'
Jan 23 14:49:09.976: INFO: stderr: ""
Jan 23 14:49:09.976: INFO: stdout: "e2e-test-crd-publish-openapi-582-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/23/24 14:49:09.976
Jan 23 14:49:09.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-3954 explain e2e-test-crd-publish-openapi-582-crds'
Jan 23 14:49:10.342: INFO: stderr: ""
Jan 23 14:49:10.342: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-582-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:49:12.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3954" for this suite. 01/23/24 14:49:12.896
------------------------------
• [SLOW TEST] [12.684 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:49:00.216
    Jan 23 14:49:00.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename crd-publish-openapi 01/23/24 14:49:00.217
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:49:00.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:49:00.227
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Jan 23 14:49:00.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/23/24 14:49:08.108
    Jan 23 14:49:08.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-3954 --namespace=crd-publish-openapi-3954 create -f -'
    Jan 23 14:49:09.369: INFO: stderr: ""
    Jan 23 14:49:09.369: INFO: stdout: "e2e-test-crd-publish-openapi-582-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 23 14:49:09.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-3954 --namespace=crd-publish-openapi-3954 delete e2e-test-crd-publish-openapi-582-crds test-cr'
    Jan 23 14:49:09.509: INFO: stderr: ""
    Jan 23 14:49:09.509: INFO: stdout: "e2e-test-crd-publish-openapi-582-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan 23 14:49:09.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-3954 --namespace=crd-publish-openapi-3954 apply -f -'
    Jan 23 14:49:09.832: INFO: stderr: ""
    Jan 23 14:49:09.832: INFO: stdout: "e2e-test-crd-publish-openapi-582-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 23 14:49:09.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-3954 --namespace=crd-publish-openapi-3954 delete e2e-test-crd-publish-openapi-582-crds test-cr'
    Jan 23 14:49:09.976: INFO: stderr: ""
    Jan 23 14:49:09.976: INFO: stdout: "e2e-test-crd-publish-openapi-582-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/23/24 14:49:09.976
    Jan 23 14:49:09.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-3954 explain e2e-test-crd-publish-openapi-582-crds'
    Jan 23 14:49:10.342: INFO: stderr: ""
    Jan 23 14:49:10.342: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-582-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:49:12.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3954" for this suite. 01/23/24 14:49:12.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:49:12.901
Jan 23 14:49:12.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename discovery 01/23/24 14:49:12.902
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:49:12.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:49:12.911
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/23/24 14:49:12.913
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan 23 14:49:13.420: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 23 14:49:13.421: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 23 14:49:13.421: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 23 14:49:13.421: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 23 14:49:13.421: INFO: Checking APIGroup: apps
Jan 23 14:49:13.421: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 23 14:49:13.421: INFO: Versions found [{apps/v1 v1}]
Jan 23 14:49:13.421: INFO: apps/v1 matches apps/v1
Jan 23 14:49:13.421: INFO: Checking APIGroup: events.k8s.io
Jan 23 14:49:13.422: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 23 14:49:13.422: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan 23 14:49:13.422: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 23 14:49:13.422: INFO: Checking APIGroup: authentication.k8s.io
Jan 23 14:49:13.422: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 23 14:49:13.422: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 23 14:49:13.422: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 23 14:49:13.422: INFO: Checking APIGroup: authorization.k8s.io
Jan 23 14:49:13.423: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 23 14:49:13.423: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 23 14:49:13.423: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 23 14:49:13.423: INFO: Checking APIGroup: autoscaling
Jan 23 14:49:13.423: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 23 14:49:13.423: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Jan 23 14:49:13.423: INFO: autoscaling/v2 matches autoscaling/v2
Jan 23 14:49:13.423: INFO: Checking APIGroup: batch
Jan 23 14:49:13.424: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 23 14:49:13.424: INFO: Versions found [{batch/v1 v1}]
Jan 23 14:49:13.424: INFO: batch/v1 matches batch/v1
Jan 23 14:49:13.424: INFO: Checking APIGroup: certificates.k8s.io
Jan 23 14:49:13.425: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 23 14:49:13.425: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 23 14:49:13.425: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 23 14:49:13.425: INFO: Checking APIGroup: networking.k8s.io
Jan 23 14:49:13.425: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 23 14:49:13.425: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 23 14:49:13.425: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 23 14:49:13.425: INFO: Checking APIGroup: policy
Jan 23 14:49:13.425: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 23 14:49:13.426: INFO: Versions found [{policy/v1 v1}]
Jan 23 14:49:13.426: INFO: policy/v1 matches policy/v1
Jan 23 14:49:13.426: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 23 14:49:13.426: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 23 14:49:13.426: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 23 14:49:13.426: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 23 14:49:13.426: INFO: Checking APIGroup: storage.k8s.io
Jan 23 14:49:13.426: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 23 14:49:13.426: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 23 14:49:13.426: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 23 14:49:13.426: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 23 14:49:13.427: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 23 14:49:13.427: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 23 14:49:13.427: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 23 14:49:13.427: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 23 14:49:13.428: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 23 14:49:13.428: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 23 14:49:13.428: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 23 14:49:13.428: INFO: Checking APIGroup: scheduling.k8s.io
Jan 23 14:49:13.428: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 23 14:49:13.428: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 23 14:49:13.428: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 23 14:49:13.428: INFO: Checking APIGroup: coordination.k8s.io
Jan 23 14:49:13.429: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 23 14:49:13.429: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 23 14:49:13.429: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 23 14:49:13.429: INFO: Checking APIGroup: node.k8s.io
Jan 23 14:49:13.429: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 23 14:49:13.429: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan 23 14:49:13.429: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 23 14:49:13.429: INFO: Checking APIGroup: discovery.k8s.io
Jan 23 14:49:13.430: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 23 14:49:13.430: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan 23 14:49:13.430: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 23 14:49:13.430: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 23 14:49:13.430: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Jan 23 14:49:13.430: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Jan 23 14:49:13.430: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Jan 23 14:49:13.430: INFO: Checking APIGroup: acme.cert-manager.io
Jan 23 14:49:13.431: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Jan 23 14:49:13.431: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
Jan 23 14:49:13.431: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Jan 23 14:49:13.431: INFO: Checking APIGroup: autoscaling.k8s.io
Jan 23 14:49:13.431: INFO: PreferredVersion.GroupVersion: autoscaling.k8s.io/v1
Jan 23 14:49:13.431: INFO: Versions found [{autoscaling.k8s.io/v1 v1} {autoscaling.k8s.io/v1beta2 v1beta2}]
Jan 23 14:49:13.431: INFO: autoscaling.k8s.io/v1 matches autoscaling.k8s.io/v1
Jan 23 14:49:13.431: INFO: Checking APIGroup: cert-manager.io
Jan 23 14:49:13.432: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Jan 23 14:49:13.432: INFO: Versions found [{cert-manager.io/v1 v1}]
Jan 23 14:49:13.432: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Jan 23 14:49:13.432: INFO: Checking APIGroup: crd.projectcalico.org
Jan 23 14:49:13.432: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan 23 14:49:13.432: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan 23 14:49:13.432: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jan 23 14:49:13.432: INFO: Checking APIGroup: kustomize.toolkit.fluxcd.io
Jan 23 14:49:13.433: INFO: PreferredVersion.GroupVersion: kustomize.toolkit.fluxcd.io/v1
Jan 23 14:49:13.433: INFO: Versions found [{kustomize.toolkit.fluxcd.io/v1 v1} {kustomize.toolkit.fluxcd.io/v1beta2 v1beta2} {kustomize.toolkit.fluxcd.io/v1beta1 v1beta1}]
Jan 23 14:49:13.433: INFO: kustomize.toolkit.fluxcd.io/v1 matches kustomize.toolkit.fluxcd.io/v1
Jan 23 14:49:13.433: INFO: Checking APIGroup: monitoring.coreos.com
Jan 23 14:49:13.433: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Jan 23 14:49:13.433: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Jan 23 14:49:13.433: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Jan 23 14:49:13.433: INFO: Checking APIGroup: neuvector.com
Jan 23 14:49:13.434: INFO: PreferredVersion.GroupVersion: neuvector.com/v1
Jan 23 14:49:13.434: INFO: Versions found [{neuvector.com/v1 v1}]
Jan 23 14:49:13.434: INFO: neuvector.com/v1 matches neuvector.com/v1
Jan 23 14:49:13.434: INFO: Checking APIGroup: notification.toolkit.fluxcd.io
Jan 23 14:49:13.434: INFO: PreferredVersion.GroupVersion: notification.toolkit.fluxcd.io/v1
Jan 23 14:49:13.434: INFO: Versions found [{notification.toolkit.fluxcd.io/v1 v1} {notification.toolkit.fluxcd.io/v1beta2 v1beta2} {notification.toolkit.fluxcd.io/v1beta1 v1beta1}]
Jan 23 14:49:13.434: INFO: notification.toolkit.fluxcd.io/v1 matches notification.toolkit.fluxcd.io/v1
Jan 23 14:49:13.434: INFO: Checking APIGroup: secrets-store.csi.x-k8s.io
Jan 23 14:49:13.435: INFO: PreferredVersion.GroupVersion: secrets-store.csi.x-k8s.io/v1
Jan 23 14:49:13.435: INFO: Versions found [{secrets-store.csi.x-k8s.io/v1 v1} {secrets-store.csi.x-k8s.io/v1alpha1 v1alpha1}]
Jan 23 14:49:13.435: INFO: secrets-store.csi.x-k8s.io/v1 matches secrets-store.csi.x-k8s.io/v1
Jan 23 14:49:13.435: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jan 23 14:49:13.435: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jan 23 14:49:13.435: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Jan 23 14:49:13.435: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jan 23 14:49:13.435: INFO: Checking APIGroup: source.toolkit.fluxcd.io
Jan 23 14:49:13.436: INFO: PreferredVersion.GroupVersion: source.toolkit.fluxcd.io/v1
Jan 23 14:49:13.436: INFO: Versions found [{source.toolkit.fluxcd.io/v1 v1} {source.toolkit.fluxcd.io/v1beta2 v1beta2} {source.toolkit.fluxcd.io/v1beta1 v1beta1}]
Jan 23 14:49:13.436: INFO: source.toolkit.fluxcd.io/v1 matches source.toolkit.fluxcd.io/v1
Jan 23 14:49:13.436: INFO: Checking APIGroup: susecloud.net
Jan 23 14:49:13.436: INFO: PreferredVersion.GroupVersion: susecloud.net/v1
Jan 23 14:49:13.436: INFO: Versions found [{susecloud.net/v1 v1}]
Jan 23 14:49:13.436: INFO: susecloud.net/v1 matches susecloud.net/v1
Jan 23 14:49:13.436: INFO: Checking APIGroup: config.nova-platform.io
Jan 23 14:49:13.437: INFO: PreferredVersion.GroupVersion: config.nova-platform.io/v1alpha2
Jan 23 14:49:13.437: INFO: Versions found [{config.nova-platform.io/v1alpha2 v1alpha2} {config.nova-platform.io/v1alpha1 v1alpha1}]
Jan 23 14:49:13.437: INFO: config.nova-platform.io/v1alpha2 matches config.nova-platform.io/v1alpha2
Jan 23 14:49:13.437: INFO: Checking APIGroup: logging-extensions.banzaicloud.io
Jan 23 14:49:13.437: INFO: PreferredVersion.GroupVersion: logging-extensions.banzaicloud.io/v1alpha1
Jan 23 14:49:13.437: INFO: Versions found [{logging-extensions.banzaicloud.io/v1alpha1 v1alpha1}]
Jan 23 14:49:13.437: INFO: logging-extensions.banzaicloud.io/v1alpha1 matches logging-extensions.banzaicloud.io/v1alpha1
Jan 23 14:49:13.437: INFO: Checking APIGroup: logging.banzaicloud.io
Jan 23 14:49:13.438: INFO: PreferredVersion.GroupVersion: logging.banzaicloud.io/v1beta1
Jan 23 14:49:13.438: INFO: Versions found [{logging.banzaicloud.io/v1beta1 v1beta1} {logging.banzaicloud.io/v1alpha1 v1alpha1}]
Jan 23 14:49:13.438: INFO: logging.banzaicloud.io/v1beta1 matches logging.banzaicloud.io/v1beta1
Jan 23 14:49:13.438: INFO: Checking APIGroup: image.toolkit.fluxcd.io
Jan 23 14:49:13.438: INFO: PreferredVersion.GroupVersion: image.toolkit.fluxcd.io/v1beta2
Jan 23 14:49:13.438: INFO: Versions found [{image.toolkit.fluxcd.io/v1beta2 v1beta2} {image.toolkit.fluxcd.io/v1beta1 v1beta1}]
Jan 23 14:49:13.438: INFO: image.toolkit.fluxcd.io/v1beta2 matches image.toolkit.fluxcd.io/v1beta2
Jan 23 14:49:13.438: INFO: Checking APIGroup: helm.toolkit.fluxcd.io
Jan 23 14:49:13.439: INFO: PreferredVersion.GroupVersion: helm.toolkit.fluxcd.io/v2beta1
Jan 23 14:49:13.439: INFO: Versions found [{helm.toolkit.fluxcd.io/v2beta1 v2beta1}]
Jan 23 14:49:13.439: INFO: helm.toolkit.fluxcd.io/v2beta1 matches helm.toolkit.fluxcd.io/v2beta1
Jan 23 14:49:13.439: INFO: Checking APIGroup: custom.metrics.k8s.io
Jan 23 14:49:13.439: INFO: PreferredVersion.GroupVersion: custom.metrics.k8s.io/v1beta1
Jan 23 14:49:13.439: INFO: Versions found [{custom.metrics.k8s.io/v1beta1 v1beta1}]
Jan 23 14:49:13.439: INFO: custom.metrics.k8s.io/v1beta1 matches custom.metrics.k8s.io/v1beta1
Jan 23 14:49:13.439: INFO: Checking APIGroup: metrics.k8s.io
Jan 23 14:49:13.440: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan 23 14:49:13.440: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan 23 14:49:13.440: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Jan 23 14:49:13.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-9632" for this suite. 01/23/24 14:49:13.442
------------------------------
• [0.544 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:49:12.901
    Jan 23 14:49:12.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename discovery 01/23/24 14:49:12.902
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:49:12.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:49:12.911
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/23/24 14:49:12.913
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan 23 14:49:13.420: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan 23 14:49:13.421: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan 23 14:49:13.421: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan 23 14:49:13.421: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan 23 14:49:13.421: INFO: Checking APIGroup: apps
    Jan 23 14:49:13.421: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan 23 14:49:13.421: INFO: Versions found [{apps/v1 v1}]
    Jan 23 14:49:13.421: INFO: apps/v1 matches apps/v1
    Jan 23 14:49:13.421: INFO: Checking APIGroup: events.k8s.io
    Jan 23 14:49:13.422: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan 23 14:49:13.422: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan 23 14:49:13.422: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan 23 14:49:13.422: INFO: Checking APIGroup: authentication.k8s.io
    Jan 23 14:49:13.422: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan 23 14:49:13.422: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan 23 14:49:13.422: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan 23 14:49:13.422: INFO: Checking APIGroup: authorization.k8s.io
    Jan 23 14:49:13.423: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan 23 14:49:13.423: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan 23 14:49:13.423: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan 23 14:49:13.423: INFO: Checking APIGroup: autoscaling
    Jan 23 14:49:13.423: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan 23 14:49:13.423: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Jan 23 14:49:13.423: INFO: autoscaling/v2 matches autoscaling/v2
    Jan 23 14:49:13.423: INFO: Checking APIGroup: batch
    Jan 23 14:49:13.424: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan 23 14:49:13.424: INFO: Versions found [{batch/v1 v1}]
    Jan 23 14:49:13.424: INFO: batch/v1 matches batch/v1
    Jan 23 14:49:13.424: INFO: Checking APIGroup: certificates.k8s.io
    Jan 23 14:49:13.425: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan 23 14:49:13.425: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan 23 14:49:13.425: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan 23 14:49:13.425: INFO: Checking APIGroup: networking.k8s.io
    Jan 23 14:49:13.425: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan 23 14:49:13.425: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan 23 14:49:13.425: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan 23 14:49:13.425: INFO: Checking APIGroup: policy
    Jan 23 14:49:13.425: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan 23 14:49:13.426: INFO: Versions found [{policy/v1 v1}]
    Jan 23 14:49:13.426: INFO: policy/v1 matches policy/v1
    Jan 23 14:49:13.426: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan 23 14:49:13.426: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan 23 14:49:13.426: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan 23 14:49:13.426: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan 23 14:49:13.426: INFO: Checking APIGroup: storage.k8s.io
    Jan 23 14:49:13.426: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan 23 14:49:13.426: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan 23 14:49:13.426: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan 23 14:49:13.426: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan 23 14:49:13.427: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan 23 14:49:13.427: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan 23 14:49:13.427: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan 23 14:49:13.427: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan 23 14:49:13.428: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan 23 14:49:13.428: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan 23 14:49:13.428: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan 23 14:49:13.428: INFO: Checking APIGroup: scheduling.k8s.io
    Jan 23 14:49:13.428: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan 23 14:49:13.428: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan 23 14:49:13.428: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan 23 14:49:13.428: INFO: Checking APIGroup: coordination.k8s.io
    Jan 23 14:49:13.429: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan 23 14:49:13.429: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan 23 14:49:13.429: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan 23 14:49:13.429: INFO: Checking APIGroup: node.k8s.io
    Jan 23 14:49:13.429: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan 23 14:49:13.429: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan 23 14:49:13.429: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan 23 14:49:13.429: INFO: Checking APIGroup: discovery.k8s.io
    Jan 23 14:49:13.430: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan 23 14:49:13.430: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan 23 14:49:13.430: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan 23 14:49:13.430: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan 23 14:49:13.430: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Jan 23 14:49:13.430: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Jan 23 14:49:13.430: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Jan 23 14:49:13.430: INFO: Checking APIGroup: acme.cert-manager.io
    Jan 23 14:49:13.431: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
    Jan 23 14:49:13.431: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
    Jan 23 14:49:13.431: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
    Jan 23 14:49:13.431: INFO: Checking APIGroup: autoscaling.k8s.io
    Jan 23 14:49:13.431: INFO: PreferredVersion.GroupVersion: autoscaling.k8s.io/v1
    Jan 23 14:49:13.431: INFO: Versions found [{autoscaling.k8s.io/v1 v1} {autoscaling.k8s.io/v1beta2 v1beta2}]
    Jan 23 14:49:13.431: INFO: autoscaling.k8s.io/v1 matches autoscaling.k8s.io/v1
    Jan 23 14:49:13.431: INFO: Checking APIGroup: cert-manager.io
    Jan 23 14:49:13.432: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
    Jan 23 14:49:13.432: INFO: Versions found [{cert-manager.io/v1 v1}]
    Jan 23 14:49:13.432: INFO: cert-manager.io/v1 matches cert-manager.io/v1
    Jan 23 14:49:13.432: INFO: Checking APIGroup: crd.projectcalico.org
    Jan 23 14:49:13.432: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jan 23 14:49:13.432: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jan 23 14:49:13.432: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Jan 23 14:49:13.432: INFO: Checking APIGroup: kustomize.toolkit.fluxcd.io
    Jan 23 14:49:13.433: INFO: PreferredVersion.GroupVersion: kustomize.toolkit.fluxcd.io/v1
    Jan 23 14:49:13.433: INFO: Versions found [{kustomize.toolkit.fluxcd.io/v1 v1} {kustomize.toolkit.fluxcd.io/v1beta2 v1beta2} {kustomize.toolkit.fluxcd.io/v1beta1 v1beta1}]
    Jan 23 14:49:13.433: INFO: kustomize.toolkit.fluxcd.io/v1 matches kustomize.toolkit.fluxcd.io/v1
    Jan 23 14:49:13.433: INFO: Checking APIGroup: monitoring.coreos.com
    Jan 23 14:49:13.433: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    Jan 23 14:49:13.433: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    Jan 23 14:49:13.433: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    Jan 23 14:49:13.433: INFO: Checking APIGroup: neuvector.com
    Jan 23 14:49:13.434: INFO: PreferredVersion.GroupVersion: neuvector.com/v1
    Jan 23 14:49:13.434: INFO: Versions found [{neuvector.com/v1 v1}]
    Jan 23 14:49:13.434: INFO: neuvector.com/v1 matches neuvector.com/v1
    Jan 23 14:49:13.434: INFO: Checking APIGroup: notification.toolkit.fluxcd.io
    Jan 23 14:49:13.434: INFO: PreferredVersion.GroupVersion: notification.toolkit.fluxcd.io/v1
    Jan 23 14:49:13.434: INFO: Versions found [{notification.toolkit.fluxcd.io/v1 v1} {notification.toolkit.fluxcd.io/v1beta2 v1beta2} {notification.toolkit.fluxcd.io/v1beta1 v1beta1}]
    Jan 23 14:49:13.434: INFO: notification.toolkit.fluxcd.io/v1 matches notification.toolkit.fluxcd.io/v1
    Jan 23 14:49:13.434: INFO: Checking APIGroup: secrets-store.csi.x-k8s.io
    Jan 23 14:49:13.435: INFO: PreferredVersion.GroupVersion: secrets-store.csi.x-k8s.io/v1
    Jan 23 14:49:13.435: INFO: Versions found [{secrets-store.csi.x-k8s.io/v1 v1} {secrets-store.csi.x-k8s.io/v1alpha1 v1alpha1}]
    Jan 23 14:49:13.435: INFO: secrets-store.csi.x-k8s.io/v1 matches secrets-store.csi.x-k8s.io/v1
    Jan 23 14:49:13.435: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Jan 23 14:49:13.435: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Jan 23 14:49:13.435: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Jan 23 14:49:13.435: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Jan 23 14:49:13.435: INFO: Checking APIGroup: source.toolkit.fluxcd.io
    Jan 23 14:49:13.436: INFO: PreferredVersion.GroupVersion: source.toolkit.fluxcd.io/v1
    Jan 23 14:49:13.436: INFO: Versions found [{source.toolkit.fluxcd.io/v1 v1} {source.toolkit.fluxcd.io/v1beta2 v1beta2} {source.toolkit.fluxcd.io/v1beta1 v1beta1}]
    Jan 23 14:49:13.436: INFO: source.toolkit.fluxcd.io/v1 matches source.toolkit.fluxcd.io/v1
    Jan 23 14:49:13.436: INFO: Checking APIGroup: susecloud.net
    Jan 23 14:49:13.436: INFO: PreferredVersion.GroupVersion: susecloud.net/v1
    Jan 23 14:49:13.436: INFO: Versions found [{susecloud.net/v1 v1}]
    Jan 23 14:49:13.436: INFO: susecloud.net/v1 matches susecloud.net/v1
    Jan 23 14:49:13.436: INFO: Checking APIGroup: config.nova-platform.io
    Jan 23 14:49:13.437: INFO: PreferredVersion.GroupVersion: config.nova-platform.io/v1alpha2
    Jan 23 14:49:13.437: INFO: Versions found [{config.nova-platform.io/v1alpha2 v1alpha2} {config.nova-platform.io/v1alpha1 v1alpha1}]
    Jan 23 14:49:13.437: INFO: config.nova-platform.io/v1alpha2 matches config.nova-platform.io/v1alpha2
    Jan 23 14:49:13.437: INFO: Checking APIGroup: logging-extensions.banzaicloud.io
    Jan 23 14:49:13.437: INFO: PreferredVersion.GroupVersion: logging-extensions.banzaicloud.io/v1alpha1
    Jan 23 14:49:13.437: INFO: Versions found [{logging-extensions.banzaicloud.io/v1alpha1 v1alpha1}]
    Jan 23 14:49:13.437: INFO: logging-extensions.banzaicloud.io/v1alpha1 matches logging-extensions.banzaicloud.io/v1alpha1
    Jan 23 14:49:13.437: INFO: Checking APIGroup: logging.banzaicloud.io
    Jan 23 14:49:13.438: INFO: PreferredVersion.GroupVersion: logging.banzaicloud.io/v1beta1
    Jan 23 14:49:13.438: INFO: Versions found [{logging.banzaicloud.io/v1beta1 v1beta1} {logging.banzaicloud.io/v1alpha1 v1alpha1}]
    Jan 23 14:49:13.438: INFO: logging.banzaicloud.io/v1beta1 matches logging.banzaicloud.io/v1beta1
    Jan 23 14:49:13.438: INFO: Checking APIGroup: image.toolkit.fluxcd.io
    Jan 23 14:49:13.438: INFO: PreferredVersion.GroupVersion: image.toolkit.fluxcd.io/v1beta2
    Jan 23 14:49:13.438: INFO: Versions found [{image.toolkit.fluxcd.io/v1beta2 v1beta2} {image.toolkit.fluxcd.io/v1beta1 v1beta1}]
    Jan 23 14:49:13.438: INFO: image.toolkit.fluxcd.io/v1beta2 matches image.toolkit.fluxcd.io/v1beta2
    Jan 23 14:49:13.438: INFO: Checking APIGroup: helm.toolkit.fluxcd.io
    Jan 23 14:49:13.439: INFO: PreferredVersion.GroupVersion: helm.toolkit.fluxcd.io/v2beta1
    Jan 23 14:49:13.439: INFO: Versions found [{helm.toolkit.fluxcd.io/v2beta1 v2beta1}]
    Jan 23 14:49:13.439: INFO: helm.toolkit.fluxcd.io/v2beta1 matches helm.toolkit.fluxcd.io/v2beta1
    Jan 23 14:49:13.439: INFO: Checking APIGroup: custom.metrics.k8s.io
    Jan 23 14:49:13.439: INFO: PreferredVersion.GroupVersion: custom.metrics.k8s.io/v1beta1
    Jan 23 14:49:13.439: INFO: Versions found [{custom.metrics.k8s.io/v1beta1 v1beta1}]
    Jan 23 14:49:13.439: INFO: custom.metrics.k8s.io/v1beta1 matches custom.metrics.k8s.io/v1beta1
    Jan 23 14:49:13.439: INFO: Checking APIGroup: metrics.k8s.io
    Jan 23 14:49:13.440: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Jan 23 14:49:13.440: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Jan 23 14:49:13.440: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:49:13.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-9632" for this suite. 01/23/24 14:49:13.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:49:13.446
Jan 23 14:49:13.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename services 01/23/24 14:49:13.447
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:49:13.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:49:13.455
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4869 01/23/24 14:49:13.457
STEP: changing the ExternalName service to type=ClusterIP 01/23/24 14:49:13.46
STEP: creating replication controller externalname-service in namespace services-4869 01/23/24 14:49:13.47
I0123 14:49:13.474043      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4869, replica count: 2
I0123 14:49:16.525097      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 23 14:49:16.525: INFO: Creating new exec pod
Jan 23 14:49:16.534: INFO: Waiting up to 5m0s for pod "execpod4l2qf" in namespace "services-4869" to be "running"
Jan 23 14:49:16.536: INFO: Pod "execpod4l2qf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.779777ms
Jan 23 14:49:18.538: INFO: Pod "execpod4l2qf": Phase="Running", Reason="", readiness=true. Elapsed: 2.004045782s
Jan 23 14:49:18.539: INFO: Pod "execpod4l2qf" satisfied condition "running"
Jan 23 14:49:19.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-4869 exec execpod4l2qf -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan 23 14:49:19.710: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 23 14:49:19.711: INFO: stdout: ""
Jan 23 14:49:19.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-4869 exec execpod4l2qf -- /bin/sh -x -c nc -v -z -w 2 10.233.27.44 80'
Jan 23 14:49:19.877: INFO: stderr: "+ nc -v -z -w 2 10.233.27.44 80\nConnection to 10.233.27.44 80 port [tcp/http] succeeded!\n"
Jan 23 14:49:19.877: INFO: stdout: ""
Jan 23 14:49:19.877: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 23 14:49:19.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4869" for this suite. 01/23/24 14:49:19.895
------------------------------
• [SLOW TEST] [6.452 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:49:13.446
    Jan 23 14:49:13.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename services 01/23/24 14:49:13.447
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:49:13.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:49:13.455
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4869 01/23/24 14:49:13.457
    STEP: changing the ExternalName service to type=ClusterIP 01/23/24 14:49:13.46
    STEP: creating replication controller externalname-service in namespace services-4869 01/23/24 14:49:13.47
    I0123 14:49:13.474043      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4869, replica count: 2
    I0123 14:49:16.525097      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 23 14:49:16.525: INFO: Creating new exec pod
    Jan 23 14:49:16.534: INFO: Waiting up to 5m0s for pod "execpod4l2qf" in namespace "services-4869" to be "running"
    Jan 23 14:49:16.536: INFO: Pod "execpod4l2qf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.779777ms
    Jan 23 14:49:18.538: INFO: Pod "execpod4l2qf": Phase="Running", Reason="", readiness=true. Elapsed: 2.004045782s
    Jan 23 14:49:18.539: INFO: Pod "execpod4l2qf" satisfied condition "running"
    Jan 23 14:49:19.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-4869 exec execpod4l2qf -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan 23 14:49:19.710: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 23 14:49:19.711: INFO: stdout: ""
    Jan 23 14:49:19.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=services-4869 exec execpod4l2qf -- /bin/sh -x -c nc -v -z -w 2 10.233.27.44 80'
    Jan 23 14:49:19.877: INFO: stderr: "+ nc -v -z -w 2 10.233.27.44 80\nConnection to 10.233.27.44 80 port [tcp/http] succeeded!\n"
    Jan 23 14:49:19.877: INFO: stdout: ""
    Jan 23 14:49:19.877: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:49:19.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4869" for this suite. 01/23/24 14:49:19.895
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:49:19.898
Jan 23 14:49:19.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename secrets 01/23/24 14:49:19.899
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:49:19.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:49:19.91
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-5d46e663-1e81-4818-806a-3e74f4d00cb3 01/23/24 14:49:19.912
STEP: Creating a pod to test consume secrets 01/23/24 14:49:19.916
Jan 23 14:49:19.927: INFO: Waiting up to 5m0s for pod "pod-secrets-8bcd1f16-87bc-4758-9725-32c34584177a" in namespace "secrets-1513" to be "Succeeded or Failed"
Jan 23 14:49:19.929: INFO: Pod "pod-secrets-8bcd1f16-87bc-4758-9725-32c34584177a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.868397ms
Jan 23 14:49:21.933: INFO: Pod "pod-secrets-8bcd1f16-87bc-4758-9725-32c34584177a": Phase="Running", Reason="", readiness=false. Elapsed: 2.005434203s
Jan 23 14:49:23.933: INFO: Pod "pod-secrets-8bcd1f16-87bc-4758-9725-32c34584177a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005579342s
STEP: Saw pod success 01/23/24 14:49:23.933
Jan 23 14:49:23.933: INFO: Pod "pod-secrets-8bcd1f16-87bc-4758-9725-32c34584177a" satisfied condition "Succeeded or Failed"
Jan 23 14:49:23.935: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-secrets-8bcd1f16-87bc-4758-9725-32c34584177a container secret-volume-test: <nil>
STEP: delete the pod 01/23/24 14:49:23.938
Jan 23 14:49:23.944: INFO: Waiting for pod pod-secrets-8bcd1f16-87bc-4758-9725-32c34584177a to disappear
Jan 23 14:49:23.945: INFO: Pod pod-secrets-8bcd1f16-87bc-4758-9725-32c34584177a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 23 14:49:23.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1513" for this suite. 01/23/24 14:49:23.947
------------------------------
• [4.052 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:49:19.898
    Jan 23 14:49:19.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename secrets 01/23/24 14:49:19.899
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:49:19.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:49:19.91
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-5d46e663-1e81-4818-806a-3e74f4d00cb3 01/23/24 14:49:19.912
    STEP: Creating a pod to test consume secrets 01/23/24 14:49:19.916
    Jan 23 14:49:19.927: INFO: Waiting up to 5m0s for pod "pod-secrets-8bcd1f16-87bc-4758-9725-32c34584177a" in namespace "secrets-1513" to be "Succeeded or Failed"
    Jan 23 14:49:19.929: INFO: Pod "pod-secrets-8bcd1f16-87bc-4758-9725-32c34584177a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.868397ms
    Jan 23 14:49:21.933: INFO: Pod "pod-secrets-8bcd1f16-87bc-4758-9725-32c34584177a": Phase="Running", Reason="", readiness=false. Elapsed: 2.005434203s
    Jan 23 14:49:23.933: INFO: Pod "pod-secrets-8bcd1f16-87bc-4758-9725-32c34584177a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005579342s
    STEP: Saw pod success 01/23/24 14:49:23.933
    Jan 23 14:49:23.933: INFO: Pod "pod-secrets-8bcd1f16-87bc-4758-9725-32c34584177a" satisfied condition "Succeeded or Failed"
    Jan 23 14:49:23.935: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-secrets-8bcd1f16-87bc-4758-9725-32c34584177a container secret-volume-test: <nil>
    STEP: delete the pod 01/23/24 14:49:23.938
    Jan 23 14:49:23.944: INFO: Waiting for pod pod-secrets-8bcd1f16-87bc-4758-9725-32c34584177a to disappear
    Jan 23 14:49:23.945: INFO: Pod pod-secrets-8bcd1f16-87bc-4758-9725-32c34584177a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:49:23.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1513" for this suite. 01/23/24 14:49:23.947
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:49:23.951
Jan 23 14:49:23.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 14:49:23.951
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:49:23.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:49:23.96
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-0402c39a-3ef3-40bd-8c46-a11b17c4a421 01/23/24 14:49:23.962
STEP: Creating a pod to test consume secrets 01/23/24 14:49:23.966
Jan 23 14:49:23.977: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1f3f6c5c-b490-4c38-a682-4b48163fc9f8" in namespace "projected-1808" to be "Succeeded or Failed"
Jan 23 14:49:23.978: INFO: Pod "pod-projected-secrets-1f3f6c5c-b490-4c38-a682-4b48163fc9f8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.324488ms
Jan 23 14:49:25.980: INFO: Pod "pod-projected-secrets-1f3f6c5c-b490-4c38-a682-4b48163fc9f8": Phase="Running", Reason="", readiness=false. Elapsed: 2.003771088s
Jan 23 14:49:27.980: INFO: Pod "pod-projected-secrets-1f3f6c5c-b490-4c38-a682-4b48163fc9f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003514374s
STEP: Saw pod success 01/23/24 14:49:27.98
Jan 23 14:49:27.980: INFO: Pod "pod-projected-secrets-1f3f6c5c-b490-4c38-a682-4b48163fc9f8" satisfied condition "Succeeded or Failed"
Jan 23 14:49:27.982: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-secrets-1f3f6c5c-b490-4c38-a682-4b48163fc9f8 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/23/24 14:49:27.985
Jan 23 14:49:27.993: INFO: Waiting for pod pod-projected-secrets-1f3f6c5c-b490-4c38-a682-4b48163fc9f8 to disappear
Jan 23 14:49:27.994: INFO: Pod pod-projected-secrets-1f3f6c5c-b490-4c38-a682-4b48163fc9f8 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 23 14:49:27.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1808" for this suite. 01/23/24 14:49:27.996
------------------------------
• [4.048 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:49:23.951
    Jan 23 14:49:23.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 14:49:23.951
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:49:23.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:49:23.96
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-0402c39a-3ef3-40bd-8c46-a11b17c4a421 01/23/24 14:49:23.962
    STEP: Creating a pod to test consume secrets 01/23/24 14:49:23.966
    Jan 23 14:49:23.977: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1f3f6c5c-b490-4c38-a682-4b48163fc9f8" in namespace "projected-1808" to be "Succeeded or Failed"
    Jan 23 14:49:23.978: INFO: Pod "pod-projected-secrets-1f3f6c5c-b490-4c38-a682-4b48163fc9f8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.324488ms
    Jan 23 14:49:25.980: INFO: Pod "pod-projected-secrets-1f3f6c5c-b490-4c38-a682-4b48163fc9f8": Phase="Running", Reason="", readiness=false. Elapsed: 2.003771088s
    Jan 23 14:49:27.980: INFO: Pod "pod-projected-secrets-1f3f6c5c-b490-4c38-a682-4b48163fc9f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003514374s
    STEP: Saw pod success 01/23/24 14:49:27.98
    Jan 23 14:49:27.980: INFO: Pod "pod-projected-secrets-1f3f6c5c-b490-4c38-a682-4b48163fc9f8" satisfied condition "Succeeded or Failed"
    Jan 23 14:49:27.982: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-projected-secrets-1f3f6c5c-b490-4c38-a682-4b48163fc9f8 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/23/24 14:49:27.985
    Jan 23 14:49:27.993: INFO: Waiting for pod pod-projected-secrets-1f3f6c5c-b490-4c38-a682-4b48163fc9f8 to disappear
    Jan 23 14:49:27.994: INFO: Pod pod-projected-secrets-1f3f6c5c-b490-4c38-a682-4b48163fc9f8 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:49:27.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1808" for this suite. 01/23/24 14:49:27.996
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:49:27.999
Jan 23 14:49:27.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename pods 01/23/24 14:49:28
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:49:28.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:49:28.008
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/23/24 14:49:28.01
STEP: submitting the pod to kubernetes 01/23/24 14:49:28.01
STEP: verifying QOS class is set on the pod 01/23/24 14:49:28.022
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Jan 23 14:49:28.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9594" for this suite. 01/23/24 14:49:28.027
------------------------------
• [0.031 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:49:27.999
    Jan 23 14:49:27.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename pods 01/23/24 14:49:28
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:49:28.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:49:28.008
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/23/24 14:49:28.01
    STEP: submitting the pod to kubernetes 01/23/24 14:49:28.01
    STEP: verifying QOS class is set on the pod 01/23/24 14:49:28.022
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:49:28.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9594" for this suite. 01/23/24 14:49:28.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:49:28.031
Jan 23 14:49:28.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename webhook 01/23/24 14:49:28.032
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:49:28.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:49:28.04
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/23/24 14:49:28.049
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:49:28.351
STEP: Deploying the webhook pod 01/23/24 14:49:28.357
STEP: Wait for the deployment to be ready 01/23/24 14:49:28.366
Jan 23 14:49:28.369: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/23/24 14:49:30.374
STEP: Verifying the service has paired with the endpoint 01/23/24 14:49:30.38
Jan 23 14:49:31.381: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Jan 23 14:49:31.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3447-crds.webhook.example.com via the AdmissionRegistration API 01/23/24 14:49:36.889
STEP: Creating a custom resource while v1 is storage version 01/23/24 14:49:36.9
STEP: Patching Custom Resource Definition to set v2 as storage 01/23/24 14:49:38.94
STEP: Patching the custom resource while v2 is storage version 01/23/24 14:49:38.956
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:49:39.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2740" for this suite. 01/23/24 14:49:39.542
STEP: Destroying namespace "webhook-2740-markers" for this suite. 01/23/24 14:49:39.547
------------------------------
• [SLOW TEST] [11.539 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:49:28.031
    Jan 23 14:49:28.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename webhook 01/23/24 14:49:28.032
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:49:28.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:49:28.04
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/23/24 14:49:28.049
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:49:28.351
    STEP: Deploying the webhook pod 01/23/24 14:49:28.357
    STEP: Wait for the deployment to be ready 01/23/24 14:49:28.366
    Jan 23 14:49:28.369: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/23/24 14:49:30.374
    STEP: Verifying the service has paired with the endpoint 01/23/24 14:49:30.38
    Jan 23 14:49:31.381: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Jan 23 14:49:31.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3447-crds.webhook.example.com via the AdmissionRegistration API 01/23/24 14:49:36.889
    STEP: Creating a custom resource while v1 is storage version 01/23/24 14:49:36.9
    STEP: Patching Custom Resource Definition to set v2 as storage 01/23/24 14:49:38.94
    STEP: Patching the custom resource while v2 is storage version 01/23/24 14:49:38.956
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:49:39.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2740" for this suite. 01/23/24 14:49:39.542
    STEP: Destroying namespace "webhook-2740-markers" for this suite. 01/23/24 14:49:39.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:49:39.571
Jan 23 14:49:39.571: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename subpath 01/23/24 14:49:39.572
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:49:39.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:49:39.588
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/23/24 14:49:39.589
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-nbs8 01/23/24 14:49:39.599
STEP: Creating a pod to test atomic-volume-subpath 01/23/24 14:49:39.6
Jan 23 14:49:39.622: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-nbs8" in namespace "subpath-9248" to be "Succeeded or Failed"
Jan 23 14:49:39.624: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.712842ms
Jan 23 14:49:41.627: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005645299s
Jan 23 14:49:43.628: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 4.005879072s
Jan 23 14:49:45.627: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 6.005243074s
Jan 23 14:49:47.643: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 8.020887274s
Jan 23 14:49:49.628: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 10.006005164s
Jan 23 14:49:51.628: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 12.005828213s
Jan 23 14:49:53.628: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 14.00649222s
Jan 23 14:49:55.627: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 16.005742838s
Jan 23 14:49:57.628: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 18.006205871s
Jan 23 14:49:59.628: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 20.006335024s
Jan 23 14:50:01.628: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 22.005956689s
Jan 23 14:50:03.628: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=false. Elapsed: 24.006118475s
Jan 23 14:50:05.627: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.005325624s
STEP: Saw pod success 01/23/24 14:50:05.627
Jan 23 14:50:05.627: INFO: Pod "pod-subpath-test-secret-nbs8" satisfied condition "Succeeded or Failed"
Jan 23 14:50:05.628: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-subpath-test-secret-nbs8 container test-container-subpath-secret-nbs8: <nil>
STEP: delete the pod 01/23/24 14:50:05.632
Jan 23 14:50:05.638: INFO: Waiting for pod pod-subpath-test-secret-nbs8 to disappear
Jan 23 14:50:05.640: INFO: Pod pod-subpath-test-secret-nbs8 no longer exists
STEP: Deleting pod pod-subpath-test-secret-nbs8 01/23/24 14:50:05.64
Jan 23 14:50:05.640: INFO: Deleting pod "pod-subpath-test-secret-nbs8" in namespace "subpath-9248"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 23 14:50:05.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9248" for this suite. 01/23/24 14:50:05.643
------------------------------
• [SLOW TEST] [26.075 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:49:39.571
    Jan 23 14:49:39.571: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename subpath 01/23/24 14:49:39.572
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:49:39.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:49:39.588
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/23/24 14:49:39.589
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-nbs8 01/23/24 14:49:39.599
    STEP: Creating a pod to test atomic-volume-subpath 01/23/24 14:49:39.6
    Jan 23 14:49:39.622: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-nbs8" in namespace "subpath-9248" to be "Succeeded or Failed"
    Jan 23 14:49:39.624: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.712842ms
    Jan 23 14:49:41.627: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005645299s
    Jan 23 14:49:43.628: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 4.005879072s
    Jan 23 14:49:45.627: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 6.005243074s
    Jan 23 14:49:47.643: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 8.020887274s
    Jan 23 14:49:49.628: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 10.006005164s
    Jan 23 14:49:51.628: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 12.005828213s
    Jan 23 14:49:53.628: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 14.00649222s
    Jan 23 14:49:55.627: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 16.005742838s
    Jan 23 14:49:57.628: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 18.006205871s
    Jan 23 14:49:59.628: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 20.006335024s
    Jan 23 14:50:01.628: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=true. Elapsed: 22.005956689s
    Jan 23 14:50:03.628: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Running", Reason="", readiness=false. Elapsed: 24.006118475s
    Jan 23 14:50:05.627: INFO: Pod "pod-subpath-test-secret-nbs8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.005325624s
    STEP: Saw pod success 01/23/24 14:50:05.627
    Jan 23 14:50:05.627: INFO: Pod "pod-subpath-test-secret-nbs8" satisfied condition "Succeeded or Failed"
    Jan 23 14:50:05.628: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-subpath-test-secret-nbs8 container test-container-subpath-secret-nbs8: <nil>
    STEP: delete the pod 01/23/24 14:50:05.632
    Jan 23 14:50:05.638: INFO: Waiting for pod pod-subpath-test-secret-nbs8 to disappear
    Jan 23 14:50:05.640: INFO: Pod pod-subpath-test-secret-nbs8 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-nbs8 01/23/24 14:50:05.64
    Jan 23 14:50:05.640: INFO: Deleting pod "pod-subpath-test-secret-nbs8" in namespace "subpath-9248"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:50:05.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9248" for this suite. 01/23/24 14:50:05.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:50:05.646
Jan 23 14:50:05.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename init-container 01/23/24 14:50:05.647
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:50:05.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:50:05.655
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 01/23/24 14:50:05.657
Jan 23 14:50:05.657: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:50:10.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-757" for this suite. 01/23/24 14:50:10.927
------------------------------
• [SLOW TEST] [5.284 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:50:05.646
    Jan 23 14:50:05.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename init-container 01/23/24 14:50:05.647
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:50:05.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:50:05.655
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 01/23/24 14:50:05.657
    Jan 23 14:50:05.657: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:50:10.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-757" for this suite. 01/23/24 14:50:10.927
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:50:10.93
Jan 23 14:50:10.930: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename var-expansion 01/23/24 14:50:10.933
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:50:10.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:50:10.941
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Jan 23 14:50:10.958: INFO: Waiting up to 2m0s for pod "var-expansion-823ba52f-8996-48c6-aaf7-759fe356ce9a" in namespace "var-expansion-6839" to be "container 0 failed with reason CreateContainerConfigError"
Jan 23 14:50:10.962: INFO: Pod "var-expansion-823ba52f-8996-48c6-aaf7-759fe356ce9a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.454182ms
Jan 23 14:50:12.964: INFO: Pod "var-expansion-823ba52f-8996-48c6-aaf7-759fe356ce9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005808755s
Jan 23 14:50:12.964: INFO: Pod "var-expansion-823ba52f-8996-48c6-aaf7-759fe356ce9a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 23 14:50:12.964: INFO: Deleting pod "var-expansion-823ba52f-8996-48c6-aaf7-759fe356ce9a" in namespace "var-expansion-6839"
Jan 23 14:50:12.969: INFO: Wait up to 5m0s for pod "var-expansion-823ba52f-8996-48c6-aaf7-759fe356ce9a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 23 14:50:16.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6839" for this suite. 01/23/24 14:50:16.976
------------------------------
• [SLOW TEST] [6.049 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:50:10.93
    Jan 23 14:50:10.930: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename var-expansion 01/23/24 14:50:10.933
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:50:10.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:50:10.941
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Jan 23 14:50:10.958: INFO: Waiting up to 2m0s for pod "var-expansion-823ba52f-8996-48c6-aaf7-759fe356ce9a" in namespace "var-expansion-6839" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 23 14:50:10.962: INFO: Pod "var-expansion-823ba52f-8996-48c6-aaf7-759fe356ce9a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.454182ms
    Jan 23 14:50:12.964: INFO: Pod "var-expansion-823ba52f-8996-48c6-aaf7-759fe356ce9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005808755s
    Jan 23 14:50:12.964: INFO: Pod "var-expansion-823ba52f-8996-48c6-aaf7-759fe356ce9a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 23 14:50:12.964: INFO: Deleting pod "var-expansion-823ba52f-8996-48c6-aaf7-759fe356ce9a" in namespace "var-expansion-6839"
    Jan 23 14:50:12.969: INFO: Wait up to 5m0s for pod "var-expansion-823ba52f-8996-48c6-aaf7-759fe356ce9a" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:50:16.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6839" for this suite. 01/23/24 14:50:16.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:50:16.98
Jan 23 14:50:16.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename var-expansion 01/23/24 14:50:16.981
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:50:16.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:50:16.991
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 01/23/24 14:50:16.992
Jan 23 14:50:17.009: INFO: Waiting up to 5m0s for pod "var-expansion-61c2b0eb-8809-4ce0-adb7-a040a315cfe3" in namespace "var-expansion-2727" to be "Succeeded or Failed"
Jan 23 14:50:17.010: INFO: Pod "var-expansion-61c2b0eb-8809-4ce0-adb7-a040a315cfe3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.326532ms
Jan 23 14:50:19.014: INFO: Pod "var-expansion-61c2b0eb-8809-4ce0-adb7-a040a315cfe3": Phase="Running", Reason="", readiness=false. Elapsed: 2.00502801s
Jan 23 14:50:21.013: INFO: Pod "var-expansion-61c2b0eb-8809-4ce0-adb7-a040a315cfe3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003982463s
STEP: Saw pod success 01/23/24 14:50:21.013
Jan 23 14:50:21.013: INFO: Pod "var-expansion-61c2b0eb-8809-4ce0-adb7-a040a315cfe3" satisfied condition "Succeeded or Failed"
Jan 23 14:50:21.015: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod var-expansion-61c2b0eb-8809-4ce0-adb7-a040a315cfe3 container dapi-container: <nil>
STEP: delete the pod 01/23/24 14:50:21.018
Jan 23 14:50:21.024: INFO: Waiting for pod var-expansion-61c2b0eb-8809-4ce0-adb7-a040a315cfe3 to disappear
Jan 23 14:50:21.026: INFO: Pod var-expansion-61c2b0eb-8809-4ce0-adb7-a040a315cfe3 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 23 14:50:21.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2727" for this suite. 01/23/24 14:50:21.028
------------------------------
• [4.051 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:50:16.98
    Jan 23 14:50:16.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename var-expansion 01/23/24 14:50:16.981
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:50:16.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:50:16.991
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 01/23/24 14:50:16.992
    Jan 23 14:50:17.009: INFO: Waiting up to 5m0s for pod "var-expansion-61c2b0eb-8809-4ce0-adb7-a040a315cfe3" in namespace "var-expansion-2727" to be "Succeeded or Failed"
    Jan 23 14:50:17.010: INFO: Pod "var-expansion-61c2b0eb-8809-4ce0-adb7-a040a315cfe3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.326532ms
    Jan 23 14:50:19.014: INFO: Pod "var-expansion-61c2b0eb-8809-4ce0-adb7-a040a315cfe3": Phase="Running", Reason="", readiness=false. Elapsed: 2.00502801s
    Jan 23 14:50:21.013: INFO: Pod "var-expansion-61c2b0eb-8809-4ce0-adb7-a040a315cfe3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003982463s
    STEP: Saw pod success 01/23/24 14:50:21.013
    Jan 23 14:50:21.013: INFO: Pod "var-expansion-61c2b0eb-8809-4ce0-adb7-a040a315cfe3" satisfied condition "Succeeded or Failed"
    Jan 23 14:50:21.015: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod var-expansion-61c2b0eb-8809-4ce0-adb7-a040a315cfe3 container dapi-container: <nil>
    STEP: delete the pod 01/23/24 14:50:21.018
    Jan 23 14:50:21.024: INFO: Waiting for pod var-expansion-61c2b0eb-8809-4ce0-adb7-a040a315cfe3 to disappear
    Jan 23 14:50:21.026: INFO: Pod var-expansion-61c2b0eb-8809-4ce0-adb7-a040a315cfe3 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:50:21.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2727" for this suite. 01/23/24 14:50:21.028
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:50:21.031
Jan 23 14:50:21.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 14:50:21.032
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:50:21.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:50:21.041
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-09208e27-1cd8-484e-be86-50b65dbf0bf3 01/23/24 14:50:21.044
STEP: Creating secret with name s-test-opt-upd-10469174-52f1-492b-aa4f-2d5932c3bf4d 01/23/24 14:50:21.048
STEP: Creating the pod 01/23/24 14:50:21.052
Jan 23 14:50:21.072: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c4016034-239e-489b-adef-be149ae741df" in namespace "projected-3583" to be "running and ready"
Jan 23 14:50:21.074: INFO: Pod "pod-projected-secrets-c4016034-239e-489b-adef-be149ae741df": Phase="Pending", Reason="", readiness=false. Elapsed: 1.297194ms
Jan 23 14:50:21.074: INFO: The phase of Pod pod-projected-secrets-c4016034-239e-489b-adef-be149ae741df is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:50:23.077: INFO: Pod "pod-projected-secrets-c4016034-239e-489b-adef-be149ae741df": Phase="Running", Reason="", readiness=true. Elapsed: 2.004401327s
Jan 23 14:50:23.077: INFO: The phase of Pod pod-projected-secrets-c4016034-239e-489b-adef-be149ae741df is Running (Ready = true)
Jan 23 14:50:23.077: INFO: Pod "pod-projected-secrets-c4016034-239e-489b-adef-be149ae741df" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-09208e27-1cd8-484e-be86-50b65dbf0bf3 01/23/24 14:50:23.088
STEP: Updating secret s-test-opt-upd-10469174-52f1-492b-aa4f-2d5932c3bf4d 01/23/24 14:50:23.091
STEP: Creating secret with name s-test-opt-create-1cc090c6-1ba4-42a9-9f57-fcaf5c6d5bd1 01/23/24 14:50:23.096
STEP: waiting to observe update in volume 01/23/24 14:50:23.1
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 23 14:50:27.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3583" for this suite. 01/23/24 14:50:27.124
------------------------------
• [SLOW TEST] [6.097 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:50:21.031
    Jan 23 14:50:21.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 14:50:21.032
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:50:21.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:50:21.041
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-09208e27-1cd8-484e-be86-50b65dbf0bf3 01/23/24 14:50:21.044
    STEP: Creating secret with name s-test-opt-upd-10469174-52f1-492b-aa4f-2d5932c3bf4d 01/23/24 14:50:21.048
    STEP: Creating the pod 01/23/24 14:50:21.052
    Jan 23 14:50:21.072: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c4016034-239e-489b-adef-be149ae741df" in namespace "projected-3583" to be "running and ready"
    Jan 23 14:50:21.074: INFO: Pod "pod-projected-secrets-c4016034-239e-489b-adef-be149ae741df": Phase="Pending", Reason="", readiness=false. Elapsed: 1.297194ms
    Jan 23 14:50:21.074: INFO: The phase of Pod pod-projected-secrets-c4016034-239e-489b-adef-be149ae741df is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:50:23.077: INFO: Pod "pod-projected-secrets-c4016034-239e-489b-adef-be149ae741df": Phase="Running", Reason="", readiness=true. Elapsed: 2.004401327s
    Jan 23 14:50:23.077: INFO: The phase of Pod pod-projected-secrets-c4016034-239e-489b-adef-be149ae741df is Running (Ready = true)
    Jan 23 14:50:23.077: INFO: Pod "pod-projected-secrets-c4016034-239e-489b-adef-be149ae741df" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-09208e27-1cd8-484e-be86-50b65dbf0bf3 01/23/24 14:50:23.088
    STEP: Updating secret s-test-opt-upd-10469174-52f1-492b-aa4f-2d5932c3bf4d 01/23/24 14:50:23.091
    STEP: Creating secret with name s-test-opt-create-1cc090c6-1ba4-42a9-9f57-fcaf5c6d5bd1 01/23/24 14:50:23.096
    STEP: waiting to observe update in volume 01/23/24 14:50:23.1
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:50:27.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3583" for this suite. 01/23/24 14:50:27.124
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:50:27.128
Jan 23 14:50:27.129: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename services 01/23/24 14:50:27.129
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:50:27.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:50:27.138
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 01/23/24 14:50:27.14
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 23 14:50:27.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9330" for this suite. 01/23/24 14:50:27.146
------------------------------
• [0.020 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:50:27.128
    Jan 23 14:50:27.129: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename services 01/23/24 14:50:27.129
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:50:27.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:50:27.138
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 01/23/24 14:50:27.14
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:50:27.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9330" for this suite. 01/23/24 14:50:27.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:50:27.149
Jan 23 14:50:27.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename pods 01/23/24 14:50:27.15
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:50:27.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:50:27.161
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 01/23/24 14:50:27.163
STEP: setting up watch 01/23/24 14:50:27.163
STEP: submitting the pod to kubernetes 01/23/24 14:50:27.265
STEP: verifying the pod is in kubernetes 01/23/24 14:50:27.277
STEP: verifying pod creation was observed 01/23/24 14:50:27.279
Jan 23 14:50:27.279: INFO: Waiting up to 5m0s for pod "pod-submit-remove-01295607-2f4a-4c0c-940b-694e0ebd848d" in namespace "pods-6886" to be "running"
Jan 23 14:50:27.280: INFO: Pod "pod-submit-remove-01295607-2f4a-4c0c-940b-694e0ebd848d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.359746ms
Jan 23 14:50:29.283: INFO: Pod "pod-submit-remove-01295607-2f4a-4c0c-940b-694e0ebd848d": Phase="Running", Reason="", readiness=true. Elapsed: 2.003983659s
Jan 23 14:50:29.283: INFO: Pod "pod-submit-remove-01295607-2f4a-4c0c-940b-694e0ebd848d" satisfied condition "running"
STEP: deleting the pod gracefully 01/23/24 14:50:29.285
STEP: verifying pod deletion was observed 01/23/24 14:50:29.289
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 23 14:50:32.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6886" for this suite. 01/23/24 14:50:32.999
------------------------------
• [SLOW TEST] [5.853 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:50:27.149
    Jan 23 14:50:27.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename pods 01/23/24 14:50:27.15
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:50:27.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:50:27.161
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 01/23/24 14:50:27.163
    STEP: setting up watch 01/23/24 14:50:27.163
    STEP: submitting the pod to kubernetes 01/23/24 14:50:27.265
    STEP: verifying the pod is in kubernetes 01/23/24 14:50:27.277
    STEP: verifying pod creation was observed 01/23/24 14:50:27.279
    Jan 23 14:50:27.279: INFO: Waiting up to 5m0s for pod "pod-submit-remove-01295607-2f4a-4c0c-940b-694e0ebd848d" in namespace "pods-6886" to be "running"
    Jan 23 14:50:27.280: INFO: Pod "pod-submit-remove-01295607-2f4a-4c0c-940b-694e0ebd848d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.359746ms
    Jan 23 14:50:29.283: INFO: Pod "pod-submit-remove-01295607-2f4a-4c0c-940b-694e0ebd848d": Phase="Running", Reason="", readiness=true. Elapsed: 2.003983659s
    Jan 23 14:50:29.283: INFO: Pod "pod-submit-remove-01295607-2f4a-4c0c-940b-694e0ebd848d" satisfied condition "running"
    STEP: deleting the pod gracefully 01/23/24 14:50:29.285
    STEP: verifying pod deletion was observed 01/23/24 14:50:29.289
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:50:32.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6886" for this suite. 01/23/24 14:50:32.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:50:33.002
Jan 23 14:50:33.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename deployment 01/23/24 14:50:33.003
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:50:33.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:50:33.012
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan 23 14:50:33.014: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 23 14:50:33.018: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 23 14:50:38.021: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/23/24 14:50:38.021
Jan 23 14:50:38.021: INFO: Creating deployment "test-rolling-update-deployment"
Jan 23 14:50:38.023: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 23 14:50:38.036: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 23 14:50:40.040: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 23 14:50:40.041: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 23 14:50:40.045: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7091  3881492b-a69f-4e6d-bc3e-4e9a82afcc0a 153851 1 2024-01-23 14:50:38 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2024-01-23 14:50:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:50:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b4b4048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-23 14:50:38 +0000 UTC,LastTransitionTime:2024-01-23 14:50:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2024-01-23 14:50:39 +0000 UTC,LastTransitionTime:2024-01-23 14:50:38 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 23 14:50:40.047: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-7091  efc63132-e759-4bec-948c-6384650c6b1d 153841 1 2024-01-23 14:50:38 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3881492b-a69f-4e6d-bc3e-4e9a82afcc0a 0xc00b3f5c17 0xc00b3f5c18}] [] [{kube-controller-manager Update apps/v1 2024-01-23 14:50:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3881492b-a69f-4e6d-bc3e-4e9a82afcc0a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:50:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b3f5cc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 23 14:50:40.047: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 23 14:50:40.047: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7091  61e172d7-e84a-477c-8e64-aea06b465965 153850 2 2024-01-23 14:50:33 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3881492b-a69f-4e6d-bc3e-4e9a82afcc0a 0xc00b3f5ae7 0xc00b3f5ae8}] [] [{e2e.test Update apps/v1 2024-01-23 14:50:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:50:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3881492b-a69f-4e6d-bc3e-4e9a82afcc0a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:50:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00b3f5ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 23 14:50:40.048: INFO: Pod "test-rolling-update-deployment-7549d9f46d-plsp5" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-plsp5 test-rolling-update-deployment-7549d9f46d- deployment-7091  d7b6666b-fd3a-487f-b7d6-66b0d96513fd 153840 0 2024-01-23 14:50:38 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:c7fe8ee0db572aefe8bfa03af3971b709666be4da099a69c8a5d7f2feb463191 cni.projectcalico.org/podIP:10.233.87.152/32 cni.projectcalico.org/podIPs:10.233.87.152/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d efc63132-e759-4bec-948c-6384650c6b1d 0xc00b5a6147 0xc00b5a6148}] [] [{calico Update v1 2024-01-23 14:50:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-23 14:50:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efc63132-e759-4bec-948c-6384650c6b1d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 14:50:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jpnw7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jpnw7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:50:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:50:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:50:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:50:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.152,StartTime:2024-01-23 14:50:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 14:50:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://e14e0051b175ec0983936f0d8138bd44578ba436faf41723005196da8f283f0b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 23 14:50:40.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7091" for this suite. 01/23/24 14:50:40.051
------------------------------
• [SLOW TEST] [7.052 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:50:33.002
    Jan 23 14:50:33.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename deployment 01/23/24 14:50:33.003
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:50:33.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:50:33.012
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan 23 14:50:33.014: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan 23 14:50:33.018: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 23 14:50:38.021: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/23/24 14:50:38.021
    Jan 23 14:50:38.021: INFO: Creating deployment "test-rolling-update-deployment"
    Jan 23 14:50:38.023: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan 23 14:50:38.036: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan 23 14:50:40.040: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan 23 14:50:40.041: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 23 14:50:40.045: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7091  3881492b-a69f-4e6d-bc3e-4e9a82afcc0a 153851 1 2024-01-23 14:50:38 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2024-01-23 14:50:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:50:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b4b4048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-23 14:50:38 +0000 UTC,LastTransitionTime:2024-01-23 14:50:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2024-01-23 14:50:39 +0000 UTC,LastTransitionTime:2024-01-23 14:50:38 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 23 14:50:40.047: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-7091  efc63132-e759-4bec-948c-6384650c6b1d 153841 1 2024-01-23 14:50:38 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3881492b-a69f-4e6d-bc3e-4e9a82afcc0a 0xc00b3f5c17 0xc00b3f5c18}] [] [{kube-controller-manager Update apps/v1 2024-01-23 14:50:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3881492b-a69f-4e6d-bc3e-4e9a82afcc0a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:50:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b3f5cc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 23 14:50:40.047: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan 23 14:50:40.047: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7091  61e172d7-e84a-477c-8e64-aea06b465965 153850 2 2024-01-23 14:50:33 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3881492b-a69f-4e6d-bc3e-4e9a82afcc0a 0xc00b3f5ae7 0xc00b3f5ae8}] [] [{e2e.test Update apps/v1 2024-01-23 14:50:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:50:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3881492b-a69f-4e6d-bc3e-4e9a82afcc0a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2024-01-23 14:50:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00b3f5ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 23 14:50:40.048: INFO: Pod "test-rolling-update-deployment-7549d9f46d-plsp5" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-plsp5 test-rolling-update-deployment-7549d9f46d- deployment-7091  d7b6666b-fd3a-487f-b7d6-66b0d96513fd 153840 0 2024-01-23 14:50:38 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:c7fe8ee0db572aefe8bfa03af3971b709666be4da099a69c8a5d7f2feb463191 cni.projectcalico.org/podIP:10.233.87.152/32 cni.projectcalico.org/podIPs:10.233.87.152/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d efc63132-e759-4bec-948c-6384650c6b1d 0xc00b5a6147 0xc00b5a6148}] [] [{calico Update v1 2024-01-23 14:50:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-23 14:50:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efc63132-e759-4bec-948c-6384650c6b1d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-23 14:50:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.87.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jpnw7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jpnw7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:50:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:50:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:50:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-23 14:50:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.67,PodIP:10.233.87.152,StartTime:2024-01-23 14:50:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-23 14:50:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://e14e0051b175ec0983936f0d8138bd44578ba436faf41723005196da8f283f0b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.87.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:50:40.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7091" for this suite. 01/23/24 14:50:40.051
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:50:40.055
Jan 23 14:50:40.055: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename statefulset 01/23/24 14:50:40.055
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:50:40.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:50:40.066
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5709 01/23/24 14:50:40.068
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-5709 01/23/24 14:50:40.075
Jan 23 14:50:40.080: INFO: Found 0 stateful pods, waiting for 1
Jan 23 14:50:50.083: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/23/24 14:50:50.086
STEP: Getting /status 01/23/24 14:50:50.091
Jan 23 14:50:50.093: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/23/24 14:50:50.093
Jan 23 14:50:50.097: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/23/24 14:50:50.097
Jan 23 14:50:50.099: INFO: Observed &StatefulSet event: ADDED
Jan 23 14:50:50.099: INFO: Found Statefulset ss in namespace statefulset-5709 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 23 14:50:50.099: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/23/24 14:50:50.099
Jan 23 14:50:50.099: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 23 14:50:50.104: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/23/24 14:50:50.104
Jan 23 14:50:50.105: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 23 14:50:50.105: INFO: Deleting all statefulset in ns statefulset-5709
Jan 23 14:50:50.106: INFO: Scaling statefulset ss to 0
Jan 23 14:51:00.116: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 14:51:00.118: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 23 14:51:00.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5709" for this suite. 01/23/24 14:51:00.137
------------------------------
• [SLOW TEST] [20.086 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:50:40.055
    Jan 23 14:50:40.055: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename statefulset 01/23/24 14:50:40.055
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:50:40.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:50:40.066
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5709 01/23/24 14:50:40.068
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-5709 01/23/24 14:50:40.075
    Jan 23 14:50:40.080: INFO: Found 0 stateful pods, waiting for 1
    Jan 23 14:50:50.083: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/23/24 14:50:50.086
    STEP: Getting /status 01/23/24 14:50:50.091
    Jan 23 14:50:50.093: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/23/24 14:50:50.093
    Jan 23 14:50:50.097: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/23/24 14:50:50.097
    Jan 23 14:50:50.099: INFO: Observed &StatefulSet event: ADDED
    Jan 23 14:50:50.099: INFO: Found Statefulset ss in namespace statefulset-5709 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 23 14:50:50.099: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/23/24 14:50:50.099
    Jan 23 14:50:50.099: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 23 14:50:50.104: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/23/24 14:50:50.104
    Jan 23 14:50:50.105: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 23 14:50:50.105: INFO: Deleting all statefulset in ns statefulset-5709
    Jan 23 14:50:50.106: INFO: Scaling statefulset ss to 0
    Jan 23 14:51:00.116: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 23 14:51:00.118: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:51:00.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5709" for this suite. 01/23/24 14:51:00.137
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:51:00.141
Jan 23 14:51:00.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename configmap 01/23/24 14:51:00.142
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:00.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:00.155
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-531cb023-6659-4c7f-bcba-7acfbf6afe5b 01/23/24 14:51:00.16
STEP: Creating configMap with name cm-test-opt-upd-4247a627-4e10-4423-a34f-66f5ab969d8e 01/23/24 14:51:00.164
STEP: Creating the pod 01/23/24 14:51:00.167
Jan 23 14:51:00.188: INFO: Waiting up to 5m0s for pod "pod-configmaps-2b93682f-4513-49d7-b1e2-09bc1b81f1c1" in namespace "configmap-445" to be "running and ready"
Jan 23 14:51:00.189: INFO: Pod "pod-configmaps-2b93682f-4513-49d7-b1e2-09bc1b81f1c1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.397181ms
Jan 23 14:51:00.189: INFO: The phase of Pod pod-configmaps-2b93682f-4513-49d7-b1e2-09bc1b81f1c1 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:51:02.192: INFO: Pod "pod-configmaps-2b93682f-4513-49d7-b1e2-09bc1b81f1c1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004299557s
Jan 23 14:51:02.192: INFO: The phase of Pod pod-configmaps-2b93682f-4513-49d7-b1e2-09bc1b81f1c1 is Running (Ready = true)
Jan 23 14:51:02.192: INFO: Pod "pod-configmaps-2b93682f-4513-49d7-b1e2-09bc1b81f1c1" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-531cb023-6659-4c7f-bcba-7acfbf6afe5b 01/23/24 14:51:02.205
STEP: Updating configmap cm-test-opt-upd-4247a627-4e10-4423-a34f-66f5ab969d8e 01/23/24 14:51:02.208
STEP: Creating configMap with name cm-test-opt-create-08e142e6-38a2-4001-9d7e-c75c9831a955 01/23/24 14:51:02.213
STEP: waiting to observe update in volume 01/23/24 14:51:02.216
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 23 14:51:06.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-445" for this suite. 01/23/24 14:51:06.241
------------------------------
• [SLOW TEST] [6.104 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:51:00.141
    Jan 23 14:51:00.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename configmap 01/23/24 14:51:00.142
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:00.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:00.155
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-531cb023-6659-4c7f-bcba-7acfbf6afe5b 01/23/24 14:51:00.16
    STEP: Creating configMap with name cm-test-opt-upd-4247a627-4e10-4423-a34f-66f5ab969d8e 01/23/24 14:51:00.164
    STEP: Creating the pod 01/23/24 14:51:00.167
    Jan 23 14:51:00.188: INFO: Waiting up to 5m0s for pod "pod-configmaps-2b93682f-4513-49d7-b1e2-09bc1b81f1c1" in namespace "configmap-445" to be "running and ready"
    Jan 23 14:51:00.189: INFO: Pod "pod-configmaps-2b93682f-4513-49d7-b1e2-09bc1b81f1c1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.397181ms
    Jan 23 14:51:00.189: INFO: The phase of Pod pod-configmaps-2b93682f-4513-49d7-b1e2-09bc1b81f1c1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:51:02.192: INFO: Pod "pod-configmaps-2b93682f-4513-49d7-b1e2-09bc1b81f1c1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004299557s
    Jan 23 14:51:02.192: INFO: The phase of Pod pod-configmaps-2b93682f-4513-49d7-b1e2-09bc1b81f1c1 is Running (Ready = true)
    Jan 23 14:51:02.192: INFO: Pod "pod-configmaps-2b93682f-4513-49d7-b1e2-09bc1b81f1c1" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-531cb023-6659-4c7f-bcba-7acfbf6afe5b 01/23/24 14:51:02.205
    STEP: Updating configmap cm-test-opt-upd-4247a627-4e10-4423-a34f-66f5ab969d8e 01/23/24 14:51:02.208
    STEP: Creating configMap with name cm-test-opt-create-08e142e6-38a2-4001-9d7e-c75c9831a955 01/23/24 14:51:02.213
    STEP: waiting to observe update in volume 01/23/24 14:51:02.216
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:51:06.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-445" for this suite. 01/23/24 14:51:06.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:51:06.245
Jan 23 14:51:06.245: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename configmap 01/23/24 14:51:06.246
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:06.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:06.255
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-5939/configmap-test-4318a684-5d33-4c6a-991d-dd0ddab467c9 01/23/24 14:51:06.257
STEP: Creating a pod to test consume configMaps 01/23/24 14:51:06.261
Jan 23 14:51:06.279: INFO: Waiting up to 5m0s for pod "pod-configmaps-8b6313d1-cc2b-4ca4-93a7-35b2d1ca8bea" in namespace "configmap-5939" to be "Succeeded or Failed"
Jan 23 14:51:06.280: INFO: Pod "pod-configmaps-8b6313d1-cc2b-4ca4-93a7-35b2d1ca8bea": Phase="Pending", Reason="", readiness=false. Elapsed: 1.450784ms
Jan 23 14:51:08.284: INFO: Pod "pod-configmaps-8b6313d1-cc2b-4ca4-93a7-35b2d1ca8bea": Phase="Running", Reason="", readiness=false. Elapsed: 2.005365964s
Jan 23 14:51:10.284: INFO: Pod "pod-configmaps-8b6313d1-cc2b-4ca4-93a7-35b2d1ca8bea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005449944s
STEP: Saw pod success 01/23/24 14:51:10.284
Jan 23 14:51:10.284: INFO: Pod "pod-configmaps-8b6313d1-cc2b-4ca4-93a7-35b2d1ca8bea" satisfied condition "Succeeded or Failed"
Jan 23 14:51:10.286: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-8b6313d1-cc2b-4ca4-93a7-35b2d1ca8bea container env-test: <nil>
STEP: delete the pod 01/23/24 14:51:10.296
Jan 23 14:51:10.304: INFO: Waiting for pod pod-configmaps-8b6313d1-cc2b-4ca4-93a7-35b2d1ca8bea to disappear
Jan 23 14:51:10.305: INFO: Pod pod-configmaps-8b6313d1-cc2b-4ca4-93a7-35b2d1ca8bea no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 23 14:51:10.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5939" for this suite. 01/23/24 14:51:10.308
------------------------------
• [4.065 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:51:06.245
    Jan 23 14:51:06.245: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename configmap 01/23/24 14:51:06.246
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:06.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:06.255
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-5939/configmap-test-4318a684-5d33-4c6a-991d-dd0ddab467c9 01/23/24 14:51:06.257
    STEP: Creating a pod to test consume configMaps 01/23/24 14:51:06.261
    Jan 23 14:51:06.279: INFO: Waiting up to 5m0s for pod "pod-configmaps-8b6313d1-cc2b-4ca4-93a7-35b2d1ca8bea" in namespace "configmap-5939" to be "Succeeded or Failed"
    Jan 23 14:51:06.280: INFO: Pod "pod-configmaps-8b6313d1-cc2b-4ca4-93a7-35b2d1ca8bea": Phase="Pending", Reason="", readiness=false. Elapsed: 1.450784ms
    Jan 23 14:51:08.284: INFO: Pod "pod-configmaps-8b6313d1-cc2b-4ca4-93a7-35b2d1ca8bea": Phase="Running", Reason="", readiness=false. Elapsed: 2.005365964s
    Jan 23 14:51:10.284: INFO: Pod "pod-configmaps-8b6313d1-cc2b-4ca4-93a7-35b2d1ca8bea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005449944s
    STEP: Saw pod success 01/23/24 14:51:10.284
    Jan 23 14:51:10.284: INFO: Pod "pod-configmaps-8b6313d1-cc2b-4ca4-93a7-35b2d1ca8bea" satisfied condition "Succeeded or Failed"
    Jan 23 14:51:10.286: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-configmaps-8b6313d1-cc2b-4ca4-93a7-35b2d1ca8bea container env-test: <nil>
    STEP: delete the pod 01/23/24 14:51:10.296
    Jan 23 14:51:10.304: INFO: Waiting for pod pod-configmaps-8b6313d1-cc2b-4ca4-93a7-35b2d1ca8bea to disappear
    Jan 23 14:51:10.305: INFO: Pod pod-configmaps-8b6313d1-cc2b-4ca4-93a7-35b2d1ca8bea no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:51:10.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5939" for this suite. 01/23/24 14:51:10.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:51:10.311
Jan 23 14:51:10.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename emptydir 01/23/24 14:51:10.312
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:10.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:10.32
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 01/23/24 14:51:10.322
Jan 23 14:51:10.336: INFO: Waiting up to 5m0s for pod "pod-cac20457-afd5-47b3-9ded-5ee7212c804c" in namespace "emptydir-68" to be "Succeeded or Failed"
Jan 23 14:51:10.337: INFO: Pod "pod-cac20457-afd5-47b3-9ded-5ee7212c804c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.26945ms
Jan 23 14:51:12.340: INFO: Pod "pod-cac20457-afd5-47b3-9ded-5ee7212c804c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004303269s
Jan 23 14:51:14.340: INFO: Pod "pod-cac20457-afd5-47b3-9ded-5ee7212c804c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00334403s
STEP: Saw pod success 01/23/24 14:51:14.34
Jan 23 14:51:14.340: INFO: Pod "pod-cac20457-afd5-47b3-9ded-5ee7212c804c" satisfied condition "Succeeded or Failed"
Jan 23 14:51:14.341: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-cac20457-afd5-47b3-9ded-5ee7212c804c container test-container: <nil>
STEP: delete the pod 01/23/24 14:51:14.344
Jan 23 14:51:14.353: INFO: Waiting for pod pod-cac20457-afd5-47b3-9ded-5ee7212c804c to disappear
Jan 23 14:51:14.355: INFO: Pod pod-cac20457-afd5-47b3-9ded-5ee7212c804c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 23 14:51:14.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-68" for this suite. 01/23/24 14:51:14.357
------------------------------
• [4.049 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:51:10.311
    Jan 23 14:51:10.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename emptydir 01/23/24 14:51:10.312
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:10.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:10.32
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/23/24 14:51:10.322
    Jan 23 14:51:10.336: INFO: Waiting up to 5m0s for pod "pod-cac20457-afd5-47b3-9ded-5ee7212c804c" in namespace "emptydir-68" to be "Succeeded or Failed"
    Jan 23 14:51:10.337: INFO: Pod "pod-cac20457-afd5-47b3-9ded-5ee7212c804c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.26945ms
    Jan 23 14:51:12.340: INFO: Pod "pod-cac20457-afd5-47b3-9ded-5ee7212c804c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004303269s
    Jan 23 14:51:14.340: INFO: Pod "pod-cac20457-afd5-47b3-9ded-5ee7212c804c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00334403s
    STEP: Saw pod success 01/23/24 14:51:14.34
    Jan 23 14:51:14.340: INFO: Pod "pod-cac20457-afd5-47b3-9ded-5ee7212c804c" satisfied condition "Succeeded or Failed"
    Jan 23 14:51:14.341: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-cac20457-afd5-47b3-9ded-5ee7212c804c container test-container: <nil>
    STEP: delete the pod 01/23/24 14:51:14.344
    Jan 23 14:51:14.353: INFO: Waiting for pod pod-cac20457-afd5-47b3-9ded-5ee7212c804c to disappear
    Jan 23 14:51:14.355: INFO: Pod pod-cac20457-afd5-47b3-9ded-5ee7212c804c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:51:14.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-68" for this suite. 01/23/24 14:51:14.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:51:14.36
Jan 23 14:51:14.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename crd-publish-openapi 01/23/24 14:51:14.361
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:14.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:14.37
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Jan 23 14:51:14.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/23/24 14:51:22.836
Jan 23 14:51:22.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5790 --namespace=crd-publish-openapi-5790 create -f -'
Jan 23 14:51:24.013: INFO: stderr: ""
Jan 23 14:51:24.013: INFO: stdout: "e2e-test-crd-publish-openapi-4096-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 23 14:51:24.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5790 --namespace=crd-publish-openapi-5790 delete e2e-test-crd-publish-openapi-4096-crds test-cr'
Jan 23 14:51:24.147: INFO: stderr: ""
Jan 23 14:51:24.147: INFO: stdout: "e2e-test-crd-publish-openapi-4096-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 23 14:51:24.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5790 --namespace=crd-publish-openapi-5790 apply -f -'
Jan 23 14:51:24.452: INFO: stderr: ""
Jan 23 14:51:24.452: INFO: stdout: "e2e-test-crd-publish-openapi-4096-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 23 14:51:24.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5790 --namespace=crd-publish-openapi-5790 delete e2e-test-crd-publish-openapi-4096-crds test-cr'
Jan 23 14:51:24.588: INFO: stderr: ""
Jan 23 14:51:24.589: INFO: stdout: "e2e-test-crd-publish-openapi-4096-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/23/24 14:51:24.589
Jan 23 14:51:24.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5790 explain e2e-test-crd-publish-openapi-4096-crds'
Jan 23 14:51:24.930: INFO: stderr: ""
Jan 23 14:51:24.930: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4096-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:51:26.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5790" for this suite. 01/23/24 14:51:26.833
------------------------------
• [SLOW TEST] [12.476 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:51:14.36
    Jan 23 14:51:14.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename crd-publish-openapi 01/23/24 14:51:14.361
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:14.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:14.37
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Jan 23 14:51:14.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/23/24 14:51:22.836
    Jan 23 14:51:22.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5790 --namespace=crd-publish-openapi-5790 create -f -'
    Jan 23 14:51:24.013: INFO: stderr: ""
    Jan 23 14:51:24.013: INFO: stdout: "e2e-test-crd-publish-openapi-4096-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 23 14:51:24.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5790 --namespace=crd-publish-openapi-5790 delete e2e-test-crd-publish-openapi-4096-crds test-cr'
    Jan 23 14:51:24.147: INFO: stderr: ""
    Jan 23 14:51:24.147: INFO: stdout: "e2e-test-crd-publish-openapi-4096-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan 23 14:51:24.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5790 --namespace=crd-publish-openapi-5790 apply -f -'
    Jan 23 14:51:24.452: INFO: stderr: ""
    Jan 23 14:51:24.452: INFO: stdout: "e2e-test-crd-publish-openapi-4096-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 23 14:51:24.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5790 --namespace=crd-publish-openapi-5790 delete e2e-test-crd-publish-openapi-4096-crds test-cr'
    Jan 23 14:51:24.588: INFO: stderr: ""
    Jan 23 14:51:24.589: INFO: stdout: "e2e-test-crd-publish-openapi-4096-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/23/24 14:51:24.589
    Jan 23 14:51:24.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=crd-publish-openapi-5790 explain e2e-test-crd-publish-openapi-4096-crds'
    Jan 23 14:51:24.930: INFO: stderr: ""
    Jan 23 14:51:24.930: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4096-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:51:26.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5790" for this suite. 01/23/24 14:51:26.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:51:26.837
Jan 23 14:51:26.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename svc-latency 01/23/24 14:51:26.838
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:26.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:26.846
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan 23 14:51:26.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4662 01/23/24 14:51:26.849
I0123 14:51:26.852889      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4662, replica count: 1
I0123 14:51:27.904210      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0123 14:51:28.905252      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 23 14:51:29.012: INFO: Created: latency-svc-cqt5g
Jan 23 14:51:29.016: INFO: Got endpoints: latency-svc-cqt5g [10.821231ms]
Jan 23 14:51:29.023: INFO: Created: latency-svc-f76ww
Jan 23 14:51:29.026: INFO: Created: latency-svc-4wd29
Jan 23 14:51:29.027: INFO: Got endpoints: latency-svc-f76ww [10.944968ms]
Jan 23 14:51:29.030: INFO: Got endpoints: latency-svc-4wd29 [12.898398ms]
Jan 23 14:51:29.031: INFO: Created: latency-svc-s65cd
Jan 23 14:51:29.034: INFO: Got endpoints: latency-svc-s65cd [16.814992ms]
Jan 23 14:51:29.089: INFO: Created: latency-svc-9spg7
Jan 23 14:51:29.097: INFO: Created: latency-svc-mnkdj
Jan 23 14:51:29.097: INFO: Created: latency-svc-8ptgz
Jan 23 14:51:29.097: INFO: Created: latency-svc-frdvj
Jan 23 14:51:29.097: INFO: Created: latency-svc-r97dl
Jan 23 14:51:29.097: INFO: Created: latency-svc-2nqtw
Jan 23 14:51:29.097: INFO: Created: latency-svc-w4vft
Jan 23 14:51:29.097: INFO: Created: latency-svc-m7cvq
Jan 23 14:51:29.097: INFO: Created: latency-svc-bf9w6
Jan 23 14:51:29.097: INFO: Created: latency-svc-v9qp8
Jan 23 14:51:29.098: INFO: Created: latency-svc-cbm2b
Jan 23 14:51:29.098: INFO: Created: latency-svc-gk7pf
Jan 23 14:51:29.098: INFO: Created: latency-svc-6g9l9
Jan 23 14:51:29.098: INFO: Created: latency-svc-jmqlt
Jan 23 14:51:29.100: INFO: Created: latency-svc-pmz2c
Jan 23 14:51:29.101: INFO: Got endpoints: latency-svc-9spg7 [82.843312ms]
Jan 23 14:51:29.103: INFO: Got endpoints: latency-svc-2nqtw [84.934336ms]
Jan 23 14:51:29.114: INFO: Got endpoints: latency-svc-mnkdj [84.269352ms]
Jan 23 14:51:29.114: INFO: Got endpoints: latency-svc-w4vft [96.372247ms]
Jan 23 14:51:29.115: INFO: Got endpoints: latency-svc-jmqlt [97.071888ms]
Jan 23 14:51:29.115: INFO: Got endpoints: latency-svc-gk7pf [97.187097ms]
Jan 23 14:51:29.115: INFO: Got endpoints: latency-svc-8ptgz [97.360706ms]
Jan 23 14:51:29.125: INFO: Got endpoints: latency-svc-m7cvq [106.766534ms]
Jan 23 14:51:29.125: INFO: Got endpoints: latency-svc-frdvj [91.045743ms]
Jan 23 14:51:29.125: INFO: Got endpoints: latency-svc-pmz2c [106.961009ms]
Jan 23 14:51:29.125: INFO: Got endpoints: latency-svc-r97dl [107.944059ms]
Jan 23 14:51:29.130: INFO: Got endpoints: latency-svc-v9qp8 [102.013843ms]
Jan 23 14:51:29.136: INFO: Created: latency-svc-42225
Jan 23 14:51:29.137: INFO: Got endpoints: latency-svc-cbm2b [119.639782ms]
Jan 23 14:51:29.137: INFO: Got endpoints: latency-svc-6g9l9 [119.540003ms]
Jan 23 14:51:29.137: INFO: Got endpoints: latency-svc-bf9w6 [119.644597ms]
Jan 23 14:51:29.149: INFO: Got endpoints: latency-svc-42225 [48.53278ms]
Jan 23 14:51:29.150: INFO: Created: latency-svc-d6nhs
Jan 23 14:51:29.159: INFO: Got endpoints: latency-svc-d6nhs [55.751331ms]
Jan 23 14:51:29.161: INFO: Created: latency-svc-mgn8r
Jan 23 14:51:29.246: INFO: Got endpoints: latency-svc-mgn8r [131.494916ms]
Jan 23 14:51:29.252: INFO: Created: latency-svc-v498m
Jan 23 14:51:29.261: INFO: Got endpoints: latency-svc-v498m [146.042493ms]
Jan 23 14:51:29.264: INFO: Created: latency-svc-c9qkf
Jan 23 14:51:29.273: INFO: Got endpoints: latency-svc-c9qkf [157.733344ms]
Jan 23 14:51:29.276: INFO: Created: latency-svc-s8457
Jan 23 14:51:29.289: INFO: Got endpoints: latency-svc-s8457 [173.548626ms]
Jan 23 14:51:29.292: INFO: Created: latency-svc-q5jpx
Jan 23 14:51:29.302: INFO: Got endpoints: latency-svc-q5jpx [187.531283ms]
Jan 23 14:51:29.305: INFO: Created: latency-svc-jjlvs
Jan 23 14:51:29.314: INFO: Got endpoints: latency-svc-jjlvs [189.05808ms]
Jan 23 14:51:29.317: INFO: Created: latency-svc-7fxtl
Jan 23 14:51:29.330: INFO: Got endpoints: latency-svc-7fxtl [205.122623ms]
Jan 23 14:51:29.333: INFO: Created: latency-svc-fqvrz
Jan 23 14:51:29.390: INFO: Got endpoints: latency-svc-fqvrz [264.710386ms]
Jan 23 14:51:29.392: INFO: Created: latency-svc-hr4bk
Jan 23 14:51:29.405: INFO: Got endpoints: latency-svc-hr4bk [279.685493ms]
Jan 23 14:51:29.410: INFO: Created: latency-svc-9vvzc
Jan 23 14:51:29.421: INFO: Got endpoints: latency-svc-9vvzc [291.143496ms]
Jan 23 14:51:29.424: INFO: Created: latency-svc-8h46b
Jan 23 14:51:29.432: INFO: Got endpoints: latency-svc-8h46b [295.088192ms]
Jan 23 14:51:29.435: INFO: Created: latency-svc-nh8xm
Jan 23 14:51:29.445: INFO: Got endpoints: latency-svc-nh8xm [307.409724ms]
Jan 23 14:51:29.451: INFO: Created: latency-svc-8gxcw
Jan 23 14:51:29.461: INFO: Got endpoints: latency-svc-8gxcw [323.742064ms]
Jan 23 14:51:29.464: INFO: Created: latency-svc-ssl79
Jan 23 14:51:29.473: INFO: Got endpoints: latency-svc-ssl79 [323.690283ms]
Jan 23 14:51:29.476: INFO: Created: latency-svc-k7ss9
Jan 23 14:51:29.485: INFO: Got endpoints: latency-svc-k7ss9 [326.871468ms]
Jan 23 14:51:29.539: INFO: Created: latency-svc-8lrxb
Jan 23 14:51:29.542: INFO: Created: latency-svc-jmlx4
Jan 23 14:51:29.556: INFO: Got endpoints: latency-svc-jmlx4 [294.450121ms]
Jan 23 14:51:29.556: INFO: Got endpoints: latency-svc-8lrxb [310.091769ms]
Jan 23 14:51:29.559: INFO: Created: latency-svc-kcwvm
Jan 23 14:51:29.570: INFO: Got endpoints: latency-svc-kcwvm [296.607744ms]
Jan 23 14:51:29.577: INFO: Created: latency-svc-xsm94
Jan 23 14:51:29.582: INFO: Got endpoints: latency-svc-xsm94 [292.97585ms]
Jan 23 14:51:29.590: INFO: Created: latency-svc-f8978
Jan 23 14:51:29.598: INFO: Got endpoints: latency-svc-f8978 [296.844751ms]
Jan 23 14:51:29.601: INFO: Created: latency-svc-rn968
Jan 23 14:51:29.611: INFO: Created: latency-svc-npjz6
Jan 23 14:51:29.619: INFO: Got endpoints: latency-svc-rn968 [305.600954ms]
Jan 23 14:51:29.623: INFO: Created: latency-svc-7njts
Jan 23 14:51:29.636: INFO: Created: latency-svc-c7j7r
Jan 23 14:51:29.692: INFO: Got endpoints: latency-svc-npjz6 [362.016929ms]
Jan 23 14:51:29.695: INFO: Created: latency-svc-8ps4d
Jan 23 14:51:29.705: INFO: Created: latency-svc-bc59k
Jan 23 14:51:29.714: INFO: Created: latency-svc-46nmx
Jan 23 14:51:29.724: INFO: Got endpoints: latency-svc-7njts [334.039356ms]
Jan 23 14:51:29.726: INFO: Created: latency-svc-xq2hc
Jan 23 14:51:29.736: INFO: Created: latency-svc-ntgkk
Jan 23 14:51:29.745: INFO: Created: latency-svc-jnnxp
Jan 23 14:51:29.755: INFO: Created: latency-svc-24scp
Jan 23 14:51:29.765: INFO: Created: latency-svc-twt9v
Jan 23 14:51:29.773: INFO: Got endpoints: latency-svc-c7j7r [368.616857ms]
Jan 23 14:51:29.776: INFO: Created: latency-svc-jp2nl
Jan 23 14:51:29.785: INFO: Created: latency-svc-bbq5j
Jan 23 14:51:29.843: INFO: Got endpoints: latency-svc-8ps4d [422.27063ms]
Jan 23 14:51:29.843: INFO: Created: latency-svc-srv6q
Jan 23 14:51:29.865: INFO: Created: latency-svc-v4662
Jan 23 14:51:29.870: INFO: Got endpoints: latency-svc-bc59k [437.119912ms]
Jan 23 14:51:29.876: INFO: Created: latency-svc-9ks7v
Jan 23 14:51:29.886: INFO: Created: latency-svc-w44qx
Jan 23 14:51:29.895: INFO: Created: latency-svc-xzlzt
Jan 23 14:51:29.905: INFO: Created: latency-svc-j8sc4
Jan 23 14:51:29.911: INFO: Created: latency-svc-v49qm
Jan 23 14:51:29.916: INFO: Got endpoints: latency-svc-46nmx [471.357543ms]
Jan 23 14:51:29.933: INFO: Created: latency-svc-n8hkm
Jan 23 14:51:29.976: INFO: Got endpoints: latency-svc-xq2hc [515.276228ms]
Jan 23 14:51:29.992: INFO: Created: latency-svc-2npgn
Jan 23 14:51:30.016: INFO: Got endpoints: latency-svc-ntgkk [542.561709ms]
Jan 23 14:51:30.024: INFO: Created: latency-svc-p7th2
Jan 23 14:51:30.064: INFO: Got endpoints: latency-svc-jnnxp [578.520404ms]
Jan 23 14:51:30.070: INFO: Created: latency-svc-wn7cj
Jan 23 14:51:30.115: INFO: Got endpoints: latency-svc-24scp [559.249432ms]
Jan 23 14:51:30.120: INFO: Created: latency-svc-szdpw
Jan 23 14:51:30.165: INFO: Got endpoints: latency-svc-twt9v [609.554908ms]
Jan 23 14:51:30.172: INFO: Created: latency-svc-vncn9
Jan 23 14:51:30.217: INFO: Got endpoints: latency-svc-jp2nl [647.43131ms]
Jan 23 14:51:30.223: INFO: Created: latency-svc-9r54m
Jan 23 14:51:30.265: INFO: Got endpoints: latency-svc-bbq5j [683.086196ms]
Jan 23 14:51:30.271: INFO: Created: latency-svc-7dzh5
Jan 23 14:51:30.315: INFO: Got endpoints: latency-svc-srv6q [716.761287ms]
Jan 23 14:51:30.320: INFO: Created: latency-svc-vmwjs
Jan 23 14:51:30.366: INFO: Got endpoints: latency-svc-v4662 [746.947812ms]
Jan 23 14:51:30.372: INFO: Created: latency-svc-t82ml
Jan 23 14:51:30.416: INFO: Got endpoints: latency-svc-9ks7v [724.205074ms]
Jan 23 14:51:30.423: INFO: Created: latency-svc-vngqj
Jan 23 14:51:30.466: INFO: Got endpoints: latency-svc-w44qx [741.889144ms]
Jan 23 14:51:30.471: INFO: Created: latency-svc-dcwj9
Jan 23 14:51:30.516: INFO: Got endpoints: latency-svc-xzlzt [742.950187ms]
Jan 23 14:51:30.522: INFO: Created: latency-svc-wn5bm
Jan 23 14:51:30.566: INFO: Got endpoints: latency-svc-j8sc4 [722.821564ms]
Jan 23 14:51:30.571: INFO: Created: latency-svc-fpvgk
Jan 23 14:51:30.616: INFO: Got endpoints: latency-svc-v49qm [746.769862ms]
Jan 23 14:51:30.623: INFO: Created: latency-svc-bt5vt
Jan 23 14:51:30.666: INFO: Got endpoints: latency-svc-n8hkm [749.519931ms]
Jan 23 14:51:30.673: INFO: Created: latency-svc-ckwx8
Jan 23 14:51:30.716: INFO: Got endpoints: latency-svc-2npgn [739.462387ms]
Jan 23 14:51:30.723: INFO: Created: latency-svc-lpmnp
Jan 23 14:51:30.767: INFO: Got endpoints: latency-svc-p7th2 [750.867831ms]
Jan 23 14:51:30.773: INFO: Created: latency-svc-mltb8
Jan 23 14:51:30.818: INFO: Got endpoints: latency-svc-wn7cj [753.738026ms]
Jan 23 14:51:30.823: INFO: Created: latency-svc-tkg6b
Jan 23 14:51:30.867: INFO: Got endpoints: latency-svc-szdpw [751.861204ms]
Jan 23 14:51:30.875: INFO: Created: latency-svc-4mmbq
Jan 23 14:51:30.915: INFO: Got endpoints: latency-svc-vncn9 [749.846885ms]
Jan 23 14:51:30.922: INFO: Created: latency-svc-54bn5
Jan 23 14:51:30.966: INFO: Got endpoints: latency-svc-9r54m [749.038883ms]
Jan 23 14:51:30.972: INFO: Created: latency-svc-982v4
Jan 23 14:51:31.020: INFO: Got endpoints: latency-svc-7dzh5 [754.90727ms]
Jan 23 14:51:31.028: INFO: Created: latency-svc-t8dnw
Jan 23 14:51:31.067: INFO: Got endpoints: latency-svc-vmwjs [751.502046ms]
Jan 23 14:51:31.072: INFO: Created: latency-svc-bb7hj
Jan 23 14:51:31.118: INFO: Got endpoints: latency-svc-t82ml [752.103074ms]
Jan 23 14:51:31.125: INFO: Created: latency-svc-ftzbv
Jan 23 14:51:31.166: INFO: Got endpoints: latency-svc-vngqj [749.366464ms]
Jan 23 14:51:31.172: INFO: Created: latency-svc-r4wjf
Jan 23 14:51:31.216: INFO: Got endpoints: latency-svc-dcwj9 [750.528767ms]
Jan 23 14:51:31.222: INFO: Created: latency-svc-l9qx7
Jan 23 14:51:31.267: INFO: Got endpoints: latency-svc-wn5bm [750.704065ms]
Jan 23 14:51:31.272: INFO: Created: latency-svc-5qhks
Jan 23 14:51:31.316: INFO: Got endpoints: latency-svc-fpvgk [750.141838ms]
Jan 23 14:51:31.323: INFO: Created: latency-svc-cvxl6
Jan 23 14:51:31.365: INFO: Got endpoints: latency-svc-bt5vt [748.855958ms]
Jan 23 14:51:31.370: INFO: Created: latency-svc-9phpm
Jan 23 14:51:31.417: INFO: Got endpoints: latency-svc-ckwx8 [751.246917ms]
Jan 23 14:51:31.423: INFO: Created: latency-svc-f9pst
Jan 23 14:51:31.467: INFO: Got endpoints: latency-svc-lpmnp [750.594628ms]
Jan 23 14:51:31.473: INFO: Created: latency-svc-rmvv8
Jan 23 14:51:31.517: INFO: Got endpoints: latency-svc-mltb8 [750.381375ms]
Jan 23 14:51:31.524: INFO: Created: latency-svc-lqjf4
Jan 23 14:51:31.565: INFO: Got endpoints: latency-svc-tkg6b [747.274668ms]
Jan 23 14:51:31.572: INFO: Created: latency-svc-hhxqp
Jan 23 14:51:31.617: INFO: Got endpoints: latency-svc-4mmbq [750.454612ms]
Jan 23 14:51:31.623: INFO: Created: latency-svc-8gkwl
Jan 23 14:51:31.667: INFO: Got endpoints: latency-svc-54bn5 [751.310016ms]
Jan 23 14:51:31.677: INFO: Created: latency-svc-qh8b5
Jan 23 14:51:31.715: INFO: Got endpoints: latency-svc-982v4 [749.33778ms]
Jan 23 14:51:31.721: INFO: Created: latency-svc-x5w87
Jan 23 14:51:31.768: INFO: Got endpoints: latency-svc-t8dnw [747.392326ms]
Jan 23 14:51:31.773: INFO: Created: latency-svc-c5t2x
Jan 23 14:51:31.816: INFO: Got endpoints: latency-svc-bb7hj [749.449755ms]
Jan 23 14:51:31.824: INFO: Created: latency-svc-6mlhk
Jan 23 14:51:31.867: INFO: Got endpoints: latency-svc-ftzbv [748.703478ms]
Jan 23 14:51:31.873: INFO: Created: latency-svc-4h6h8
Jan 23 14:51:31.917: INFO: Got endpoints: latency-svc-r4wjf [751.532084ms]
Jan 23 14:51:31.923: INFO: Created: latency-svc-qwm5d
Jan 23 14:51:31.966: INFO: Got endpoints: latency-svc-l9qx7 [749.688315ms]
Jan 23 14:51:31.971: INFO: Created: latency-svc-r9d4l
Jan 23 14:51:32.015: INFO: Got endpoints: latency-svc-5qhks [748.358614ms]
Jan 23 14:51:32.022: INFO: Created: latency-svc-pr7sh
Jan 23 14:51:32.066: INFO: Got endpoints: latency-svc-cvxl6 [750.32246ms]
Jan 23 14:51:32.072: INFO: Created: latency-svc-c7894
Jan 23 14:51:32.117: INFO: Got endpoints: latency-svc-9phpm [752.148962ms]
Jan 23 14:51:32.125: INFO: Created: latency-svc-mnccl
Jan 23 14:51:32.166: INFO: Got endpoints: latency-svc-f9pst [749.148013ms]
Jan 23 14:51:32.174: INFO: Created: latency-svc-9vpdm
Jan 23 14:51:32.216: INFO: Got endpoints: latency-svc-rmvv8 [749.567191ms]
Jan 23 14:51:32.222: INFO: Created: latency-svc-cx2h2
Jan 23 14:51:32.265: INFO: Got endpoints: latency-svc-lqjf4 [748.484158ms]
Jan 23 14:51:32.272: INFO: Created: latency-svc-jtl82
Jan 23 14:51:32.317: INFO: Got endpoints: latency-svc-hhxqp [751.633896ms]
Jan 23 14:51:32.325: INFO: Created: latency-svc-f4btv
Jan 23 14:51:32.367: INFO: Got endpoints: latency-svc-8gkwl [749.91347ms]
Jan 23 14:51:32.373: INFO: Created: latency-svc-x8db2
Jan 23 14:51:32.417: INFO: Got endpoints: latency-svc-qh8b5 [750.61589ms]
Jan 23 14:51:32.424: INFO: Created: latency-svc-9vzzd
Jan 23 14:51:32.465: INFO: Got endpoints: latency-svc-x5w87 [749.970354ms]
Jan 23 14:51:32.471: INFO: Created: latency-svc-9vd9l
Jan 23 14:51:32.518: INFO: Got endpoints: latency-svc-c5t2x [750.080519ms]
Jan 23 14:51:32.523: INFO: Created: latency-svc-kmmgd
Jan 23 14:51:32.566: INFO: Got endpoints: latency-svc-6mlhk [749.369058ms]
Jan 23 14:51:32.572: INFO: Created: latency-svc-xn9ls
Jan 23 14:51:32.616: INFO: Got endpoints: latency-svc-4h6h8 [748.647962ms]
Jan 23 14:51:32.622: INFO: Created: latency-svc-bfjpq
Jan 23 14:51:32.667: INFO: Got endpoints: latency-svc-qwm5d [749.56962ms]
Jan 23 14:51:32.673: INFO: Created: latency-svc-x4lhx
Jan 23 14:51:32.716: INFO: Got endpoints: latency-svc-r9d4l [749.6486ms]
Jan 23 14:51:32.723: INFO: Created: latency-svc-jhnsf
Jan 23 14:51:32.767: INFO: Got endpoints: latency-svc-pr7sh [752.032885ms]
Jan 23 14:51:32.773: INFO: Created: latency-svc-x74m6
Jan 23 14:51:32.818: INFO: Got endpoints: latency-svc-c7894 [751.984648ms]
Jan 23 14:51:32.825: INFO: Created: latency-svc-dwrv8
Jan 23 14:51:32.867: INFO: Got endpoints: latency-svc-mnccl [749.002756ms]
Jan 23 14:51:32.873: INFO: Created: latency-svc-b9wh4
Jan 23 14:51:32.917: INFO: Got endpoints: latency-svc-9vpdm [750.541843ms]
Jan 23 14:51:32.923: INFO: Created: latency-svc-tls85
Jan 23 14:51:32.966: INFO: Got endpoints: latency-svc-cx2h2 [750.196137ms]
Jan 23 14:51:32.972: INFO: Created: latency-svc-z2qqh
Jan 23 14:51:33.017: INFO: Got endpoints: latency-svc-jtl82 [751.098693ms]
Jan 23 14:51:33.024: INFO: Created: latency-svc-fctsw
Jan 23 14:51:33.067: INFO: Got endpoints: latency-svc-f4btv [750.563906ms]
Jan 23 14:51:33.074: INFO: Created: latency-svc-vtvd6
Jan 23 14:51:33.119: INFO: Got endpoints: latency-svc-x8db2 [751.354148ms]
Jan 23 14:51:33.125: INFO: Created: latency-svc-xtpgb
Jan 23 14:51:33.165: INFO: Got endpoints: latency-svc-9vzzd [748.166919ms]
Jan 23 14:51:33.172: INFO: Created: latency-svc-48jxf
Jan 23 14:51:33.217: INFO: Got endpoints: latency-svc-9vd9l [751.257924ms]
Jan 23 14:51:33.227: INFO: Created: latency-svc-q5rgx
Jan 23 14:51:33.266: INFO: Got endpoints: latency-svc-kmmgd [748.684905ms]
Jan 23 14:51:33.272: INFO: Created: latency-svc-cfhl2
Jan 23 14:51:33.315: INFO: Got endpoints: latency-svc-xn9ls [749.795734ms]
Jan 23 14:51:33.322: INFO: Created: latency-svc-clmw5
Jan 23 14:51:33.368: INFO: Got endpoints: latency-svc-bfjpq [752.197747ms]
Jan 23 14:51:33.379: INFO: Created: latency-svc-dsbtg
Jan 23 14:51:33.416: INFO: Got endpoints: latency-svc-x4lhx [749.491615ms]
Jan 23 14:51:33.423: INFO: Created: latency-svc-lb4js
Jan 23 14:51:33.465: INFO: Got endpoints: latency-svc-jhnsf [749.783017ms]
Jan 23 14:51:33.472: INFO: Created: latency-svc-6xs6w
Jan 23 14:51:33.516: INFO: Got endpoints: latency-svc-x74m6 [749.018043ms]
Jan 23 14:51:33.522: INFO: Created: latency-svc-64ffk
Jan 23 14:51:33.571: INFO: Got endpoints: latency-svc-dwrv8 [752.972321ms]
Jan 23 14:51:33.579: INFO: Created: latency-svc-kgk9t
Jan 23 14:51:33.616: INFO: Got endpoints: latency-svc-b9wh4 [749.312501ms]
Jan 23 14:51:33.621: INFO: Created: latency-svc-vs4sd
Jan 23 14:51:33.666: INFO: Got endpoints: latency-svc-tls85 [748.991991ms]
Jan 23 14:51:33.672: INFO: Created: latency-svc-cx2tl
Jan 23 14:51:33.716: INFO: Got endpoints: latency-svc-z2qqh [749.950123ms]
Jan 23 14:51:33.723: INFO: Created: latency-svc-92dvf
Jan 23 14:51:33.765: INFO: Got endpoints: latency-svc-fctsw [748.274655ms]
Jan 23 14:51:33.771: INFO: Created: latency-svc-cvzzn
Jan 23 14:51:33.816: INFO: Got endpoints: latency-svc-vtvd6 [748.13765ms]
Jan 23 14:51:33.821: INFO: Created: latency-svc-gzwcz
Jan 23 14:51:33.867: INFO: Got endpoints: latency-svc-xtpgb [748.129818ms]
Jan 23 14:51:33.873: INFO: Created: latency-svc-t27nq
Jan 23 14:51:33.917: INFO: Got endpoints: latency-svc-48jxf [751.771124ms]
Jan 23 14:51:33.923: INFO: Created: latency-svc-swg7l
Jan 23 14:51:33.966: INFO: Got endpoints: latency-svc-q5rgx [748.817496ms]
Jan 23 14:51:33.971: INFO: Created: latency-svc-j97gc
Jan 23 14:51:34.017: INFO: Got endpoints: latency-svc-cfhl2 [750.388418ms]
Jan 23 14:51:34.023: INFO: Created: latency-svc-csbgh
Jan 23 14:51:34.067: INFO: Got endpoints: latency-svc-clmw5 [751.687165ms]
Jan 23 14:51:34.074: INFO: Created: latency-svc-kpf7q
Jan 23 14:51:34.116: INFO: Got endpoints: latency-svc-dsbtg [747.418121ms]
Jan 23 14:51:34.123: INFO: Created: latency-svc-gs4sr
Jan 23 14:51:34.167: INFO: Got endpoints: latency-svc-lb4js [751.110039ms]
Jan 23 14:51:34.173: INFO: Created: latency-svc-j8h5q
Jan 23 14:51:34.215: INFO: Got endpoints: latency-svc-6xs6w [749.827094ms]
Jan 23 14:51:34.220: INFO: Created: latency-svc-84h42
Jan 23 14:51:34.266: INFO: Got endpoints: latency-svc-64ffk [749.956791ms]
Jan 23 14:51:34.272: INFO: Created: latency-svc-wqcgs
Jan 23 14:51:34.317: INFO: Got endpoints: latency-svc-kgk9t [745.456821ms]
Jan 23 14:51:34.323: INFO: Created: latency-svc-skl5g
Jan 23 14:51:34.366: INFO: Got endpoints: latency-svc-vs4sd [750.116864ms]
Jan 23 14:51:34.372: INFO: Created: latency-svc-78zvv
Jan 23 14:51:34.416: INFO: Got endpoints: latency-svc-cx2tl [749.888554ms]
Jan 23 14:51:34.422: INFO: Created: latency-svc-4w5mk
Jan 23 14:51:34.466: INFO: Got endpoints: latency-svc-92dvf [749.974316ms]
Jan 23 14:51:34.473: INFO: Created: latency-svc-rt4f4
Jan 23 14:51:34.515: INFO: Got endpoints: latency-svc-cvzzn [750.287721ms]
Jan 23 14:51:34.522: INFO: Created: latency-svc-b6b4z
Jan 23 14:51:34.565: INFO: Got endpoints: latency-svc-gzwcz [749.055982ms]
Jan 23 14:51:34.573: INFO: Created: latency-svc-rk7jr
Jan 23 14:51:34.617: INFO: Got endpoints: latency-svc-t27nq [750.736145ms]
Jan 23 14:51:34.624: INFO: Created: latency-svc-mrpps
Jan 23 14:51:34.667: INFO: Got endpoints: latency-svc-swg7l [749.735585ms]
Jan 23 14:51:34.673: INFO: Created: latency-svc-8bzwv
Jan 23 14:51:34.716: INFO: Got endpoints: latency-svc-j97gc [750.659718ms]
Jan 23 14:51:34.722: INFO: Created: latency-svc-4rpx8
Jan 23 14:51:34.768: INFO: Got endpoints: latency-svc-csbgh [750.932823ms]
Jan 23 14:51:34.774: INFO: Created: latency-svc-54q9r
Jan 23 14:51:34.816: INFO: Got endpoints: latency-svc-kpf7q [748.648671ms]
Jan 23 14:51:34.822: INFO: Created: latency-svc-tq2v2
Jan 23 14:51:34.866: INFO: Got endpoints: latency-svc-gs4sr [750.020761ms]
Jan 23 14:51:34.871: INFO: Created: latency-svc-2gdgn
Jan 23 14:51:34.916: INFO: Got endpoints: latency-svc-j8h5q [748.475361ms]
Jan 23 14:51:34.922: INFO: Created: latency-svc-pgfc7
Jan 23 14:51:34.966: INFO: Got endpoints: latency-svc-84h42 [751.074011ms]
Jan 23 14:51:34.975: INFO: Created: latency-svc-sp86r
Jan 23 14:51:35.016: INFO: Got endpoints: latency-svc-wqcgs [749.248003ms]
Jan 23 14:51:35.022: INFO: Created: latency-svc-jqc4k
Jan 23 14:51:35.126: INFO: Got endpoints: latency-svc-skl5g [809.455803ms]
Jan 23 14:51:35.154: INFO: Got endpoints: latency-svc-78zvv [787.897171ms]
Jan 23 14:51:35.187: INFO: Got endpoints: latency-svc-4w5mk [770.926478ms]
Jan 23 14:51:35.187: INFO: Created: latency-svc-7hplj
Jan 23 14:51:35.198: INFO: Created: latency-svc-7j6ch
Jan 23 14:51:35.213: INFO: Created: latency-svc-xnbkn
Jan 23 14:51:35.216: INFO: Got endpoints: latency-svc-rt4f4 [749.548476ms]
Jan 23 14:51:35.224: INFO: Created: latency-svc-vd5qs
Jan 23 14:51:35.268: INFO: Got endpoints: latency-svc-b6b4z [752.177428ms]
Jan 23 14:51:35.274: INFO: Created: latency-svc-g2l2l
Jan 23 14:51:35.316: INFO: Got endpoints: latency-svc-rk7jr [751.418431ms]
Jan 23 14:51:35.323: INFO: Created: latency-svc-drfsc
Jan 23 14:51:35.367: INFO: Got endpoints: latency-svc-mrpps [749.522515ms]
Jan 23 14:51:35.373: INFO: Created: latency-svc-kqcjq
Jan 23 14:51:35.416: INFO: Got endpoints: latency-svc-8bzwv [748.996592ms]
Jan 23 14:51:35.423: INFO: Created: latency-svc-4p9q4
Jan 23 14:51:35.466: INFO: Got endpoints: latency-svc-4rpx8 [750.008761ms]
Jan 23 14:51:35.473: INFO: Created: latency-svc-cjrkl
Jan 23 14:51:35.518: INFO: Got endpoints: latency-svc-54q9r [750.607792ms]
Jan 23 14:51:35.525: INFO: Created: latency-svc-5lbns
Jan 23 14:51:35.565: INFO: Got endpoints: latency-svc-tq2v2 [749.412569ms]
Jan 23 14:51:35.574: INFO: Created: latency-svc-l5f7n
Jan 23 14:51:35.617: INFO: Got endpoints: latency-svc-2gdgn [750.98907ms]
Jan 23 14:51:35.623: INFO: Created: latency-svc-72stt
Jan 23 14:51:35.666: INFO: Got endpoints: latency-svc-pgfc7 [750.386935ms]
Jan 23 14:51:35.672: INFO: Created: latency-svc-lkshz
Jan 23 14:51:35.716: INFO: Got endpoints: latency-svc-sp86r [749.885516ms]
Jan 23 14:51:35.726: INFO: Created: latency-svc-q7rv4
Jan 23 14:51:35.766: INFO: Got endpoints: latency-svc-jqc4k [750.478333ms]
Jan 23 14:51:35.773: INFO: Created: latency-svc-dfhsk
Jan 23 14:51:35.816: INFO: Got endpoints: latency-svc-7hplj [689.546016ms]
Jan 23 14:51:35.822: INFO: Created: latency-svc-7kc6f
Jan 23 14:51:35.866: INFO: Got endpoints: latency-svc-7j6ch [712.051278ms]
Jan 23 14:51:35.873: INFO: Created: latency-svc-c9fhx
Jan 23 14:51:35.916: INFO: Got endpoints: latency-svc-xnbkn [729.448434ms]
Jan 23 14:51:35.922: INFO: Created: latency-svc-pfq8s
Jan 23 14:51:35.966: INFO: Got endpoints: latency-svc-vd5qs [749.658867ms]
Jan 23 14:51:35.971: INFO: Created: latency-svc-bzf7b
Jan 23 14:51:36.018: INFO: Got endpoints: latency-svc-g2l2l [750.178919ms]
Jan 23 14:51:36.027: INFO: Created: latency-svc-9mk29
Jan 23 14:51:36.068: INFO: Got endpoints: latency-svc-drfsc [751.990536ms]
Jan 23 14:51:36.073: INFO: Created: latency-svc-5rjh7
Jan 23 14:51:36.118: INFO: Got endpoints: latency-svc-kqcjq [750.538936ms]
Jan 23 14:51:36.124: INFO: Created: latency-svc-rnnmq
Jan 23 14:51:36.165: INFO: Got endpoints: latency-svc-4p9q4 [749.344466ms]
Jan 23 14:51:36.173: INFO: Created: latency-svc-v7l8m
Jan 23 14:51:36.215: INFO: Got endpoints: latency-svc-cjrkl [748.565312ms]
Jan 23 14:51:36.222: INFO: Created: latency-svc-7m5ck
Jan 23 14:51:36.266: INFO: Got endpoints: latency-svc-5lbns [747.733109ms]
Jan 23 14:51:36.275: INFO: Created: latency-svc-zrb2j
Jan 23 14:51:36.317: INFO: Got endpoints: latency-svc-l5f7n [751.260693ms]
Jan 23 14:51:36.323: INFO: Created: latency-svc-9spvz
Jan 23 14:51:36.365: INFO: Got endpoints: latency-svc-72stt [747.907413ms]
Jan 23 14:51:36.370: INFO: Created: latency-svc-6phzl
Jan 23 14:51:36.415: INFO: Got endpoints: latency-svc-lkshz [748.590779ms]
Jan 23 14:51:36.421: INFO: Created: latency-svc-gpf75
Jan 23 14:51:36.466: INFO: Got endpoints: latency-svc-q7rv4 [749.75418ms]
Jan 23 14:51:36.472: INFO: Created: latency-svc-pbctb
Jan 23 14:51:36.514: INFO: Got endpoints: latency-svc-dfhsk [748.181252ms]
Jan 23 14:51:36.521: INFO: Created: latency-svc-ldvgm
Jan 23 14:51:36.566: INFO: Got endpoints: latency-svc-7kc6f [749.498512ms]
Jan 23 14:51:36.572: INFO: Created: latency-svc-8rjhz
Jan 23 14:51:36.617: INFO: Got endpoints: latency-svc-c9fhx [750.58761ms]
Jan 23 14:51:36.623: INFO: Created: latency-svc-d89j9
Jan 23 14:51:36.668: INFO: Got endpoints: latency-svc-pfq8s [751.977054ms]
Jan 23 14:51:36.673: INFO: Created: latency-svc-6hgjk
Jan 23 14:51:36.717: INFO: Got endpoints: latency-svc-bzf7b [750.968041ms]
Jan 23 14:51:36.722: INFO: Created: latency-svc-rqqjv
Jan 23 14:51:36.767: INFO: Got endpoints: latency-svc-9mk29 [749.21512ms]
Jan 23 14:51:36.774: INFO: Created: latency-svc-jmnkj
Jan 23 14:51:36.818: INFO: Got endpoints: latency-svc-5rjh7 [749.60254ms]
Jan 23 14:51:36.824: INFO: Created: latency-svc-6g62s
Jan 23 14:51:36.866: INFO: Got endpoints: latency-svc-rnnmq [748.689562ms]
Jan 23 14:51:36.915: INFO: Got endpoints: latency-svc-v7l8m [749.195582ms]
Jan 23 14:51:36.965: INFO: Got endpoints: latency-svc-7m5ck [750.486373ms]
Jan 23 14:51:37.015: INFO: Got endpoints: latency-svc-zrb2j [749.155626ms]
Jan 23 14:51:37.065: INFO: Got endpoints: latency-svc-9spvz [748.447519ms]
Jan 23 14:51:37.116: INFO: Got endpoints: latency-svc-6phzl [751.30368ms]
Jan 23 14:51:37.164: INFO: Got endpoints: latency-svc-gpf75 [749.186635ms]
Jan 23 14:51:37.215: INFO: Got endpoints: latency-svc-pbctb [748.444204ms]
Jan 23 14:51:37.267: INFO: Got endpoints: latency-svc-ldvgm [752.522422ms]
Jan 23 14:51:37.316: INFO: Got endpoints: latency-svc-8rjhz [749.874976ms]
Jan 23 14:51:37.366: INFO: Got endpoints: latency-svc-d89j9 [749.632447ms]
Jan 23 14:51:37.415: INFO: Got endpoints: latency-svc-6hgjk [746.944325ms]
Jan 23 14:51:37.464: INFO: Got endpoints: latency-svc-rqqjv [747.710647ms]
Jan 23 14:51:37.516: INFO: Got endpoints: latency-svc-jmnkj [748.821359ms]
Jan 23 14:51:37.565: INFO: Got endpoints: latency-svc-6g62s [746.687113ms]
Jan 23 14:51:37.565: INFO: Latencies: [10.944968ms 12.898398ms 16.814992ms 48.53278ms 55.751331ms 82.843312ms 84.269352ms 84.934336ms 91.045743ms 96.372247ms 97.071888ms 97.187097ms 97.360706ms 102.013843ms 106.766534ms 106.961009ms 107.944059ms 119.540003ms 119.639782ms 119.644597ms 131.494916ms 146.042493ms 157.733344ms 173.548626ms 187.531283ms 189.05808ms 205.122623ms 264.710386ms 279.685493ms 291.143496ms 292.97585ms 294.450121ms 295.088192ms 296.607744ms 296.844751ms 305.600954ms 307.409724ms 310.091769ms 323.690283ms 323.742064ms 326.871468ms 334.039356ms 362.016929ms 368.616857ms 422.27063ms 437.119912ms 471.357543ms 515.276228ms 542.561709ms 559.249432ms 578.520404ms 609.554908ms 647.43131ms 683.086196ms 689.546016ms 712.051278ms 716.761287ms 722.821564ms 724.205074ms 729.448434ms 739.462387ms 741.889144ms 742.950187ms 745.456821ms 746.687113ms 746.769862ms 746.944325ms 746.947812ms 747.274668ms 747.392326ms 747.418121ms 747.710647ms 747.733109ms 747.907413ms 748.129818ms 748.13765ms 748.166919ms 748.181252ms 748.274655ms 748.358614ms 748.444204ms 748.447519ms 748.475361ms 748.484158ms 748.565312ms 748.590779ms 748.647962ms 748.648671ms 748.684905ms 748.689562ms 748.703478ms 748.817496ms 748.821359ms 748.855958ms 748.991991ms 748.996592ms 749.002756ms 749.018043ms 749.038883ms 749.055982ms 749.148013ms 749.155626ms 749.186635ms 749.195582ms 749.21512ms 749.248003ms 749.312501ms 749.33778ms 749.344466ms 749.366464ms 749.369058ms 749.412569ms 749.449755ms 749.491615ms 749.498512ms 749.519931ms 749.522515ms 749.548476ms 749.567191ms 749.56962ms 749.60254ms 749.632447ms 749.6486ms 749.658867ms 749.688315ms 749.735585ms 749.75418ms 749.783017ms 749.795734ms 749.827094ms 749.846885ms 749.874976ms 749.885516ms 749.888554ms 749.91347ms 749.950123ms 749.956791ms 749.970354ms 749.974316ms 750.008761ms 750.020761ms 750.080519ms 750.116864ms 750.141838ms 750.178919ms 750.196137ms 750.287721ms 750.32246ms 750.381375ms 750.386935ms 750.388418ms 750.454612ms 750.478333ms 750.486373ms 750.528767ms 750.538936ms 750.541843ms 750.563906ms 750.58761ms 750.594628ms 750.607792ms 750.61589ms 750.659718ms 750.704065ms 750.736145ms 750.867831ms 750.932823ms 750.968041ms 750.98907ms 751.074011ms 751.098693ms 751.110039ms 751.246917ms 751.257924ms 751.260693ms 751.30368ms 751.310016ms 751.354148ms 751.418431ms 751.502046ms 751.532084ms 751.633896ms 751.687165ms 751.771124ms 751.861204ms 751.977054ms 751.984648ms 751.990536ms 752.032885ms 752.103074ms 752.148962ms 752.177428ms 752.197747ms 752.522422ms 752.972321ms 753.738026ms 754.90727ms 770.926478ms 787.897171ms 809.455803ms]
Jan 23 14:51:37.565: INFO: 50 %ile: 749.148013ms
Jan 23 14:51:37.565: INFO: 90 %ile: 751.532084ms
Jan 23 14:51:37.565: INFO: 99 %ile: 787.897171ms
Jan 23 14:51:37.565: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Jan 23 14:51:37.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-4662" for this suite. 01/23/24 14:51:37.569
------------------------------
• [SLOW TEST] [10.735 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:51:26.837
    Jan 23 14:51:26.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename svc-latency 01/23/24 14:51:26.838
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:26.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:26.846
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan 23 14:51:26.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-4662 01/23/24 14:51:26.849
    I0123 14:51:26.852889      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4662, replica count: 1
    I0123 14:51:27.904210      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0123 14:51:28.905252      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 23 14:51:29.012: INFO: Created: latency-svc-cqt5g
    Jan 23 14:51:29.016: INFO: Got endpoints: latency-svc-cqt5g [10.821231ms]
    Jan 23 14:51:29.023: INFO: Created: latency-svc-f76ww
    Jan 23 14:51:29.026: INFO: Created: latency-svc-4wd29
    Jan 23 14:51:29.027: INFO: Got endpoints: latency-svc-f76ww [10.944968ms]
    Jan 23 14:51:29.030: INFO: Got endpoints: latency-svc-4wd29 [12.898398ms]
    Jan 23 14:51:29.031: INFO: Created: latency-svc-s65cd
    Jan 23 14:51:29.034: INFO: Got endpoints: latency-svc-s65cd [16.814992ms]
    Jan 23 14:51:29.089: INFO: Created: latency-svc-9spg7
    Jan 23 14:51:29.097: INFO: Created: latency-svc-mnkdj
    Jan 23 14:51:29.097: INFO: Created: latency-svc-8ptgz
    Jan 23 14:51:29.097: INFO: Created: latency-svc-frdvj
    Jan 23 14:51:29.097: INFO: Created: latency-svc-r97dl
    Jan 23 14:51:29.097: INFO: Created: latency-svc-2nqtw
    Jan 23 14:51:29.097: INFO: Created: latency-svc-w4vft
    Jan 23 14:51:29.097: INFO: Created: latency-svc-m7cvq
    Jan 23 14:51:29.097: INFO: Created: latency-svc-bf9w6
    Jan 23 14:51:29.097: INFO: Created: latency-svc-v9qp8
    Jan 23 14:51:29.098: INFO: Created: latency-svc-cbm2b
    Jan 23 14:51:29.098: INFO: Created: latency-svc-gk7pf
    Jan 23 14:51:29.098: INFO: Created: latency-svc-6g9l9
    Jan 23 14:51:29.098: INFO: Created: latency-svc-jmqlt
    Jan 23 14:51:29.100: INFO: Created: latency-svc-pmz2c
    Jan 23 14:51:29.101: INFO: Got endpoints: latency-svc-9spg7 [82.843312ms]
    Jan 23 14:51:29.103: INFO: Got endpoints: latency-svc-2nqtw [84.934336ms]
    Jan 23 14:51:29.114: INFO: Got endpoints: latency-svc-mnkdj [84.269352ms]
    Jan 23 14:51:29.114: INFO: Got endpoints: latency-svc-w4vft [96.372247ms]
    Jan 23 14:51:29.115: INFO: Got endpoints: latency-svc-jmqlt [97.071888ms]
    Jan 23 14:51:29.115: INFO: Got endpoints: latency-svc-gk7pf [97.187097ms]
    Jan 23 14:51:29.115: INFO: Got endpoints: latency-svc-8ptgz [97.360706ms]
    Jan 23 14:51:29.125: INFO: Got endpoints: latency-svc-m7cvq [106.766534ms]
    Jan 23 14:51:29.125: INFO: Got endpoints: latency-svc-frdvj [91.045743ms]
    Jan 23 14:51:29.125: INFO: Got endpoints: latency-svc-pmz2c [106.961009ms]
    Jan 23 14:51:29.125: INFO: Got endpoints: latency-svc-r97dl [107.944059ms]
    Jan 23 14:51:29.130: INFO: Got endpoints: latency-svc-v9qp8 [102.013843ms]
    Jan 23 14:51:29.136: INFO: Created: latency-svc-42225
    Jan 23 14:51:29.137: INFO: Got endpoints: latency-svc-cbm2b [119.639782ms]
    Jan 23 14:51:29.137: INFO: Got endpoints: latency-svc-6g9l9 [119.540003ms]
    Jan 23 14:51:29.137: INFO: Got endpoints: latency-svc-bf9w6 [119.644597ms]
    Jan 23 14:51:29.149: INFO: Got endpoints: latency-svc-42225 [48.53278ms]
    Jan 23 14:51:29.150: INFO: Created: latency-svc-d6nhs
    Jan 23 14:51:29.159: INFO: Got endpoints: latency-svc-d6nhs [55.751331ms]
    Jan 23 14:51:29.161: INFO: Created: latency-svc-mgn8r
    Jan 23 14:51:29.246: INFO: Got endpoints: latency-svc-mgn8r [131.494916ms]
    Jan 23 14:51:29.252: INFO: Created: latency-svc-v498m
    Jan 23 14:51:29.261: INFO: Got endpoints: latency-svc-v498m [146.042493ms]
    Jan 23 14:51:29.264: INFO: Created: latency-svc-c9qkf
    Jan 23 14:51:29.273: INFO: Got endpoints: latency-svc-c9qkf [157.733344ms]
    Jan 23 14:51:29.276: INFO: Created: latency-svc-s8457
    Jan 23 14:51:29.289: INFO: Got endpoints: latency-svc-s8457 [173.548626ms]
    Jan 23 14:51:29.292: INFO: Created: latency-svc-q5jpx
    Jan 23 14:51:29.302: INFO: Got endpoints: latency-svc-q5jpx [187.531283ms]
    Jan 23 14:51:29.305: INFO: Created: latency-svc-jjlvs
    Jan 23 14:51:29.314: INFO: Got endpoints: latency-svc-jjlvs [189.05808ms]
    Jan 23 14:51:29.317: INFO: Created: latency-svc-7fxtl
    Jan 23 14:51:29.330: INFO: Got endpoints: latency-svc-7fxtl [205.122623ms]
    Jan 23 14:51:29.333: INFO: Created: latency-svc-fqvrz
    Jan 23 14:51:29.390: INFO: Got endpoints: latency-svc-fqvrz [264.710386ms]
    Jan 23 14:51:29.392: INFO: Created: latency-svc-hr4bk
    Jan 23 14:51:29.405: INFO: Got endpoints: latency-svc-hr4bk [279.685493ms]
    Jan 23 14:51:29.410: INFO: Created: latency-svc-9vvzc
    Jan 23 14:51:29.421: INFO: Got endpoints: latency-svc-9vvzc [291.143496ms]
    Jan 23 14:51:29.424: INFO: Created: latency-svc-8h46b
    Jan 23 14:51:29.432: INFO: Got endpoints: latency-svc-8h46b [295.088192ms]
    Jan 23 14:51:29.435: INFO: Created: latency-svc-nh8xm
    Jan 23 14:51:29.445: INFO: Got endpoints: latency-svc-nh8xm [307.409724ms]
    Jan 23 14:51:29.451: INFO: Created: latency-svc-8gxcw
    Jan 23 14:51:29.461: INFO: Got endpoints: latency-svc-8gxcw [323.742064ms]
    Jan 23 14:51:29.464: INFO: Created: latency-svc-ssl79
    Jan 23 14:51:29.473: INFO: Got endpoints: latency-svc-ssl79 [323.690283ms]
    Jan 23 14:51:29.476: INFO: Created: latency-svc-k7ss9
    Jan 23 14:51:29.485: INFO: Got endpoints: latency-svc-k7ss9 [326.871468ms]
    Jan 23 14:51:29.539: INFO: Created: latency-svc-8lrxb
    Jan 23 14:51:29.542: INFO: Created: latency-svc-jmlx4
    Jan 23 14:51:29.556: INFO: Got endpoints: latency-svc-jmlx4 [294.450121ms]
    Jan 23 14:51:29.556: INFO: Got endpoints: latency-svc-8lrxb [310.091769ms]
    Jan 23 14:51:29.559: INFO: Created: latency-svc-kcwvm
    Jan 23 14:51:29.570: INFO: Got endpoints: latency-svc-kcwvm [296.607744ms]
    Jan 23 14:51:29.577: INFO: Created: latency-svc-xsm94
    Jan 23 14:51:29.582: INFO: Got endpoints: latency-svc-xsm94 [292.97585ms]
    Jan 23 14:51:29.590: INFO: Created: latency-svc-f8978
    Jan 23 14:51:29.598: INFO: Got endpoints: latency-svc-f8978 [296.844751ms]
    Jan 23 14:51:29.601: INFO: Created: latency-svc-rn968
    Jan 23 14:51:29.611: INFO: Created: latency-svc-npjz6
    Jan 23 14:51:29.619: INFO: Got endpoints: latency-svc-rn968 [305.600954ms]
    Jan 23 14:51:29.623: INFO: Created: latency-svc-7njts
    Jan 23 14:51:29.636: INFO: Created: latency-svc-c7j7r
    Jan 23 14:51:29.692: INFO: Got endpoints: latency-svc-npjz6 [362.016929ms]
    Jan 23 14:51:29.695: INFO: Created: latency-svc-8ps4d
    Jan 23 14:51:29.705: INFO: Created: latency-svc-bc59k
    Jan 23 14:51:29.714: INFO: Created: latency-svc-46nmx
    Jan 23 14:51:29.724: INFO: Got endpoints: latency-svc-7njts [334.039356ms]
    Jan 23 14:51:29.726: INFO: Created: latency-svc-xq2hc
    Jan 23 14:51:29.736: INFO: Created: latency-svc-ntgkk
    Jan 23 14:51:29.745: INFO: Created: latency-svc-jnnxp
    Jan 23 14:51:29.755: INFO: Created: latency-svc-24scp
    Jan 23 14:51:29.765: INFO: Created: latency-svc-twt9v
    Jan 23 14:51:29.773: INFO: Got endpoints: latency-svc-c7j7r [368.616857ms]
    Jan 23 14:51:29.776: INFO: Created: latency-svc-jp2nl
    Jan 23 14:51:29.785: INFO: Created: latency-svc-bbq5j
    Jan 23 14:51:29.843: INFO: Got endpoints: latency-svc-8ps4d [422.27063ms]
    Jan 23 14:51:29.843: INFO: Created: latency-svc-srv6q
    Jan 23 14:51:29.865: INFO: Created: latency-svc-v4662
    Jan 23 14:51:29.870: INFO: Got endpoints: latency-svc-bc59k [437.119912ms]
    Jan 23 14:51:29.876: INFO: Created: latency-svc-9ks7v
    Jan 23 14:51:29.886: INFO: Created: latency-svc-w44qx
    Jan 23 14:51:29.895: INFO: Created: latency-svc-xzlzt
    Jan 23 14:51:29.905: INFO: Created: latency-svc-j8sc4
    Jan 23 14:51:29.911: INFO: Created: latency-svc-v49qm
    Jan 23 14:51:29.916: INFO: Got endpoints: latency-svc-46nmx [471.357543ms]
    Jan 23 14:51:29.933: INFO: Created: latency-svc-n8hkm
    Jan 23 14:51:29.976: INFO: Got endpoints: latency-svc-xq2hc [515.276228ms]
    Jan 23 14:51:29.992: INFO: Created: latency-svc-2npgn
    Jan 23 14:51:30.016: INFO: Got endpoints: latency-svc-ntgkk [542.561709ms]
    Jan 23 14:51:30.024: INFO: Created: latency-svc-p7th2
    Jan 23 14:51:30.064: INFO: Got endpoints: latency-svc-jnnxp [578.520404ms]
    Jan 23 14:51:30.070: INFO: Created: latency-svc-wn7cj
    Jan 23 14:51:30.115: INFO: Got endpoints: latency-svc-24scp [559.249432ms]
    Jan 23 14:51:30.120: INFO: Created: latency-svc-szdpw
    Jan 23 14:51:30.165: INFO: Got endpoints: latency-svc-twt9v [609.554908ms]
    Jan 23 14:51:30.172: INFO: Created: latency-svc-vncn9
    Jan 23 14:51:30.217: INFO: Got endpoints: latency-svc-jp2nl [647.43131ms]
    Jan 23 14:51:30.223: INFO: Created: latency-svc-9r54m
    Jan 23 14:51:30.265: INFO: Got endpoints: latency-svc-bbq5j [683.086196ms]
    Jan 23 14:51:30.271: INFO: Created: latency-svc-7dzh5
    Jan 23 14:51:30.315: INFO: Got endpoints: latency-svc-srv6q [716.761287ms]
    Jan 23 14:51:30.320: INFO: Created: latency-svc-vmwjs
    Jan 23 14:51:30.366: INFO: Got endpoints: latency-svc-v4662 [746.947812ms]
    Jan 23 14:51:30.372: INFO: Created: latency-svc-t82ml
    Jan 23 14:51:30.416: INFO: Got endpoints: latency-svc-9ks7v [724.205074ms]
    Jan 23 14:51:30.423: INFO: Created: latency-svc-vngqj
    Jan 23 14:51:30.466: INFO: Got endpoints: latency-svc-w44qx [741.889144ms]
    Jan 23 14:51:30.471: INFO: Created: latency-svc-dcwj9
    Jan 23 14:51:30.516: INFO: Got endpoints: latency-svc-xzlzt [742.950187ms]
    Jan 23 14:51:30.522: INFO: Created: latency-svc-wn5bm
    Jan 23 14:51:30.566: INFO: Got endpoints: latency-svc-j8sc4 [722.821564ms]
    Jan 23 14:51:30.571: INFO: Created: latency-svc-fpvgk
    Jan 23 14:51:30.616: INFO: Got endpoints: latency-svc-v49qm [746.769862ms]
    Jan 23 14:51:30.623: INFO: Created: latency-svc-bt5vt
    Jan 23 14:51:30.666: INFO: Got endpoints: latency-svc-n8hkm [749.519931ms]
    Jan 23 14:51:30.673: INFO: Created: latency-svc-ckwx8
    Jan 23 14:51:30.716: INFO: Got endpoints: latency-svc-2npgn [739.462387ms]
    Jan 23 14:51:30.723: INFO: Created: latency-svc-lpmnp
    Jan 23 14:51:30.767: INFO: Got endpoints: latency-svc-p7th2 [750.867831ms]
    Jan 23 14:51:30.773: INFO: Created: latency-svc-mltb8
    Jan 23 14:51:30.818: INFO: Got endpoints: latency-svc-wn7cj [753.738026ms]
    Jan 23 14:51:30.823: INFO: Created: latency-svc-tkg6b
    Jan 23 14:51:30.867: INFO: Got endpoints: latency-svc-szdpw [751.861204ms]
    Jan 23 14:51:30.875: INFO: Created: latency-svc-4mmbq
    Jan 23 14:51:30.915: INFO: Got endpoints: latency-svc-vncn9 [749.846885ms]
    Jan 23 14:51:30.922: INFO: Created: latency-svc-54bn5
    Jan 23 14:51:30.966: INFO: Got endpoints: latency-svc-9r54m [749.038883ms]
    Jan 23 14:51:30.972: INFO: Created: latency-svc-982v4
    Jan 23 14:51:31.020: INFO: Got endpoints: latency-svc-7dzh5 [754.90727ms]
    Jan 23 14:51:31.028: INFO: Created: latency-svc-t8dnw
    Jan 23 14:51:31.067: INFO: Got endpoints: latency-svc-vmwjs [751.502046ms]
    Jan 23 14:51:31.072: INFO: Created: latency-svc-bb7hj
    Jan 23 14:51:31.118: INFO: Got endpoints: latency-svc-t82ml [752.103074ms]
    Jan 23 14:51:31.125: INFO: Created: latency-svc-ftzbv
    Jan 23 14:51:31.166: INFO: Got endpoints: latency-svc-vngqj [749.366464ms]
    Jan 23 14:51:31.172: INFO: Created: latency-svc-r4wjf
    Jan 23 14:51:31.216: INFO: Got endpoints: latency-svc-dcwj9 [750.528767ms]
    Jan 23 14:51:31.222: INFO: Created: latency-svc-l9qx7
    Jan 23 14:51:31.267: INFO: Got endpoints: latency-svc-wn5bm [750.704065ms]
    Jan 23 14:51:31.272: INFO: Created: latency-svc-5qhks
    Jan 23 14:51:31.316: INFO: Got endpoints: latency-svc-fpvgk [750.141838ms]
    Jan 23 14:51:31.323: INFO: Created: latency-svc-cvxl6
    Jan 23 14:51:31.365: INFO: Got endpoints: latency-svc-bt5vt [748.855958ms]
    Jan 23 14:51:31.370: INFO: Created: latency-svc-9phpm
    Jan 23 14:51:31.417: INFO: Got endpoints: latency-svc-ckwx8 [751.246917ms]
    Jan 23 14:51:31.423: INFO: Created: latency-svc-f9pst
    Jan 23 14:51:31.467: INFO: Got endpoints: latency-svc-lpmnp [750.594628ms]
    Jan 23 14:51:31.473: INFO: Created: latency-svc-rmvv8
    Jan 23 14:51:31.517: INFO: Got endpoints: latency-svc-mltb8 [750.381375ms]
    Jan 23 14:51:31.524: INFO: Created: latency-svc-lqjf4
    Jan 23 14:51:31.565: INFO: Got endpoints: latency-svc-tkg6b [747.274668ms]
    Jan 23 14:51:31.572: INFO: Created: latency-svc-hhxqp
    Jan 23 14:51:31.617: INFO: Got endpoints: latency-svc-4mmbq [750.454612ms]
    Jan 23 14:51:31.623: INFO: Created: latency-svc-8gkwl
    Jan 23 14:51:31.667: INFO: Got endpoints: latency-svc-54bn5 [751.310016ms]
    Jan 23 14:51:31.677: INFO: Created: latency-svc-qh8b5
    Jan 23 14:51:31.715: INFO: Got endpoints: latency-svc-982v4 [749.33778ms]
    Jan 23 14:51:31.721: INFO: Created: latency-svc-x5w87
    Jan 23 14:51:31.768: INFO: Got endpoints: latency-svc-t8dnw [747.392326ms]
    Jan 23 14:51:31.773: INFO: Created: latency-svc-c5t2x
    Jan 23 14:51:31.816: INFO: Got endpoints: latency-svc-bb7hj [749.449755ms]
    Jan 23 14:51:31.824: INFO: Created: latency-svc-6mlhk
    Jan 23 14:51:31.867: INFO: Got endpoints: latency-svc-ftzbv [748.703478ms]
    Jan 23 14:51:31.873: INFO: Created: latency-svc-4h6h8
    Jan 23 14:51:31.917: INFO: Got endpoints: latency-svc-r4wjf [751.532084ms]
    Jan 23 14:51:31.923: INFO: Created: latency-svc-qwm5d
    Jan 23 14:51:31.966: INFO: Got endpoints: latency-svc-l9qx7 [749.688315ms]
    Jan 23 14:51:31.971: INFO: Created: latency-svc-r9d4l
    Jan 23 14:51:32.015: INFO: Got endpoints: latency-svc-5qhks [748.358614ms]
    Jan 23 14:51:32.022: INFO: Created: latency-svc-pr7sh
    Jan 23 14:51:32.066: INFO: Got endpoints: latency-svc-cvxl6 [750.32246ms]
    Jan 23 14:51:32.072: INFO: Created: latency-svc-c7894
    Jan 23 14:51:32.117: INFO: Got endpoints: latency-svc-9phpm [752.148962ms]
    Jan 23 14:51:32.125: INFO: Created: latency-svc-mnccl
    Jan 23 14:51:32.166: INFO: Got endpoints: latency-svc-f9pst [749.148013ms]
    Jan 23 14:51:32.174: INFO: Created: latency-svc-9vpdm
    Jan 23 14:51:32.216: INFO: Got endpoints: latency-svc-rmvv8 [749.567191ms]
    Jan 23 14:51:32.222: INFO: Created: latency-svc-cx2h2
    Jan 23 14:51:32.265: INFO: Got endpoints: latency-svc-lqjf4 [748.484158ms]
    Jan 23 14:51:32.272: INFO: Created: latency-svc-jtl82
    Jan 23 14:51:32.317: INFO: Got endpoints: latency-svc-hhxqp [751.633896ms]
    Jan 23 14:51:32.325: INFO: Created: latency-svc-f4btv
    Jan 23 14:51:32.367: INFO: Got endpoints: latency-svc-8gkwl [749.91347ms]
    Jan 23 14:51:32.373: INFO: Created: latency-svc-x8db2
    Jan 23 14:51:32.417: INFO: Got endpoints: latency-svc-qh8b5 [750.61589ms]
    Jan 23 14:51:32.424: INFO: Created: latency-svc-9vzzd
    Jan 23 14:51:32.465: INFO: Got endpoints: latency-svc-x5w87 [749.970354ms]
    Jan 23 14:51:32.471: INFO: Created: latency-svc-9vd9l
    Jan 23 14:51:32.518: INFO: Got endpoints: latency-svc-c5t2x [750.080519ms]
    Jan 23 14:51:32.523: INFO: Created: latency-svc-kmmgd
    Jan 23 14:51:32.566: INFO: Got endpoints: latency-svc-6mlhk [749.369058ms]
    Jan 23 14:51:32.572: INFO: Created: latency-svc-xn9ls
    Jan 23 14:51:32.616: INFO: Got endpoints: latency-svc-4h6h8 [748.647962ms]
    Jan 23 14:51:32.622: INFO: Created: latency-svc-bfjpq
    Jan 23 14:51:32.667: INFO: Got endpoints: latency-svc-qwm5d [749.56962ms]
    Jan 23 14:51:32.673: INFO: Created: latency-svc-x4lhx
    Jan 23 14:51:32.716: INFO: Got endpoints: latency-svc-r9d4l [749.6486ms]
    Jan 23 14:51:32.723: INFO: Created: latency-svc-jhnsf
    Jan 23 14:51:32.767: INFO: Got endpoints: latency-svc-pr7sh [752.032885ms]
    Jan 23 14:51:32.773: INFO: Created: latency-svc-x74m6
    Jan 23 14:51:32.818: INFO: Got endpoints: latency-svc-c7894 [751.984648ms]
    Jan 23 14:51:32.825: INFO: Created: latency-svc-dwrv8
    Jan 23 14:51:32.867: INFO: Got endpoints: latency-svc-mnccl [749.002756ms]
    Jan 23 14:51:32.873: INFO: Created: latency-svc-b9wh4
    Jan 23 14:51:32.917: INFO: Got endpoints: latency-svc-9vpdm [750.541843ms]
    Jan 23 14:51:32.923: INFO: Created: latency-svc-tls85
    Jan 23 14:51:32.966: INFO: Got endpoints: latency-svc-cx2h2 [750.196137ms]
    Jan 23 14:51:32.972: INFO: Created: latency-svc-z2qqh
    Jan 23 14:51:33.017: INFO: Got endpoints: latency-svc-jtl82 [751.098693ms]
    Jan 23 14:51:33.024: INFO: Created: latency-svc-fctsw
    Jan 23 14:51:33.067: INFO: Got endpoints: latency-svc-f4btv [750.563906ms]
    Jan 23 14:51:33.074: INFO: Created: latency-svc-vtvd6
    Jan 23 14:51:33.119: INFO: Got endpoints: latency-svc-x8db2 [751.354148ms]
    Jan 23 14:51:33.125: INFO: Created: latency-svc-xtpgb
    Jan 23 14:51:33.165: INFO: Got endpoints: latency-svc-9vzzd [748.166919ms]
    Jan 23 14:51:33.172: INFO: Created: latency-svc-48jxf
    Jan 23 14:51:33.217: INFO: Got endpoints: latency-svc-9vd9l [751.257924ms]
    Jan 23 14:51:33.227: INFO: Created: latency-svc-q5rgx
    Jan 23 14:51:33.266: INFO: Got endpoints: latency-svc-kmmgd [748.684905ms]
    Jan 23 14:51:33.272: INFO: Created: latency-svc-cfhl2
    Jan 23 14:51:33.315: INFO: Got endpoints: latency-svc-xn9ls [749.795734ms]
    Jan 23 14:51:33.322: INFO: Created: latency-svc-clmw5
    Jan 23 14:51:33.368: INFO: Got endpoints: latency-svc-bfjpq [752.197747ms]
    Jan 23 14:51:33.379: INFO: Created: latency-svc-dsbtg
    Jan 23 14:51:33.416: INFO: Got endpoints: latency-svc-x4lhx [749.491615ms]
    Jan 23 14:51:33.423: INFO: Created: latency-svc-lb4js
    Jan 23 14:51:33.465: INFO: Got endpoints: latency-svc-jhnsf [749.783017ms]
    Jan 23 14:51:33.472: INFO: Created: latency-svc-6xs6w
    Jan 23 14:51:33.516: INFO: Got endpoints: latency-svc-x74m6 [749.018043ms]
    Jan 23 14:51:33.522: INFO: Created: latency-svc-64ffk
    Jan 23 14:51:33.571: INFO: Got endpoints: latency-svc-dwrv8 [752.972321ms]
    Jan 23 14:51:33.579: INFO: Created: latency-svc-kgk9t
    Jan 23 14:51:33.616: INFO: Got endpoints: latency-svc-b9wh4 [749.312501ms]
    Jan 23 14:51:33.621: INFO: Created: latency-svc-vs4sd
    Jan 23 14:51:33.666: INFO: Got endpoints: latency-svc-tls85 [748.991991ms]
    Jan 23 14:51:33.672: INFO: Created: latency-svc-cx2tl
    Jan 23 14:51:33.716: INFO: Got endpoints: latency-svc-z2qqh [749.950123ms]
    Jan 23 14:51:33.723: INFO: Created: latency-svc-92dvf
    Jan 23 14:51:33.765: INFO: Got endpoints: latency-svc-fctsw [748.274655ms]
    Jan 23 14:51:33.771: INFO: Created: latency-svc-cvzzn
    Jan 23 14:51:33.816: INFO: Got endpoints: latency-svc-vtvd6 [748.13765ms]
    Jan 23 14:51:33.821: INFO: Created: latency-svc-gzwcz
    Jan 23 14:51:33.867: INFO: Got endpoints: latency-svc-xtpgb [748.129818ms]
    Jan 23 14:51:33.873: INFO: Created: latency-svc-t27nq
    Jan 23 14:51:33.917: INFO: Got endpoints: latency-svc-48jxf [751.771124ms]
    Jan 23 14:51:33.923: INFO: Created: latency-svc-swg7l
    Jan 23 14:51:33.966: INFO: Got endpoints: latency-svc-q5rgx [748.817496ms]
    Jan 23 14:51:33.971: INFO: Created: latency-svc-j97gc
    Jan 23 14:51:34.017: INFO: Got endpoints: latency-svc-cfhl2 [750.388418ms]
    Jan 23 14:51:34.023: INFO: Created: latency-svc-csbgh
    Jan 23 14:51:34.067: INFO: Got endpoints: latency-svc-clmw5 [751.687165ms]
    Jan 23 14:51:34.074: INFO: Created: latency-svc-kpf7q
    Jan 23 14:51:34.116: INFO: Got endpoints: latency-svc-dsbtg [747.418121ms]
    Jan 23 14:51:34.123: INFO: Created: latency-svc-gs4sr
    Jan 23 14:51:34.167: INFO: Got endpoints: latency-svc-lb4js [751.110039ms]
    Jan 23 14:51:34.173: INFO: Created: latency-svc-j8h5q
    Jan 23 14:51:34.215: INFO: Got endpoints: latency-svc-6xs6w [749.827094ms]
    Jan 23 14:51:34.220: INFO: Created: latency-svc-84h42
    Jan 23 14:51:34.266: INFO: Got endpoints: latency-svc-64ffk [749.956791ms]
    Jan 23 14:51:34.272: INFO: Created: latency-svc-wqcgs
    Jan 23 14:51:34.317: INFO: Got endpoints: latency-svc-kgk9t [745.456821ms]
    Jan 23 14:51:34.323: INFO: Created: latency-svc-skl5g
    Jan 23 14:51:34.366: INFO: Got endpoints: latency-svc-vs4sd [750.116864ms]
    Jan 23 14:51:34.372: INFO: Created: latency-svc-78zvv
    Jan 23 14:51:34.416: INFO: Got endpoints: latency-svc-cx2tl [749.888554ms]
    Jan 23 14:51:34.422: INFO: Created: latency-svc-4w5mk
    Jan 23 14:51:34.466: INFO: Got endpoints: latency-svc-92dvf [749.974316ms]
    Jan 23 14:51:34.473: INFO: Created: latency-svc-rt4f4
    Jan 23 14:51:34.515: INFO: Got endpoints: latency-svc-cvzzn [750.287721ms]
    Jan 23 14:51:34.522: INFO: Created: latency-svc-b6b4z
    Jan 23 14:51:34.565: INFO: Got endpoints: latency-svc-gzwcz [749.055982ms]
    Jan 23 14:51:34.573: INFO: Created: latency-svc-rk7jr
    Jan 23 14:51:34.617: INFO: Got endpoints: latency-svc-t27nq [750.736145ms]
    Jan 23 14:51:34.624: INFO: Created: latency-svc-mrpps
    Jan 23 14:51:34.667: INFO: Got endpoints: latency-svc-swg7l [749.735585ms]
    Jan 23 14:51:34.673: INFO: Created: latency-svc-8bzwv
    Jan 23 14:51:34.716: INFO: Got endpoints: latency-svc-j97gc [750.659718ms]
    Jan 23 14:51:34.722: INFO: Created: latency-svc-4rpx8
    Jan 23 14:51:34.768: INFO: Got endpoints: latency-svc-csbgh [750.932823ms]
    Jan 23 14:51:34.774: INFO: Created: latency-svc-54q9r
    Jan 23 14:51:34.816: INFO: Got endpoints: latency-svc-kpf7q [748.648671ms]
    Jan 23 14:51:34.822: INFO: Created: latency-svc-tq2v2
    Jan 23 14:51:34.866: INFO: Got endpoints: latency-svc-gs4sr [750.020761ms]
    Jan 23 14:51:34.871: INFO: Created: latency-svc-2gdgn
    Jan 23 14:51:34.916: INFO: Got endpoints: latency-svc-j8h5q [748.475361ms]
    Jan 23 14:51:34.922: INFO: Created: latency-svc-pgfc7
    Jan 23 14:51:34.966: INFO: Got endpoints: latency-svc-84h42 [751.074011ms]
    Jan 23 14:51:34.975: INFO: Created: latency-svc-sp86r
    Jan 23 14:51:35.016: INFO: Got endpoints: latency-svc-wqcgs [749.248003ms]
    Jan 23 14:51:35.022: INFO: Created: latency-svc-jqc4k
    Jan 23 14:51:35.126: INFO: Got endpoints: latency-svc-skl5g [809.455803ms]
    Jan 23 14:51:35.154: INFO: Got endpoints: latency-svc-78zvv [787.897171ms]
    Jan 23 14:51:35.187: INFO: Got endpoints: latency-svc-4w5mk [770.926478ms]
    Jan 23 14:51:35.187: INFO: Created: latency-svc-7hplj
    Jan 23 14:51:35.198: INFO: Created: latency-svc-7j6ch
    Jan 23 14:51:35.213: INFO: Created: latency-svc-xnbkn
    Jan 23 14:51:35.216: INFO: Got endpoints: latency-svc-rt4f4 [749.548476ms]
    Jan 23 14:51:35.224: INFO: Created: latency-svc-vd5qs
    Jan 23 14:51:35.268: INFO: Got endpoints: latency-svc-b6b4z [752.177428ms]
    Jan 23 14:51:35.274: INFO: Created: latency-svc-g2l2l
    Jan 23 14:51:35.316: INFO: Got endpoints: latency-svc-rk7jr [751.418431ms]
    Jan 23 14:51:35.323: INFO: Created: latency-svc-drfsc
    Jan 23 14:51:35.367: INFO: Got endpoints: latency-svc-mrpps [749.522515ms]
    Jan 23 14:51:35.373: INFO: Created: latency-svc-kqcjq
    Jan 23 14:51:35.416: INFO: Got endpoints: latency-svc-8bzwv [748.996592ms]
    Jan 23 14:51:35.423: INFO: Created: latency-svc-4p9q4
    Jan 23 14:51:35.466: INFO: Got endpoints: latency-svc-4rpx8 [750.008761ms]
    Jan 23 14:51:35.473: INFO: Created: latency-svc-cjrkl
    Jan 23 14:51:35.518: INFO: Got endpoints: latency-svc-54q9r [750.607792ms]
    Jan 23 14:51:35.525: INFO: Created: latency-svc-5lbns
    Jan 23 14:51:35.565: INFO: Got endpoints: latency-svc-tq2v2 [749.412569ms]
    Jan 23 14:51:35.574: INFO: Created: latency-svc-l5f7n
    Jan 23 14:51:35.617: INFO: Got endpoints: latency-svc-2gdgn [750.98907ms]
    Jan 23 14:51:35.623: INFO: Created: latency-svc-72stt
    Jan 23 14:51:35.666: INFO: Got endpoints: latency-svc-pgfc7 [750.386935ms]
    Jan 23 14:51:35.672: INFO: Created: latency-svc-lkshz
    Jan 23 14:51:35.716: INFO: Got endpoints: latency-svc-sp86r [749.885516ms]
    Jan 23 14:51:35.726: INFO: Created: latency-svc-q7rv4
    Jan 23 14:51:35.766: INFO: Got endpoints: latency-svc-jqc4k [750.478333ms]
    Jan 23 14:51:35.773: INFO: Created: latency-svc-dfhsk
    Jan 23 14:51:35.816: INFO: Got endpoints: latency-svc-7hplj [689.546016ms]
    Jan 23 14:51:35.822: INFO: Created: latency-svc-7kc6f
    Jan 23 14:51:35.866: INFO: Got endpoints: latency-svc-7j6ch [712.051278ms]
    Jan 23 14:51:35.873: INFO: Created: latency-svc-c9fhx
    Jan 23 14:51:35.916: INFO: Got endpoints: latency-svc-xnbkn [729.448434ms]
    Jan 23 14:51:35.922: INFO: Created: latency-svc-pfq8s
    Jan 23 14:51:35.966: INFO: Got endpoints: latency-svc-vd5qs [749.658867ms]
    Jan 23 14:51:35.971: INFO: Created: latency-svc-bzf7b
    Jan 23 14:51:36.018: INFO: Got endpoints: latency-svc-g2l2l [750.178919ms]
    Jan 23 14:51:36.027: INFO: Created: latency-svc-9mk29
    Jan 23 14:51:36.068: INFO: Got endpoints: latency-svc-drfsc [751.990536ms]
    Jan 23 14:51:36.073: INFO: Created: latency-svc-5rjh7
    Jan 23 14:51:36.118: INFO: Got endpoints: latency-svc-kqcjq [750.538936ms]
    Jan 23 14:51:36.124: INFO: Created: latency-svc-rnnmq
    Jan 23 14:51:36.165: INFO: Got endpoints: latency-svc-4p9q4 [749.344466ms]
    Jan 23 14:51:36.173: INFO: Created: latency-svc-v7l8m
    Jan 23 14:51:36.215: INFO: Got endpoints: latency-svc-cjrkl [748.565312ms]
    Jan 23 14:51:36.222: INFO: Created: latency-svc-7m5ck
    Jan 23 14:51:36.266: INFO: Got endpoints: latency-svc-5lbns [747.733109ms]
    Jan 23 14:51:36.275: INFO: Created: latency-svc-zrb2j
    Jan 23 14:51:36.317: INFO: Got endpoints: latency-svc-l5f7n [751.260693ms]
    Jan 23 14:51:36.323: INFO: Created: latency-svc-9spvz
    Jan 23 14:51:36.365: INFO: Got endpoints: latency-svc-72stt [747.907413ms]
    Jan 23 14:51:36.370: INFO: Created: latency-svc-6phzl
    Jan 23 14:51:36.415: INFO: Got endpoints: latency-svc-lkshz [748.590779ms]
    Jan 23 14:51:36.421: INFO: Created: latency-svc-gpf75
    Jan 23 14:51:36.466: INFO: Got endpoints: latency-svc-q7rv4 [749.75418ms]
    Jan 23 14:51:36.472: INFO: Created: latency-svc-pbctb
    Jan 23 14:51:36.514: INFO: Got endpoints: latency-svc-dfhsk [748.181252ms]
    Jan 23 14:51:36.521: INFO: Created: latency-svc-ldvgm
    Jan 23 14:51:36.566: INFO: Got endpoints: latency-svc-7kc6f [749.498512ms]
    Jan 23 14:51:36.572: INFO: Created: latency-svc-8rjhz
    Jan 23 14:51:36.617: INFO: Got endpoints: latency-svc-c9fhx [750.58761ms]
    Jan 23 14:51:36.623: INFO: Created: latency-svc-d89j9
    Jan 23 14:51:36.668: INFO: Got endpoints: latency-svc-pfq8s [751.977054ms]
    Jan 23 14:51:36.673: INFO: Created: latency-svc-6hgjk
    Jan 23 14:51:36.717: INFO: Got endpoints: latency-svc-bzf7b [750.968041ms]
    Jan 23 14:51:36.722: INFO: Created: latency-svc-rqqjv
    Jan 23 14:51:36.767: INFO: Got endpoints: latency-svc-9mk29 [749.21512ms]
    Jan 23 14:51:36.774: INFO: Created: latency-svc-jmnkj
    Jan 23 14:51:36.818: INFO: Got endpoints: latency-svc-5rjh7 [749.60254ms]
    Jan 23 14:51:36.824: INFO: Created: latency-svc-6g62s
    Jan 23 14:51:36.866: INFO: Got endpoints: latency-svc-rnnmq [748.689562ms]
    Jan 23 14:51:36.915: INFO: Got endpoints: latency-svc-v7l8m [749.195582ms]
    Jan 23 14:51:36.965: INFO: Got endpoints: latency-svc-7m5ck [750.486373ms]
    Jan 23 14:51:37.015: INFO: Got endpoints: latency-svc-zrb2j [749.155626ms]
    Jan 23 14:51:37.065: INFO: Got endpoints: latency-svc-9spvz [748.447519ms]
    Jan 23 14:51:37.116: INFO: Got endpoints: latency-svc-6phzl [751.30368ms]
    Jan 23 14:51:37.164: INFO: Got endpoints: latency-svc-gpf75 [749.186635ms]
    Jan 23 14:51:37.215: INFO: Got endpoints: latency-svc-pbctb [748.444204ms]
    Jan 23 14:51:37.267: INFO: Got endpoints: latency-svc-ldvgm [752.522422ms]
    Jan 23 14:51:37.316: INFO: Got endpoints: latency-svc-8rjhz [749.874976ms]
    Jan 23 14:51:37.366: INFO: Got endpoints: latency-svc-d89j9 [749.632447ms]
    Jan 23 14:51:37.415: INFO: Got endpoints: latency-svc-6hgjk [746.944325ms]
    Jan 23 14:51:37.464: INFO: Got endpoints: latency-svc-rqqjv [747.710647ms]
    Jan 23 14:51:37.516: INFO: Got endpoints: latency-svc-jmnkj [748.821359ms]
    Jan 23 14:51:37.565: INFO: Got endpoints: latency-svc-6g62s [746.687113ms]
    Jan 23 14:51:37.565: INFO: Latencies: [10.944968ms 12.898398ms 16.814992ms 48.53278ms 55.751331ms 82.843312ms 84.269352ms 84.934336ms 91.045743ms 96.372247ms 97.071888ms 97.187097ms 97.360706ms 102.013843ms 106.766534ms 106.961009ms 107.944059ms 119.540003ms 119.639782ms 119.644597ms 131.494916ms 146.042493ms 157.733344ms 173.548626ms 187.531283ms 189.05808ms 205.122623ms 264.710386ms 279.685493ms 291.143496ms 292.97585ms 294.450121ms 295.088192ms 296.607744ms 296.844751ms 305.600954ms 307.409724ms 310.091769ms 323.690283ms 323.742064ms 326.871468ms 334.039356ms 362.016929ms 368.616857ms 422.27063ms 437.119912ms 471.357543ms 515.276228ms 542.561709ms 559.249432ms 578.520404ms 609.554908ms 647.43131ms 683.086196ms 689.546016ms 712.051278ms 716.761287ms 722.821564ms 724.205074ms 729.448434ms 739.462387ms 741.889144ms 742.950187ms 745.456821ms 746.687113ms 746.769862ms 746.944325ms 746.947812ms 747.274668ms 747.392326ms 747.418121ms 747.710647ms 747.733109ms 747.907413ms 748.129818ms 748.13765ms 748.166919ms 748.181252ms 748.274655ms 748.358614ms 748.444204ms 748.447519ms 748.475361ms 748.484158ms 748.565312ms 748.590779ms 748.647962ms 748.648671ms 748.684905ms 748.689562ms 748.703478ms 748.817496ms 748.821359ms 748.855958ms 748.991991ms 748.996592ms 749.002756ms 749.018043ms 749.038883ms 749.055982ms 749.148013ms 749.155626ms 749.186635ms 749.195582ms 749.21512ms 749.248003ms 749.312501ms 749.33778ms 749.344466ms 749.366464ms 749.369058ms 749.412569ms 749.449755ms 749.491615ms 749.498512ms 749.519931ms 749.522515ms 749.548476ms 749.567191ms 749.56962ms 749.60254ms 749.632447ms 749.6486ms 749.658867ms 749.688315ms 749.735585ms 749.75418ms 749.783017ms 749.795734ms 749.827094ms 749.846885ms 749.874976ms 749.885516ms 749.888554ms 749.91347ms 749.950123ms 749.956791ms 749.970354ms 749.974316ms 750.008761ms 750.020761ms 750.080519ms 750.116864ms 750.141838ms 750.178919ms 750.196137ms 750.287721ms 750.32246ms 750.381375ms 750.386935ms 750.388418ms 750.454612ms 750.478333ms 750.486373ms 750.528767ms 750.538936ms 750.541843ms 750.563906ms 750.58761ms 750.594628ms 750.607792ms 750.61589ms 750.659718ms 750.704065ms 750.736145ms 750.867831ms 750.932823ms 750.968041ms 750.98907ms 751.074011ms 751.098693ms 751.110039ms 751.246917ms 751.257924ms 751.260693ms 751.30368ms 751.310016ms 751.354148ms 751.418431ms 751.502046ms 751.532084ms 751.633896ms 751.687165ms 751.771124ms 751.861204ms 751.977054ms 751.984648ms 751.990536ms 752.032885ms 752.103074ms 752.148962ms 752.177428ms 752.197747ms 752.522422ms 752.972321ms 753.738026ms 754.90727ms 770.926478ms 787.897171ms 809.455803ms]
    Jan 23 14:51:37.565: INFO: 50 %ile: 749.148013ms
    Jan 23 14:51:37.565: INFO: 90 %ile: 751.532084ms
    Jan 23 14:51:37.565: INFO: 99 %ile: 787.897171ms
    Jan 23 14:51:37.565: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:51:37.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-4662" for this suite. 01/23/24 14:51:37.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:51:37.573
Jan 23 14:51:37.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename dns 01/23/24 14:51:37.574
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:37.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:37.584
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/23/24 14:51:37.585
Jan 23 14:51:37.596: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2871  6e1fe413-0370-4e8d-bd94-9bccc1d1a969 155220 0 2024-01-23 14:51:37 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2024-01-23 14:51:37 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-89wgb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-89wgb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 14:51:37.596: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-2871" to be "running and ready"
Jan 23 14:51:37.597: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 1.142128ms
Jan 23 14:51:37.597: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:51:39.600: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.003579424s
Jan 23 14:51:39.600: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan 23 14:51:39.600: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/23/24 14:51:39.6
Jan 23 14:51:39.600: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2871 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 14:51:39.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:51:39.600: INFO: ExecWithOptions: Clientset creation
Jan 23 14:51:39.601: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-2871/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/23/24 14:51:39.663
Jan 23 14:51:39.664: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2871 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 14:51:39.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:51:39.664: INFO: ExecWithOptions: Clientset creation
Jan 23 14:51:39.664: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-2871/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 23 14:51:39.727: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 23 14:51:39.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2871" for this suite. 01/23/24 14:51:39.738
------------------------------
• [2.169 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:51:37.573
    Jan 23 14:51:37.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename dns 01/23/24 14:51:37.574
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:37.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:37.584
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/23/24 14:51:37.585
    Jan 23 14:51:37.596: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2871  6e1fe413-0370-4e8d-bd94-9bccc1d1a969 155220 0 2024-01-23 14:51:37 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2024-01-23 14:51:37 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-89wgb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-89wgb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:nova-user-critical,Priority:*1000000000,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 23 14:51:37.596: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-2871" to be "running and ready"
    Jan 23 14:51:37.597: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 1.142128ms
    Jan 23 14:51:37.597: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:51:39.600: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.003579424s
    Jan 23 14:51:39.600: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan 23 14:51:39.600: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/23/24 14:51:39.6
    Jan 23 14:51:39.600: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2871 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 14:51:39.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:51:39.600: INFO: ExecWithOptions: Clientset creation
    Jan 23 14:51:39.601: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-2871/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/23/24 14:51:39.663
    Jan 23 14:51:39.664: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2871 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 14:51:39.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:51:39.664: INFO: ExecWithOptions: Clientset creation
    Jan 23 14:51:39.664: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-2871/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 23 14:51:39.727: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:51:39.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2871" for this suite. 01/23/24 14:51:39.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:51:39.742
Jan 23 14:51:39.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename emptydir 01/23/24 14:51:39.743
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:39.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:39.751
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 01/23/24 14:51:39.753
Jan 23 14:51:39.766: INFO: Waiting up to 5m0s for pod "pod-8ab6c967-465c-4d70-b3db-782c06bbce79" in namespace "emptydir-2507" to be "Succeeded or Failed"
Jan 23 14:51:39.768: INFO: Pod "pod-8ab6c967-465c-4d70-b3db-782c06bbce79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110983ms
Jan 23 14:51:41.771: INFO: Pod "pod-8ab6c967-465c-4d70-b3db-782c06bbce79": Phase="Running", Reason="", readiness=true. Elapsed: 2.005085024s
Jan 23 14:51:43.798: INFO: Pod "pod-8ab6c967-465c-4d70-b3db-782c06bbce79": Phase="Running", Reason="", readiness=false. Elapsed: 4.032037957s
Jan 23 14:51:45.778: INFO: Pod "pod-8ab6c967-465c-4d70-b3db-782c06bbce79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012234382s
STEP: Saw pod success 01/23/24 14:51:45.778
Jan 23 14:51:45.778: INFO: Pod "pod-8ab6c967-465c-4d70-b3db-782c06bbce79" satisfied condition "Succeeded or Failed"
Jan 23 14:51:45.809: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-8ab6c967-465c-4d70-b3db-782c06bbce79 container test-container: <nil>
STEP: delete the pod 01/23/24 14:51:45.815
Jan 23 14:51:45.836: INFO: Waiting for pod pod-8ab6c967-465c-4d70-b3db-782c06bbce79 to disappear
Jan 23 14:51:45.848: INFO: Pod pod-8ab6c967-465c-4d70-b3db-782c06bbce79 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 23 14:51:45.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2507" for this suite. 01/23/24 14:51:45.857
------------------------------
• [SLOW TEST] [6.121 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:51:39.742
    Jan 23 14:51:39.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename emptydir 01/23/24 14:51:39.743
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:39.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:39.751
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/23/24 14:51:39.753
    Jan 23 14:51:39.766: INFO: Waiting up to 5m0s for pod "pod-8ab6c967-465c-4d70-b3db-782c06bbce79" in namespace "emptydir-2507" to be "Succeeded or Failed"
    Jan 23 14:51:39.768: INFO: Pod "pod-8ab6c967-465c-4d70-b3db-782c06bbce79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110983ms
    Jan 23 14:51:41.771: INFO: Pod "pod-8ab6c967-465c-4d70-b3db-782c06bbce79": Phase="Running", Reason="", readiness=true. Elapsed: 2.005085024s
    Jan 23 14:51:43.798: INFO: Pod "pod-8ab6c967-465c-4d70-b3db-782c06bbce79": Phase="Running", Reason="", readiness=false. Elapsed: 4.032037957s
    Jan 23 14:51:45.778: INFO: Pod "pod-8ab6c967-465c-4d70-b3db-782c06bbce79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012234382s
    STEP: Saw pod success 01/23/24 14:51:45.778
    Jan 23 14:51:45.778: INFO: Pod "pod-8ab6c967-465c-4d70-b3db-782c06bbce79" satisfied condition "Succeeded or Failed"
    Jan 23 14:51:45.809: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-8ab6c967-465c-4d70-b3db-782c06bbce79 container test-container: <nil>
    STEP: delete the pod 01/23/24 14:51:45.815
    Jan 23 14:51:45.836: INFO: Waiting for pod pod-8ab6c967-465c-4d70-b3db-782c06bbce79 to disappear
    Jan 23 14:51:45.848: INFO: Pod pod-8ab6c967-465c-4d70-b3db-782c06bbce79 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:51:45.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2507" for this suite. 01/23/24 14:51:45.857
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:51:45.864
Jan 23 14:51:45.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename pods 01/23/24 14:51:45.865
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:45.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:45.887
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 01/23/24 14:51:45.891
STEP: submitting the pod to kubernetes 01/23/24 14:51:45.891
Jan 23 14:51:45.936: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3" in namespace "pods-2167" to be "running and ready"
Jan 23 14:51:45.950: INFO: Pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.147116ms
Jan 23 14:51:45.950: INFO: The phase of Pod pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:51:47.952: INFO: Pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.015226987s
Jan 23 14:51:47.952: INFO: The phase of Pod pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3 is Running (Ready = true)
Jan 23 14:51:47.952: INFO: Pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/23/24 14:51:47.953
STEP: updating the pod 01/23/24 14:51:47.954
Jan 23 14:51:48.464: INFO: Successfully updated pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3"
Jan 23 14:51:48.464: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3" in namespace "pods-2167" to be "terminated with reason DeadlineExceeded"
Jan 23 14:51:48.465: INFO: Pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3": Phase="Running", Reason="", readiness=true. Elapsed: 1.396049ms
Jan 23 14:51:50.468: INFO: Pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3": Phase="Running", Reason="", readiness=false. Elapsed: 2.003838168s
Jan 23 14:51:52.469: INFO: Pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.005394513s
Jan 23 14:51:52.469: INFO: Pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 23 14:51:52.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2167" for this suite. 01/23/24 14:51:52.473
------------------------------
• [SLOW TEST] [6.613 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:51:45.864
    Jan 23 14:51:45.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename pods 01/23/24 14:51:45.865
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:45.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:45.887
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 01/23/24 14:51:45.891
    STEP: submitting the pod to kubernetes 01/23/24 14:51:45.891
    Jan 23 14:51:45.936: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3" in namespace "pods-2167" to be "running and ready"
    Jan 23 14:51:45.950: INFO: Pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.147116ms
    Jan 23 14:51:45.950: INFO: The phase of Pod pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:51:47.952: INFO: Pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.015226987s
    Jan 23 14:51:47.952: INFO: The phase of Pod pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3 is Running (Ready = true)
    Jan 23 14:51:47.952: INFO: Pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/23/24 14:51:47.953
    STEP: updating the pod 01/23/24 14:51:47.954
    Jan 23 14:51:48.464: INFO: Successfully updated pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3"
    Jan 23 14:51:48.464: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3" in namespace "pods-2167" to be "terminated with reason DeadlineExceeded"
    Jan 23 14:51:48.465: INFO: Pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3": Phase="Running", Reason="", readiness=true. Elapsed: 1.396049ms
    Jan 23 14:51:50.468: INFO: Pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3": Phase="Running", Reason="", readiness=false. Elapsed: 2.003838168s
    Jan 23 14:51:52.469: INFO: Pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.005394513s
    Jan 23 14:51:52.469: INFO: Pod "pod-update-activedeadlineseconds-69d4d3d3-be8a-40b0-9d68-f28e7d82e1b3" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:51:52.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2167" for this suite. 01/23/24 14:51:52.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:51:52.477
Jan 23 14:51:52.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename downward-api 01/23/24 14:51:52.477
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:52.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:52.487
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 01/23/24 14:51:52.491
Jan 23 14:51:52.505: INFO: Waiting up to 5m0s for pod "annotationupdate9747db0e-1106-4206-8054-59f88cddee11" in namespace "downward-api-6049" to be "running and ready"
Jan 23 14:51:52.507: INFO: Pod "annotationupdate9747db0e-1106-4206-8054-59f88cddee11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.69026ms
Jan 23 14:51:52.507: INFO: The phase of Pod annotationupdate9747db0e-1106-4206-8054-59f88cddee11 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:51:54.511: INFO: Pod "annotationupdate9747db0e-1106-4206-8054-59f88cddee11": Phase="Running", Reason="", readiness=true. Elapsed: 2.005943655s
Jan 23 14:51:54.511: INFO: The phase of Pod annotationupdate9747db0e-1106-4206-8054-59f88cddee11 is Running (Ready = true)
Jan 23 14:51:54.511: INFO: Pod "annotationupdate9747db0e-1106-4206-8054-59f88cddee11" satisfied condition "running and ready"
Jan 23 14:51:55.025: INFO: Successfully updated pod "annotationupdate9747db0e-1106-4206-8054-59f88cddee11"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 23 14:51:59.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6049" for this suite. 01/23/24 14:51:59.042
------------------------------
• [SLOW TEST] [6.569 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:51:52.477
    Jan 23 14:51:52.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename downward-api 01/23/24 14:51:52.477
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:52.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:52.487
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 01/23/24 14:51:52.491
    Jan 23 14:51:52.505: INFO: Waiting up to 5m0s for pod "annotationupdate9747db0e-1106-4206-8054-59f88cddee11" in namespace "downward-api-6049" to be "running and ready"
    Jan 23 14:51:52.507: INFO: Pod "annotationupdate9747db0e-1106-4206-8054-59f88cddee11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.69026ms
    Jan 23 14:51:52.507: INFO: The phase of Pod annotationupdate9747db0e-1106-4206-8054-59f88cddee11 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:51:54.511: INFO: Pod "annotationupdate9747db0e-1106-4206-8054-59f88cddee11": Phase="Running", Reason="", readiness=true. Elapsed: 2.005943655s
    Jan 23 14:51:54.511: INFO: The phase of Pod annotationupdate9747db0e-1106-4206-8054-59f88cddee11 is Running (Ready = true)
    Jan 23 14:51:54.511: INFO: Pod "annotationupdate9747db0e-1106-4206-8054-59f88cddee11" satisfied condition "running and ready"
    Jan 23 14:51:55.025: INFO: Successfully updated pod "annotationupdate9747db0e-1106-4206-8054-59f88cddee11"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:51:59.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6049" for this suite. 01/23/24 14:51:59.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:51:59.047
Jan 23 14:51:59.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubectl 01/23/24 14:51:59.047
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:59.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:59.061
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/23/24 14:51:59.062
Jan 23 14:51:59.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-2798 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 23 14:51:59.163: INFO: stderr: ""
Jan 23 14:51:59.163: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/23/24 14:51:59.163
STEP: verifying the pod e2e-test-httpd-pod was created 01/23/24 14:52:04.215
Jan 23 14:52:04.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-2798 get pod e2e-test-httpd-pod -o json'
Jan 23 14:52:04.364: INFO: stderr: ""
Jan 23 14:52:04.364: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"22e6a4e17df9b228bd1a3b576ffff47e9736be35e2954ec47a8b35a61e4d6676\",\n            \"cni.projectcalico.org/podIP\": \"10.233.87.160/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.87.160/32\"\n        },\n        \"creationTimestamp\": \"2024-01-23T14:51:59Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2798\",\n        \"resourceVersion\": \"156291\",\n        \"uid\": \"0e88ce7a-92a2-45e8-96bf-7762d0f45097\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-7wwjr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 1000000000,\n        \"priorityClassName\": \"nova-user-critical\",\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-7wwjr\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-23T14:51:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-23T14:52:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-23T14:52:00Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-23T14:51:59Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://48cf982367fe6b8985c2e8b588c777b57c51214949970034ed898e775fd542d7\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2024-01-23T14:51:59Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.11.67\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.87.160\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.87.160\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2024-01-23T14:51:59Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/23/24 14:52:04.364
Jan 23 14:52:04.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-2798 replace -f -'
Jan 23 14:52:05.453: INFO: stderr: ""
Jan 23 14:52:05.453: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/23/24 14:52:05.453
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Jan 23 14:52:05.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-2798 delete pods e2e-test-httpd-pod'
Jan 23 14:52:07.358: INFO: stderr: ""
Jan 23 14:52:07.358: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 23 14:52:07.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2798" for this suite. 01/23/24 14:52:07.361
------------------------------
• [SLOW TEST] [8.318 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:51:59.047
    Jan 23 14:51:59.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubectl 01/23/24 14:51:59.047
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:51:59.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:51:59.061
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/23/24 14:51:59.062
    Jan 23 14:51:59.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-2798 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 23 14:51:59.163: INFO: stderr: ""
    Jan 23 14:51:59.163: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/23/24 14:51:59.163
    STEP: verifying the pod e2e-test-httpd-pod was created 01/23/24 14:52:04.215
    Jan 23 14:52:04.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-2798 get pod e2e-test-httpd-pod -o json'
    Jan 23 14:52:04.364: INFO: stderr: ""
    Jan 23 14:52:04.364: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"22e6a4e17df9b228bd1a3b576ffff47e9736be35e2954ec47a8b35a61e4d6676\",\n            \"cni.projectcalico.org/podIP\": \"10.233.87.160/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.87.160/32\"\n        },\n        \"creationTimestamp\": \"2024-01-23T14:51:59Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2798\",\n        \"resourceVersion\": \"156291\",\n        \"uid\": \"0e88ce7a-92a2-45e8-96bf-7762d0f45097\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-7wwjr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 1000000000,\n        \"priorityClassName\": \"nova-user-critical\",\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-7wwjr\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-23T14:51:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-23T14:52:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-23T14:52:00Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-23T14:51:59Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://48cf982367fe6b8985c2e8b588c777b57c51214949970034ed898e775fd542d7\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2024-01-23T14:51:59Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.11.67\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.87.160\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.87.160\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2024-01-23T14:51:59Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/23/24 14:52:04.364
    Jan 23 14:52:04.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-2798 replace -f -'
    Jan 23 14:52:05.453: INFO: stderr: ""
    Jan 23 14:52:05.453: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/23/24 14:52:05.453
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Jan 23 14:52:05.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-2798 delete pods e2e-test-httpd-pod'
    Jan 23 14:52:07.358: INFO: stderr: ""
    Jan 23 14:52:07.358: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:52:07.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2798" for this suite. 01/23/24 14:52:07.361
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:52:07.365
Jan 23 14:52:07.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename cronjob 01/23/24 14:52:07.366
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:52:07.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:52:07.375
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/23/24 14:52:07.376
STEP: creating 01/23/24 14:52:07.376
STEP: getting 01/23/24 14:52:07.387
STEP: listing 01/23/24 14:52:07.389
STEP: watching 01/23/24 14:52:07.39
Jan 23 14:52:07.390: INFO: starting watch
STEP: cluster-wide listing 01/23/24 14:52:07.391
STEP: cluster-wide watching 01/23/24 14:52:07.392
Jan 23 14:52:07.392: INFO: starting watch
STEP: patching 01/23/24 14:52:07.393
STEP: updating 01/23/24 14:52:07.397
Jan 23 14:52:07.401: INFO: waiting for watch events with expected annotations
Jan 23 14:52:07.401: INFO: saw patched and updated annotations
STEP: patching /status 01/23/24 14:52:07.401
STEP: updating /status 01/23/24 14:52:07.406
STEP: get /status 01/23/24 14:52:07.409
STEP: deleting 01/23/24 14:52:07.411
STEP: deleting a collection 01/23/24 14:52:07.418
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 23 14:52:07.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2904" for this suite. 01/23/24 14:52:07.426
------------------------------
• [0.065 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:52:07.365
    Jan 23 14:52:07.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename cronjob 01/23/24 14:52:07.366
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:52:07.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:52:07.375
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/23/24 14:52:07.376
    STEP: creating 01/23/24 14:52:07.376
    STEP: getting 01/23/24 14:52:07.387
    STEP: listing 01/23/24 14:52:07.389
    STEP: watching 01/23/24 14:52:07.39
    Jan 23 14:52:07.390: INFO: starting watch
    STEP: cluster-wide listing 01/23/24 14:52:07.391
    STEP: cluster-wide watching 01/23/24 14:52:07.392
    Jan 23 14:52:07.392: INFO: starting watch
    STEP: patching 01/23/24 14:52:07.393
    STEP: updating 01/23/24 14:52:07.397
    Jan 23 14:52:07.401: INFO: waiting for watch events with expected annotations
    Jan 23 14:52:07.401: INFO: saw patched and updated annotations
    STEP: patching /status 01/23/24 14:52:07.401
    STEP: updating /status 01/23/24 14:52:07.406
    STEP: get /status 01/23/24 14:52:07.409
    STEP: deleting 01/23/24 14:52:07.411
    STEP: deleting a collection 01/23/24 14:52:07.418
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:52:07.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2904" for this suite. 01/23/24 14:52:07.426
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:52:07.43
Jan 23 14:52:07.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubectl 01/23/24 14:52:07.431
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:52:07.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:52:07.439
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 01/23/24 14:52:07.441
Jan 23 14:52:07.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1226 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 23 14:52:07.549: INFO: stderr: ""
Jan 23 14:52:07.549: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 01/23/24 14:52:07.549
Jan 23 14:52:07.549: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 23 14:52:07.549: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1226" to be "running and ready, or succeeded"
Jan 23 14:52:07.551: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.575688ms
Jan 23 14:52:07.551: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local' to be 'Running' but was 'Pending'
Jan 23 14:52:09.554: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.004833265s
Jan 23 14:52:09.554: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 23 14:52:09.554: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/23/24 14:52:09.554
Jan 23 14:52:09.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1226 logs logs-generator logs-generator'
Jan 23 14:52:09.665: INFO: stderr: ""
Jan 23 14:52:09.665: INFO: stdout: "I0123 14:52:08.398397       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/qkf 214\nI0123 14:52:08.598532       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/88lk 424\nI0123 14:52:08.799148       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/rvj 563\nI0123 14:52:08.999483       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/wqz 206\nI0123 14:52:09.198834       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/7vdq 242\nI0123 14:52:09.399215       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/h79p 589\nI0123 14:52:09.598494       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/rhk 262\n"
STEP: limiting log lines 01/23/24 14:52:09.665
Jan 23 14:52:09.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1226 logs logs-generator logs-generator --tail=1'
Jan 23 14:52:09.776: INFO: stderr: ""
Jan 23 14:52:09.776: INFO: stdout: "I0123 14:52:09.598494       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/rhk 262\n"
Jan 23 14:52:09.776: INFO: got output "I0123 14:52:09.598494       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/rhk 262\n"
STEP: limiting log bytes 01/23/24 14:52:09.776
Jan 23 14:52:09.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1226 logs logs-generator logs-generator --limit-bytes=1'
Jan 23 14:52:09.889: INFO: stderr: ""
Jan 23 14:52:09.889: INFO: stdout: "I"
Jan 23 14:52:09.889: INFO: got output "I"
STEP: exposing timestamps 01/23/24 14:52:09.889
Jan 23 14:52:09.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1226 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 23 14:52:09.998: INFO: stderr: ""
Jan 23 14:52:09.998: INFO: stdout: "2024-01-23T17:52:09.798956727+03:00 I0123 14:52:09.798827       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/rstk 313\n"
Jan 23 14:52:09.998: INFO: got output "2024-01-23T17:52:09.798956727+03:00 I0123 14:52:09.798827       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/rstk 313\n"
STEP: restricting to a time range 01/23/24 14:52:09.998
Jan 23 14:52:12.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1226 logs logs-generator logs-generator --since=1s'
Jan 23 14:52:12.625: INFO: stderr: ""
Jan 23 14:52:12.625: INFO: stdout: "I0123 14:52:11.798935       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/brx 575\nI0123 14:52:11.999267       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/n86m 387\nI0123 14:52:12.198546       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/9xw 412\nI0123 14:52:12.398870       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/4kg 204\nI0123 14:52:12.599211       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/mc24 225\n"
Jan 23 14:52:12.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1226 logs logs-generator logs-generator --since=24h'
Jan 23 14:52:12.750: INFO: stderr: ""
Jan 23 14:52:12.750: INFO: stdout: "I0123 14:52:08.398397       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/qkf 214\nI0123 14:52:08.598532       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/88lk 424\nI0123 14:52:08.799148       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/rvj 563\nI0123 14:52:08.999483       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/wqz 206\nI0123 14:52:09.198834       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/7vdq 242\nI0123 14:52:09.399215       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/h79p 589\nI0123 14:52:09.598494       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/rhk 262\nI0123 14:52:09.798827       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/rstk 313\nI0123 14:52:09.999139       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/b4qp 524\nI0123 14:52:10.198431       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/2jb 421\nI0123 14:52:10.398749       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/hkf 473\nI0123 14:52:10.599078       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/bqw 401\nI0123 14:52:10.799383       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/6zz7 346\nI0123 14:52:10.998669       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/zz6t 552\nI0123 14:52:11.198999       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/fw5 549\nI0123 14:52:11.399336       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/qf5 200\nI0123 14:52:11.598602       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/9ffc 419\nI0123 14:52:11.798935       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/brx 575\nI0123 14:52:11.999267       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/n86m 387\nI0123 14:52:12.198546       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/9xw 412\nI0123 14:52:12.398870       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/4kg 204\nI0123 14:52:12.599211       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/mc24 225\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Jan 23 14:52:12.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1226 delete pod logs-generator'
Jan 23 14:52:14.277: INFO: stderr: ""
Jan 23 14:52:14.277: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 23 14:52:14.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1226" for this suite. 01/23/24 14:52:14.28
------------------------------
• [SLOW TEST] [6.853 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:52:07.43
    Jan 23 14:52:07.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubectl 01/23/24 14:52:07.431
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:52:07.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:52:07.439
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 01/23/24 14:52:07.441
    Jan 23 14:52:07.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1226 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan 23 14:52:07.549: INFO: stderr: ""
    Jan 23 14:52:07.549: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 01/23/24 14:52:07.549
    Jan 23 14:52:07.549: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan 23 14:52:07.549: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1226" to be "running and ready, or succeeded"
    Jan 23 14:52:07.551: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.575688ms
    Jan 23 14:52:07.551: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local' to be 'Running' but was 'Pending'
    Jan 23 14:52:09.554: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.004833265s
    Jan 23 14:52:09.554: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan 23 14:52:09.554: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/23/24 14:52:09.554
    Jan 23 14:52:09.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1226 logs logs-generator logs-generator'
    Jan 23 14:52:09.665: INFO: stderr: ""
    Jan 23 14:52:09.665: INFO: stdout: "I0123 14:52:08.398397       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/qkf 214\nI0123 14:52:08.598532       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/88lk 424\nI0123 14:52:08.799148       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/rvj 563\nI0123 14:52:08.999483       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/wqz 206\nI0123 14:52:09.198834       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/7vdq 242\nI0123 14:52:09.399215       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/h79p 589\nI0123 14:52:09.598494       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/rhk 262\n"
    STEP: limiting log lines 01/23/24 14:52:09.665
    Jan 23 14:52:09.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1226 logs logs-generator logs-generator --tail=1'
    Jan 23 14:52:09.776: INFO: stderr: ""
    Jan 23 14:52:09.776: INFO: stdout: "I0123 14:52:09.598494       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/rhk 262\n"
    Jan 23 14:52:09.776: INFO: got output "I0123 14:52:09.598494       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/rhk 262\n"
    STEP: limiting log bytes 01/23/24 14:52:09.776
    Jan 23 14:52:09.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1226 logs logs-generator logs-generator --limit-bytes=1'
    Jan 23 14:52:09.889: INFO: stderr: ""
    Jan 23 14:52:09.889: INFO: stdout: "I"
    Jan 23 14:52:09.889: INFO: got output "I"
    STEP: exposing timestamps 01/23/24 14:52:09.889
    Jan 23 14:52:09.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1226 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan 23 14:52:09.998: INFO: stderr: ""
    Jan 23 14:52:09.998: INFO: stdout: "2024-01-23T17:52:09.798956727+03:00 I0123 14:52:09.798827       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/rstk 313\n"
    Jan 23 14:52:09.998: INFO: got output "2024-01-23T17:52:09.798956727+03:00 I0123 14:52:09.798827       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/rstk 313\n"
    STEP: restricting to a time range 01/23/24 14:52:09.998
    Jan 23 14:52:12.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1226 logs logs-generator logs-generator --since=1s'
    Jan 23 14:52:12.625: INFO: stderr: ""
    Jan 23 14:52:12.625: INFO: stdout: "I0123 14:52:11.798935       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/brx 575\nI0123 14:52:11.999267       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/n86m 387\nI0123 14:52:12.198546       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/9xw 412\nI0123 14:52:12.398870       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/4kg 204\nI0123 14:52:12.599211       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/mc24 225\n"
    Jan 23 14:52:12.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1226 logs logs-generator logs-generator --since=24h'
    Jan 23 14:52:12.750: INFO: stderr: ""
    Jan 23 14:52:12.750: INFO: stdout: "I0123 14:52:08.398397       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/qkf 214\nI0123 14:52:08.598532       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/88lk 424\nI0123 14:52:08.799148       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/rvj 563\nI0123 14:52:08.999483       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/wqz 206\nI0123 14:52:09.198834       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/7vdq 242\nI0123 14:52:09.399215       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/h79p 589\nI0123 14:52:09.598494       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/rhk 262\nI0123 14:52:09.798827       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/rstk 313\nI0123 14:52:09.999139       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/b4qp 524\nI0123 14:52:10.198431       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/2jb 421\nI0123 14:52:10.398749       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/hkf 473\nI0123 14:52:10.599078       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/bqw 401\nI0123 14:52:10.799383       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/6zz7 346\nI0123 14:52:10.998669       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/zz6t 552\nI0123 14:52:11.198999       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/fw5 549\nI0123 14:52:11.399336       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/qf5 200\nI0123 14:52:11.598602       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/9ffc 419\nI0123 14:52:11.798935       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/brx 575\nI0123 14:52:11.999267       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/n86m 387\nI0123 14:52:12.198546       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/9xw 412\nI0123 14:52:12.398870       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/4kg 204\nI0123 14:52:12.599211       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/mc24 225\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Jan 23 14:52:12.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=kubectl-1226 delete pod logs-generator'
    Jan 23 14:52:14.277: INFO: stderr: ""
    Jan 23 14:52:14.277: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:52:14.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1226" for this suite. 01/23/24 14:52:14.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:52:14.283
Jan 23 14:52:14.284: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename var-expansion 01/23/24 14:52:14.284
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:52:14.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:52:14.293
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 01/23/24 14:52:14.295
Jan 23 14:52:14.322: INFO: Waiting up to 5m0s for pod "var-expansion-5500eae0-44b1-4e09-af90-3a3a96cad91f" in namespace "var-expansion-7091" to be "Succeeded or Failed"
Jan 23 14:52:14.324: INFO: Pod "var-expansion-5500eae0-44b1-4e09-af90-3a3a96cad91f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.873093ms
Jan 23 14:52:16.328: INFO: Pod "var-expansion-5500eae0-44b1-4e09-af90-3a3a96cad91f": Phase="Running", Reason="", readiness=false. Elapsed: 2.005992563s
Jan 23 14:52:18.328: INFO: Pod "var-expansion-5500eae0-44b1-4e09-af90-3a3a96cad91f": Phase="Running", Reason="", readiness=false. Elapsed: 4.005954507s
Jan 23 14:52:20.328: INFO: Pod "var-expansion-5500eae0-44b1-4e09-af90-3a3a96cad91f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00621061s
STEP: Saw pod success 01/23/24 14:52:20.328
Jan 23 14:52:20.328: INFO: Pod "var-expansion-5500eae0-44b1-4e09-af90-3a3a96cad91f" satisfied condition "Succeeded or Failed"
Jan 23 14:52:20.329: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod var-expansion-5500eae0-44b1-4e09-af90-3a3a96cad91f container dapi-container: <nil>
STEP: delete the pod 01/23/24 14:52:20.333
Jan 23 14:52:20.340: INFO: Waiting for pod var-expansion-5500eae0-44b1-4e09-af90-3a3a96cad91f to disappear
Jan 23 14:52:20.341: INFO: Pod var-expansion-5500eae0-44b1-4e09-af90-3a3a96cad91f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 23 14:52:20.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7091" for this suite. 01/23/24 14:52:20.343
------------------------------
• [SLOW TEST] [6.062 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:52:14.283
    Jan 23 14:52:14.284: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename var-expansion 01/23/24 14:52:14.284
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:52:14.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:52:14.293
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 01/23/24 14:52:14.295
    Jan 23 14:52:14.322: INFO: Waiting up to 5m0s for pod "var-expansion-5500eae0-44b1-4e09-af90-3a3a96cad91f" in namespace "var-expansion-7091" to be "Succeeded or Failed"
    Jan 23 14:52:14.324: INFO: Pod "var-expansion-5500eae0-44b1-4e09-af90-3a3a96cad91f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.873093ms
    Jan 23 14:52:16.328: INFO: Pod "var-expansion-5500eae0-44b1-4e09-af90-3a3a96cad91f": Phase="Running", Reason="", readiness=false. Elapsed: 2.005992563s
    Jan 23 14:52:18.328: INFO: Pod "var-expansion-5500eae0-44b1-4e09-af90-3a3a96cad91f": Phase="Running", Reason="", readiness=false. Elapsed: 4.005954507s
    Jan 23 14:52:20.328: INFO: Pod "var-expansion-5500eae0-44b1-4e09-af90-3a3a96cad91f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00621061s
    STEP: Saw pod success 01/23/24 14:52:20.328
    Jan 23 14:52:20.328: INFO: Pod "var-expansion-5500eae0-44b1-4e09-af90-3a3a96cad91f" satisfied condition "Succeeded or Failed"
    Jan 23 14:52:20.329: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod var-expansion-5500eae0-44b1-4e09-af90-3a3a96cad91f container dapi-container: <nil>
    STEP: delete the pod 01/23/24 14:52:20.333
    Jan 23 14:52:20.340: INFO: Waiting for pod var-expansion-5500eae0-44b1-4e09-af90-3a3a96cad91f to disappear
    Jan 23 14:52:20.341: INFO: Pod var-expansion-5500eae0-44b1-4e09-af90-3a3a96cad91f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:52:20.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7091" for this suite. 01/23/24 14:52:20.343
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:52:20.346
Jan 23 14:52:20.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/23/24 14:52:20.347
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:52:20.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:52:20.355
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/23/24 14:52:20.357
STEP: Creating hostNetwork=false pod 01/23/24 14:52:20.357
Jan 23 14:52:20.375: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-6882" to be "running and ready"
Jan 23 14:52:20.376: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.268108ms
Jan 23 14:52:20.376: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:52:22.380: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004939305s
Jan 23 14:52:22.380: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan 23 14:52:22.380: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/23/24 14:52:22.381
Jan 23 14:52:22.395: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-6882" to be "running and ready"
Jan 23 14:52:22.397: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.748754ms
Jan 23 14:52:22.397: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:52:24.400: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004950507s
Jan 23 14:52:24.400: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan 23 14:52:24.400: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/23/24 14:52:24.402
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/23/24 14:52:24.402
Jan 23 14:52:24.402: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 14:52:24.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:52:24.402: INFO: ExecWithOptions: Clientset creation
Jan 23 14:52:24.402: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 23 14:52:24.461: INFO: Exec stderr: ""
Jan 23 14:52:24.461: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 14:52:24.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:52:24.461: INFO: ExecWithOptions: Clientset creation
Jan 23 14:52:24.461: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 23 14:52:24.513: INFO: Exec stderr: ""
Jan 23 14:52:24.513: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 14:52:24.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:52:24.513: INFO: ExecWithOptions: Clientset creation
Jan 23 14:52:24.514: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 23 14:52:24.577: INFO: Exec stderr: ""
Jan 23 14:52:24.577: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 14:52:24.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:52:24.577: INFO: ExecWithOptions: Clientset creation
Jan 23 14:52:24.577: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 23 14:52:24.630: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/23/24 14:52:24.63
Jan 23 14:52:24.630: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 14:52:24.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:52:24.630: INFO: ExecWithOptions: Clientset creation
Jan 23 14:52:24.630: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 23 14:52:24.681: INFO: Exec stderr: ""
Jan 23 14:52:24.681: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 14:52:24.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:52:24.681: INFO: ExecWithOptions: Clientset creation
Jan 23 14:52:24.681: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 23 14:52:24.744: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/23/24 14:52:24.744
Jan 23 14:52:24.744: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 14:52:24.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:52:24.745: INFO: ExecWithOptions: Clientset creation
Jan 23 14:52:24.745: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 23 14:52:24.799: INFO: Exec stderr: ""
Jan 23 14:52:24.799: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 14:52:24.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:52:24.799: INFO: ExecWithOptions: Clientset creation
Jan 23 14:52:24.799: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 23 14:52:24.853: INFO: Exec stderr: ""
Jan 23 14:52:24.854: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 14:52:24.854: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:52:24.854: INFO: ExecWithOptions: Clientset creation
Jan 23 14:52:24.854: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 23 14:52:24.914: INFO: Exec stderr: ""
Jan 23 14:52:24.914: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 14:52:24.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:52:24.915: INFO: ExecWithOptions: Clientset creation
Jan 23 14:52:24.915: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 23 14:52:24.975: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Jan 23 14:52:24.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6882" for this suite. 01/23/24 14:52:24.978
------------------------------
• [4.635 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:52:20.346
    Jan 23 14:52:20.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/23/24 14:52:20.347
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:52:20.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:52:20.355
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/23/24 14:52:20.357
    STEP: Creating hostNetwork=false pod 01/23/24 14:52:20.357
    Jan 23 14:52:20.375: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-6882" to be "running and ready"
    Jan 23 14:52:20.376: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.268108ms
    Jan 23 14:52:20.376: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:52:22.380: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004939305s
    Jan 23 14:52:22.380: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan 23 14:52:22.380: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/23/24 14:52:22.381
    Jan 23 14:52:22.395: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-6882" to be "running and ready"
    Jan 23 14:52:22.397: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.748754ms
    Jan 23 14:52:22.397: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:52:24.400: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004950507s
    Jan 23 14:52:24.400: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan 23 14:52:24.400: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/23/24 14:52:24.402
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/23/24 14:52:24.402
    Jan 23 14:52:24.402: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 14:52:24.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:52:24.402: INFO: ExecWithOptions: Clientset creation
    Jan 23 14:52:24.402: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 23 14:52:24.461: INFO: Exec stderr: ""
    Jan 23 14:52:24.461: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 14:52:24.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:52:24.461: INFO: ExecWithOptions: Clientset creation
    Jan 23 14:52:24.461: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 23 14:52:24.513: INFO: Exec stderr: ""
    Jan 23 14:52:24.513: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 14:52:24.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:52:24.513: INFO: ExecWithOptions: Clientset creation
    Jan 23 14:52:24.514: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 23 14:52:24.577: INFO: Exec stderr: ""
    Jan 23 14:52:24.577: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 14:52:24.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:52:24.577: INFO: ExecWithOptions: Clientset creation
    Jan 23 14:52:24.577: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 23 14:52:24.630: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/23/24 14:52:24.63
    Jan 23 14:52:24.630: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 14:52:24.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:52:24.630: INFO: ExecWithOptions: Clientset creation
    Jan 23 14:52:24.630: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 23 14:52:24.681: INFO: Exec stderr: ""
    Jan 23 14:52:24.681: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 14:52:24.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:52:24.681: INFO: ExecWithOptions: Clientset creation
    Jan 23 14:52:24.681: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 23 14:52:24.744: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/23/24 14:52:24.744
    Jan 23 14:52:24.744: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 14:52:24.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:52:24.745: INFO: ExecWithOptions: Clientset creation
    Jan 23 14:52:24.745: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 23 14:52:24.799: INFO: Exec stderr: ""
    Jan 23 14:52:24.799: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 14:52:24.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:52:24.799: INFO: ExecWithOptions: Clientset creation
    Jan 23 14:52:24.799: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 23 14:52:24.853: INFO: Exec stderr: ""
    Jan 23 14:52:24.854: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 14:52:24.854: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:52:24.854: INFO: ExecWithOptions: Clientset creation
    Jan 23 14:52:24.854: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 23 14:52:24.914: INFO: Exec stderr: ""
    Jan 23 14:52:24.914: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6882 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 14:52:24.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:52:24.915: INFO: ExecWithOptions: Clientset creation
    Jan 23 14:52:24.915: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6882/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 23 14:52:24.975: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:52:24.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-6882" for this suite. 01/23/24 14:52:24.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:52:24.982
Jan 23 14:52:24.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename resourcequota 01/23/24 14:52:24.983
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:52:24.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:52:24.995
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 01/23/24 14:52:24.996
STEP: Creating a ResourceQuota 01/23/24 14:52:29.998
STEP: Ensuring resource quota status is calculated 01/23/24 14:52:30.01
STEP: Creating a Service 01/23/24 14:52:32.014
STEP: Creating a NodePort Service 01/23/24 14:52:32.024
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/23/24 14:52:32.036
STEP: Ensuring resource quota status captures service creation 01/23/24 14:52:32.045
STEP: Deleting Services 01/23/24 14:52:34.048
STEP: Ensuring resource quota status released usage 01/23/24 14:52:34.066
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 23 14:52:36.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-271" for this suite. 01/23/24 14:52:36.071
------------------------------
• [SLOW TEST] [11.092 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:52:24.982
    Jan 23 14:52:24.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename resourcequota 01/23/24 14:52:24.983
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:52:24.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:52:24.995
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 01/23/24 14:52:24.996
    STEP: Creating a ResourceQuota 01/23/24 14:52:29.998
    STEP: Ensuring resource quota status is calculated 01/23/24 14:52:30.01
    STEP: Creating a Service 01/23/24 14:52:32.014
    STEP: Creating a NodePort Service 01/23/24 14:52:32.024
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/23/24 14:52:32.036
    STEP: Ensuring resource quota status captures service creation 01/23/24 14:52:32.045
    STEP: Deleting Services 01/23/24 14:52:34.048
    STEP: Ensuring resource quota status released usage 01/23/24 14:52:34.066
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:52:36.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-271" for this suite. 01/23/24 14:52:36.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:52:36.075
Jan 23 14:52:36.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename resourcequota 01/23/24 14:52:36.076
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:52:36.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:52:36.086
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 01/23/24 14:52:36.088
STEP: Ensuring ResourceQuota status is calculated 01/23/24 14:52:36.091
STEP: Creating a ResourceQuota with not terminating scope 01/23/24 14:52:38.095
STEP: Ensuring ResourceQuota status is calculated 01/23/24 14:52:38.099
STEP: Creating a long running pod 01/23/24 14:52:40.102
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/23/24 14:52:40.118
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/23/24 14:52:42.121
STEP: Deleting the pod 01/23/24 14:52:44.124
STEP: Ensuring resource quota status released the pod usage 01/23/24 14:52:44.133
STEP: Creating a terminating pod 01/23/24 14:52:46.137
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/23/24 14:52:46.158
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/23/24 14:52:48.161
STEP: Deleting the pod 01/23/24 14:52:50.165
STEP: Ensuring resource quota status released the pod usage 01/23/24 14:52:50.172
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 23 14:52:52.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7703" for this suite. 01/23/24 14:52:52.177
------------------------------
• [SLOW TEST] [16.106 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:52:36.075
    Jan 23 14:52:36.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename resourcequota 01/23/24 14:52:36.076
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:52:36.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:52:36.086
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 01/23/24 14:52:36.088
    STEP: Ensuring ResourceQuota status is calculated 01/23/24 14:52:36.091
    STEP: Creating a ResourceQuota with not terminating scope 01/23/24 14:52:38.095
    STEP: Ensuring ResourceQuota status is calculated 01/23/24 14:52:38.099
    STEP: Creating a long running pod 01/23/24 14:52:40.102
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/23/24 14:52:40.118
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/23/24 14:52:42.121
    STEP: Deleting the pod 01/23/24 14:52:44.124
    STEP: Ensuring resource quota status released the pod usage 01/23/24 14:52:44.133
    STEP: Creating a terminating pod 01/23/24 14:52:46.137
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/23/24 14:52:46.158
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/23/24 14:52:48.161
    STEP: Deleting the pod 01/23/24 14:52:50.165
    STEP: Ensuring resource quota status released the pod usage 01/23/24 14:52:50.172
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:52:52.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7703" for this suite. 01/23/24 14:52:52.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:52:52.183
Jan 23 14:52:52.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename gc 01/23/24 14:52:52.184
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:52:52.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:52:52.193
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/23/24 14:52:52.196
STEP: create the rc2 01/23/24 14:52:52.199
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/23/24 14:52:57.205
STEP: delete the rc simpletest-rc-to-be-deleted 01/23/24 14:52:57.563
STEP: wait for the rc to be deleted 01/23/24 14:52:57.571
Jan 23 14:53:02.578: INFO: 62 pods remaining
Jan 23 14:53:02.578: INFO: 62 pods has nil DeletionTimestamp
Jan 23 14:53:02.578: INFO: 
STEP: Gathering metrics 01/23/24 14:53:07.578
Jan 23 14:53:07.591: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" in namespace "kube-system" to be "running and ready"
Jan 23 14:53:07.593: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local": Phase="Running", Reason="", readiness=true. Elapsed: 1.591091ms
Jan 23 14:53:07.593: INFO: The phase of Pod kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local is Running (Ready = true)
Jan 23 14:53:07.593: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" satisfied condition "running and ready"
Jan 23 14:53:07.635: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 23 14:53:07.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-29sz8" in namespace "gc-7180"
Jan 23 14:53:07.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-2b4wk" in namespace "gc-7180"
Jan 23 14:53:07.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lrqc" in namespace "gc-7180"
Jan 23 14:53:07.659: INFO: Deleting pod "simpletest-rc-to-be-deleted-4dhdf" in namespace "gc-7180"
Jan 23 14:53:07.664: INFO: Deleting pod "simpletest-rc-to-be-deleted-4pvd4" in namespace "gc-7180"
Jan 23 14:53:07.671: INFO: Deleting pod "simpletest-rc-to-be-deleted-4sp52" in namespace "gc-7180"
Jan 23 14:53:07.676: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wjsb" in namespace "gc-7180"
Jan 23 14:53:07.684: INFO: Deleting pod "simpletest-rc-to-be-deleted-57m46" in namespace "gc-7180"
Jan 23 14:53:07.691: INFO: Deleting pod "simpletest-rc-to-be-deleted-5djmz" in namespace "gc-7180"
Jan 23 14:53:07.695: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hhn5" in namespace "gc-7180"
Jan 23 14:53:07.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-5k2d8" in namespace "gc-7180"
Jan 23 14:53:07.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qff5" in namespace "gc-7180"
Jan 23 14:53:07.710: INFO: Deleting pod "simpletest-rc-to-be-deleted-6b6kd" in namespace "gc-7180"
Jan 23 14:53:07.718: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fzbs" in namespace "gc-7180"
Jan 23 14:53:07.723: INFO: Deleting pod "simpletest-rc-to-be-deleted-6nvtc" in namespace "gc-7180"
Jan 23 14:53:07.728: INFO: Deleting pod "simpletest-rc-to-be-deleted-6w8jn" in namespace "gc-7180"
Jan 23 14:53:07.736: INFO: Deleting pod "simpletest-rc-to-be-deleted-72hcw" in namespace "gc-7180"
Jan 23 14:53:07.745: INFO: Deleting pod "simpletest-rc-to-be-deleted-77f75" in namespace "gc-7180"
Jan 23 14:53:07.758: INFO: Deleting pod "simpletest-rc-to-be-deleted-7b7zx" in namespace "gc-7180"
Jan 23 14:53:07.770: INFO: Deleting pod "simpletest-rc-to-be-deleted-7fvqn" in namespace "gc-7180"
Jan 23 14:53:07.776: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xtw8" in namespace "gc-7180"
Jan 23 14:53:07.786: INFO: Deleting pod "simpletest-rc-to-be-deleted-8894m" in namespace "gc-7180"
Jan 23 14:53:07.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bzhm" in namespace "gc-7180"
Jan 23 14:53:07.805: INFO: Deleting pod "simpletest-rc-to-be-deleted-8j45b" in namespace "gc-7180"
Jan 23 14:53:07.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-97vg2" in namespace "gc-7180"
Jan 23 14:53:07.870: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ctlc" in namespace "gc-7180"
Jan 23 14:53:07.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-9f2lb" in namespace "gc-7180"
Jan 23 14:53:07.887: INFO: Deleting pod "simpletest-rc-to-be-deleted-9g928" in namespace "gc-7180"
Jan 23 14:53:07.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rjcd" in namespace "gc-7180"
Jan 23 14:53:07.911: INFO: Deleting pod "simpletest-rc-to-be-deleted-9zgkx" in namespace "gc-7180"
Jan 23 14:53:07.918: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgh84" in namespace "gc-7180"
Jan 23 14:53:07.927: INFO: Deleting pod "simpletest-rc-to-be-deleted-blfl9" in namespace "gc-7180"
Jan 23 14:53:07.940: INFO: Deleting pod "simpletest-rc-to-be-deleted-c69f7" in namespace "gc-7180"
Jan 23 14:53:07.989: INFO: Deleting pod "simpletest-rc-to-be-deleted-cdn6k" in namespace "gc-7180"
Jan 23 14:53:07.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-cf2dn" in namespace "gc-7180"
Jan 23 14:53:08.013: INFO: Deleting pod "simpletest-rc-to-be-deleted-cf2mh" in namespace "gc-7180"
Jan 23 14:53:08.023: INFO: Deleting pod "simpletest-rc-to-be-deleted-ck2sz" in namespace "gc-7180"
Jan 23 14:53:08.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9pfj" in namespace "gc-7180"
Jan 23 14:53:08.049: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgjjr" in namespace "gc-7180"
Jan 23 14:53:08.055: INFO: Deleting pod "simpletest-rc-to-be-deleted-dm7qc" in namespace "gc-7180"
Jan 23 14:53:08.067: INFO: Deleting pod "simpletest-rc-to-be-deleted-dszpw" in namespace "gc-7180"
Jan 23 14:53:08.076: INFO: Deleting pod "simpletest-rc-to-be-deleted-fd76f" in namespace "gc-7180"
Jan 23 14:53:08.114: INFO: Deleting pod "simpletest-rc-to-be-deleted-fg5wt" in namespace "gc-7180"
Jan 23 14:53:08.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkhzd" in namespace "gc-7180"
Jan 23 14:53:08.133: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvr5p" in namespace "gc-7180"
Jan 23 14:53:08.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-fx2nc" in namespace "gc-7180"
Jan 23 14:53:08.151: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzxtr" in namespace "gc-7180"
Jan 23 14:53:08.165: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzzm9" in namespace "gc-7180"
Jan 23 14:53:08.177: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6m8c" in namespace "gc-7180"
Jan 23 14:53:08.186: INFO: Deleting pod "simpletest-rc-to-be-deleted-gl75p" in namespace "gc-7180"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 23 14:53:08.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7180" for this suite. 01/23/24 14:53:08.201
------------------------------
• [SLOW TEST] [16.020 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:52:52.183
    Jan 23 14:52:52.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename gc 01/23/24 14:52:52.184
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:52:52.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:52:52.193
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/23/24 14:52:52.196
    STEP: create the rc2 01/23/24 14:52:52.199
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/23/24 14:52:57.205
    STEP: delete the rc simpletest-rc-to-be-deleted 01/23/24 14:52:57.563
    STEP: wait for the rc to be deleted 01/23/24 14:52:57.571
    Jan 23 14:53:02.578: INFO: 62 pods remaining
    Jan 23 14:53:02.578: INFO: 62 pods has nil DeletionTimestamp
    Jan 23 14:53:02.578: INFO: 
    STEP: Gathering metrics 01/23/24 14:53:07.578
    Jan 23 14:53:07.591: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" in namespace "kube-system" to be "running and ready"
    Jan 23 14:53:07.593: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local": Phase="Running", Reason="", readiness=true. Elapsed: 1.591091ms
    Jan 23 14:53:07.593: INFO: The phase of Pod kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local is Running (Ready = true)
    Jan 23 14:53:07.593: INFO: Pod "kube-controller-manager-node-master-ob7j1nuv.nova-ht9xu6tk2ptb.local" satisfied condition "running and ready"
    Jan 23 14:53:07.635: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 23 14:53:07.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-29sz8" in namespace "gc-7180"
    Jan 23 14:53:07.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-2b4wk" in namespace "gc-7180"
    Jan 23 14:53:07.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lrqc" in namespace "gc-7180"
    Jan 23 14:53:07.659: INFO: Deleting pod "simpletest-rc-to-be-deleted-4dhdf" in namespace "gc-7180"
    Jan 23 14:53:07.664: INFO: Deleting pod "simpletest-rc-to-be-deleted-4pvd4" in namespace "gc-7180"
    Jan 23 14:53:07.671: INFO: Deleting pod "simpletest-rc-to-be-deleted-4sp52" in namespace "gc-7180"
    Jan 23 14:53:07.676: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wjsb" in namespace "gc-7180"
    Jan 23 14:53:07.684: INFO: Deleting pod "simpletest-rc-to-be-deleted-57m46" in namespace "gc-7180"
    Jan 23 14:53:07.691: INFO: Deleting pod "simpletest-rc-to-be-deleted-5djmz" in namespace "gc-7180"
    Jan 23 14:53:07.695: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hhn5" in namespace "gc-7180"
    Jan 23 14:53:07.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-5k2d8" in namespace "gc-7180"
    Jan 23 14:53:07.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qff5" in namespace "gc-7180"
    Jan 23 14:53:07.710: INFO: Deleting pod "simpletest-rc-to-be-deleted-6b6kd" in namespace "gc-7180"
    Jan 23 14:53:07.718: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fzbs" in namespace "gc-7180"
    Jan 23 14:53:07.723: INFO: Deleting pod "simpletest-rc-to-be-deleted-6nvtc" in namespace "gc-7180"
    Jan 23 14:53:07.728: INFO: Deleting pod "simpletest-rc-to-be-deleted-6w8jn" in namespace "gc-7180"
    Jan 23 14:53:07.736: INFO: Deleting pod "simpletest-rc-to-be-deleted-72hcw" in namespace "gc-7180"
    Jan 23 14:53:07.745: INFO: Deleting pod "simpletest-rc-to-be-deleted-77f75" in namespace "gc-7180"
    Jan 23 14:53:07.758: INFO: Deleting pod "simpletest-rc-to-be-deleted-7b7zx" in namespace "gc-7180"
    Jan 23 14:53:07.770: INFO: Deleting pod "simpletest-rc-to-be-deleted-7fvqn" in namespace "gc-7180"
    Jan 23 14:53:07.776: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xtw8" in namespace "gc-7180"
    Jan 23 14:53:07.786: INFO: Deleting pod "simpletest-rc-to-be-deleted-8894m" in namespace "gc-7180"
    Jan 23 14:53:07.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bzhm" in namespace "gc-7180"
    Jan 23 14:53:07.805: INFO: Deleting pod "simpletest-rc-to-be-deleted-8j45b" in namespace "gc-7180"
    Jan 23 14:53:07.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-97vg2" in namespace "gc-7180"
    Jan 23 14:53:07.870: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ctlc" in namespace "gc-7180"
    Jan 23 14:53:07.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-9f2lb" in namespace "gc-7180"
    Jan 23 14:53:07.887: INFO: Deleting pod "simpletest-rc-to-be-deleted-9g928" in namespace "gc-7180"
    Jan 23 14:53:07.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rjcd" in namespace "gc-7180"
    Jan 23 14:53:07.911: INFO: Deleting pod "simpletest-rc-to-be-deleted-9zgkx" in namespace "gc-7180"
    Jan 23 14:53:07.918: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgh84" in namespace "gc-7180"
    Jan 23 14:53:07.927: INFO: Deleting pod "simpletest-rc-to-be-deleted-blfl9" in namespace "gc-7180"
    Jan 23 14:53:07.940: INFO: Deleting pod "simpletest-rc-to-be-deleted-c69f7" in namespace "gc-7180"
    Jan 23 14:53:07.989: INFO: Deleting pod "simpletest-rc-to-be-deleted-cdn6k" in namespace "gc-7180"
    Jan 23 14:53:07.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-cf2dn" in namespace "gc-7180"
    Jan 23 14:53:08.013: INFO: Deleting pod "simpletest-rc-to-be-deleted-cf2mh" in namespace "gc-7180"
    Jan 23 14:53:08.023: INFO: Deleting pod "simpletest-rc-to-be-deleted-ck2sz" in namespace "gc-7180"
    Jan 23 14:53:08.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9pfj" in namespace "gc-7180"
    Jan 23 14:53:08.049: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgjjr" in namespace "gc-7180"
    Jan 23 14:53:08.055: INFO: Deleting pod "simpletest-rc-to-be-deleted-dm7qc" in namespace "gc-7180"
    Jan 23 14:53:08.067: INFO: Deleting pod "simpletest-rc-to-be-deleted-dszpw" in namespace "gc-7180"
    Jan 23 14:53:08.076: INFO: Deleting pod "simpletest-rc-to-be-deleted-fd76f" in namespace "gc-7180"
    Jan 23 14:53:08.114: INFO: Deleting pod "simpletest-rc-to-be-deleted-fg5wt" in namespace "gc-7180"
    Jan 23 14:53:08.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkhzd" in namespace "gc-7180"
    Jan 23 14:53:08.133: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvr5p" in namespace "gc-7180"
    Jan 23 14:53:08.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-fx2nc" in namespace "gc-7180"
    Jan 23 14:53:08.151: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzxtr" in namespace "gc-7180"
    Jan 23 14:53:08.165: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzzm9" in namespace "gc-7180"
    Jan 23 14:53:08.177: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6m8c" in namespace "gc-7180"
    Jan 23 14:53:08.186: INFO: Deleting pod "simpletest-rc-to-be-deleted-gl75p" in namespace "gc-7180"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:53:08.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7180" for this suite. 01/23/24 14:53:08.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:53:08.205
Jan 23 14:53:08.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename subpath 01/23/24 14:53:08.206
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:53:08.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:53:08.214
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/23/24 14:53:08.252
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-rlbt 01/23/24 14:53:08.27
STEP: Creating a pod to test atomic-volume-subpath 01/23/24 14:53:08.27
Jan 23 14:53:08.292: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rlbt" in namespace "subpath-8761" to be "Succeeded or Failed"
Jan 23 14:53:08.293: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Pending", Reason="", readiness=false. Elapsed: 1.753191ms
Jan 23 14:53:10.295: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003698972s
Jan 23 14:53:12.298: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006780603s
Jan 23 14:53:14.296: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003946998s
Jan 23 14:53:16.295: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Pending", Reason="", readiness=false. Elapsed: 8.003685596s
Jan 23 14:53:18.296: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00418509s
Jan 23 14:53:20.296: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004408972s
Jan 23 14:53:22.297: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005789791s
Jan 23 14:53:24.298: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=true. Elapsed: 16.00601475s
Jan 23 14:53:26.296: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=true. Elapsed: 18.004888445s
Jan 23 14:53:28.296: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=true. Elapsed: 20.004890678s
Jan 23 14:53:30.296: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=true. Elapsed: 22.004500536s
Jan 23 14:53:32.297: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=true. Elapsed: 24.005110819s
Jan 23 14:53:34.297: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=true. Elapsed: 26.005459202s
Jan 23 14:53:36.297: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=true. Elapsed: 28.005771154s
Jan 23 14:53:38.297: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=true. Elapsed: 30.005236995s
Jan 23 14:53:40.297: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=true. Elapsed: 32.005210122s
Jan 23 14:53:42.297: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=false. Elapsed: 34.00578842s
Jan 23 14:53:44.297: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 36.0051322s
STEP: Saw pod success 01/23/24 14:53:44.297
Jan 23 14:53:44.297: INFO: Pod "pod-subpath-test-downwardapi-rlbt" satisfied condition "Succeeded or Failed"
Jan 23 14:53:44.299: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-subpath-test-downwardapi-rlbt container test-container-subpath-downwardapi-rlbt: <nil>
STEP: delete the pod 01/23/24 14:53:44.305
Jan 23 14:53:44.313: INFO: Waiting for pod pod-subpath-test-downwardapi-rlbt to disappear
Jan 23 14:53:44.315: INFO: Pod pod-subpath-test-downwardapi-rlbt no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-rlbt 01/23/24 14:53:44.315
Jan 23 14:53:44.315: INFO: Deleting pod "pod-subpath-test-downwardapi-rlbt" in namespace "subpath-8761"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 23 14:53:44.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8761" for this suite. 01/23/24 14:53:44.318
------------------------------
• [SLOW TEST] [36.116 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:53:08.205
    Jan 23 14:53:08.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename subpath 01/23/24 14:53:08.206
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:53:08.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:53:08.214
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/23/24 14:53:08.252
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-rlbt 01/23/24 14:53:08.27
    STEP: Creating a pod to test atomic-volume-subpath 01/23/24 14:53:08.27
    Jan 23 14:53:08.292: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rlbt" in namespace "subpath-8761" to be "Succeeded or Failed"
    Jan 23 14:53:08.293: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Pending", Reason="", readiness=false. Elapsed: 1.753191ms
    Jan 23 14:53:10.295: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003698972s
    Jan 23 14:53:12.298: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006780603s
    Jan 23 14:53:14.296: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003946998s
    Jan 23 14:53:16.295: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Pending", Reason="", readiness=false. Elapsed: 8.003685596s
    Jan 23 14:53:18.296: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00418509s
    Jan 23 14:53:20.296: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004408972s
    Jan 23 14:53:22.297: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005789791s
    Jan 23 14:53:24.298: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=true. Elapsed: 16.00601475s
    Jan 23 14:53:26.296: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=true. Elapsed: 18.004888445s
    Jan 23 14:53:28.296: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=true. Elapsed: 20.004890678s
    Jan 23 14:53:30.296: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=true. Elapsed: 22.004500536s
    Jan 23 14:53:32.297: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=true. Elapsed: 24.005110819s
    Jan 23 14:53:34.297: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=true. Elapsed: 26.005459202s
    Jan 23 14:53:36.297: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=true. Elapsed: 28.005771154s
    Jan 23 14:53:38.297: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=true. Elapsed: 30.005236995s
    Jan 23 14:53:40.297: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=true. Elapsed: 32.005210122s
    Jan 23 14:53:42.297: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Running", Reason="", readiness=false. Elapsed: 34.00578842s
    Jan 23 14:53:44.297: INFO: Pod "pod-subpath-test-downwardapi-rlbt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 36.0051322s
    STEP: Saw pod success 01/23/24 14:53:44.297
    Jan 23 14:53:44.297: INFO: Pod "pod-subpath-test-downwardapi-rlbt" satisfied condition "Succeeded or Failed"
    Jan 23 14:53:44.299: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-subpath-test-downwardapi-rlbt container test-container-subpath-downwardapi-rlbt: <nil>
    STEP: delete the pod 01/23/24 14:53:44.305
    Jan 23 14:53:44.313: INFO: Waiting for pod pod-subpath-test-downwardapi-rlbt to disappear
    Jan 23 14:53:44.315: INFO: Pod pod-subpath-test-downwardapi-rlbt no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-rlbt 01/23/24 14:53:44.315
    Jan 23 14:53:44.315: INFO: Deleting pod "pod-subpath-test-downwardapi-rlbt" in namespace "subpath-8761"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:53:44.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8761" for this suite. 01/23/24 14:53:44.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:53:44.322
Jan 23 14:53:44.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename custom-resource-definition 01/23/24 14:53:44.322
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:53:44.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:53:44.334
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan 23 14:53:44.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:53:52.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5367" for this suite. 01/23/24 14:53:52.425
------------------------------
• [SLOW TEST] [8.107 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:53:44.322
    Jan 23 14:53:44.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename custom-resource-definition 01/23/24 14:53:44.322
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:53:44.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:53:44.334
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan 23 14:53:44.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:53:52.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5367" for this suite. 01/23/24 14:53:52.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:53:52.43
Jan 23 14:53:52.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename webhook 01/23/24 14:53:52.43
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:53:52.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:53:52.438
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/23/24 14:53:52.447
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:53:52.74
STEP: Deploying the webhook pod 01/23/24 14:53:52.753
STEP: Wait for the deployment to be ready 01/23/24 14:53:52.849
Jan 23 14:53:52.873: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/23/24 14:53:54.878
STEP: Verifying the service has paired with the endpoint 01/23/24 14:53:54.883
Jan 23 14:53:55.884: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/23/24 14:53:55.887
STEP: create a pod that should be updated by the webhook 01/23/24 14:53:55.898
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:53:55.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7733" for this suite. 01/23/24 14:53:55.951
STEP: Destroying namespace "webhook-7733-markers" for this suite. 01/23/24 14:53:55.965
------------------------------
• [3.541 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:53:52.43
    Jan 23 14:53:52.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename webhook 01/23/24 14:53:52.43
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:53:52.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:53:52.438
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/23/24 14:53:52.447
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:53:52.74
    STEP: Deploying the webhook pod 01/23/24 14:53:52.753
    STEP: Wait for the deployment to be ready 01/23/24 14:53:52.849
    Jan 23 14:53:52.873: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/23/24 14:53:54.878
    STEP: Verifying the service has paired with the endpoint 01/23/24 14:53:54.883
    Jan 23 14:53:55.884: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/23/24 14:53:55.887
    STEP: create a pod that should be updated by the webhook 01/23/24 14:53:55.898
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:53:55.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7733" for this suite. 01/23/24 14:53:55.951
    STEP: Destroying namespace "webhook-7733-markers" for this suite. 01/23/24 14:53:55.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:53:55.971
Jan 23 14:53:55.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename statefulset 01/23/24 14:53:55.972
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:53:55.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:53:55.983
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9833 01/23/24 14:53:55.985
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 01/23/24 14:53:55.987
Jan 23 14:53:55.994: INFO: Found 0 stateful pods, waiting for 3
Jan 23 14:54:05.998: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 14:54:05.998: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 14:54:05.998: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 14:54:06.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-9833 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 14:54:06.202: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 14:54:06.202: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 14:54:06.202: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/23/24 14:54:16.213
Jan 23 14:54:16.229: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/23/24 14:54:16.229
STEP: Updating Pods in reverse ordinal order 01/23/24 14:54:26.238
Jan 23 14:54:26.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-9833 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 23 14:54:26.427: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 23 14:54:26.427: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 23 14:54:26.427: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 01/23/24 14:54:36.44
Jan 23 14:54:36.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-9833 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 14:54:36.624: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 14:54:36.624: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 14:54:36.624: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 23 14:54:46.649: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/23/24 14:54:56.659
Jan 23 14:54:56.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-9833 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 23 14:54:56.847: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 23 14:54:56.847: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 23 14:54:56.847: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 23 14:55:16.858: INFO: Deleting all statefulset in ns statefulset-9833
Jan 23 14:55:16.859: INFO: Scaling statefulset ss2 to 0
Jan 23 14:55:26.869: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 14:55:26.871: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 23 14:55:26.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9833" for this suite. 01/23/24 14:55:26.886
------------------------------
• [SLOW TEST] [90.918 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:53:55.971
    Jan 23 14:53:55.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename statefulset 01/23/24 14:53:55.972
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:53:55.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:53:55.983
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9833 01/23/24 14:53:55.985
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 01/23/24 14:53:55.987
    Jan 23 14:53:55.994: INFO: Found 0 stateful pods, waiting for 3
    Jan 23 14:54:05.998: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 23 14:54:05.998: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 23 14:54:05.998: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan 23 14:54:06.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-9833 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 23 14:54:06.202: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 23 14:54:06.202: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 23 14:54:06.202: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/23/24 14:54:16.213
    Jan 23 14:54:16.229: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/23/24 14:54:16.229
    STEP: Updating Pods in reverse ordinal order 01/23/24 14:54:26.238
    Jan 23 14:54:26.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-9833 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 23 14:54:26.427: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 23 14:54:26.427: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 23 14:54:26.427: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 01/23/24 14:54:36.44
    Jan 23 14:54:36.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-9833 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 23 14:54:36.624: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 23 14:54:36.624: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 23 14:54:36.624: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 23 14:54:46.649: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/23/24 14:54:56.659
    Jan 23 14:54:56.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-9833 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 23 14:54:56.847: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 23 14:54:56.847: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 23 14:54:56.847: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 23 14:55:16.858: INFO: Deleting all statefulset in ns statefulset-9833
    Jan 23 14:55:16.859: INFO: Scaling statefulset ss2 to 0
    Jan 23 14:55:26.869: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 23 14:55:26.871: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:55:26.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9833" for this suite. 01/23/24 14:55:26.886
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:55:26.889
Jan 23 14:55:26.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename namespaces 01/23/24 14:55:26.89
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:55:26.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:55:26.9
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 01/23/24 14:55:26.902
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:55:26.909
STEP: Creating a service in the namespace 01/23/24 14:55:26.911
STEP: Deleting the namespace 01/23/24 14:55:26.917
STEP: Waiting for the namespace to be removed. 01/23/24 14:55:26.924
STEP: Recreating the namespace 01/23/24 14:55:32.927
STEP: Verifying there is no service in the namespace 01/23/24 14:55:32.935
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:55:32.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9858" for this suite. 01/23/24 14:55:32.939
STEP: Destroying namespace "nsdeletetest-9241" for this suite. 01/23/24 14:55:32.942
Jan 23 14:55:32.944: INFO: Namespace nsdeletetest-9241 was already deleted
STEP: Destroying namespace "nsdeletetest-9513" for this suite. 01/23/24 14:55:32.944
------------------------------
• [SLOW TEST] [6.062 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:55:26.889
    Jan 23 14:55:26.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename namespaces 01/23/24 14:55:26.89
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:55:26.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:55:26.9
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 01/23/24 14:55:26.902
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:55:26.909
    STEP: Creating a service in the namespace 01/23/24 14:55:26.911
    STEP: Deleting the namespace 01/23/24 14:55:26.917
    STEP: Waiting for the namespace to be removed. 01/23/24 14:55:26.924
    STEP: Recreating the namespace 01/23/24 14:55:32.927
    STEP: Verifying there is no service in the namespace 01/23/24 14:55:32.935
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:55:32.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9858" for this suite. 01/23/24 14:55:32.939
    STEP: Destroying namespace "nsdeletetest-9241" for this suite. 01/23/24 14:55:32.942
    Jan 23 14:55:32.944: INFO: Namespace nsdeletetest-9241 was already deleted
    STEP: Destroying namespace "nsdeletetest-9513" for this suite. 01/23/24 14:55:32.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:55:32.951
Jan 23 14:55:32.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename crd-publish-openapi 01/23/24 14:55:32.952
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:55:32.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:55:32.962
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/23/24 14:55:32.964
Jan 23 14:55:32.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:55:41.482: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:55:55.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7059" for this suite. 01/23/24 14:55:55.23
------------------------------
• [SLOW TEST] [22.282 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:55:32.951
    Jan 23 14:55:32.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename crd-publish-openapi 01/23/24 14:55:32.952
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:55:32.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:55:32.962
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/23/24 14:55:32.964
    Jan 23 14:55:32.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:55:41.482: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:55:55.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7059" for this suite. 01/23/24 14:55:55.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:55:55.234
Jan 23 14:55:55.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename proxy 01/23/24 14:55:55.235
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:55:55.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:55:55.245
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/23/24 14:55:55.252
STEP: creating replication controller proxy-service-899l4 in namespace proxy-5094 01/23/24 14:55:55.252
I0123 14:55:55.258281      22 runners.go:193] Created replication controller with name: proxy-service-899l4, namespace: proxy-5094, replica count: 1
I0123 14:55:56.309035      22 runners.go:193] proxy-service-899l4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0123 14:55:57.309385      22 runners.go:193] proxy-service-899l4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0123 14:55:58.310508      22 runners.go:193] proxy-service-899l4 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 23 14:55:58.313: INFO: setup took 3.066178453s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/23/24 14:55:58.313
Jan 23 14:55:58.317: INFO: (0) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 4.043505ms)
Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 4.044336ms)
Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.068267ms)
Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 4.222374ms)
Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 4.261099ms)
Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 4.140394ms)
Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.298059ms)
Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 4.370788ms)
Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 4.571814ms)
Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 4.803648ms)
Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.754966ms)
Jan 23 14:55:58.322: INFO: (0) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 7.995134ms)
Jan 23 14:55:58.322: INFO: (0) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 8.141353ms)
Jan 23 14:55:58.322: INFO: (0) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 8.283728ms)
Jan 23 14:55:58.322: INFO: (0) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 8.138699ms)
Jan 23 14:55:58.322: INFO: (0) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 8.178192ms)
Jan 23 14:55:58.324: INFO: (1) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 1.888922ms)
Jan 23 14:55:58.325: INFO: (1) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.54918ms)
Jan 23 14:55:58.325: INFO: (1) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.621638ms)
Jan 23 14:55:58.325: INFO: (1) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.70135ms)
Jan 23 14:55:58.328: INFO: (1) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 6.557348ms)
Jan 23 14:55:58.328: INFO: (1) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 6.618785ms)
Jan 23 14:55:58.328: INFO: (1) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 6.58697ms)
Jan 23 14:55:58.328: INFO: (1) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 6.577301ms)
Jan 23 14:55:58.328: INFO: (1) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 6.718502ms)
Jan 23 14:55:58.329: INFO: (1) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 6.577036ms)
Jan 23 14:55:58.329: INFO: (1) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 6.962623ms)
Jan 23 14:55:58.331: INFO: (1) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 9.032682ms)
Jan 23 14:55:58.331: INFO: (1) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 9.023508ms)
Jan 23 14:55:58.331: INFO: (1) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 9.029327ms)
Jan 23 14:55:58.331: INFO: (1) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 9.025417ms)
Jan 23 14:55:58.331: INFO: (1) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 8.990489ms)
Jan 23 14:55:58.335: INFO: (2) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.18926ms)
Jan 23 14:55:58.335: INFO: (2) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 4.224625ms)
Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 4.731029ms)
Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 4.851523ms)
Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.789368ms)
Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 5.042242ms)
Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 5.875769ms)
Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 4.991701ms)
Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 5.314294ms)
Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 5.180819ms)
Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 5.439674ms)
Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 5.531977ms)
Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 6.402121ms)
Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 5.445548ms)
Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 5.430078ms)
Jan 23 14:55:58.338: INFO: (2) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 5.736765ms)
Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.453425ms)
Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.345293ms)
Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.495044ms)
Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 3.551588ms)
Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.344938ms)
Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.507667ms)
Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 3.697191ms)
Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.431094ms)
Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.514388ms)
Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.559758ms)
Jan 23 14:55:58.342: INFO: (3) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 4.626589ms)
Jan 23 14:55:58.342: INFO: (3) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.506549ms)
Jan 23 14:55:58.343: INFO: (3) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 4.665375ms)
Jan 23 14:55:58.343: INFO: (3) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 4.742787ms)
Jan 23 14:55:58.343: INFO: (3) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 4.780944ms)
Jan 23 14:55:58.343: INFO: (3) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 5.087755ms)
Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.288005ms)
Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.274775ms)
Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 3.310706ms)
Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 3.285832ms)
Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 3.424246ms)
Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.276593ms)
Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.325318ms)
Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.310002ms)
Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.29129ms)
Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.317192ms)
Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.396897ms)
Jan 23 14:55:58.347: INFO: (4) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 3.759333ms)
Jan 23 14:55:58.347: INFO: (4) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 3.848504ms)
Jan 23 14:55:58.347: INFO: (4) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 3.806437ms)
Jan 23 14:55:58.347: INFO: (4) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 3.816465ms)
Jan 23 14:55:58.347: INFO: (4) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 3.866408ms)
Jan 23 14:55:58.349: INFO: (5) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 2.661931ms)
Jan 23 14:55:58.350: INFO: (5) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 2.753457ms)
Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.266384ms)
Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.318241ms)
Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.554756ms)
Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.536353ms)
Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.353603ms)
Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.795669ms)
Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 3.910195ms)
Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 3.812641ms)
Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 3.894695ms)
Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.943558ms)
Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 4.081651ms)
Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 3.715259ms)
Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.133046ms)
Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 3.693181ms)
Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 3.864117ms)
Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.583067ms)
Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.828926ms)
Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 3.796589ms)
Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.906678ms)
Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.604093ms)
Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.579214ms)
Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.784825ms)
Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.762969ms)
Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 4.089947ms)
Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 3.824071ms)
Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.860525ms)
Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 3.822172ms)
Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.254179ms)
Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 3.996279ms)
Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 3.846079ms)
Jan 23 14:55:58.358: INFO: (7) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 2.068072ms)
Jan 23 14:55:58.358: INFO: (7) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 2.141874ms)
Jan 23 14:55:58.358: INFO: (7) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 2.447739ms)
Jan 23 14:55:58.358: INFO: (7) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 2.443677ms)
Jan 23 14:55:58.358: INFO: (7) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 2.50338ms)
Jan 23 14:55:58.359: INFO: (7) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.555517ms)
Jan 23 14:55:58.359: INFO: (7) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.751094ms)
Jan 23 14:55:58.359: INFO: (7) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.593467ms)
Jan 23 14:55:58.359: INFO: (7) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.586788ms)
Jan 23 14:55:58.359: INFO: (7) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.620117ms)
Jan 23 14:55:58.360: INFO: (7) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 4.119551ms)
Jan 23 14:55:58.360: INFO: (7) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 4.012403ms)
Jan 23 14:55:58.360: INFO: (7) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.065762ms)
Jan 23 14:55:58.360: INFO: (7) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.067738ms)
Jan 23 14:55:58.360: INFO: (7) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 4.161293ms)
Jan 23 14:55:58.360: INFO: (7) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 4.149655ms)
Jan 23 14:55:58.361: INFO: (8) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 1.770083ms)
Jan 23 14:55:58.363: INFO: (8) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.289292ms)
Jan 23 14:55:58.363: INFO: (8) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.272981ms)
Jan 23 14:55:58.363: INFO: (8) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 3.51583ms)
Jan 23 14:55:58.363: INFO: (8) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.392077ms)
Jan 23 14:55:58.363: INFO: (8) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 3.279694ms)
Jan 23 14:55:58.363: INFO: (8) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.383475ms)
Jan 23 14:55:58.363: INFO: (8) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 3.485081ms)
Jan 23 14:55:58.363: INFO: (8) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.524246ms)
Jan 23 14:55:58.363: INFO: (8) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 3.618547ms)
Jan 23 14:55:58.364: INFO: (8) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.48627ms)
Jan 23 14:55:58.364: INFO: (8) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 3.570382ms)
Jan 23 14:55:58.364: INFO: (8) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 3.83717ms)
Jan 23 14:55:58.364: INFO: (8) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.830418ms)
Jan 23 14:55:58.364: INFO: (8) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 3.817602ms)
Jan 23 14:55:58.364: INFO: (8) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.182ms)
Jan 23 14:55:58.367: INFO: (9) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 3.076474ms)
Jan 23 14:55:58.367: INFO: (9) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.085951ms)
Jan 23 14:55:58.367: INFO: (9) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 3.100174ms)
Jan 23 14:55:58.368: INFO: (9) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.347721ms)
Jan 23 14:55:58.368: INFO: (9) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.667697ms)
Jan 23 14:55:58.368: INFO: (9) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.715018ms)
Jan 23 14:55:58.368: INFO: (9) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.781942ms)
Jan 23 14:55:58.368: INFO: (9) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.693345ms)
Jan 23 14:55:58.368: INFO: (9) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.862958ms)
Jan 23 14:55:58.368: INFO: (9) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.845235ms)
Jan 23 14:55:58.369: INFO: (9) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.783624ms)
Jan 23 14:55:58.369: INFO: (9) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 4.926077ms)
Jan 23 14:55:58.369: INFO: (9) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 4.906588ms)
Jan 23 14:55:58.369: INFO: (9) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 4.929441ms)
Jan 23 14:55:58.369: INFO: (9) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.988984ms)
Jan 23 14:55:58.369: INFO: (9) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 4.927369ms)
Jan 23 14:55:58.373: INFO: (10) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.092176ms)
Jan 23 14:55:58.373: INFO: (10) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 2.968774ms)
Jan 23 14:55:58.373: INFO: (10) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 2.966873ms)
Jan 23 14:55:58.373: INFO: (10) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 2.808381ms)
Jan 23 14:55:58.373: INFO: (10) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 2.835069ms)
Jan 23 14:55:58.373: INFO: (10) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 2.860894ms)
Jan 23 14:55:58.373: INFO: (10) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 2.930346ms)
Jan 23 14:55:58.373: INFO: (10) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.145541ms)
Jan 23 14:55:58.373: INFO: (10) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 2.966348ms)
Jan 23 14:55:58.374: INFO: (10) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 3.750004ms)
Jan 23 14:55:58.374: INFO: (10) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 4.035137ms)
Jan 23 14:55:58.374: INFO: (10) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 3.88489ms)
Jan 23 14:55:58.374: INFO: (10) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.218681ms)
Jan 23 14:55:58.374: INFO: (10) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 4.265088ms)
Jan 23 14:55:58.374: INFO: (10) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.172749ms)
Jan 23 14:55:58.374: INFO: (10) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 4.379497ms)
Jan 23 14:55:58.376: INFO: (11) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 2.136033ms)
Jan 23 14:55:58.376: INFO: (11) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 2.070651ms)
Jan 23 14:55:58.376: INFO: (11) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 2.313272ms)
Jan 23 14:55:58.376: INFO: (11) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 2.343524ms)
Jan 23 14:55:58.377: INFO: (11) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.328626ms)
Jan 23 14:55:58.377: INFO: (11) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 3.440177ms)
Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.499941ms)
Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 3.551008ms)
Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.638745ms)
Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 3.358424ms)
Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.420931ms)
Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 3.561379ms)
Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.546869ms)
Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 3.633083ms)
Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.435268ms)
Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 3.573605ms)
Jan 23 14:55:58.381: INFO: (12) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.070241ms)
Jan 23 14:55:58.381: INFO: (12) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.107803ms)
Jan 23 14:55:58.381: INFO: (12) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.035035ms)
Jan 23 14:55:58.381: INFO: (12) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 3.12093ms)
Jan 23 14:55:58.381: INFO: (12) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 2.952352ms)
Jan 23 14:55:58.381: INFO: (12) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 3.110729ms)
Jan 23 14:55:58.381: INFO: (12) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.422701ms)
Jan 23 14:55:58.382: INFO: (12) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.770819ms)
Jan 23 14:55:58.382: INFO: (12) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.746184ms)
Jan 23 14:55:58.382: INFO: (12) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.813176ms)
Jan 23 14:55:58.382: INFO: (12) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.225166ms)
Jan 23 14:55:58.382: INFO: (12) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 4.137053ms)
Jan 23 14:55:58.382: INFO: (12) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 4.190065ms)
Jan 23 14:55:58.382: INFO: (12) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 4.25896ms)
Jan 23 14:55:58.382: INFO: (12) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.396903ms)
Jan 23 14:55:58.382: INFO: (12) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 4.339798ms)
Jan 23 14:55:58.385: INFO: (13) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.023837ms)
Jan 23 14:55:58.385: INFO: (13) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.005335ms)
Jan 23 14:55:58.385: INFO: (13) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.061688ms)
Jan 23 14:55:58.385: INFO: (13) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.013515ms)
Jan 23 14:55:58.386: INFO: (13) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.081606ms)
Jan 23 14:55:58.386: INFO: (13) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 4.160785ms)
Jan 23 14:55:58.386: INFO: (13) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 4.201809ms)
Jan 23 14:55:58.387: INFO: (13) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 4.139961ms)
Jan 23 14:55:58.387: INFO: (13) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 4.157171ms)
Jan 23 14:55:58.387: INFO: (13) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 4.116848ms)
Jan 23 14:55:58.387: INFO: (13) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 4.135936ms)
Jan 23 14:55:58.387: INFO: (13) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.227591ms)
Jan 23 14:55:58.387: INFO: (13) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 4.292229ms)
Jan 23 14:55:58.387: INFO: (13) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 4.262527ms)
Jan 23 14:55:58.387: INFO: (13) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.319497ms)
Jan 23 14:55:58.387: INFO: (13) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 4.394823ms)
Jan 23 14:55:58.389: INFO: (14) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 1.994135ms)
Jan 23 14:55:58.390: INFO: (14) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.498688ms)
Jan 23 14:55:58.390: INFO: (14) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 3.210851ms)
Jan 23 14:55:58.391: INFO: (14) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 3.747799ms)
Jan 23 14:55:58.391: INFO: (14) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.643635ms)
Jan 23 14:55:58.391: INFO: (14) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.48114ms)
Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 4.88667ms)
Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 5.001925ms)
Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.832738ms)
Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 5.043868ms)
Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 5.221393ms)
Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 5.108975ms)
Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.928405ms)
Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 5.081626ms)
Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 5.391885ms)
Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 5.288636ms)
Jan 23 14:55:58.395: INFO: (15) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 2.361322ms)
Jan 23 14:55:58.395: INFO: (15) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 2.657712ms)
Jan 23 14:55:58.395: INFO: (15) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 2.717841ms)
Jan 23 14:55:58.395: INFO: (15) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 2.756842ms)
Jan 23 14:55:58.395: INFO: (15) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 2.725865ms)
Jan 23 14:55:58.395: INFO: (15) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 2.740595ms)
Jan 23 14:55:58.395: INFO: (15) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 2.842928ms)
Jan 23 14:55:58.395: INFO: (15) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 2.966423ms)
Jan 23 14:55:58.396: INFO: (15) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 3.099669ms)
Jan 23 14:55:58.396: INFO: (15) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.338961ms)
Jan 23 14:55:58.396: INFO: (15) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.571189ms)
Jan 23 14:55:58.396: INFO: (15) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.680657ms)
Jan 23 14:55:58.396: INFO: (15) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 3.671458ms)
Jan 23 14:55:58.396: INFO: (15) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 3.942918ms)
Jan 23 14:55:58.396: INFO: (15) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.031009ms)
Jan 23 14:55:58.396: INFO: (15) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 4.012854ms)
Jan 23 14:55:58.399: INFO: (16) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 2.574165ms)
Jan 23 14:55:58.399: INFO: (16) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 2.62082ms)
Jan 23 14:55:58.399: INFO: (16) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 2.558943ms)
Jan 23 14:55:58.399: INFO: (16) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 2.583197ms)
Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.467853ms)
Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.544297ms)
Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.563709ms)
Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.56674ms)
Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 3.608915ms)
Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.649702ms)
Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.572586ms)
Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 3.863977ms)
Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 3.982878ms)
Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 3.886361ms)
Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 3.964845ms)
Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 3.955144ms)
Jan 23 14:55:58.402: INFO: (17) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 1.385882ms)
Jan 23 14:55:58.404: INFO: (17) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.911439ms)
Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 3.514525ms)
Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 3.814282ms)
Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.767045ms)
Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.825764ms)
Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.967172ms)
Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.82895ms)
Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 3.666543ms)
Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 3.655247ms)
Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 3.820929ms)
Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 3.985286ms)
Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.792049ms)
Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 4.034626ms)
Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 3.83689ms)
Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.069183ms)
Jan 23 14:55:58.409: INFO: (18) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 4.195634ms)
Jan 23 14:55:58.409: INFO: (18) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 4.016405ms)
Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 4.251934ms)
Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 4.360211ms)
Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 4.478279ms)
Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 4.425191ms)
Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 4.35805ms)
Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 4.615406ms)
Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.375872ms)
Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.558399ms)
Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 4.605081ms)
Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 4.544876ms)
Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 4.732745ms)
Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.604074ms)
Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 4.803592ms)
Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.693412ms)
Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 4.85484ms)
Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.906348ms)
Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 4.957775ms)
Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 4.650266ms)
Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 4.532571ms)
Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.712928ms)
Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 4.52922ms)
Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 4.598748ms)
Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.872056ms)
Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 4.848013ms)
Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 4.727136ms)
Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 4.975647ms)
Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 5.149923ms)
Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 4.707612ms)
Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 4.982075ms)
Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.87169ms)
STEP: deleting ReplicationController proxy-service-899l4 in namespace proxy-5094, will wait for the garbage collector to delete the pods 01/23/24 14:55:58.415
Jan 23 14:55:58.475: INFO: Deleting ReplicationController proxy-service-899l4 took: 7.759228ms
Jan 23 14:55:58.576: INFO: Terminating ReplicationController proxy-service-899l4 pods took: 100.203024ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 23 14:56:01.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-5094" for this suite. 01/23/24 14:56:01.78
------------------------------
• [SLOW TEST] [6.550 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:55:55.234
    Jan 23 14:55:55.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename proxy 01/23/24 14:55:55.235
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:55:55.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:55:55.245
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/23/24 14:55:55.252
    STEP: creating replication controller proxy-service-899l4 in namespace proxy-5094 01/23/24 14:55:55.252
    I0123 14:55:55.258281      22 runners.go:193] Created replication controller with name: proxy-service-899l4, namespace: proxy-5094, replica count: 1
    I0123 14:55:56.309035      22 runners.go:193] proxy-service-899l4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0123 14:55:57.309385      22 runners.go:193] proxy-service-899l4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0123 14:55:58.310508      22 runners.go:193] proxy-service-899l4 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 23 14:55:58.313: INFO: setup took 3.066178453s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/23/24 14:55:58.313
    Jan 23 14:55:58.317: INFO: (0) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 4.043505ms)
    Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 4.044336ms)
    Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.068267ms)
    Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 4.222374ms)
    Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 4.261099ms)
    Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 4.140394ms)
    Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.298059ms)
    Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 4.370788ms)
    Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 4.571814ms)
    Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 4.803648ms)
    Jan 23 14:55:58.318: INFO: (0) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.754966ms)
    Jan 23 14:55:58.322: INFO: (0) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 7.995134ms)
    Jan 23 14:55:58.322: INFO: (0) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 8.141353ms)
    Jan 23 14:55:58.322: INFO: (0) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 8.283728ms)
    Jan 23 14:55:58.322: INFO: (0) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 8.138699ms)
    Jan 23 14:55:58.322: INFO: (0) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 8.178192ms)
    Jan 23 14:55:58.324: INFO: (1) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 1.888922ms)
    Jan 23 14:55:58.325: INFO: (1) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.54918ms)
    Jan 23 14:55:58.325: INFO: (1) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.621638ms)
    Jan 23 14:55:58.325: INFO: (1) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.70135ms)
    Jan 23 14:55:58.328: INFO: (1) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 6.557348ms)
    Jan 23 14:55:58.328: INFO: (1) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 6.618785ms)
    Jan 23 14:55:58.328: INFO: (1) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 6.58697ms)
    Jan 23 14:55:58.328: INFO: (1) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 6.577301ms)
    Jan 23 14:55:58.328: INFO: (1) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 6.718502ms)
    Jan 23 14:55:58.329: INFO: (1) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 6.577036ms)
    Jan 23 14:55:58.329: INFO: (1) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 6.962623ms)
    Jan 23 14:55:58.331: INFO: (1) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 9.032682ms)
    Jan 23 14:55:58.331: INFO: (1) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 9.023508ms)
    Jan 23 14:55:58.331: INFO: (1) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 9.029327ms)
    Jan 23 14:55:58.331: INFO: (1) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 9.025417ms)
    Jan 23 14:55:58.331: INFO: (1) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 8.990489ms)
    Jan 23 14:55:58.335: INFO: (2) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.18926ms)
    Jan 23 14:55:58.335: INFO: (2) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 4.224625ms)
    Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 4.731029ms)
    Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 4.851523ms)
    Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.789368ms)
    Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 5.042242ms)
    Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 5.875769ms)
    Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 4.991701ms)
    Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 5.314294ms)
    Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 5.180819ms)
    Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 5.439674ms)
    Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 5.531977ms)
    Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 6.402121ms)
    Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 5.445548ms)
    Jan 23 14:55:58.337: INFO: (2) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 5.430078ms)
    Jan 23 14:55:58.338: INFO: (2) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 5.736765ms)
    Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.453425ms)
    Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.345293ms)
    Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.495044ms)
    Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 3.551588ms)
    Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.344938ms)
    Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.507667ms)
    Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 3.697191ms)
    Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.431094ms)
    Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.514388ms)
    Jan 23 14:55:58.341: INFO: (3) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.559758ms)
    Jan 23 14:55:58.342: INFO: (3) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 4.626589ms)
    Jan 23 14:55:58.342: INFO: (3) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.506549ms)
    Jan 23 14:55:58.343: INFO: (3) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 4.665375ms)
    Jan 23 14:55:58.343: INFO: (3) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 4.742787ms)
    Jan 23 14:55:58.343: INFO: (3) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 4.780944ms)
    Jan 23 14:55:58.343: INFO: (3) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 5.087755ms)
    Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.288005ms)
    Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.274775ms)
    Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 3.310706ms)
    Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 3.285832ms)
    Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 3.424246ms)
    Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.276593ms)
    Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.325318ms)
    Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.310002ms)
    Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.29129ms)
    Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.317192ms)
    Jan 23 14:55:58.346: INFO: (4) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.396897ms)
    Jan 23 14:55:58.347: INFO: (4) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 3.759333ms)
    Jan 23 14:55:58.347: INFO: (4) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 3.848504ms)
    Jan 23 14:55:58.347: INFO: (4) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 3.806437ms)
    Jan 23 14:55:58.347: INFO: (4) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 3.816465ms)
    Jan 23 14:55:58.347: INFO: (4) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 3.866408ms)
    Jan 23 14:55:58.349: INFO: (5) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 2.661931ms)
    Jan 23 14:55:58.350: INFO: (5) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 2.753457ms)
    Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.266384ms)
    Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.318241ms)
    Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.554756ms)
    Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.536353ms)
    Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.353603ms)
    Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.795669ms)
    Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 3.910195ms)
    Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 3.812641ms)
    Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 3.894695ms)
    Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.943558ms)
    Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 4.081651ms)
    Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 3.715259ms)
    Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.133046ms)
    Jan 23 14:55:58.351: INFO: (5) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 3.693181ms)
    Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 3.864117ms)
    Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.583067ms)
    Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.828926ms)
    Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 3.796589ms)
    Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.906678ms)
    Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.604093ms)
    Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.579214ms)
    Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.784825ms)
    Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.762969ms)
    Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 4.089947ms)
    Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 3.824071ms)
    Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.860525ms)
    Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 3.822172ms)
    Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.254179ms)
    Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 3.996279ms)
    Jan 23 14:55:58.355: INFO: (6) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 3.846079ms)
    Jan 23 14:55:58.358: INFO: (7) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 2.068072ms)
    Jan 23 14:55:58.358: INFO: (7) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 2.141874ms)
    Jan 23 14:55:58.358: INFO: (7) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 2.447739ms)
    Jan 23 14:55:58.358: INFO: (7) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 2.443677ms)
    Jan 23 14:55:58.358: INFO: (7) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 2.50338ms)
    Jan 23 14:55:58.359: INFO: (7) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.555517ms)
    Jan 23 14:55:58.359: INFO: (7) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.751094ms)
    Jan 23 14:55:58.359: INFO: (7) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.593467ms)
    Jan 23 14:55:58.359: INFO: (7) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.586788ms)
    Jan 23 14:55:58.359: INFO: (7) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.620117ms)
    Jan 23 14:55:58.360: INFO: (7) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 4.119551ms)
    Jan 23 14:55:58.360: INFO: (7) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 4.012403ms)
    Jan 23 14:55:58.360: INFO: (7) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.065762ms)
    Jan 23 14:55:58.360: INFO: (7) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.067738ms)
    Jan 23 14:55:58.360: INFO: (7) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 4.161293ms)
    Jan 23 14:55:58.360: INFO: (7) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 4.149655ms)
    Jan 23 14:55:58.361: INFO: (8) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 1.770083ms)
    Jan 23 14:55:58.363: INFO: (8) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.289292ms)
    Jan 23 14:55:58.363: INFO: (8) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.272981ms)
    Jan 23 14:55:58.363: INFO: (8) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 3.51583ms)
    Jan 23 14:55:58.363: INFO: (8) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.392077ms)
    Jan 23 14:55:58.363: INFO: (8) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 3.279694ms)
    Jan 23 14:55:58.363: INFO: (8) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.383475ms)
    Jan 23 14:55:58.363: INFO: (8) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 3.485081ms)
    Jan 23 14:55:58.363: INFO: (8) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.524246ms)
    Jan 23 14:55:58.363: INFO: (8) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 3.618547ms)
    Jan 23 14:55:58.364: INFO: (8) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.48627ms)
    Jan 23 14:55:58.364: INFO: (8) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 3.570382ms)
    Jan 23 14:55:58.364: INFO: (8) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 3.83717ms)
    Jan 23 14:55:58.364: INFO: (8) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.830418ms)
    Jan 23 14:55:58.364: INFO: (8) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 3.817602ms)
    Jan 23 14:55:58.364: INFO: (8) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.182ms)
    Jan 23 14:55:58.367: INFO: (9) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 3.076474ms)
    Jan 23 14:55:58.367: INFO: (9) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.085951ms)
    Jan 23 14:55:58.367: INFO: (9) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 3.100174ms)
    Jan 23 14:55:58.368: INFO: (9) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.347721ms)
    Jan 23 14:55:58.368: INFO: (9) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.667697ms)
    Jan 23 14:55:58.368: INFO: (9) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.715018ms)
    Jan 23 14:55:58.368: INFO: (9) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.781942ms)
    Jan 23 14:55:58.368: INFO: (9) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.693345ms)
    Jan 23 14:55:58.368: INFO: (9) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.862958ms)
    Jan 23 14:55:58.368: INFO: (9) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.845235ms)
    Jan 23 14:55:58.369: INFO: (9) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.783624ms)
    Jan 23 14:55:58.369: INFO: (9) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 4.926077ms)
    Jan 23 14:55:58.369: INFO: (9) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 4.906588ms)
    Jan 23 14:55:58.369: INFO: (9) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 4.929441ms)
    Jan 23 14:55:58.369: INFO: (9) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.988984ms)
    Jan 23 14:55:58.369: INFO: (9) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 4.927369ms)
    Jan 23 14:55:58.373: INFO: (10) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.092176ms)
    Jan 23 14:55:58.373: INFO: (10) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 2.968774ms)
    Jan 23 14:55:58.373: INFO: (10) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 2.966873ms)
    Jan 23 14:55:58.373: INFO: (10) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 2.808381ms)
    Jan 23 14:55:58.373: INFO: (10) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 2.835069ms)
    Jan 23 14:55:58.373: INFO: (10) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 2.860894ms)
    Jan 23 14:55:58.373: INFO: (10) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 2.930346ms)
    Jan 23 14:55:58.373: INFO: (10) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.145541ms)
    Jan 23 14:55:58.373: INFO: (10) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 2.966348ms)
    Jan 23 14:55:58.374: INFO: (10) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 3.750004ms)
    Jan 23 14:55:58.374: INFO: (10) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 4.035137ms)
    Jan 23 14:55:58.374: INFO: (10) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 3.88489ms)
    Jan 23 14:55:58.374: INFO: (10) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.218681ms)
    Jan 23 14:55:58.374: INFO: (10) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 4.265088ms)
    Jan 23 14:55:58.374: INFO: (10) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.172749ms)
    Jan 23 14:55:58.374: INFO: (10) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 4.379497ms)
    Jan 23 14:55:58.376: INFO: (11) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 2.136033ms)
    Jan 23 14:55:58.376: INFO: (11) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 2.070651ms)
    Jan 23 14:55:58.376: INFO: (11) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 2.313272ms)
    Jan 23 14:55:58.376: INFO: (11) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 2.343524ms)
    Jan 23 14:55:58.377: INFO: (11) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.328626ms)
    Jan 23 14:55:58.377: INFO: (11) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 3.440177ms)
    Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.499941ms)
    Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 3.551008ms)
    Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.638745ms)
    Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 3.358424ms)
    Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.420931ms)
    Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 3.561379ms)
    Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.546869ms)
    Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 3.633083ms)
    Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.435268ms)
    Jan 23 14:55:58.378: INFO: (11) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 3.573605ms)
    Jan 23 14:55:58.381: INFO: (12) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.070241ms)
    Jan 23 14:55:58.381: INFO: (12) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.107803ms)
    Jan 23 14:55:58.381: INFO: (12) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.035035ms)
    Jan 23 14:55:58.381: INFO: (12) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 3.12093ms)
    Jan 23 14:55:58.381: INFO: (12) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 2.952352ms)
    Jan 23 14:55:58.381: INFO: (12) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 3.110729ms)
    Jan 23 14:55:58.381: INFO: (12) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.422701ms)
    Jan 23 14:55:58.382: INFO: (12) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.770819ms)
    Jan 23 14:55:58.382: INFO: (12) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.746184ms)
    Jan 23 14:55:58.382: INFO: (12) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.813176ms)
    Jan 23 14:55:58.382: INFO: (12) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.225166ms)
    Jan 23 14:55:58.382: INFO: (12) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 4.137053ms)
    Jan 23 14:55:58.382: INFO: (12) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 4.190065ms)
    Jan 23 14:55:58.382: INFO: (12) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 4.25896ms)
    Jan 23 14:55:58.382: INFO: (12) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.396903ms)
    Jan 23 14:55:58.382: INFO: (12) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 4.339798ms)
    Jan 23 14:55:58.385: INFO: (13) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.023837ms)
    Jan 23 14:55:58.385: INFO: (13) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.005335ms)
    Jan 23 14:55:58.385: INFO: (13) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.061688ms)
    Jan 23 14:55:58.385: INFO: (13) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.013515ms)
    Jan 23 14:55:58.386: INFO: (13) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.081606ms)
    Jan 23 14:55:58.386: INFO: (13) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 4.160785ms)
    Jan 23 14:55:58.386: INFO: (13) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 4.201809ms)
    Jan 23 14:55:58.387: INFO: (13) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 4.139961ms)
    Jan 23 14:55:58.387: INFO: (13) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 4.157171ms)
    Jan 23 14:55:58.387: INFO: (13) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 4.116848ms)
    Jan 23 14:55:58.387: INFO: (13) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 4.135936ms)
    Jan 23 14:55:58.387: INFO: (13) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.227591ms)
    Jan 23 14:55:58.387: INFO: (13) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 4.292229ms)
    Jan 23 14:55:58.387: INFO: (13) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 4.262527ms)
    Jan 23 14:55:58.387: INFO: (13) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.319497ms)
    Jan 23 14:55:58.387: INFO: (13) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 4.394823ms)
    Jan 23 14:55:58.389: INFO: (14) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 1.994135ms)
    Jan 23 14:55:58.390: INFO: (14) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.498688ms)
    Jan 23 14:55:58.390: INFO: (14) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 3.210851ms)
    Jan 23 14:55:58.391: INFO: (14) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 3.747799ms)
    Jan 23 14:55:58.391: INFO: (14) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.643635ms)
    Jan 23 14:55:58.391: INFO: (14) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.48114ms)
    Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 4.88667ms)
    Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 5.001925ms)
    Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.832738ms)
    Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 5.043868ms)
    Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 5.221393ms)
    Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 5.108975ms)
    Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.928405ms)
    Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 5.081626ms)
    Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 5.391885ms)
    Jan 23 14:55:58.392: INFO: (14) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 5.288636ms)
    Jan 23 14:55:58.395: INFO: (15) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 2.361322ms)
    Jan 23 14:55:58.395: INFO: (15) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 2.657712ms)
    Jan 23 14:55:58.395: INFO: (15) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 2.717841ms)
    Jan 23 14:55:58.395: INFO: (15) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 2.756842ms)
    Jan 23 14:55:58.395: INFO: (15) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 2.725865ms)
    Jan 23 14:55:58.395: INFO: (15) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 2.740595ms)
    Jan 23 14:55:58.395: INFO: (15) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 2.842928ms)
    Jan 23 14:55:58.395: INFO: (15) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 2.966423ms)
    Jan 23 14:55:58.396: INFO: (15) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 3.099669ms)
    Jan 23 14:55:58.396: INFO: (15) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.338961ms)
    Jan 23 14:55:58.396: INFO: (15) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.571189ms)
    Jan 23 14:55:58.396: INFO: (15) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.680657ms)
    Jan 23 14:55:58.396: INFO: (15) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 3.671458ms)
    Jan 23 14:55:58.396: INFO: (15) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 3.942918ms)
    Jan 23 14:55:58.396: INFO: (15) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.031009ms)
    Jan 23 14:55:58.396: INFO: (15) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 4.012854ms)
    Jan 23 14:55:58.399: INFO: (16) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 2.574165ms)
    Jan 23 14:55:58.399: INFO: (16) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 2.62082ms)
    Jan 23 14:55:58.399: INFO: (16) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 2.558943ms)
    Jan 23 14:55:58.399: INFO: (16) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 2.583197ms)
    Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.467853ms)
    Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.544297ms)
    Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 3.563709ms)
    Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.56674ms)
    Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 3.608915ms)
    Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.649702ms)
    Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.572586ms)
    Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 3.863977ms)
    Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 3.982878ms)
    Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 3.886361ms)
    Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 3.964845ms)
    Jan 23 14:55:58.400: INFO: (16) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 3.955144ms)
    Jan 23 14:55:58.402: INFO: (17) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 1.385882ms)
    Jan 23 14:55:58.404: INFO: (17) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.911439ms)
    Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 3.514525ms)
    Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 3.814282ms)
    Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 3.767045ms)
    Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 3.825764ms)
    Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 3.967172ms)
    Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 3.82895ms)
    Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 3.666543ms)
    Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 3.655247ms)
    Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 3.820929ms)
    Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 3.985286ms)
    Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 3.792049ms)
    Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 4.034626ms)
    Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 3.83689ms)
    Jan 23 14:55:58.405: INFO: (17) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.069183ms)
    Jan 23 14:55:58.409: INFO: (18) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 4.195634ms)
    Jan 23 14:55:58.409: INFO: (18) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 4.016405ms)
    Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 4.251934ms)
    Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 4.360211ms)
    Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 4.478279ms)
    Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 4.425191ms)
    Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 4.35805ms)
    Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 4.615406ms)
    Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.375872ms)
    Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.558399ms)
    Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 4.605081ms)
    Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 4.544876ms)
    Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 4.732745ms)
    Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.604074ms)
    Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 4.803592ms)
    Jan 23 14:55:58.410: INFO: (18) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.693412ms)
    Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:162/proxy/: bar (200; 4.85484ms)
    Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.906348ms)
    Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc/proxy/rewriteme">test</a> (200; 4.957775ms)
    Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:1080/proxy/rewriteme">test<... (200; 4.650266ms)
    Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:462/proxy/: tls qux (200; 4.532571ms)
    Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname1/proxy/: tls baz (200; 4.712928ms)
    Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname2/proxy/: bar (200; 4.52922ms)
    Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:443/proxy/tlsrewritem... (200; 4.598748ms)
    Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/services/proxy-service-899l4:portname1/proxy/: foo (200; 4.872056ms)
    Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/https:proxy-service-899l4-kqhxc:460/proxy/: tls baz (200; 4.848013ms)
    Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:1080/proxy/rewriteme">... (200; 4.727136ms)
    Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname1/proxy/: foo (200; 4.975647ms)
    Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/services/https:proxy-service-899l4:tlsportname2/proxy/: tls qux (200; 5.149923ms)
    Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/proxy-service-899l4-kqhxc:162/proxy/: bar (200; 4.707612ms)
    Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/services/http:proxy-service-899l4:portname2/proxy/: bar (200; 4.982075ms)
    Jan 23 14:55:58.415: INFO: (19) /api/v1/namespaces/proxy-5094/pods/http:proxy-service-899l4-kqhxc:160/proxy/: foo (200; 4.87169ms)
    STEP: deleting ReplicationController proxy-service-899l4 in namespace proxy-5094, will wait for the garbage collector to delete the pods 01/23/24 14:55:58.415
    Jan 23 14:55:58.475: INFO: Deleting ReplicationController proxy-service-899l4 took: 7.759228ms
    Jan 23 14:55:58.576: INFO: Terminating ReplicationController proxy-service-899l4 pods took: 100.203024ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:56:01.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-5094" for this suite. 01/23/24 14:56:01.78
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:56:01.785
Jan 23 14:56:01.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename events 01/23/24 14:56:01.785
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:56:01.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:56:01.795
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/23/24 14:56:01.796
STEP: get a list of Events with a label in the current namespace 01/23/24 14:56:01.806
STEP: delete a list of events 01/23/24 14:56:01.808
Jan 23 14:56:01.808: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/23/24 14:56:01.818
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan 23 14:56:01.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6945" for this suite. 01/23/24 14:56:01.829
------------------------------
• [0.048 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:56:01.785
    Jan 23 14:56:01.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename events 01/23/24 14:56:01.785
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:56:01.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:56:01.795
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/23/24 14:56:01.796
    STEP: get a list of Events with a label in the current namespace 01/23/24 14:56:01.806
    STEP: delete a list of events 01/23/24 14:56:01.808
    Jan 23 14:56:01.808: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/23/24 14:56:01.818
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:56:01.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6945" for this suite. 01/23/24 14:56:01.829
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:56:01.833
Jan 23 14:56:01.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 14:56:01.834
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:56:01.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:56:01.843
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 01/23/24 14:56:01.845
Jan 23 14:56:01.859: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6373afbf-d622-4cf8-9bb8-23ed1e573719" in namespace "projected-4538" to be "Succeeded or Failed"
Jan 23 14:56:01.862: INFO: Pod "downwardapi-volume-6373afbf-d622-4cf8-9bb8-23ed1e573719": Phase="Pending", Reason="", readiness=false. Elapsed: 2.468971ms
Jan 23 14:56:03.865: INFO: Pod "downwardapi-volume-6373afbf-d622-4cf8-9bb8-23ed1e573719": Phase="Running", Reason="", readiness=false. Elapsed: 2.00578568s
Jan 23 14:56:05.865: INFO: Pod "downwardapi-volume-6373afbf-d622-4cf8-9bb8-23ed1e573719": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005657216s
STEP: Saw pod success 01/23/24 14:56:05.865
Jan 23 14:56:05.865: INFO: Pod "downwardapi-volume-6373afbf-d622-4cf8-9bb8-23ed1e573719" satisfied condition "Succeeded or Failed"
Jan 23 14:56:05.867: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-6373afbf-d622-4cf8-9bb8-23ed1e573719 container client-container: <nil>
STEP: delete the pod 01/23/24 14:56:05.878
Jan 23 14:56:05.885: INFO: Waiting for pod downwardapi-volume-6373afbf-d622-4cf8-9bb8-23ed1e573719 to disappear
Jan 23 14:56:05.887: INFO: Pod downwardapi-volume-6373afbf-d622-4cf8-9bb8-23ed1e573719 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 23 14:56:05.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4538" for this suite. 01/23/24 14:56:05.889
------------------------------
• [4.060 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:56:01.833
    Jan 23 14:56:01.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 14:56:01.834
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:56:01.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:56:01.843
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 01/23/24 14:56:01.845
    Jan 23 14:56:01.859: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6373afbf-d622-4cf8-9bb8-23ed1e573719" in namespace "projected-4538" to be "Succeeded or Failed"
    Jan 23 14:56:01.862: INFO: Pod "downwardapi-volume-6373afbf-d622-4cf8-9bb8-23ed1e573719": Phase="Pending", Reason="", readiness=false. Elapsed: 2.468971ms
    Jan 23 14:56:03.865: INFO: Pod "downwardapi-volume-6373afbf-d622-4cf8-9bb8-23ed1e573719": Phase="Running", Reason="", readiness=false. Elapsed: 2.00578568s
    Jan 23 14:56:05.865: INFO: Pod "downwardapi-volume-6373afbf-d622-4cf8-9bb8-23ed1e573719": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005657216s
    STEP: Saw pod success 01/23/24 14:56:05.865
    Jan 23 14:56:05.865: INFO: Pod "downwardapi-volume-6373afbf-d622-4cf8-9bb8-23ed1e573719" satisfied condition "Succeeded or Failed"
    Jan 23 14:56:05.867: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-6373afbf-d622-4cf8-9bb8-23ed1e573719 container client-container: <nil>
    STEP: delete the pod 01/23/24 14:56:05.878
    Jan 23 14:56:05.885: INFO: Waiting for pod downwardapi-volume-6373afbf-d622-4cf8-9bb8-23ed1e573719 to disappear
    Jan 23 14:56:05.887: INFO: Pod downwardapi-volume-6373afbf-d622-4cf8-9bb8-23ed1e573719 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:56:05.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4538" for this suite. 01/23/24 14:56:05.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:56:05.893
Jan 23 14:56:05.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename webhook 01/23/24 14:56:05.894
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:56:05.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:56:05.902
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/23/24 14:56:05.912
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:56:06.388
STEP: Deploying the webhook pod 01/23/24 14:56:06.393
STEP: Wait for the deployment to be ready 01/23/24 14:56:06.403
Jan 23 14:56:06.406: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/23/24 14:56:08.413
STEP: Verifying the service has paired with the endpoint 01/23/24 14:56:08.42
Jan 23 14:56:09.420: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Jan 23 14:56:09.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2282-crds.webhook.example.com via the AdmissionRegistration API 01/23/24 14:56:14.929
STEP: Creating a custom resource that should be mutated by the webhook 01/23/24 14:56:14.94
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:56:17.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4317" for this suite. 01/23/24 14:56:17.502
STEP: Destroying namespace "webhook-4317-markers" for this suite. 01/23/24 14:56:17.514
------------------------------
• [SLOW TEST] [11.630 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:56:05.893
    Jan 23 14:56:05.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename webhook 01/23/24 14:56:05.894
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:56:05.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:56:05.902
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/23/24 14:56:05.912
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:56:06.388
    STEP: Deploying the webhook pod 01/23/24 14:56:06.393
    STEP: Wait for the deployment to be ready 01/23/24 14:56:06.403
    Jan 23 14:56:06.406: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/23/24 14:56:08.413
    STEP: Verifying the service has paired with the endpoint 01/23/24 14:56:08.42
    Jan 23 14:56:09.420: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Jan 23 14:56:09.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2282-crds.webhook.example.com via the AdmissionRegistration API 01/23/24 14:56:14.929
    STEP: Creating a custom resource that should be mutated by the webhook 01/23/24 14:56:14.94
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:56:17.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4317" for this suite. 01/23/24 14:56:17.502
    STEP: Destroying namespace "webhook-4317-markers" for this suite. 01/23/24 14:56:17.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:56:17.523
Jan 23 14:56:17.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename kubelet-test 01/23/24 14:56:17.524
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:56:17.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:56:17.533
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan 23 14:56:17.550: INFO: Waiting up to 5m0s for pod "busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614" in namespace "kubelet-test-8391" to be "running and ready"
Jan 23 14:56:17.553: INFO: Pod "busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614": Phase="Pending", Reason="", readiness=false. Elapsed: 3.533407ms
Jan 23 14:56:17.553: INFO: The phase of Pod busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:56:19.557: INFO: Pod "busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007712076s
Jan 23 14:56:19.557: INFO: The phase of Pod busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:56:21.557: INFO: Pod "busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614": Phase="Running", Reason="", readiness=true. Elapsed: 4.007003027s
Jan 23 14:56:21.557: INFO: The phase of Pod busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614 is Running (Ready = true)
Jan 23 14:56:21.557: INFO: Pod "busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 23 14:56:21.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8391" for this suite. 01/23/24 14:56:21.564
------------------------------
• [4.045 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:56:17.523
    Jan 23 14:56:17.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename kubelet-test 01/23/24 14:56:17.524
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:56:17.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:56:17.533
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan 23 14:56:17.550: INFO: Waiting up to 5m0s for pod "busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614" in namespace "kubelet-test-8391" to be "running and ready"
    Jan 23 14:56:17.553: INFO: Pod "busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614": Phase="Pending", Reason="", readiness=false. Elapsed: 3.533407ms
    Jan 23 14:56:17.553: INFO: The phase of Pod busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:56:19.557: INFO: Pod "busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007712076s
    Jan 23 14:56:19.557: INFO: The phase of Pod busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:56:21.557: INFO: Pod "busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614": Phase="Running", Reason="", readiness=true. Elapsed: 4.007003027s
    Jan 23 14:56:21.557: INFO: The phase of Pod busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614 is Running (Ready = true)
    Jan 23 14:56:21.557: INFO: Pod "busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:56:21.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8391" for this suite. 01/23/24 14:56:21.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:56:21.568
Jan 23 14:56:21.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename projected 01/23/24 14:56:21.569
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:56:21.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:56:21.577
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 01/23/24 14:56:21.579
Jan 23 14:56:21.601: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e2b5f45a-b7f2-4a07-b7bd-3a709ddfd0ab" in namespace "projected-939" to be "Succeeded or Failed"
Jan 23 14:56:21.603: INFO: Pod "downwardapi-volume-e2b5f45a-b7f2-4a07-b7bd-3a709ddfd0ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1.317827ms
Jan 23 14:56:23.606: INFO: Pod "downwardapi-volume-e2b5f45a-b7f2-4a07-b7bd-3a709ddfd0ab": Phase="Running", Reason="", readiness=true. Elapsed: 2.005000283s
Jan 23 14:56:25.606: INFO: Pod "downwardapi-volume-e2b5f45a-b7f2-4a07-b7bd-3a709ddfd0ab": Phase="Running", Reason="", readiness=false. Elapsed: 4.004803326s
Jan 23 14:56:27.606: INFO: Pod "downwardapi-volume-e2b5f45a-b7f2-4a07-b7bd-3a709ddfd0ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004403672s
STEP: Saw pod success 01/23/24 14:56:27.606
Jan 23 14:56:27.606: INFO: Pod "downwardapi-volume-e2b5f45a-b7f2-4a07-b7bd-3a709ddfd0ab" satisfied condition "Succeeded or Failed"
Jan 23 14:56:27.608: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-e2b5f45a-b7f2-4a07-b7bd-3a709ddfd0ab container client-container: <nil>
STEP: delete the pod 01/23/24 14:56:27.611
Jan 23 14:56:27.620: INFO: Waiting for pod downwardapi-volume-e2b5f45a-b7f2-4a07-b7bd-3a709ddfd0ab to disappear
Jan 23 14:56:27.621: INFO: Pod downwardapi-volume-e2b5f45a-b7f2-4a07-b7bd-3a709ddfd0ab no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 23 14:56:27.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-939" for this suite. 01/23/24 14:56:27.624
------------------------------
• [SLOW TEST] [6.058 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:56:21.568
    Jan 23 14:56:21.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename projected 01/23/24 14:56:21.569
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:56:21.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:56:21.577
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 01/23/24 14:56:21.579
    Jan 23 14:56:21.601: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e2b5f45a-b7f2-4a07-b7bd-3a709ddfd0ab" in namespace "projected-939" to be "Succeeded or Failed"
    Jan 23 14:56:21.603: INFO: Pod "downwardapi-volume-e2b5f45a-b7f2-4a07-b7bd-3a709ddfd0ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1.317827ms
    Jan 23 14:56:23.606: INFO: Pod "downwardapi-volume-e2b5f45a-b7f2-4a07-b7bd-3a709ddfd0ab": Phase="Running", Reason="", readiness=true. Elapsed: 2.005000283s
    Jan 23 14:56:25.606: INFO: Pod "downwardapi-volume-e2b5f45a-b7f2-4a07-b7bd-3a709ddfd0ab": Phase="Running", Reason="", readiness=false. Elapsed: 4.004803326s
    Jan 23 14:56:27.606: INFO: Pod "downwardapi-volume-e2b5f45a-b7f2-4a07-b7bd-3a709ddfd0ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004403672s
    STEP: Saw pod success 01/23/24 14:56:27.606
    Jan 23 14:56:27.606: INFO: Pod "downwardapi-volume-e2b5f45a-b7f2-4a07-b7bd-3a709ddfd0ab" satisfied condition "Succeeded or Failed"
    Jan 23 14:56:27.608: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod downwardapi-volume-e2b5f45a-b7f2-4a07-b7bd-3a709ddfd0ab container client-container: <nil>
    STEP: delete the pod 01/23/24 14:56:27.611
    Jan 23 14:56:27.620: INFO: Waiting for pod downwardapi-volume-e2b5f45a-b7f2-4a07-b7bd-3a709ddfd0ab to disappear
    Jan 23 14:56:27.621: INFO: Pod downwardapi-volume-e2b5f45a-b7f2-4a07-b7bd-3a709ddfd0ab no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:56:27.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-939" for this suite. 01/23/24 14:56:27.624
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:56:27.627
Jan 23 14:56:27.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename sched-pred 01/23/24 14:56:27.628
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:56:27.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:56:27.636
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 23 14:56:27.638: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 23 14:56:27.642: INFO: Waiting for terminating namespaces to be deleted...
Jan 23 14:56:27.645: INFO: 
Logging pods the apiserver thinks is on node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local before test
Jan 23 14:56:27.662: INFO: calico-kube-controllers-7d4c856855-qrf8w from kube-system started at 2024-01-23 09:02:23 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.662: INFO: 	Container calico-kube-controllers ready: true, restart count 4
Jan 23 14:56:27.662: INFO: calico-node-hx9hg from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.662: INFO: 	Container calico-node ready: true, restart count 1
Jan 23 14:56:27.662: INFO: coredns-8446d7bc66-zglp7 from kube-system started at 2024-01-23 09:02:28 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container coredns ready: true, restart count 1
Jan 23 14:56:27.663: INFO: kube-proxy-5p4pt from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container kube-proxy ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nginx-proxy-node-worker-hohyvwot.nova-ht9xu6tk2ptb.local from kube-system started at 2024-01-23 09:02:32 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container nginx-proxy ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-dns-667b6f9dd9-f4wkr from kube-system started at 2024-01-23 09:02:53 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container nova-dns ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-dns-667b6f9dd9-fc8vq from kube-system started at 2024-01-23 09:02:53 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container nova-dns ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-reflector-7bc9b5d4dd-vgml8 from nova-automation started at 2024-01-23 09:06:13 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container reflector ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-release-git-main-0 from nova-automation started at 2024-01-23 09:08:44 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container gitea ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-reloader-744fcf7b8f-ztn6c from nova-automation started at 2024-01-23 09:06:13 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container nova-reloader ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-cert-manager-74fb9fd7f9-q6q9f from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container cert-manager-controller ready: true, restart count 2
Jan 23 14:56:27.663: INFO: nova-cert-manager-cainjector-74465474c6-pjhbz from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container cert-manager-cainjector ready: true, restart count 4
Jan 23 14:56:27.663: INFO: nova-cert-manager-webhook-74b6ccdf8-f5bt2 from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container cert-manager-webhook ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-console-675844f8c4-p92th from nova-console started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container nova-console ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-local-path-provisioner-59754bbcb5-nm6ps from nova-csi-drivers started at 2024-01-23 09:06:07 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container local-path-provisioner ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-oauth-csi-provider-msq2s from nova-csi-drivers started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container vault-csi-provider ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-secrets-store-csi-driver-s4f8p from nova-csi-drivers started at 2024-01-23 09:03:57 +0000 UTC (3 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container liveness-probe ready: true, restart count 1
Jan 23 14:56:27.663: INFO: 	Container node-driver-registrar ready: true, restart count 1
Jan 23 14:56:27.663: INFO: 	Container secrets-store ready: true, restart count 1
Jan 23 14:56:27.663: INFO: secrets-store-csi-driver-upgrade-crds-jvnt9 from nova-csi-drivers started at 2024-01-23 09:03:57 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container crds-upgrade ready: false, restart count 0
Jan 23 14:56:27.663: INFO: nova-descheduler-575f46487d-wcml5 from nova-descheduler started at 2024-01-23 09:06:07 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container descheduler ready: true, restart count 4
Jan 23 14:56:27.663: INFO: helm-controller-6ffdd7974c-ndrg6 from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container manager ready: true, restart count 5
Jan 23 14:56:27.663: INFO: image-automation-controller-79bb688dbd-sq8gm from nova-gitops started at 2024-01-23 09:09:27 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container manager ready: true, restart count 4
Jan 23 14:56:27.663: INFO: image-reflector-controller-6b744758c7-nwppg from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container manager ready: true, restart count 4
Jan 23 14:56:27.663: INFO: kustomize-controller-5d5bb4d48-99q8z from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container manager ready: true, restart count 4
Jan 23 14:56:27.663: INFO: notification-controller-5974fbb84-8wcdc from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container manager ready: true, restart count 4
Jan 23 14:56:27.663: INFO: source-controller-7f8d6bc9d7-dbb88 from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container manager ready: true, restart count 4
Jan 23 14:56:27.663: INFO: nova-ingress-internal-controller-g8rl8 from nova-ingress-internal started at 2024-01-23 09:08:09 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container controller ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-logging-operator-7587849584-mq59r from nova-logging-operator started at 2024-01-23 09:06:18 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container logging-operator ready: true, restart count 5
Jan 23 14:56:27.663: INFO: alertmanager-main-0 from nova-monitoring started at 2024-01-23 09:09:10 +0000 UTC (3 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container alertmanager ready: true, restart count 1
Jan 23 14:56:27.663: INFO: 	Container config-reloader ready: true, restart count 1
Jan 23 14:56:27.663: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 23 14:56:27.663: INFO: monitoring-plugin-6dcd875fb-6sn96 from nova-monitoring started at 2024-01-23 09:08:38 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container monitoring-plugin ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-cadvisor-jvkj7 from nova-monitoring started at 2024-01-23 09:09:34 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container nova-cadvisor ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-grafana-5fcb766f99-xlrkr from nova-monitoring started at 2024-01-23 09:09:09 +0000 UTC (3 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container grafana ready: true, restart count 1
Jan 23 14:56:27.663: INFO: 	Container nova-release-grafana-sc-dashboard ready: true, restart count 1
Jan 23 14:56:27.663: INFO: 	Container nova-release-grafana-sc-datasources ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-kube-state-metrics-6c99956449-vwb5x from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container nova-release-kube-state-metrics ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-metrics-server-6fc9cb6c86-6dfwd from nova-monitoring started at 2024-01-23 09:06:11 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container metrics-server ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-prometheus-adapter-55b7c8779-b9ffb from nova-monitoring started at 2024-01-23 09:09:27 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container prometheus-adapter ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-prometheus-main-operator-fcb966c79-2wnc9 from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container main ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-prometheus-node-exporter-cc7w5 from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container node-exporter ready: true, restart count 1
Jan 23 14:56:27.663: INFO: prometheus-main-0 from nova-monitoring started at 2024-01-23 09:09:42 +0000 UTC (5 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container config-reloader ready: true, restart count 1
Jan 23 14:56:27.663: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 23 14:56:27.663: INFO: 	Container prometheus ready: true, restart count 1
Jan 23 14:56:27.663: INFO: 	Container thanos-sidecar ready: true, restart count 1
Jan 23 14:56:27.663: INFO: 	Container vault-agent-auth ready: true, restart count 1
Jan 23 14:56:27.663: INFO: nova-oauth-secrets-webhook-gmmxd from nova-secrets-webhook started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container vault-secrets-webhook ready: true, restart count 1
Jan 23 14:56:27.663: INFO: sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-4htxf from sonobuoy started at 2024-01-23 13:21:23 +0000 UTC (2 container statuses recorded)
Jan 23 14:56:27.663: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 14:56:27.663: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 23 14:56:27.663: INFO: 
Logging pods the apiserver thinks is on node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local before test
Jan 23 14:56:27.670: INFO: calico-node-44dpv from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.670: INFO: 	Container calico-node ready: true, restart count 1
Jan 23 14:56:27.670: INFO: coredns-8446d7bc66-rpjs8 from kube-system started at 2024-01-23 09:02:28 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.670: INFO: 	Container coredns ready: true, restart count 1
Jan 23 14:56:27.670: INFO: kube-proxy-dvptq from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.670: INFO: 	Container kube-proxy ready: true, restart count 1
Jan 23 14:56:27.670: INFO: nginx-proxy-node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local from kube-system started at 2024-01-23 09:02:45 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.670: INFO: 	Container nginx-proxy ready: true, restart count 1
Jan 23 14:56:27.670: INFO: busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614 from kubelet-test-8391 started at 2024-01-23 14:56:17 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.670: INFO: 	Container busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614 ready: true, restart count 0
Jan 23 14:56:27.670: INFO: nova-oauth-csi-provider-pj45z from nova-csi-drivers started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.670: INFO: 	Container vault-csi-provider ready: true, restart count 1
Jan 23 14:56:27.670: INFO: nova-secrets-store-csi-driver-ft87l from nova-csi-drivers started at 2024-01-23 09:03:58 +0000 UTC (3 container statuses recorded)
Jan 23 14:56:27.670: INFO: 	Container liveness-probe ready: true, restart count 1
Jan 23 14:56:27.670: INFO: 	Container node-driver-registrar ready: true, restart count 1
Jan 23 14:56:27.670: INFO: 	Container secrets-store ready: true, restart count 1
Jan 23 14:56:27.670: INFO: nova-ingress-public-controller-l6fx2 from nova-ingress-public started at 2024-01-23 14:26:19 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.670: INFO: 	Container controller ready: true, restart count 0
Jan 23 14:56:27.670: INFO: nova-cadvisor-tq2rp from nova-monitoring started at 2024-01-23 09:09:34 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.670: INFO: 	Container nova-cadvisor ready: true, restart count 1
Jan 23 14:56:27.670: INFO: nova-prometheus-node-exporter-xj6bh from nova-monitoring started at 2024-01-23 14:26:08 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.670: INFO: 	Container node-exporter ready: true, restart count 0
Jan 23 14:56:27.670: INFO: nova-oauth-secrets-webhook-h4wz9 from nova-secrets-webhook started at 2024-01-23 14:26:08 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.670: INFO: 	Container vault-secrets-webhook ready: true, restart count 0
Jan 23 14:56:27.670: INFO: sonobuoy from sonobuoy started at 2024-01-23 13:21:17 +0000 UTC (1 container statuses recorded)
Jan 23 14:56:27.670: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 23 14:56:27.670: INFO: sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-7csgd from sonobuoy started at 2024-01-23 13:21:23 +0000 UTC (2 container statuses recorded)
Jan 23 14:56:27.670: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 14:56:27.670: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local 01/23/24 14:56:27.693
STEP: verifying the node has the label node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local 01/23/24 14:56:27.702
Jan 23 14:56:27.718: INFO: Pod calico-kube-controllers-7d4c856855-qrf8w requesting resource cpu=0m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod calico-node-44dpv requesting resource cpu=250m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod calico-node-hx9hg requesting resource cpu=250m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod coredns-8446d7bc66-rpjs8 requesting resource cpu=100m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod coredns-8446d7bc66-zglp7 requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod kube-proxy-5p4pt requesting resource cpu=0m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod kube-proxy-dvptq requesting resource cpu=0m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nginx-proxy-node-worker-hohyvwot.nova-ht9xu6tk2ptb.local requesting resource cpu=25m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nginx-proxy-node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local requesting resource cpu=25m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-dns-667b6f9dd9-f4wkr requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-dns-667b6f9dd9-fc8vq requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614 requesting resource cpu=0m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-reflector-7bc9b5d4dd-vgml8 requesting resource cpu=50m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-release-git-main-0 requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-reloader-744fcf7b8f-ztn6c requesting resource cpu=50m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-cert-manager-74fb9fd7f9-q6q9f requesting resource cpu=10m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-cert-manager-cainjector-74465474c6-pjhbz requesting resource cpu=10m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-cert-manager-webhook-74b6ccdf8-f5bt2 requesting resource cpu=10m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-console-675844f8c4-p92th requesting resource cpu=10m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-local-path-provisioner-59754bbcb5-nm6ps requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-oauth-csi-provider-msq2s requesting resource cpu=50m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-oauth-csi-provider-pj45z requesting resource cpu=50m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-secrets-store-csi-driver-ft87l requesting resource cpu=70m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-secrets-store-csi-driver-s4f8p requesting resource cpu=70m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-descheduler-575f46487d-wcml5 requesting resource cpu=10m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod helm-controller-6ffdd7974c-ndrg6 requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod image-automation-controller-79bb688dbd-sq8gm requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod image-reflector-controller-6b744758c7-nwppg requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod kustomize-controller-5d5bb4d48-99q8z requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod notification-controller-5974fbb84-8wcdc requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod source-controller-7f8d6bc9d7-dbb88 requesting resource cpu=50m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-ingress-internal-controller-g8rl8 requesting resource cpu=50m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-ingress-public-controller-l6fx2 requesting resource cpu=100m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-logging-operator-7587849584-mq59r requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod alertmanager-main-0 requesting resource cpu=400m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod monitoring-plugin-6dcd875fb-6sn96 requesting resource cpu=10m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-cadvisor-jvkj7 requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-cadvisor-tq2rp requesting resource cpu=100m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-grafana-5fcb766f99-xlrkr requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-kube-state-metrics-6c99956449-vwb5x requesting resource cpu=10m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-metrics-server-6fc9cb6c86-6dfwd requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-prometheus-adapter-55b7c8779-b9ffb requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-prometheus-main-operator-fcb966c79-2wnc9 requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-prometheus-node-exporter-cc7w5 requesting resource cpu=10m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-prometheus-node-exporter-xj6bh requesting resource cpu=10m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod prometheus-main-0 requesting resource cpu=850m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-oauth-secrets-webhook-gmmxd requesting resource cpu=50m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod nova-oauth-secrets-webhook-h4wz9 requesting resource cpu=50m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod sonobuoy requesting resource cpu=0m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-4htxf requesting resource cpu=0m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.718: INFO: Pod sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-7csgd requesting resource cpu=0m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
STEP: Starting Pods to consume most of the cluster CPU. 01/23/24 14:56:27.718
Jan 23 14:56:27.718: INFO: Creating a pod which consumes cpu=1697m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.730: INFO: Creating a pod which consumes cpu=2271m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
Jan 23 14:56:27.740: INFO: Waiting up to 5m0s for pod "filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a" in namespace "sched-pred-5620" to be "running"
Jan 23 14:56:27.743: INFO: Pod "filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037165ms
Jan 23 14:56:29.746: INFO: Pod "filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a": Phase="Running", Reason="", readiness=true. Elapsed: 2.005616146s
Jan 23 14:56:29.746: INFO: Pod "filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a" satisfied condition "running"
Jan 23 14:56:29.746: INFO: Waiting up to 5m0s for pod "filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e" in namespace "sched-pred-5620" to be "running"
Jan 23 14:56:29.748: INFO: Pod "filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e": Phase="Running", Reason="", readiness=true. Elapsed: 1.427477ms
Jan 23 14:56:29.748: INFO: Pod "filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/23/24 14:56:29.748
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e.17ad01c6ed9dce93], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5620/filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e to node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local] 01/23/24 14:56:29.749
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e.17ad01c712205755], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/23/24 14:56:29.75
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e.17ad01c7153debd4], Reason = [Created], Message = [Created container filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e] 01/23/24 14:56:29.75
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e.17ad01c71a7ac7ef], Reason = [Started], Message = [Started container filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e] 01/23/24 14:56:29.75
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a.17ad01c6ecf3799c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5620/filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a to node-worker-hohyvwot.nova-ht9xu6tk2ptb.local] 01/23/24 14:56:29.75
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a.17ad01c7156476b6], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/23/24 14:56:29.75
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a.17ad01c7195d4c56], Reason = [Created], Message = [Created container filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a] 01/23/24 14:56:29.75
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a.17ad01c71fbd1b59], Reason = [Started], Message = [Started container filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a] 01/23/24 14:56:29.75
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17ad01c765e9bc9a], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod..] 01/23/24 14:56:29.765
STEP: removing the label node off the node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local 01/23/24 14:56:30.766
STEP: verifying the node doesn't have the label node 01/23/24 14:56:30.774
STEP: removing the label node off the node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local 01/23/24 14:56:30.776
STEP: verifying the node doesn't have the label node 01/23/24 14:56:30.785
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:56:30.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5620" for this suite. 01/23/24 14:56:30.788
------------------------------
• [3.164 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:56:27.627
    Jan 23 14:56:27.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename sched-pred 01/23/24 14:56:27.628
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:56:27.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:56:27.636
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 23 14:56:27.638: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 23 14:56:27.642: INFO: Waiting for terminating namespaces to be deleted...
    Jan 23 14:56:27.645: INFO: 
    Logging pods the apiserver thinks is on node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local before test
    Jan 23 14:56:27.662: INFO: calico-kube-controllers-7d4c856855-qrf8w from kube-system started at 2024-01-23 09:02:23 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.662: INFO: 	Container calico-kube-controllers ready: true, restart count 4
    Jan 23 14:56:27.662: INFO: calico-node-hx9hg from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.662: INFO: 	Container calico-node ready: true, restart count 1
    Jan 23 14:56:27.662: INFO: coredns-8446d7bc66-zglp7 from kube-system started at 2024-01-23 09:02:28 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container coredns ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: kube-proxy-5p4pt from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container kube-proxy ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nginx-proxy-node-worker-hohyvwot.nova-ht9xu6tk2ptb.local from kube-system started at 2024-01-23 09:02:32 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container nginx-proxy ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-dns-667b6f9dd9-f4wkr from kube-system started at 2024-01-23 09:02:53 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container nova-dns ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-dns-667b6f9dd9-fc8vq from kube-system started at 2024-01-23 09:02:53 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container nova-dns ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-reflector-7bc9b5d4dd-vgml8 from nova-automation started at 2024-01-23 09:06:13 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container reflector ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-release-git-main-0 from nova-automation started at 2024-01-23 09:08:44 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container gitea ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-reloader-744fcf7b8f-ztn6c from nova-automation started at 2024-01-23 09:06:13 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container nova-reloader ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-cert-manager-74fb9fd7f9-q6q9f from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container cert-manager-controller ready: true, restart count 2
    Jan 23 14:56:27.663: INFO: nova-cert-manager-cainjector-74465474c6-pjhbz from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container cert-manager-cainjector ready: true, restart count 4
    Jan 23 14:56:27.663: INFO: nova-cert-manager-webhook-74b6ccdf8-f5bt2 from nova-cert-management started at 2024-01-23 09:06:34 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container cert-manager-webhook ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-console-675844f8c4-p92th from nova-console started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container nova-console ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-local-path-provisioner-59754bbcb5-nm6ps from nova-csi-drivers started at 2024-01-23 09:06:07 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container local-path-provisioner ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-oauth-csi-provider-msq2s from nova-csi-drivers started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container vault-csi-provider ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-secrets-store-csi-driver-s4f8p from nova-csi-drivers started at 2024-01-23 09:03:57 +0000 UTC (3 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container liveness-probe ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: 	Container node-driver-registrar ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: 	Container secrets-store ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: secrets-store-csi-driver-upgrade-crds-jvnt9 from nova-csi-drivers started at 2024-01-23 09:03:57 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container crds-upgrade ready: false, restart count 0
    Jan 23 14:56:27.663: INFO: nova-descheduler-575f46487d-wcml5 from nova-descheduler started at 2024-01-23 09:06:07 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container descheduler ready: true, restart count 4
    Jan 23 14:56:27.663: INFO: helm-controller-6ffdd7974c-ndrg6 from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container manager ready: true, restart count 5
    Jan 23 14:56:27.663: INFO: image-automation-controller-79bb688dbd-sq8gm from nova-gitops started at 2024-01-23 09:09:27 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container manager ready: true, restart count 4
    Jan 23 14:56:27.663: INFO: image-reflector-controller-6b744758c7-nwppg from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container manager ready: true, restart count 4
    Jan 23 14:56:27.663: INFO: kustomize-controller-5d5bb4d48-99q8z from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container manager ready: true, restart count 4
    Jan 23 14:56:27.663: INFO: notification-controller-5974fbb84-8wcdc from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container manager ready: true, restart count 4
    Jan 23 14:56:27.663: INFO: source-controller-7f8d6bc9d7-dbb88 from nova-gitops started at 2024-01-23 09:05:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container manager ready: true, restart count 4
    Jan 23 14:56:27.663: INFO: nova-ingress-internal-controller-g8rl8 from nova-ingress-internal started at 2024-01-23 09:08:09 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container controller ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-logging-operator-7587849584-mq59r from nova-logging-operator started at 2024-01-23 09:06:18 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container logging-operator ready: true, restart count 5
    Jan 23 14:56:27.663: INFO: alertmanager-main-0 from nova-monitoring started at 2024-01-23 09:09:10 +0000 UTC (3 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: 	Container config-reloader ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: monitoring-plugin-6dcd875fb-6sn96 from nova-monitoring started at 2024-01-23 09:08:38 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container monitoring-plugin ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-cadvisor-jvkj7 from nova-monitoring started at 2024-01-23 09:09:34 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container nova-cadvisor ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-grafana-5fcb766f99-xlrkr from nova-monitoring started at 2024-01-23 09:09:09 +0000 UTC (3 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container grafana ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: 	Container nova-release-grafana-sc-dashboard ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: 	Container nova-release-grafana-sc-datasources ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-kube-state-metrics-6c99956449-vwb5x from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container nova-release-kube-state-metrics ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-metrics-server-6fc9cb6c86-6dfwd from nova-monitoring started at 2024-01-23 09:06:11 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container metrics-server ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-prometheus-adapter-55b7c8779-b9ffb from nova-monitoring started at 2024-01-23 09:09:27 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container prometheus-adapter ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-prometheus-main-operator-fcb966c79-2wnc9 from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container main ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-prometheus-node-exporter-cc7w5 from nova-monitoring started at 2024-01-23 09:08:39 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: prometheus-main-0 from nova-monitoring started at 2024-01-23 09:09:42 +0000 UTC (5 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container config-reloader ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: 	Container prometheus ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: 	Container thanos-sidecar ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: 	Container vault-agent-auth ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: nova-oauth-secrets-webhook-gmmxd from nova-secrets-webhook started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container vault-secrets-webhook ready: true, restart count 1
    Jan 23 14:56:27.663: INFO: sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-4htxf from sonobuoy started at 2024-01-23 13:21:23 +0000 UTC (2 container statuses recorded)
    Jan 23 14:56:27.663: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 23 14:56:27.663: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 23 14:56:27.663: INFO: 
    Logging pods the apiserver thinks is on node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local before test
    Jan 23 14:56:27.670: INFO: calico-node-44dpv from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.670: INFO: 	Container calico-node ready: true, restart count 1
    Jan 23 14:56:27.670: INFO: coredns-8446d7bc66-rpjs8 from kube-system started at 2024-01-23 09:02:28 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.670: INFO: 	Container coredns ready: true, restart count 1
    Jan 23 14:56:27.670: INFO: kube-proxy-dvptq from kube-system started at 2024-01-23 09:02:12 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.670: INFO: 	Container kube-proxy ready: true, restart count 1
    Jan 23 14:56:27.670: INFO: nginx-proxy-node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local from kube-system started at 2024-01-23 09:02:45 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.670: INFO: 	Container nginx-proxy ready: true, restart count 1
    Jan 23 14:56:27.670: INFO: busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614 from kubelet-test-8391 started at 2024-01-23 14:56:17 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.670: INFO: 	Container busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614 ready: true, restart count 0
    Jan 23 14:56:27.670: INFO: nova-oauth-csi-provider-pj45z from nova-csi-drivers started at 2024-01-23 09:03:56 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.670: INFO: 	Container vault-csi-provider ready: true, restart count 1
    Jan 23 14:56:27.670: INFO: nova-secrets-store-csi-driver-ft87l from nova-csi-drivers started at 2024-01-23 09:03:58 +0000 UTC (3 container statuses recorded)
    Jan 23 14:56:27.670: INFO: 	Container liveness-probe ready: true, restart count 1
    Jan 23 14:56:27.670: INFO: 	Container node-driver-registrar ready: true, restart count 1
    Jan 23 14:56:27.670: INFO: 	Container secrets-store ready: true, restart count 1
    Jan 23 14:56:27.670: INFO: nova-ingress-public-controller-l6fx2 from nova-ingress-public started at 2024-01-23 14:26:19 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.670: INFO: 	Container controller ready: true, restart count 0
    Jan 23 14:56:27.670: INFO: nova-cadvisor-tq2rp from nova-monitoring started at 2024-01-23 09:09:34 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.670: INFO: 	Container nova-cadvisor ready: true, restart count 1
    Jan 23 14:56:27.670: INFO: nova-prometheus-node-exporter-xj6bh from nova-monitoring started at 2024-01-23 14:26:08 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.670: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 23 14:56:27.670: INFO: nova-oauth-secrets-webhook-h4wz9 from nova-secrets-webhook started at 2024-01-23 14:26:08 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.670: INFO: 	Container vault-secrets-webhook ready: true, restart count 0
    Jan 23 14:56:27.670: INFO: sonobuoy from sonobuoy started at 2024-01-23 13:21:17 +0000 UTC (1 container statuses recorded)
    Jan 23 14:56:27.670: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 23 14:56:27.670: INFO: sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-7csgd from sonobuoy started at 2024-01-23 13:21:23 +0000 UTC (2 container statuses recorded)
    Jan 23 14:56:27.670: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 23 14:56:27.670: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local 01/23/24 14:56:27.693
    STEP: verifying the node has the label node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local 01/23/24 14:56:27.702
    Jan 23 14:56:27.718: INFO: Pod calico-kube-controllers-7d4c856855-qrf8w requesting resource cpu=0m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod calico-node-44dpv requesting resource cpu=250m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod calico-node-hx9hg requesting resource cpu=250m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod coredns-8446d7bc66-rpjs8 requesting resource cpu=100m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod coredns-8446d7bc66-zglp7 requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod kube-proxy-5p4pt requesting resource cpu=0m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod kube-proxy-dvptq requesting resource cpu=0m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nginx-proxy-node-worker-hohyvwot.nova-ht9xu6tk2ptb.local requesting resource cpu=25m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nginx-proxy-node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local requesting resource cpu=25m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-dns-667b6f9dd9-f4wkr requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-dns-667b6f9dd9-fc8vq requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod busybox-scheduling-fc5752a8-372a-4c22-aa6b-37cf08434614 requesting resource cpu=0m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-reflector-7bc9b5d4dd-vgml8 requesting resource cpu=50m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-release-git-main-0 requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-reloader-744fcf7b8f-ztn6c requesting resource cpu=50m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-cert-manager-74fb9fd7f9-q6q9f requesting resource cpu=10m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-cert-manager-cainjector-74465474c6-pjhbz requesting resource cpu=10m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-cert-manager-webhook-74b6ccdf8-f5bt2 requesting resource cpu=10m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-console-675844f8c4-p92th requesting resource cpu=10m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-local-path-provisioner-59754bbcb5-nm6ps requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-oauth-csi-provider-msq2s requesting resource cpu=50m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-oauth-csi-provider-pj45z requesting resource cpu=50m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-secrets-store-csi-driver-ft87l requesting resource cpu=70m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-secrets-store-csi-driver-s4f8p requesting resource cpu=70m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-descheduler-575f46487d-wcml5 requesting resource cpu=10m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod helm-controller-6ffdd7974c-ndrg6 requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod image-automation-controller-79bb688dbd-sq8gm requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod image-reflector-controller-6b744758c7-nwppg requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod kustomize-controller-5d5bb4d48-99q8z requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod notification-controller-5974fbb84-8wcdc requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod source-controller-7f8d6bc9d7-dbb88 requesting resource cpu=50m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-ingress-internal-controller-g8rl8 requesting resource cpu=50m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-ingress-public-controller-l6fx2 requesting resource cpu=100m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-logging-operator-7587849584-mq59r requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod alertmanager-main-0 requesting resource cpu=400m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod monitoring-plugin-6dcd875fb-6sn96 requesting resource cpu=10m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-cadvisor-jvkj7 requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-cadvisor-tq2rp requesting resource cpu=100m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-grafana-5fcb766f99-xlrkr requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-kube-state-metrics-6c99956449-vwb5x requesting resource cpu=10m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-metrics-server-6fc9cb6c86-6dfwd requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-prometheus-adapter-55b7c8779-b9ffb requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-prometheus-main-operator-fcb966c79-2wnc9 requesting resource cpu=100m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-prometheus-node-exporter-cc7w5 requesting resource cpu=10m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-prometheus-node-exporter-xj6bh requesting resource cpu=10m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod prometheus-main-0 requesting resource cpu=850m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-oauth-secrets-webhook-gmmxd requesting resource cpu=50m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod nova-oauth-secrets-webhook-h4wz9 requesting resource cpu=50m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod sonobuoy requesting resource cpu=0m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-4htxf requesting resource cpu=0m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.718: INFO: Pod sonobuoy-systemd-logs-daemon-set-7eb9afbdcedc4e65-7csgd requesting resource cpu=0m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
    STEP: Starting Pods to consume most of the cluster CPU. 01/23/24 14:56:27.718
    Jan 23 14:56:27.718: INFO: Creating a pod which consumes cpu=1697m on Node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.730: INFO: Creating a pod which consumes cpu=2271m on Node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local
    Jan 23 14:56:27.740: INFO: Waiting up to 5m0s for pod "filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a" in namespace "sched-pred-5620" to be "running"
    Jan 23 14:56:27.743: INFO: Pod "filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037165ms
    Jan 23 14:56:29.746: INFO: Pod "filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a": Phase="Running", Reason="", readiness=true. Elapsed: 2.005616146s
    Jan 23 14:56:29.746: INFO: Pod "filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a" satisfied condition "running"
    Jan 23 14:56:29.746: INFO: Waiting up to 5m0s for pod "filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e" in namespace "sched-pred-5620" to be "running"
    Jan 23 14:56:29.748: INFO: Pod "filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e": Phase="Running", Reason="", readiness=true. Elapsed: 1.427477ms
    Jan 23 14:56:29.748: INFO: Pod "filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/23/24 14:56:29.748
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e.17ad01c6ed9dce93], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5620/filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e to node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local] 01/23/24 14:56:29.749
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e.17ad01c712205755], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/23/24 14:56:29.75
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e.17ad01c7153debd4], Reason = [Created], Message = [Created container filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e] 01/23/24 14:56:29.75
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e.17ad01c71a7ac7ef], Reason = [Started], Message = [Started container filler-pod-9cdee825-5393-4cff-bc37-c3fefc30e88e] 01/23/24 14:56:29.75
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a.17ad01c6ecf3799c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5620/filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a to node-worker-hohyvwot.nova-ht9xu6tk2ptb.local] 01/23/24 14:56:29.75
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a.17ad01c7156476b6], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/23/24 14:56:29.75
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a.17ad01c7195d4c56], Reason = [Created], Message = [Created container filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a] 01/23/24 14:56:29.75
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a.17ad01c71fbd1b59], Reason = [Started], Message = [Started container filler-pod-b0a1eee0-3283-462c-a495-d2d68285318a] 01/23/24 14:56:29.75
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17ad01c765e9bc9a], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod..] 01/23/24 14:56:29.765
    STEP: removing the label node off the node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local 01/23/24 14:56:30.766
    STEP: verifying the node doesn't have the label node 01/23/24 14:56:30.774
    STEP: removing the label node off the node node-worker-hohyvwot.nova-ht9xu6tk2ptb.local 01/23/24 14:56:30.776
    STEP: verifying the node doesn't have the label node 01/23/24 14:56:30.785
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:56:30.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5620" for this suite. 01/23/24 14:56:30.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:56:30.792
Jan 23 14:56:30.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename emptydir 01/23/24 14:56:30.793
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:56:30.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:56:30.801
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/23/24 14:56:30.803
Jan 23 14:56:30.814: INFO: Waiting up to 5m0s for pod "pod-f1c57f45-b10b-4f32-a52a-84e8c7c1c7d8" in namespace "emptydir-2142" to be "Succeeded or Failed"
Jan 23 14:56:30.817: INFO: Pod "pod-f1c57f45-b10b-4f32-a52a-84e8c7c1c7d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.777319ms
Jan 23 14:56:32.819: INFO: Pod "pod-f1c57f45-b10b-4f32-a52a-84e8c7c1c7d8": Phase="Running", Reason="", readiness=false. Elapsed: 2.004664884s
Jan 23 14:56:34.819: INFO: Pod "pod-f1c57f45-b10b-4f32-a52a-84e8c7c1c7d8": Phase="Running", Reason="", readiness=false. Elapsed: 4.004842487s
Jan 23 14:56:36.819: INFO: Pod "pod-f1c57f45-b10b-4f32-a52a-84e8c7c1c7d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005152786s
STEP: Saw pod success 01/23/24 14:56:36.819
Jan 23 14:56:36.819: INFO: Pod "pod-f1c57f45-b10b-4f32-a52a-84e8c7c1c7d8" satisfied condition "Succeeded or Failed"
Jan 23 14:56:36.821: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-f1c57f45-b10b-4f32-a52a-84e8c7c1c7d8 container test-container: <nil>
STEP: delete the pod 01/23/24 14:56:36.824
Jan 23 14:56:36.833: INFO: Waiting for pod pod-f1c57f45-b10b-4f32-a52a-84e8c7c1c7d8 to disappear
Jan 23 14:56:36.834: INFO: Pod pod-f1c57f45-b10b-4f32-a52a-84e8c7c1c7d8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 23 14:56:36.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2142" for this suite. 01/23/24 14:56:36.839
------------------------------
• [SLOW TEST] [6.051 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:56:30.792
    Jan 23 14:56:30.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename emptydir 01/23/24 14:56:30.793
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:56:30.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:56:30.801
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/23/24 14:56:30.803
    Jan 23 14:56:30.814: INFO: Waiting up to 5m0s for pod "pod-f1c57f45-b10b-4f32-a52a-84e8c7c1c7d8" in namespace "emptydir-2142" to be "Succeeded or Failed"
    Jan 23 14:56:30.817: INFO: Pod "pod-f1c57f45-b10b-4f32-a52a-84e8c7c1c7d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.777319ms
    Jan 23 14:56:32.819: INFO: Pod "pod-f1c57f45-b10b-4f32-a52a-84e8c7c1c7d8": Phase="Running", Reason="", readiness=false. Elapsed: 2.004664884s
    Jan 23 14:56:34.819: INFO: Pod "pod-f1c57f45-b10b-4f32-a52a-84e8c7c1c7d8": Phase="Running", Reason="", readiness=false. Elapsed: 4.004842487s
    Jan 23 14:56:36.819: INFO: Pod "pod-f1c57f45-b10b-4f32-a52a-84e8c7c1c7d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005152786s
    STEP: Saw pod success 01/23/24 14:56:36.819
    Jan 23 14:56:36.819: INFO: Pod "pod-f1c57f45-b10b-4f32-a52a-84e8c7c1c7d8" satisfied condition "Succeeded or Failed"
    Jan 23 14:56:36.821: INFO: Trying to get logs from node node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local pod pod-f1c57f45-b10b-4f32-a52a-84e8c7c1c7d8 container test-container: <nil>
    STEP: delete the pod 01/23/24 14:56:36.824
    Jan 23 14:56:36.833: INFO: Waiting for pod pod-f1c57f45-b10b-4f32-a52a-84e8c7c1c7d8 to disappear
    Jan 23 14:56:36.834: INFO: Pod pod-f1c57f45-b10b-4f32-a52a-84e8c7c1c7d8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:56:36.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2142" for this suite. 01/23/24 14:56:36.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:56:36.848
Jan 23 14:56:36.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename webhook 01/23/24 14:56:36.849
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:56:36.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:56:36.857
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/23/24 14:56:36.865
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:56:37.243
STEP: Deploying the webhook pod 01/23/24 14:56:37.248
STEP: Wait for the deployment to be ready 01/23/24 14:56:37.256
Jan 23 14:56:37.260: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/23/24 14:56:39.266
STEP: Verifying the service has paired with the endpoint 01/23/24 14:56:39.272
Jan 23 14:56:40.272: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/23/24 14:56:40.275
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/23/24 14:56:40.287
STEP: Creating a dummy validating-webhook-configuration object 01/23/24 14:56:40.297
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/23/24 14:56:40.303
STEP: Creating a dummy mutating-webhook-configuration object 01/23/24 14:56:40.305
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/23/24 14:56:40.311
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 23 14:56:40.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9765" for this suite. 01/23/24 14:56:40.347
STEP: Destroying namespace "webhook-9765-markers" for this suite. 01/23/24 14:56:40.35
------------------------------
• [3.506 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:56:36.848
    Jan 23 14:56:36.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename webhook 01/23/24 14:56:36.849
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:56:36.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:56:36.857
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/23/24 14:56:36.865
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/23/24 14:56:37.243
    STEP: Deploying the webhook pod 01/23/24 14:56:37.248
    STEP: Wait for the deployment to be ready 01/23/24 14:56:37.256
    Jan 23 14:56:37.260: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/23/24 14:56:39.266
    STEP: Verifying the service has paired with the endpoint 01/23/24 14:56:39.272
    Jan 23 14:56:40.272: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/23/24 14:56:40.275
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/23/24 14:56:40.287
    STEP: Creating a dummy validating-webhook-configuration object 01/23/24 14:56:40.297
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/23/24 14:56:40.303
    STEP: Creating a dummy mutating-webhook-configuration object 01/23/24 14:56:40.305
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/23/24 14:56:40.311
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:56:40.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9765" for this suite. 01/23/24 14:56:40.347
    STEP: Destroying namespace "webhook-9765-markers" for this suite. 01/23/24 14:56:40.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:56:40.354
Jan 23 14:56:40.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename statefulset 01/23/24 14:56:40.355
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:56:40.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:56:40.371
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7324 01/23/24 14:56:40.373
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-7324 01/23/24 14:56:40.378
Jan 23 14:56:40.387: INFO: Found 0 stateful pods, waiting for 1
Jan 23 14:56:50.392: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/23/24 14:56:50.395
STEP: updating a scale subresource 01/23/24 14:56:50.397
STEP: verifying the statefulset Spec.Replicas was modified 01/23/24 14:56:50.401
STEP: Patch a scale subresource 01/23/24 14:56:50.402
STEP: verifying the statefulset Spec.Replicas was modified 01/23/24 14:56:50.406
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 23 14:56:50.408: INFO: Deleting all statefulset in ns statefulset-7324
Jan 23 14:56:50.409: INFO: Scaling statefulset ss to 0
Jan 23 14:57:00.423: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 14:57:00.425: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 23 14:57:00.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7324" for this suite. 01/23/24 14:57:00.439
------------------------------
• [SLOW TEST] [20.089 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:56:40.354
    Jan 23 14:56:40.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename statefulset 01/23/24 14:56:40.355
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:56:40.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:56:40.371
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7324 01/23/24 14:56:40.373
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-7324 01/23/24 14:56:40.378
    Jan 23 14:56:40.387: INFO: Found 0 stateful pods, waiting for 1
    Jan 23 14:56:50.392: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/23/24 14:56:50.395
    STEP: updating a scale subresource 01/23/24 14:56:50.397
    STEP: verifying the statefulset Spec.Replicas was modified 01/23/24 14:56:50.401
    STEP: Patch a scale subresource 01/23/24 14:56:50.402
    STEP: verifying the statefulset Spec.Replicas was modified 01/23/24 14:56:50.406
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 23 14:56:50.408: INFO: Deleting all statefulset in ns statefulset-7324
    Jan 23 14:56:50.409: INFO: Scaling statefulset ss to 0
    Jan 23 14:57:00.423: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 23 14:57:00.425: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:57:00.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7324" for this suite. 01/23/24 14:57:00.439
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:57:00.443
Jan 23 14:57:00.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename statefulset 01/23/24 14:57:00.444
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:57:00.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:57:00.455
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4678 01/23/24 14:57:00.457
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-4678 01/23/24 14:57:00.46
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4678 01/23/24 14:57:00.465
Jan 23 14:57:00.468: INFO: Found 0 stateful pods, waiting for 1
Jan 23 14:57:10.472: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/23/24 14:57:10.472
Jan 23 14:57:10.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-4678 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 14:57:10.667: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 14:57:10.667: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 14:57:10.667: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 23 14:57:10.670: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 23 14:57:20.673: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 14:57:20.674: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 14:57:20.682: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Jan 23 14:57:20.682: INFO: ss-0  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:00 +0000 UTC  }]
Jan 23 14:57:20.682: INFO: 
Jan 23 14:57:20.682: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 23 14:57:21.684: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998254206s
Jan 23 14:57:22.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995845846s
Jan 23 14:57:23.690: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.992921164s
Jan 23 14:57:24.693: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.989839028s
Jan 23 14:57:25.695: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.986800149s
Jan 23 14:57:26.699: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.984454271s
Jan 23 14:57:27.702: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.98125145s
Jan 23 14:57:28.705: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.977659099s
Jan 23 14:57:29.710: INFO: Verifying statefulset ss doesn't scale past 3 for another 973.463732ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4678 01/23/24 14:57:30.711
Jan 23 14:57:30.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-4678 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 23 14:57:30.890: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 23 14:57:30.890: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 23 14:57:30.890: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 23 14:57:30.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-4678 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 23 14:57:31.075: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 23 14:57:31.075: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 23 14:57:31.075: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 23 14:57:31.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-4678 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 23 14:57:31.283: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 23 14:57:31.283: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 23 14:57:31.283: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 23 14:57:31.286: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 23 14:57:41.290: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 14:57:41.290: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 14:57:41.290: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/23/24 14:57:41.29
Jan 23 14:57:41.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-4678 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 14:57:41.487: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 14:57:41.487: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 14:57:41.487: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 23 14:57:41.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-4678 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 14:57:41.679: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 14:57:41.679: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 14:57:41.679: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 23 14:57:41.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-4678 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 14:57:41.844: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 14:57:41.844: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 14:57:41.844: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 23 14:57:41.844: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 14:57:41.845: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 23 14:57:51.851: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 14:57:51.851: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 14:57:51.851: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 14:57:51.859: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Jan 23 14:57:51.859: INFO: ss-0  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:00 +0000 UTC  }]
Jan 23 14:57:51.859: INFO: ss-1  node-worker-hohyvwot.nova-ht9xu6tk2ptb.local  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  }]
Jan 23 14:57:51.859: INFO: ss-2  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  }]
Jan 23 14:57:51.859: INFO: 
Jan 23 14:57:51.859: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 23 14:57:52.863: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Jan 23 14:57:52.863: INFO: ss-0  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:00 +0000 UTC  }]
Jan 23 14:57:52.863: INFO: ss-1  node-worker-hohyvwot.nova-ht9xu6tk2ptb.local  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  }]
Jan 23 14:57:52.863: INFO: ss-2  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  }]
Jan 23 14:57:52.863: INFO: 
Jan 23 14:57:52.863: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 23 14:57:53.865: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Jan 23 14:57:53.865: INFO: ss-0  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:00 +0000 UTC  }]
Jan 23 14:57:53.865: INFO: ss-2  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  }]
Jan 23 14:57:53.865: INFO: 
Jan 23 14:57:53.865: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 23 14:57:54.868: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.991278032s
Jan 23 14:57:55.870: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.988352434s
Jan 23 14:57:56.873: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.985222097s
Jan 23 14:57:57.875: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.98364823s
Jan 23 14:57:58.879: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.980338817s
Jan 23 14:57:59.882: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.977186096s
Jan 23 14:58:00.884: INFO: Verifying statefulset ss doesn't scale past 0 for another 974.215876ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4678 01/23/24 14:58:01.884
Jan 23 14:58:01.886: INFO: Scaling statefulset ss to 0
Jan 23 14:58:01.891: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 23 14:58:01.893: INFO: Deleting all statefulset in ns statefulset-4678
Jan 23 14:58:01.894: INFO: Scaling statefulset ss to 0
Jan 23 14:58:01.899: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 14:58:01.900: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 23 14:58:01.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4678" for this suite. 01/23/24 14:58:01.917
------------------------------
• [SLOW TEST] [61.476 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:57:00.443
    Jan 23 14:57:00.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename statefulset 01/23/24 14:57:00.444
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:57:00.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:57:00.455
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4678 01/23/24 14:57:00.457
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-4678 01/23/24 14:57:00.46
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4678 01/23/24 14:57:00.465
    Jan 23 14:57:00.468: INFO: Found 0 stateful pods, waiting for 1
    Jan 23 14:57:10.472: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/23/24 14:57:10.472
    Jan 23 14:57:10.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-4678 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 23 14:57:10.667: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 23 14:57:10.667: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 23 14:57:10.667: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 23 14:57:10.670: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 23 14:57:20.673: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 23 14:57:20.674: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 23 14:57:20.682: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
    Jan 23 14:57:20.682: INFO: ss-0  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:00 +0000 UTC  }]
    Jan 23 14:57:20.682: INFO: 
    Jan 23 14:57:20.682: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan 23 14:57:21.684: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998254206s
    Jan 23 14:57:22.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995845846s
    Jan 23 14:57:23.690: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.992921164s
    Jan 23 14:57:24.693: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.989839028s
    Jan 23 14:57:25.695: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.986800149s
    Jan 23 14:57:26.699: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.984454271s
    Jan 23 14:57:27.702: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.98125145s
    Jan 23 14:57:28.705: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.977659099s
    Jan 23 14:57:29.710: INFO: Verifying statefulset ss doesn't scale past 3 for another 973.463732ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4678 01/23/24 14:57:30.711
    Jan 23 14:57:30.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-4678 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 23 14:57:30.890: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 23 14:57:30.890: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 23 14:57:30.890: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 23 14:57:30.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-4678 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 23 14:57:31.075: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 23 14:57:31.075: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 23 14:57:31.075: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 23 14:57:31.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-4678 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 23 14:57:31.283: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 23 14:57:31.283: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 23 14:57:31.283: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 23 14:57:31.286: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jan 23 14:57:41.290: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 23 14:57:41.290: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 23 14:57:41.290: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/23/24 14:57:41.29
    Jan 23 14:57:41.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-4678 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 23 14:57:41.487: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 23 14:57:41.487: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 23 14:57:41.487: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 23 14:57:41.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-4678 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 23 14:57:41.679: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 23 14:57:41.679: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 23 14:57:41.679: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 23 14:57:41.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1347667206 --namespace=statefulset-4678 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 23 14:57:41.844: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 23 14:57:41.844: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 23 14:57:41.844: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 23 14:57:41.844: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 23 14:57:41.845: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jan 23 14:57:51.851: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 23 14:57:51.851: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 23 14:57:51.851: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 23 14:57:51.859: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
    Jan 23 14:57:51.859: INFO: ss-0  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:00 +0000 UTC  }]
    Jan 23 14:57:51.859: INFO: ss-1  node-worker-hohyvwot.nova-ht9xu6tk2ptb.local  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  }]
    Jan 23 14:57:51.859: INFO: ss-2  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  }]
    Jan 23 14:57:51.859: INFO: 
    Jan 23 14:57:51.859: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 23 14:57:52.863: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
    Jan 23 14:57:52.863: INFO: ss-0  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:00 +0000 UTC  }]
    Jan 23 14:57:52.863: INFO: ss-1  node-worker-hohyvwot.nova-ht9xu6tk2ptb.local  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  }]
    Jan 23 14:57:52.863: INFO: ss-2  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  }]
    Jan 23 14:57:52.863: INFO: 
    Jan 23 14:57:52.863: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 23 14:57:53.865: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
    Jan 23 14:57:53.865: INFO: ss-0  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:00 +0000 UTC  }]
    Jan 23 14:57:53.865: INFO: ss-2  node-worker-s8kfghkv.nova-ht9xu6tk2ptb.local  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-23 14:57:20 +0000 UTC  }]
    Jan 23 14:57:53.865: INFO: 
    Jan 23 14:57:53.865: INFO: StatefulSet ss has not reached scale 0, at 2
    Jan 23 14:57:54.868: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.991278032s
    Jan 23 14:57:55.870: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.988352434s
    Jan 23 14:57:56.873: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.985222097s
    Jan 23 14:57:57.875: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.98364823s
    Jan 23 14:57:58.879: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.980338817s
    Jan 23 14:57:59.882: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.977186096s
    Jan 23 14:58:00.884: INFO: Verifying statefulset ss doesn't scale past 0 for another 974.215876ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4678 01/23/24 14:58:01.884
    Jan 23 14:58:01.886: INFO: Scaling statefulset ss to 0
    Jan 23 14:58:01.891: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 23 14:58:01.893: INFO: Deleting all statefulset in ns statefulset-4678
    Jan 23 14:58:01.894: INFO: Scaling statefulset ss to 0
    Jan 23 14:58:01.899: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 23 14:58:01.900: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:58:01.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4678" for this suite. 01/23/24 14:58:01.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/23/24 14:58:01.92
Jan 23 14:58:01.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
STEP: Building a namespace api object, basename pod-network-test 01/23/24 14:58:01.921
STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:58:01.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:58:01.939
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-1123 01/23/24 14:58:01.941
STEP: creating a selector 01/23/24 14:58:01.941
STEP: Creating the service pods in kubernetes 01/23/24 14:58:01.941
Jan 23 14:58:01.941: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 23 14:58:01.977: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1123" to be "running and ready"
Jan 23 14:58:01.979: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.671297ms
Jan 23 14:58:01.979: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 23 14:58:03.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005249755s
Jan 23 14:58:03.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 14:58:05.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004743914s
Jan 23 14:58:05.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 14:58:07.983: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006131378s
Jan 23 14:58:07.983: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 14:58:09.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004772276s
Jan 23 14:58:09.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 14:58:11.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004740305s
Jan 23 14:58:11.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 23 14:58:13.981: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.004458803s
Jan 23 14:58:13.981: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 23 14:58:13.981: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 23 14:58:13.983: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1123" to be "running and ready"
Jan 23 14:58:13.984: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.332332ms
Jan 23 14:58:13.984: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 23 14:58:13.984: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/23/24 14:58:13.985
Jan 23 14:58:13.994: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1123" to be "running"
Jan 23 14:58:13.995: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.218023ms
Jan 23 14:58:15.998: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004032016s
Jan 23 14:58:15.998: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 23 14:58:15.999: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 23 14:58:15.999: INFO: Breadth first check of 10.233.75.183 on host 172.31.11.40...
Jan 23 14:58:16.000: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.87.232:9080/dial?request=hostname&protocol=http&host=10.233.75.183&port=8083&tries=1'] Namespace:pod-network-test-1123 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 14:58:16.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:58:16.001: INFO: ExecWithOptions: Clientset creation
Jan 23 14:58:16.001: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1123/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.87.232%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.75.183%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 23 14:58:16.056: INFO: Waiting for responses: map[]
Jan 23 14:58:16.056: INFO: reached 10.233.75.183 after 0/1 tries
Jan 23 14:58:16.056: INFO: Breadth first check of 10.233.87.240 on host 172.31.11.67...
Jan 23 14:58:16.058: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.87.232:9080/dial?request=hostname&protocol=http&host=10.233.87.240&port=8083&tries=1'] Namespace:pod-network-test-1123 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 23 14:58:16.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
Jan 23 14:58:16.059: INFO: ExecWithOptions: Clientset creation
Jan 23 14:58:16.059: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1123/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.87.232%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.87.240%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 23 14:58:16.122: INFO: Waiting for responses: map[]
Jan 23 14:58:16.122: INFO: reached 10.233.87.240 after 0/1 tries
Jan 23 14:58:16.122: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 23 14:58:16.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1123" for this suite. 01/23/24 14:58:16.125
------------------------------
• [SLOW TEST] [14.209 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/23/24 14:58:01.92
    Jan 23 14:58:01.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    STEP: Building a namespace api object, basename pod-network-test 01/23/24 14:58:01.921
    STEP: Waiting for a default service account to be provisioned in namespace 01/23/24 14:58:01.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/23/24 14:58:01.939
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-1123 01/23/24 14:58:01.941
    STEP: creating a selector 01/23/24 14:58:01.941
    STEP: Creating the service pods in kubernetes 01/23/24 14:58:01.941
    Jan 23 14:58:01.941: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 23 14:58:01.977: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1123" to be "running and ready"
    Jan 23 14:58:01.979: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.671297ms
    Jan 23 14:58:01.979: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 23 14:58:03.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005249755s
    Jan 23 14:58:03.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 14:58:05.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004743914s
    Jan 23 14:58:05.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 14:58:07.983: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006131378s
    Jan 23 14:58:07.983: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 14:58:09.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004772276s
    Jan 23 14:58:09.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 14:58:11.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004740305s
    Jan 23 14:58:11.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 23 14:58:13.981: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.004458803s
    Jan 23 14:58:13.981: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 23 14:58:13.981: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 23 14:58:13.983: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1123" to be "running and ready"
    Jan 23 14:58:13.984: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.332332ms
    Jan 23 14:58:13.984: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 23 14:58:13.984: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/23/24 14:58:13.985
    Jan 23 14:58:13.994: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1123" to be "running"
    Jan 23 14:58:13.995: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.218023ms
    Jan 23 14:58:15.998: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004032016s
    Jan 23 14:58:15.998: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 23 14:58:15.999: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 23 14:58:15.999: INFO: Breadth first check of 10.233.75.183 on host 172.31.11.40...
    Jan 23 14:58:16.000: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.87.232:9080/dial?request=hostname&protocol=http&host=10.233.75.183&port=8083&tries=1'] Namespace:pod-network-test-1123 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 14:58:16.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:58:16.001: INFO: ExecWithOptions: Clientset creation
    Jan 23 14:58:16.001: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1123/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.87.232%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.75.183%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 23 14:58:16.056: INFO: Waiting for responses: map[]
    Jan 23 14:58:16.056: INFO: reached 10.233.75.183 after 0/1 tries
    Jan 23 14:58:16.056: INFO: Breadth first check of 10.233.87.240 on host 172.31.11.67...
    Jan 23 14:58:16.058: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.87.232:9080/dial?request=hostname&protocol=http&host=10.233.87.240&port=8083&tries=1'] Namespace:pod-network-test-1123 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 23 14:58:16.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1347667206
    Jan 23 14:58:16.059: INFO: ExecWithOptions: Clientset creation
    Jan 23 14:58:16.059: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1123/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.87.232%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.87.240%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 23 14:58:16.122: INFO: Waiting for responses: map[]
    Jan 23 14:58:16.122: INFO: reached 10.233.87.240 after 0/1 tries
    Jan 23 14:58:16.122: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 23 14:58:16.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1123" for this suite. 01/23/24 14:58:16.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Jan 23 14:58:16.131: INFO: Running AfterSuite actions on node 1
Jan 23 14:58:16.131: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Jan 23 14:58:16.131: INFO: Running AfterSuite actions on node 1
    Jan 23 14:58:16.131: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.077 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5810.492 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h36m51.625861771s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

