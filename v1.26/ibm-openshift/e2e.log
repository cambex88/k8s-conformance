I1115 05:52:22.323456      22 e2e.go:126] Starting e2e run "ecce4ee9-09c9-4c3e-878b-b9821315fe37" on Ginkgo node 1
Nov 15 05:52:22.336: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1700027542 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Nov 15 05:52:22.464: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 05:52:22.466: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E1115 05:52:22.466257      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Nov 15 05:52:22.536: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 15 05:52:22.629: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 15 05:52:22.629: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Nov 15 05:52:22.629: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Nov 15 05:52:22.643: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Nov 15 05:52:22.643: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Nov 15 05:52:22.643: INFO: e2e test version: v1.26.9
Nov 15 05:52:22.649: INFO: kube-apiserver version: v1.26.9+636f2be
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Nov 15 05:52:22.650: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 05:52:22.671: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.207 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Nov 15 05:52:22.464: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 05:52:22.466: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E1115 05:52:22.466257      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Nov 15 05:52:22.536: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Nov 15 05:52:22.629: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Nov 15 05:52:22.629: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
    Nov 15 05:52:22.629: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Nov 15 05:52:22.643: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
    Nov 15 05:52:22.643: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
    Nov 15 05:52:22.643: INFO: e2e test version: v1.26.9
    Nov 15 05:52:22.649: INFO: kube-apiserver version: v1.26.9+636f2be
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Nov 15 05:52:22.650: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 05:52:22.671: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:52:22.699
Nov 15 05:52:22.699: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename certificates 11/15/23 05:52:22.699
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:52:22.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:52:22.765
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 11/15/23 05:52:23.319
STEP: getting /apis/certificates.k8s.io 11/15/23 05:52:23.334
STEP: getting /apis/certificates.k8s.io/v1 11/15/23 05:52:23.341
STEP: creating 11/15/23 05:52:23.351
STEP: getting 11/15/23 05:52:23.422
STEP: listing 11/15/23 05:52:23.442
STEP: watching 11/15/23 05:52:23.464
Nov 15 05:52:23.465: INFO: starting watch
STEP: patching 11/15/23 05:52:23.472
STEP: updating 11/15/23 05:52:23.489
Nov 15 05:52:23.507: INFO: waiting for watch events with expected annotations
Nov 15 05:52:23.507: INFO: saw patched and updated annotations
STEP: getting /approval 11/15/23 05:52:23.507
STEP: patching /approval 11/15/23 05:52:23.528
STEP: updating /approval 11/15/23 05:52:23.548
STEP: getting /status 11/15/23 05:52:23.569
STEP: patching /status 11/15/23 05:52:23.59
STEP: updating /status 11/15/23 05:52:23.611
STEP: deleting 11/15/23 05:52:23.634
STEP: deleting a collection 11/15/23 05:52:23.698
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 05:52:23.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-4354" for this suite. 11/15/23 05:52:23.841
------------------------------
â€¢ [1.173 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:52:22.699
    Nov 15 05:52:22.699: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename certificates 11/15/23 05:52:22.699
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:52:22.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:52:22.765
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 11/15/23 05:52:23.319
    STEP: getting /apis/certificates.k8s.io 11/15/23 05:52:23.334
    STEP: getting /apis/certificates.k8s.io/v1 11/15/23 05:52:23.341
    STEP: creating 11/15/23 05:52:23.351
    STEP: getting 11/15/23 05:52:23.422
    STEP: listing 11/15/23 05:52:23.442
    STEP: watching 11/15/23 05:52:23.464
    Nov 15 05:52:23.465: INFO: starting watch
    STEP: patching 11/15/23 05:52:23.472
    STEP: updating 11/15/23 05:52:23.489
    Nov 15 05:52:23.507: INFO: waiting for watch events with expected annotations
    Nov 15 05:52:23.507: INFO: saw patched and updated annotations
    STEP: getting /approval 11/15/23 05:52:23.507
    STEP: patching /approval 11/15/23 05:52:23.528
    STEP: updating /approval 11/15/23 05:52:23.548
    STEP: getting /status 11/15/23 05:52:23.569
    STEP: patching /status 11/15/23 05:52:23.59
    STEP: updating /status 11/15/23 05:52:23.611
    STEP: deleting 11/15/23 05:52:23.634
    STEP: deleting a collection 11/15/23 05:52:23.698
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:52:23.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-4354" for this suite. 11/15/23 05:52:23.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:52:23.874
Nov 15 05:52:23.874: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubectl 11/15/23 05:52:23.875
E1115 05:52:23.876308      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:52:23.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:52:24.046
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 11/15/23 05:52:24.067
Nov 15 05:52:24.067: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4467 proxy --unix-socket=/tmp/kubectl-proxy-unix1986802233/test'
STEP: retrieving proxy /api/ output 11/15/23 05:52:24.153
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 05:52:24.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4467" for this suite. 11/15/23 05:52:24.189
------------------------------
â€¢ [0.343 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:52:23.874
    Nov 15 05:52:23.874: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubectl 11/15/23 05:52:23.875
    E1115 05:52:23.876308      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:52:23.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:52:24.046
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 11/15/23 05:52:24.067
    Nov 15 05:52:24.067: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4467 proxy --unix-socket=/tmp/kubectl-proxy-unix1986802233/test'
    STEP: retrieving proxy /api/ output 11/15/23 05:52:24.153
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:52:24.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4467" for this suite. 11/15/23 05:52:24.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:52:24.217
Nov 15 05:52:24.217: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
E1115 05:52:24.219477      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
STEP: Building a namespace api object, basename downward-api 11/15/23 05:52:24.22
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:52:24.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:52:24.312
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 11/15/23 05:52:24.328
Nov 15 05:52:24.369: INFO: Waiting up to 5m0s for pod "downward-api-7e09f579-6199-4e86-9e00-06f92ba56d4b" in namespace "downward-api-2909" to be "Succeeded or Failed"
Nov 15 05:52:24.392: INFO: Pod "downward-api-7e09f579-6199-4e86-9e00-06f92ba56d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 23.724645ms
Nov 15 05:52:26.413: INFO: Pod "downward-api-7e09f579-6199-4e86-9e00-06f92ba56d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044154169s
Nov 15 05:52:28.510: INFO: Pod "downward-api-7e09f579-6199-4e86-9e00-06f92ba56d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.141667237s
Nov 15 05:52:30.413: INFO: Pod "downward-api-7e09f579-6199-4e86-9e00-06f92ba56d4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044681743s
STEP: Saw pod success 11/15/23 05:52:30.413
Nov 15 05:52:30.414: INFO: Pod "downward-api-7e09f579-6199-4e86-9e00-06f92ba56d4b" satisfied condition "Succeeded or Failed"
Nov 15 05:52:30.432: INFO: Trying to get logs from node 10.72.152.81 pod downward-api-7e09f579-6199-4e86-9e00-06f92ba56d4b container dapi-container: <nil>
STEP: delete the pod 11/15/23 05:52:30.518
Nov 15 05:52:30.576: INFO: Waiting for pod downward-api-7e09f579-6199-4e86-9e00-06f92ba56d4b to disappear
Nov 15 05:52:30.602: INFO: Pod downward-api-7e09f579-6199-4e86-9e00-06f92ba56d4b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Nov 15 05:52:30.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2909" for this suite. 11/15/23 05:52:30.632
------------------------------
â€¢ [SLOW TEST] [6.444 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:52:24.217
    Nov 15 05:52:24.217: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    E1115 05:52:24.219477      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    STEP: Building a namespace api object, basename downward-api 11/15/23 05:52:24.22
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:52:24.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:52:24.312
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 11/15/23 05:52:24.328
    Nov 15 05:52:24.369: INFO: Waiting up to 5m0s for pod "downward-api-7e09f579-6199-4e86-9e00-06f92ba56d4b" in namespace "downward-api-2909" to be "Succeeded or Failed"
    Nov 15 05:52:24.392: INFO: Pod "downward-api-7e09f579-6199-4e86-9e00-06f92ba56d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 23.724645ms
    Nov 15 05:52:26.413: INFO: Pod "downward-api-7e09f579-6199-4e86-9e00-06f92ba56d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044154169s
    Nov 15 05:52:28.510: INFO: Pod "downward-api-7e09f579-6199-4e86-9e00-06f92ba56d4b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.141667237s
    Nov 15 05:52:30.413: INFO: Pod "downward-api-7e09f579-6199-4e86-9e00-06f92ba56d4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044681743s
    STEP: Saw pod success 11/15/23 05:52:30.413
    Nov 15 05:52:30.414: INFO: Pod "downward-api-7e09f579-6199-4e86-9e00-06f92ba56d4b" satisfied condition "Succeeded or Failed"
    Nov 15 05:52:30.432: INFO: Trying to get logs from node 10.72.152.81 pod downward-api-7e09f579-6199-4e86-9e00-06f92ba56d4b container dapi-container: <nil>
    STEP: delete the pod 11/15/23 05:52:30.518
    Nov 15 05:52:30.576: INFO: Waiting for pod downward-api-7e09f579-6199-4e86-9e00-06f92ba56d4b to disappear
    Nov 15 05:52:30.602: INFO: Pod downward-api-7e09f579-6199-4e86-9e00-06f92ba56d4b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:52:30.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2909" for this suite. 11/15/23 05:52:30.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:52:30.672
Nov 15 05:52:30.673: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename webhook 11/15/23 05:52:30.674
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:52:30.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:52:30.747
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 05:52:30.819
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 05:52:30.976
STEP: Deploying the webhook pod 11/15/23 05:52:31.029
STEP: Wait for the deployment to be ready 11/15/23 05:52:31.065
Nov 15 05:52:31.104: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 05:52:33.153: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 5, 52, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 5, 52, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 5, 52, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 5, 52, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 05:52:35.168
STEP: Verifying the service has paired with the endpoint 11/15/23 05:52:35.206
Nov 15 05:52:36.207: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 11/15/23 05:52:36.225
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 11/15/23 05:52:36.294
STEP: Creating a dummy validating-webhook-configuration object 11/15/23 05:52:36.348
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 11/15/23 05:52:36.396
STEP: Creating a dummy mutating-webhook-configuration object 11/15/23 05:52:36.432
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 11/15/23 05:52:36.47
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 05:52:36.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1407" for this suite. 11/15/23 05:52:36.725
STEP: Destroying namespace "webhook-1407-markers" for this suite. 11/15/23 05:52:36.762
------------------------------
â€¢ [SLOW TEST] [6.119 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:52:30.672
    Nov 15 05:52:30.673: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename webhook 11/15/23 05:52:30.674
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:52:30.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:52:30.747
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 05:52:30.819
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 05:52:30.976
    STEP: Deploying the webhook pod 11/15/23 05:52:31.029
    STEP: Wait for the deployment to be ready 11/15/23 05:52:31.065
    Nov 15 05:52:31.104: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 05:52:33.153: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 5, 52, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 5, 52, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 5, 52, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 5, 52, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 05:52:35.168
    STEP: Verifying the service has paired with the endpoint 11/15/23 05:52:35.206
    Nov 15 05:52:36.207: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 11/15/23 05:52:36.225
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 11/15/23 05:52:36.294
    STEP: Creating a dummy validating-webhook-configuration object 11/15/23 05:52:36.348
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 11/15/23 05:52:36.396
    STEP: Creating a dummy mutating-webhook-configuration object 11/15/23 05:52:36.432
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 11/15/23 05:52:36.47
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:52:36.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1407" for this suite. 11/15/23 05:52:36.725
    STEP: Destroying namespace "webhook-1407-markers" for this suite. 11/15/23 05:52:36.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:52:36.796
Nov 15 05:52:36.796: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename statefulset 11/15/23 05:52:36.797
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:52:36.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:52:36.864
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-477 11/15/23 05:52:36.877
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-477 11/15/23 05:52:36.908
Nov 15 05:52:36.953: INFO: Found 0 stateful pods, waiting for 1
Nov 15 05:52:46.982: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 11/15/23 05:52:47.052
STEP: updating a scale subresource 11/15/23 05:52:47.085
STEP: verifying the statefulset Spec.Replicas was modified 11/15/23 05:52:47.132
STEP: Patch a scale subresource 11/15/23 05:52:47.183
STEP: verifying the statefulset Spec.Replicas was modified 11/15/23 05:52:47.293
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 15 05:52:47.319: INFO: Deleting all statefulset in ns statefulset-477
Nov 15 05:52:47.335: INFO: Scaling statefulset ss to 0
Nov 15 05:52:57.421: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 05:52:57.444: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 15 05:52:57.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-477" for this suite. 11/15/23 05:52:57.541
------------------------------
â€¢ [SLOW TEST] [20.774 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:52:36.796
    Nov 15 05:52:36.796: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename statefulset 11/15/23 05:52:36.797
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:52:36.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:52:36.864
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-477 11/15/23 05:52:36.877
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-477 11/15/23 05:52:36.908
    Nov 15 05:52:36.953: INFO: Found 0 stateful pods, waiting for 1
    Nov 15 05:52:46.982: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 11/15/23 05:52:47.052
    STEP: updating a scale subresource 11/15/23 05:52:47.085
    STEP: verifying the statefulset Spec.Replicas was modified 11/15/23 05:52:47.132
    STEP: Patch a scale subresource 11/15/23 05:52:47.183
    STEP: verifying the statefulset Spec.Replicas was modified 11/15/23 05:52:47.293
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 15 05:52:47.319: INFO: Deleting all statefulset in ns statefulset-477
    Nov 15 05:52:47.335: INFO: Scaling statefulset ss to 0
    Nov 15 05:52:57.421: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 05:52:57.444: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:52:57.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-477" for this suite. 11/15/23 05:52:57.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:52:57.57
Nov 15 05:52:57.570: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename services 11/15/23 05:52:57.571
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:52:57.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:52:57.64
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 05:52:57.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5927" for this suite. 11/15/23 05:52:57.702
------------------------------
â€¢ [0.161 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:52:57.57
    Nov 15 05:52:57.570: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename services 11/15/23 05:52:57.571
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:52:57.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:52:57.64
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:52:57.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5927" for this suite. 11/15/23 05:52:57.702
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:52:57.731
Nov 15 05:52:57.731: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 05:52:57.732
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:52:57.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:52:57.796
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 11/15/23 05:52:57.81
Nov 15 05:52:57.851: INFO: Waiting up to 5m0s for pod "labelsupdate78c20573-5cac-4ad5-be12-a0e684acc104" in namespace "projected-948" to be "running and ready"
Nov 15 05:52:57.868: INFO: Pod "labelsupdate78c20573-5cac-4ad5-be12-a0e684acc104": Phase="Pending", Reason="", readiness=false. Elapsed: 17.240472ms
Nov 15 05:52:57.868: INFO: The phase of Pod labelsupdate78c20573-5cac-4ad5-be12-a0e684acc104 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 05:52:59.895: INFO: Pod "labelsupdate78c20573-5cac-4ad5-be12-a0e684acc104": Phase="Running", Reason="", readiness=true. Elapsed: 2.043453571s
Nov 15 05:52:59.895: INFO: The phase of Pod labelsupdate78c20573-5cac-4ad5-be12-a0e684acc104 is Running (Ready = true)
Nov 15 05:52:59.895: INFO: Pod "labelsupdate78c20573-5cac-4ad5-be12-a0e684acc104" satisfied condition "running and ready"
Nov 15 05:53:00.547: INFO: Successfully updated pod "labelsupdate78c20573-5cac-4ad5-be12-a0e684acc104"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 05:53:02.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-948" for this suite. 11/15/23 05:53:02.663
------------------------------
â€¢ [4.958 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:52:57.731
    Nov 15 05:52:57.731: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 05:52:57.732
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:52:57.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:52:57.796
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 11/15/23 05:52:57.81
    Nov 15 05:52:57.851: INFO: Waiting up to 5m0s for pod "labelsupdate78c20573-5cac-4ad5-be12-a0e684acc104" in namespace "projected-948" to be "running and ready"
    Nov 15 05:52:57.868: INFO: Pod "labelsupdate78c20573-5cac-4ad5-be12-a0e684acc104": Phase="Pending", Reason="", readiness=false. Elapsed: 17.240472ms
    Nov 15 05:52:57.868: INFO: The phase of Pod labelsupdate78c20573-5cac-4ad5-be12-a0e684acc104 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 05:52:59.895: INFO: Pod "labelsupdate78c20573-5cac-4ad5-be12-a0e684acc104": Phase="Running", Reason="", readiness=true. Elapsed: 2.043453571s
    Nov 15 05:52:59.895: INFO: The phase of Pod labelsupdate78c20573-5cac-4ad5-be12-a0e684acc104 is Running (Ready = true)
    Nov 15 05:52:59.895: INFO: Pod "labelsupdate78c20573-5cac-4ad5-be12-a0e684acc104" satisfied condition "running and ready"
    Nov 15 05:53:00.547: INFO: Successfully updated pod "labelsupdate78c20573-5cac-4ad5-be12-a0e684acc104"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:53:02.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-948" for this suite. 11/15/23 05:53:02.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:53:02.693
Nov 15 05:53:02.693: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename controllerrevisions 11/15/23 05:53:02.694
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:53:02.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:53:02.771
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-2b64z-daemon-set" 11/15/23 05:53:02.899
STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 05:53:02.919
Nov 15 05:53:02.962: INFO: Number of nodes with available pods controlled by daemonset e2e-2b64z-daemon-set: 0
Nov 15 05:53:02.962: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 05:53:04.017: INFO: Number of nodes with available pods controlled by daemonset e2e-2b64z-daemon-set: 0
Nov 15 05:53:04.017: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 05:53:05.021: INFO: Number of nodes with available pods controlled by daemonset e2e-2b64z-daemon-set: 1
Nov 15 05:53:05.021: INFO: Node 10.72.152.86 is running 0 daemon pod, expected 1
Nov 15 05:53:06.041: INFO: Number of nodes with available pods controlled by daemonset e2e-2b64z-daemon-set: 2
Nov 15 05:53:06.041: INFO: Node 10.72.152.88 is running 0 daemon pod, expected 1
Nov 15 05:53:07.021: INFO: Number of nodes with available pods controlled by daemonset e2e-2b64z-daemon-set: 2
Nov 15 05:53:07.021: INFO: Node 10.72.152.88 is running 0 daemon pod, expected 1
Nov 15 05:53:08.031: INFO: Number of nodes with available pods controlled by daemonset e2e-2b64z-daemon-set: 2
Nov 15 05:53:08.032: INFO: Node 10.72.152.88 is running 0 daemon pod, expected 1
Nov 15 05:53:09.013: INFO: Number of nodes with available pods controlled by daemonset e2e-2b64z-daemon-set: 3
Nov 15 05:53:09.013: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-2b64z-daemon-set
STEP: Confirm DaemonSet "e2e-2b64z-daemon-set" successfully created with "daemonset-name=e2e-2b64z-daemon-set" label 11/15/23 05:53:09.026
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-2b64z-daemon-set" 11/15/23 05:53:09.054
Nov 15 05:53:09.070: INFO: Located ControllerRevision: "e2e-2b64z-daemon-set-59b75c5d97"
STEP: Patching ControllerRevision "e2e-2b64z-daemon-set-59b75c5d97" 11/15/23 05:53:09.082
Nov 15 05:53:09.101: INFO: e2e-2b64z-daemon-set-59b75c5d97 has been patched
STEP: Create a new ControllerRevision 11/15/23 05:53:09.101
Nov 15 05:53:09.120: INFO: Created ControllerRevision: e2e-2b64z-daemon-set-7858999c45
STEP: Confirm that there are two ControllerRevisions 11/15/23 05:53:09.12
Nov 15 05:53:09.120: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 15 05:53:09.133: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-2b64z-daemon-set-59b75c5d97" 11/15/23 05:53:09.133
STEP: Confirm that there is only one ControllerRevision 11/15/23 05:53:09.154
Nov 15 05:53:09.154: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 15 05:53:09.167: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-2b64z-daemon-set-7858999c45" 11/15/23 05:53:09.18
Nov 15 05:53:09.214: INFO: e2e-2b64z-daemon-set-7858999c45 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 11/15/23 05:53:09.214
W1115 05:53:09.232667      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 11/15/23 05:53:09.232
Nov 15 05:53:09.232: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 15 05:53:10.246: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 15 05:53:10.264: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-2b64z-daemon-set-7858999c45=updated" 11/15/23 05:53:10.264
STEP: Confirm that there is only one ControllerRevision 11/15/23 05:53:10.283
Nov 15 05:53:10.283: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 15 05:53:10.296: INFO: Found 1 ControllerRevisions
Nov 15 05:53:10.308: INFO: ControllerRevision "e2e-2b64z-daemon-set-5c987649b8" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-2b64z-daemon-set" 11/15/23 05:53:10.32
STEP: deleting DaemonSet.extensions e2e-2b64z-daemon-set in namespace controllerrevisions-6978, will wait for the garbage collector to delete the pods 11/15/23 05:53:10.32
Nov 15 05:53:10.403: INFO: Deleting DaemonSet.extensions e2e-2b64z-daemon-set took: 18.861283ms
Nov 15 05:53:10.503: INFO: Terminating DaemonSet.extensions e2e-2b64z-daemon-set pods took: 100.614225ms
Nov 15 05:53:12.323: INFO: Number of nodes with available pods controlled by daemonset e2e-2b64z-daemon-set: 0
Nov 15 05:53:12.323: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-2b64z-daemon-set
Nov 15 05:53:12.337: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"60376"},"items":null}

Nov 15 05:53:12.356: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"60376"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 05:53:12.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-6978" for this suite. 11/15/23 05:53:12.474
------------------------------
â€¢ [SLOW TEST] [9.810 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:53:02.693
    Nov 15 05:53:02.693: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename controllerrevisions 11/15/23 05:53:02.694
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:53:02.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:53:02.771
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-2b64z-daemon-set" 11/15/23 05:53:02.899
    STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 05:53:02.919
    Nov 15 05:53:02.962: INFO: Number of nodes with available pods controlled by daemonset e2e-2b64z-daemon-set: 0
    Nov 15 05:53:02.962: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 05:53:04.017: INFO: Number of nodes with available pods controlled by daemonset e2e-2b64z-daemon-set: 0
    Nov 15 05:53:04.017: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 05:53:05.021: INFO: Number of nodes with available pods controlled by daemonset e2e-2b64z-daemon-set: 1
    Nov 15 05:53:05.021: INFO: Node 10.72.152.86 is running 0 daemon pod, expected 1
    Nov 15 05:53:06.041: INFO: Number of nodes with available pods controlled by daemonset e2e-2b64z-daemon-set: 2
    Nov 15 05:53:06.041: INFO: Node 10.72.152.88 is running 0 daemon pod, expected 1
    Nov 15 05:53:07.021: INFO: Number of nodes with available pods controlled by daemonset e2e-2b64z-daemon-set: 2
    Nov 15 05:53:07.021: INFO: Node 10.72.152.88 is running 0 daemon pod, expected 1
    Nov 15 05:53:08.031: INFO: Number of nodes with available pods controlled by daemonset e2e-2b64z-daemon-set: 2
    Nov 15 05:53:08.032: INFO: Node 10.72.152.88 is running 0 daemon pod, expected 1
    Nov 15 05:53:09.013: INFO: Number of nodes with available pods controlled by daemonset e2e-2b64z-daemon-set: 3
    Nov 15 05:53:09.013: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-2b64z-daemon-set
    STEP: Confirm DaemonSet "e2e-2b64z-daemon-set" successfully created with "daemonset-name=e2e-2b64z-daemon-set" label 11/15/23 05:53:09.026
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-2b64z-daemon-set" 11/15/23 05:53:09.054
    Nov 15 05:53:09.070: INFO: Located ControllerRevision: "e2e-2b64z-daemon-set-59b75c5d97"
    STEP: Patching ControllerRevision "e2e-2b64z-daemon-set-59b75c5d97" 11/15/23 05:53:09.082
    Nov 15 05:53:09.101: INFO: e2e-2b64z-daemon-set-59b75c5d97 has been patched
    STEP: Create a new ControllerRevision 11/15/23 05:53:09.101
    Nov 15 05:53:09.120: INFO: Created ControllerRevision: e2e-2b64z-daemon-set-7858999c45
    STEP: Confirm that there are two ControllerRevisions 11/15/23 05:53:09.12
    Nov 15 05:53:09.120: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 15 05:53:09.133: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-2b64z-daemon-set-59b75c5d97" 11/15/23 05:53:09.133
    STEP: Confirm that there is only one ControllerRevision 11/15/23 05:53:09.154
    Nov 15 05:53:09.154: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 15 05:53:09.167: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-2b64z-daemon-set-7858999c45" 11/15/23 05:53:09.18
    Nov 15 05:53:09.214: INFO: e2e-2b64z-daemon-set-7858999c45 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 11/15/23 05:53:09.214
    W1115 05:53:09.232667      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 11/15/23 05:53:09.232
    Nov 15 05:53:09.232: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 15 05:53:10.246: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 15 05:53:10.264: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-2b64z-daemon-set-7858999c45=updated" 11/15/23 05:53:10.264
    STEP: Confirm that there is only one ControllerRevision 11/15/23 05:53:10.283
    Nov 15 05:53:10.283: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 15 05:53:10.296: INFO: Found 1 ControllerRevisions
    Nov 15 05:53:10.308: INFO: ControllerRevision "e2e-2b64z-daemon-set-5c987649b8" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-2b64z-daemon-set" 11/15/23 05:53:10.32
    STEP: deleting DaemonSet.extensions e2e-2b64z-daemon-set in namespace controllerrevisions-6978, will wait for the garbage collector to delete the pods 11/15/23 05:53:10.32
    Nov 15 05:53:10.403: INFO: Deleting DaemonSet.extensions e2e-2b64z-daemon-set took: 18.861283ms
    Nov 15 05:53:10.503: INFO: Terminating DaemonSet.extensions e2e-2b64z-daemon-set pods took: 100.614225ms
    Nov 15 05:53:12.323: INFO: Number of nodes with available pods controlled by daemonset e2e-2b64z-daemon-set: 0
    Nov 15 05:53:12.323: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-2b64z-daemon-set
    Nov 15 05:53:12.337: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"60376"},"items":null}

    Nov 15 05:53:12.356: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"60376"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:53:12.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-6978" for this suite. 11/15/23 05:53:12.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:53:12.504
Nov 15 05:53:12.504: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename watch 11/15/23 05:53:12.505
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:53:12.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:53:12.581
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 11/15/23 05:53:12.595
STEP: creating a watch on configmaps with label B 11/15/23 05:53:12.602
STEP: creating a watch on configmaps with label A or B 11/15/23 05:53:12.609
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 11/15/23 05:53:12.621
Nov 15 05:53:12.642: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6779  d33c9987-e1ee-4ac8-a346-5a0faba0e310 60399 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 05:53:12.642: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6779  d33c9987-e1ee-4ac8-a346-5a0faba0e310 60399 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 11/15/23 05:53:12.642
Nov 15 05:53:12.687: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6779  d33c9987-e1ee-4ac8-a346-5a0faba0e310 60407 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 05:53:12.687: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6779  d33c9987-e1ee-4ac8-a346-5a0faba0e310 60407 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 11/15/23 05:53:12.687
Nov 15 05:53:12.727: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6779  d33c9987-e1ee-4ac8-a346-5a0faba0e310 60410 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 05:53:12.727: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6779  d33c9987-e1ee-4ac8-a346-5a0faba0e310 60410 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 11/15/23 05:53:12.727
Nov 15 05:53:12.766: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6779  d33c9987-e1ee-4ac8-a346-5a0faba0e310 60411 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 05:53:12.766: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6779  d33c9987-e1ee-4ac8-a346-5a0faba0e310 60411 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 11/15/23 05:53:12.766
Nov 15 05:53:12.786: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6779  067ebbb4-73d4-4c1b-acc5-29aa3206e72e 60412 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 05:53:12.786: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6779  067ebbb4-73d4-4c1b-acc5-29aa3206e72e 60412 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 11/15/23 05:53:22.787
Nov 15 05:53:22.819: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6779  067ebbb4-73d4-4c1b-acc5-29aa3206e72e 60508 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 05:53:22.819: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6779  067ebbb4-73d4-4c1b-acc5-29aa3206e72e 60508 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Nov 15 05:53:32.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6779" for this suite. 11/15/23 05:53:32.851
------------------------------
â€¢ [SLOW TEST] [20.374 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:53:12.504
    Nov 15 05:53:12.504: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename watch 11/15/23 05:53:12.505
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:53:12.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:53:12.581
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 11/15/23 05:53:12.595
    STEP: creating a watch on configmaps with label B 11/15/23 05:53:12.602
    STEP: creating a watch on configmaps with label A or B 11/15/23 05:53:12.609
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 11/15/23 05:53:12.621
    Nov 15 05:53:12.642: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6779  d33c9987-e1ee-4ac8-a346-5a0faba0e310 60399 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 05:53:12.642: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6779  d33c9987-e1ee-4ac8-a346-5a0faba0e310 60399 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 11/15/23 05:53:12.642
    Nov 15 05:53:12.687: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6779  d33c9987-e1ee-4ac8-a346-5a0faba0e310 60407 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 05:53:12.687: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6779  d33c9987-e1ee-4ac8-a346-5a0faba0e310 60407 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 11/15/23 05:53:12.687
    Nov 15 05:53:12.727: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6779  d33c9987-e1ee-4ac8-a346-5a0faba0e310 60410 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 05:53:12.727: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6779  d33c9987-e1ee-4ac8-a346-5a0faba0e310 60410 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 11/15/23 05:53:12.727
    Nov 15 05:53:12.766: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6779  d33c9987-e1ee-4ac8-a346-5a0faba0e310 60411 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 05:53:12.766: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6779  d33c9987-e1ee-4ac8-a346-5a0faba0e310 60411 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 11/15/23 05:53:12.766
    Nov 15 05:53:12.786: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6779  067ebbb4-73d4-4c1b-acc5-29aa3206e72e 60412 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 05:53:12.786: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6779  067ebbb4-73d4-4c1b-acc5-29aa3206e72e 60412 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 11/15/23 05:53:22.787
    Nov 15 05:53:22.819: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6779  067ebbb4-73d4-4c1b-acc5-29aa3206e72e 60508 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 05:53:22.819: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6779  067ebbb4-73d4-4c1b-acc5-29aa3206e72e 60508 0 2023-11-15 05:53:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-15 05:53:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:53:32.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6779" for this suite. 11/15/23 05:53:32.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:53:32.878
Nov 15 05:53:32.878: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename var-expansion 11/15/23 05:53:32.879
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:53:32.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:53:32.953
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Nov 15 05:53:33.002: INFO: Waiting up to 2m0s for pod "var-expansion-5a4b46ae-ad9a-4569-ad02-37db9e5a1d95" in namespace "var-expansion-691" to be "container 0 failed with reason CreateContainerConfigError"
Nov 15 05:53:33.022: INFO: Pod "var-expansion-5a4b46ae-ad9a-4569-ad02-37db9e5a1d95": Phase="Pending", Reason="", readiness=false. Elapsed: 19.224958ms
Nov 15 05:53:35.042: INFO: Pod "var-expansion-5a4b46ae-ad9a-4569-ad02-37db9e5a1d95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039265887s
Nov 15 05:53:35.042: INFO: Pod "var-expansion-5a4b46ae-ad9a-4569-ad02-37db9e5a1d95" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Nov 15 05:53:35.042: INFO: Deleting pod "var-expansion-5a4b46ae-ad9a-4569-ad02-37db9e5a1d95" in namespace "var-expansion-691"
Nov 15 05:53:35.076: INFO: Wait up to 5m0s for pod "var-expansion-5a4b46ae-ad9a-4569-ad02-37db9e5a1d95" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 15 05:53:39.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-691" for this suite. 11/15/23 05:53:39.152
------------------------------
â€¢ [SLOW TEST] [6.302 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:53:32.878
    Nov 15 05:53:32.878: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename var-expansion 11/15/23 05:53:32.879
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:53:32.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:53:32.953
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Nov 15 05:53:33.002: INFO: Waiting up to 2m0s for pod "var-expansion-5a4b46ae-ad9a-4569-ad02-37db9e5a1d95" in namespace "var-expansion-691" to be "container 0 failed with reason CreateContainerConfigError"
    Nov 15 05:53:33.022: INFO: Pod "var-expansion-5a4b46ae-ad9a-4569-ad02-37db9e5a1d95": Phase="Pending", Reason="", readiness=false. Elapsed: 19.224958ms
    Nov 15 05:53:35.042: INFO: Pod "var-expansion-5a4b46ae-ad9a-4569-ad02-37db9e5a1d95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039265887s
    Nov 15 05:53:35.042: INFO: Pod "var-expansion-5a4b46ae-ad9a-4569-ad02-37db9e5a1d95" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Nov 15 05:53:35.042: INFO: Deleting pod "var-expansion-5a4b46ae-ad9a-4569-ad02-37db9e5a1d95" in namespace "var-expansion-691"
    Nov 15 05:53:35.076: INFO: Wait up to 5m0s for pod "var-expansion-5a4b46ae-ad9a-4569-ad02-37db9e5a1d95" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:53:39.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-691" for this suite. 11/15/23 05:53:39.152
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:53:39.182
Nov 15 05:53:39.182: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename configmap 11/15/23 05:53:39.183
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:53:39.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:53:39.257
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-72bf5ea9-a931-4cfa-b7c9-a794f4f736aa 11/15/23 05:53:39.273
STEP: Creating a pod to test consume configMaps 11/15/23 05:53:39.293
Nov 15 05:53:39.332: INFO: Waiting up to 5m0s for pod "pod-configmaps-d9549c49-6f6b-4a2c-843b-15bda5575551" in namespace "configmap-2207" to be "Succeeded or Failed"
Nov 15 05:53:39.355: INFO: Pod "pod-configmaps-d9549c49-6f6b-4a2c-843b-15bda5575551": Phase="Pending", Reason="", readiness=false. Elapsed: 22.583237ms
Nov 15 05:53:41.376: INFO: Pod "pod-configmaps-d9549c49-6f6b-4a2c-843b-15bda5575551": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043545609s
Nov 15 05:53:43.387: INFO: Pod "pod-configmaps-d9549c49-6f6b-4a2c-843b-15bda5575551": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054972524s
STEP: Saw pod success 11/15/23 05:53:43.387
Nov 15 05:53:43.387: INFO: Pod "pod-configmaps-d9549c49-6f6b-4a2c-843b-15bda5575551" satisfied condition "Succeeded or Failed"
Nov 15 05:53:43.406: INFO: Trying to get logs from node 10.72.152.81 pod pod-configmaps-d9549c49-6f6b-4a2c-843b-15bda5575551 container configmap-volume-test: <nil>
STEP: delete the pod 11/15/23 05:53:43.453
Nov 15 05:53:43.511: INFO: Waiting for pod pod-configmaps-d9549c49-6f6b-4a2c-843b-15bda5575551 to disappear
Nov 15 05:53:43.528: INFO: Pod pod-configmaps-d9549c49-6f6b-4a2c-843b-15bda5575551 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 05:53:43.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2207" for this suite. 11/15/23 05:53:43.563
------------------------------
â€¢ [4.409 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:53:39.182
    Nov 15 05:53:39.182: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename configmap 11/15/23 05:53:39.183
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:53:39.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:53:39.257
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-72bf5ea9-a931-4cfa-b7c9-a794f4f736aa 11/15/23 05:53:39.273
    STEP: Creating a pod to test consume configMaps 11/15/23 05:53:39.293
    Nov 15 05:53:39.332: INFO: Waiting up to 5m0s for pod "pod-configmaps-d9549c49-6f6b-4a2c-843b-15bda5575551" in namespace "configmap-2207" to be "Succeeded or Failed"
    Nov 15 05:53:39.355: INFO: Pod "pod-configmaps-d9549c49-6f6b-4a2c-843b-15bda5575551": Phase="Pending", Reason="", readiness=false. Elapsed: 22.583237ms
    Nov 15 05:53:41.376: INFO: Pod "pod-configmaps-d9549c49-6f6b-4a2c-843b-15bda5575551": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043545609s
    Nov 15 05:53:43.387: INFO: Pod "pod-configmaps-d9549c49-6f6b-4a2c-843b-15bda5575551": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054972524s
    STEP: Saw pod success 11/15/23 05:53:43.387
    Nov 15 05:53:43.387: INFO: Pod "pod-configmaps-d9549c49-6f6b-4a2c-843b-15bda5575551" satisfied condition "Succeeded or Failed"
    Nov 15 05:53:43.406: INFO: Trying to get logs from node 10.72.152.81 pod pod-configmaps-d9549c49-6f6b-4a2c-843b-15bda5575551 container configmap-volume-test: <nil>
    STEP: delete the pod 11/15/23 05:53:43.453
    Nov 15 05:53:43.511: INFO: Waiting for pod pod-configmaps-d9549c49-6f6b-4a2c-843b-15bda5575551 to disappear
    Nov 15 05:53:43.528: INFO: Pod pod-configmaps-d9549c49-6f6b-4a2c-843b-15bda5575551 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:53:43.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2207" for this suite. 11/15/23 05:53:43.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:53:43.593
Nov 15 05:53:43.593: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename dns 11/15/23 05:53:43.594
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:53:43.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:53:43.658
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 11/15/23 05:53:43.672
Nov 15 05:53:43.710: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3858  3429ec0c-6d21-4da1-abe1-49e52416ac3b 60738 0 2023-11-15 05:53:43 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] [] [{e2e.test Update v1 2023-11-15 05:53:43 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n8w4w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n8w4w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c31,c25,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 05:53:43.711: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3858" to be "running and ready"
Nov 15 05:53:43.728: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 16.968236ms
Nov 15 05:53:43.728: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Nov 15 05:53:45.747: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036245931s
Nov 15 05:53:45.747: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Nov 15 05:53:47.747: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.036117392s
Nov 15 05:53:47.747: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Nov 15 05:53:47.747: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 11/15/23 05:53:47.747
Nov 15 05:53:47.747: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3858 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 05:53:47.747: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 05:53:47.748: INFO: ExecWithOptions: Clientset creation
Nov 15 05:53:47.748: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-3858/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 11/15/23 05:53:47.969
Nov 15 05:53:47.969: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3858 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 05:53:47.969: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 05:53:47.970: INFO: ExecWithOptions: Clientset creation
Nov 15 05:53:47.970: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-3858/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 15 05:53:48.155: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 15 05:53:48.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3858" for this suite. 11/15/23 05:53:48.234
------------------------------
â€¢ [4.668 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:53:43.593
    Nov 15 05:53:43.593: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename dns 11/15/23 05:53:43.594
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:53:43.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:53:43.658
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 11/15/23 05:53:43.672
    Nov 15 05:53:43.710: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3858  3429ec0c-6d21-4da1-abe1-49e52416ac3b 60738 0 2023-11-15 05:53:43 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] [] [{e2e.test Update v1 2023-11-15 05:53:43 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n8w4w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n8w4w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c31,c25,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 05:53:43.711: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3858" to be "running and ready"
    Nov 15 05:53:43.728: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 16.968236ms
    Nov 15 05:53:43.728: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 05:53:45.747: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036245931s
    Nov 15 05:53:45.747: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 05:53:47.747: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.036117392s
    Nov 15 05:53:47.747: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Nov 15 05:53:47.747: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 11/15/23 05:53:47.747
    Nov 15 05:53:47.747: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3858 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 05:53:47.747: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 05:53:47.748: INFO: ExecWithOptions: Clientset creation
    Nov 15 05:53:47.748: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-3858/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 11/15/23 05:53:47.969
    Nov 15 05:53:47.969: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3858 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 05:53:47.969: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 05:53:47.970: INFO: ExecWithOptions: Clientset creation
    Nov 15 05:53:47.970: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-3858/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 15 05:53:48.155: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:53:48.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3858" for this suite. 11/15/23 05:53:48.234
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:53:48.262
Nov 15 05:53:48.262: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 05:53:48.263
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:53:48.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:53:48.332
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 11/15/23 05:53:48.346
Nov 15 05:53:48.347: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: mark a version not serverd 11/15/23 05:53:57.532
STEP: check the unserved version gets removed 11/15/23 05:53:57.575
STEP: check the other version is not changed 11/15/23 05:54:01.615
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 05:54:08.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7063" for this suite. 11/15/23 05:54:08.49
------------------------------
â€¢ [SLOW TEST] [20.259 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:53:48.262
    Nov 15 05:53:48.262: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 05:53:48.263
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:53:48.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:53:48.332
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 11/15/23 05:53:48.346
    Nov 15 05:53:48.347: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: mark a version not serverd 11/15/23 05:53:57.532
    STEP: check the unserved version gets removed 11/15/23 05:53:57.575
    STEP: check the other version is not changed 11/15/23 05:54:01.615
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:54:08.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7063" for this suite. 11/15/23 05:54:08.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:54:08.525
Nov 15 05:54:08.525: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename daemonsets 11/15/23 05:54:08.526
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:54:08.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:54:08.604
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
STEP: Creating simple DaemonSet "daemon-set" 11/15/23 05:54:08.722
STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 05:54:08.738
Nov 15 05:54:08.785: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 05:54:08.785: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 05:54:09.868: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 05:54:09.868: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 05:54:10.859: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 15 05:54:10.859: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 05:54:11.844: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 15 05:54:11.844: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 11/15/23 05:54:11.874
STEP: DeleteCollection of the DaemonSets 11/15/23 05:54:11.888
STEP: Verify that ReplicaSets have been deleted 11/15/23 05:54:11.944
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
Nov 15 05:54:11.981: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"61112"},"items":null}

Nov 15 05:54:12.002: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"61114"},"items":[{"metadata":{"name":"daemon-set-cftq6","generateName":"daemon-set-","namespace":"daemonsets-5135","uid":"e090305b-52c9-4329-a53f-95f81043d07a","resourceVersion":"61114","creationTimestamp":"2023-11-15T05:54:08Z","deletionTimestamp":"2023-11-15T05:54:41Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"48569deb378acf0b2ba9956154783196d4a034a0dddf802fadb3507592673a5a","cni.projectcalico.org/podIP":"172.30.10.158/32","cni.projectcalico.org/podIPs":"172.30.10.158/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.10.158\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"3ea106d7-00f0-47c1-9fe5-684dd59fafa8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ea106d7-00f0-47c1-9fe5-684dd59fafa8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.10.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6mbhv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6mbhv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.72.152.88","securityContext":{"seLinuxOptions":{"level":"s0:c32,c4"}},"imagePullSecrets":[{"name":"default-dockercfg-xl4bv"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.72.152.88"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:08Z"}],"hostIP":"10.72.152.88","podIP":"172.30.10.158","podIPs":[{"ip":"172.30.10.158"}],"startTime":"2023-11-15T05:54:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-11-15T05:54:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://6871b119fd4083e2e736ff87c45d6e1f1b84ee07dd11268bf1ab44849ac726d8","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-svp9t","generateName":"daemon-set-","namespace":"daemonsets-5135","uid":"b5fba8c4-3e67-4b35-ae0a-91959b54f281","resourceVersion":"61113","creationTimestamp":"2023-11-15T05:54:08Z","deletionTimestamp":"2023-11-15T05:54:41Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"e67ae7248a3107d45a5e5ae2efebc9354057f14d2d8d61dbbab6427ccac9f902","cni.projectcalico.org/podIP":"172.30.213.174/32","cni.projectcalico.org/podIPs":"172.30.213.174/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.213.174\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"3ea106d7-00f0-47c1-9fe5-684dd59fafa8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ea106d7-00f0-47c1-9fe5-684dd59fafa8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ldhlj","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ldhlj","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.72.152.86","securityContext":{"seLinuxOptions":{"level":"s0:c32,c4"}},"imagePullSecrets":[{"name":"default-dockercfg-xl4bv"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.72.152.86"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:08Z"}],"hostIP":"10.72.152.86","podIP":"172.30.213.174","podIPs":[{"ip":"172.30.213.174"}],"startTime":"2023-11-15T05:54:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-11-15T05:54:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://287c569542ebd9d2c8bf30f6dbce3c54acb4a514f93a884ac4183573dd2f3f3a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-wm5x4","generateName":"daemon-set-","namespace":"daemonsets-5135","uid":"ae05ddbe-a104-4232-809f-18c6afa47aae","resourceVersion":"61112","creationTimestamp":"2023-11-15T05:54:08Z","deletionTimestamp":"2023-11-15T05:54:41Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"9ed3423b657334d28af30d1fe500c5046c174fcd2bf9b783f39a62c8a24d3ed6","cni.projectcalico.org/podIP":"172.30.214.168/32","cni.projectcalico.org/podIPs":"172.30.214.168/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.214.168\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"3ea106d7-00f0-47c1-9fe5-684dd59fafa8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ea106d7-00f0-47c1-9fe5-684dd59fafa8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.214.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pgj7b","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pgj7b","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.72.152.81","securityContext":{"seLinuxOptions":{"level":"s0:c32,c4"}},"imagePullSecrets":[{"name":"default-dockercfg-xl4bv"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.72.152.81"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:08Z"}],"hostIP":"10.72.152.81","podIP":"172.30.214.168","podIPs":[{"ip":"172.30.214.168"}],"startTime":"2023-11-15T05:54:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-11-15T05:54:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://ab93011e031f5f5eb7c572145ae4a6e80a7fd057881c0084ec8140f7408fb72a","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 05:54:12.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5135" for this suite. 11/15/23 05:54:12.125
------------------------------
â€¢ [3.642 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:54:08.525
    Nov 15 05:54:08.525: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename daemonsets 11/15/23 05:54:08.526
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:54:08.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:54:08.604
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:834
    STEP: Creating simple DaemonSet "daemon-set" 11/15/23 05:54:08.722
    STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 05:54:08.738
    Nov 15 05:54:08.785: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 05:54:08.785: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 05:54:09.868: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 05:54:09.868: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 05:54:10.859: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 15 05:54:10.859: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 05:54:11.844: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 15 05:54:11.844: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 11/15/23 05:54:11.874
    STEP: DeleteCollection of the DaemonSets 11/15/23 05:54:11.888
    STEP: Verify that ReplicaSets have been deleted 11/15/23 05:54:11.944
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    Nov 15 05:54:11.981: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"61112"},"items":null}

    Nov 15 05:54:12.002: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"61114"},"items":[{"metadata":{"name":"daemon-set-cftq6","generateName":"daemon-set-","namespace":"daemonsets-5135","uid":"e090305b-52c9-4329-a53f-95f81043d07a","resourceVersion":"61114","creationTimestamp":"2023-11-15T05:54:08Z","deletionTimestamp":"2023-11-15T05:54:41Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"48569deb378acf0b2ba9956154783196d4a034a0dddf802fadb3507592673a5a","cni.projectcalico.org/podIP":"172.30.10.158/32","cni.projectcalico.org/podIPs":"172.30.10.158/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.10.158\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"3ea106d7-00f0-47c1-9fe5-684dd59fafa8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ea106d7-00f0-47c1-9fe5-684dd59fafa8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.10.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6mbhv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6mbhv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.72.152.88","securityContext":{"seLinuxOptions":{"level":"s0:c32,c4"}},"imagePullSecrets":[{"name":"default-dockercfg-xl4bv"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.72.152.88"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:08Z"}],"hostIP":"10.72.152.88","podIP":"172.30.10.158","podIPs":[{"ip":"172.30.10.158"}],"startTime":"2023-11-15T05:54:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-11-15T05:54:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://6871b119fd4083e2e736ff87c45d6e1f1b84ee07dd11268bf1ab44849ac726d8","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-svp9t","generateName":"daemon-set-","namespace":"daemonsets-5135","uid":"b5fba8c4-3e67-4b35-ae0a-91959b54f281","resourceVersion":"61113","creationTimestamp":"2023-11-15T05:54:08Z","deletionTimestamp":"2023-11-15T05:54:41Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"e67ae7248a3107d45a5e5ae2efebc9354057f14d2d8d61dbbab6427ccac9f902","cni.projectcalico.org/podIP":"172.30.213.174/32","cni.projectcalico.org/podIPs":"172.30.213.174/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.213.174\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"3ea106d7-00f0-47c1-9fe5-684dd59fafa8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ea106d7-00f0-47c1-9fe5-684dd59fafa8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ldhlj","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ldhlj","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.72.152.86","securityContext":{"seLinuxOptions":{"level":"s0:c32,c4"}},"imagePullSecrets":[{"name":"default-dockercfg-xl4bv"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.72.152.86"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:08Z"}],"hostIP":"10.72.152.86","podIP":"172.30.213.174","podIPs":[{"ip":"172.30.213.174"}],"startTime":"2023-11-15T05:54:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-11-15T05:54:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://287c569542ebd9d2c8bf30f6dbce3c54acb4a514f93a884ac4183573dd2f3f3a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-wm5x4","generateName":"daemon-set-","namespace":"daemonsets-5135","uid":"ae05ddbe-a104-4232-809f-18c6afa47aae","resourceVersion":"61112","creationTimestamp":"2023-11-15T05:54:08Z","deletionTimestamp":"2023-11-15T05:54:41Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"9ed3423b657334d28af30d1fe500c5046c174fcd2bf9b783f39a62c8a24d3ed6","cni.projectcalico.org/podIP":"172.30.214.168/32","cni.projectcalico.org/podIPs":"172.30.214.168/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.214.168\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"3ea106d7-00f0-47c1-9fe5-684dd59fafa8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ea106d7-00f0-47c1-9fe5-684dd59fafa8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-11-15T05:54:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.214.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pgj7b","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pgj7b","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.72.152.81","securityContext":{"seLinuxOptions":{"level":"s0:c32,c4"}},"imagePullSecrets":[{"name":"default-dockercfg-xl4bv"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.72.152.81"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T05:54:08Z"}],"hostIP":"10.72.152.81","podIP":"172.30.214.168","podIPs":[{"ip":"172.30.214.168"}],"startTime":"2023-11-15T05:54:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-11-15T05:54:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://ab93011e031f5f5eb7c572145ae4a6e80a7fd057881c0084ec8140f7408fb72a","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:54:12.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5135" for this suite. 11/15/23 05:54:12.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:54:12.169
Nov 15 05:54:12.169: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename replication-controller 11/15/23 05:54:12.17
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:54:12.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:54:12.265
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Nov 15 05:54:12.283: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 11/15/23 05:54:12.328
STEP: Checking rc "condition-test" has the desired failure condition set 11/15/23 05:54:12.347
STEP: Scaling down rc "condition-test" to satisfy pod quota 11/15/23 05:54:13.384
Nov 15 05:54:13.423: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 11/15/23 05:54:13.423
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Nov 15 05:54:14.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8904" for this suite. 11/15/23 05:54:14.487
------------------------------
â€¢ [2.344 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:54:12.169
    Nov 15 05:54:12.169: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename replication-controller 11/15/23 05:54:12.17
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:54:12.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:54:12.265
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Nov 15 05:54:12.283: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 11/15/23 05:54:12.328
    STEP: Checking rc "condition-test" has the desired failure condition set 11/15/23 05:54:12.347
    STEP: Scaling down rc "condition-test" to satisfy pod quota 11/15/23 05:54:13.384
    Nov 15 05:54:13.423: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 11/15/23 05:54:13.423
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:54:14.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8904" for this suite. 11/15/23 05:54:14.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:54:14.516
Nov 15 05:54:14.517: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename downward-api 11/15/23 05:54:14.518
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:54:14.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:54:14.588
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 11/15/23 05:54:14.601
Nov 15 05:54:14.639: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ebd258e7-3827-4b0e-9267-362e0d59e88b" in namespace "downward-api-3053" to be "Succeeded or Failed"
Nov 15 05:54:14.661: INFO: Pod "downwardapi-volume-ebd258e7-3827-4b0e-9267-362e0d59e88b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.496565ms
Nov 15 05:54:16.682: INFO: Pod "downwardapi-volume-ebd258e7-3827-4b0e-9267-362e0d59e88b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042181885s
Nov 15 05:54:18.699: INFO: Pod "downwardapi-volume-ebd258e7-3827-4b0e-9267-362e0d59e88b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059752909s
Nov 15 05:54:20.680: INFO: Pod "downwardapi-volume-ebd258e7-3827-4b0e-9267-362e0d59e88b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040647631s
STEP: Saw pod success 11/15/23 05:54:20.68
Nov 15 05:54:20.680: INFO: Pod "downwardapi-volume-ebd258e7-3827-4b0e-9267-362e0d59e88b" satisfied condition "Succeeded or Failed"
Nov 15 05:54:20.698: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-ebd258e7-3827-4b0e-9267-362e0d59e88b container client-container: <nil>
STEP: delete the pod 11/15/23 05:54:20.771
Nov 15 05:54:20.819: INFO: Waiting for pod downwardapi-volume-ebd258e7-3827-4b0e-9267-362e0d59e88b to disappear
Nov 15 05:54:20.843: INFO: Pod downwardapi-volume-ebd258e7-3827-4b0e-9267-362e0d59e88b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 05:54:20.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3053" for this suite. 11/15/23 05:54:20.873
------------------------------
â€¢ [SLOW TEST] [6.385 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:54:14.516
    Nov 15 05:54:14.517: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename downward-api 11/15/23 05:54:14.518
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:54:14.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:54:14.588
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 11/15/23 05:54:14.601
    Nov 15 05:54:14.639: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ebd258e7-3827-4b0e-9267-362e0d59e88b" in namespace "downward-api-3053" to be "Succeeded or Failed"
    Nov 15 05:54:14.661: INFO: Pod "downwardapi-volume-ebd258e7-3827-4b0e-9267-362e0d59e88b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.496565ms
    Nov 15 05:54:16.682: INFO: Pod "downwardapi-volume-ebd258e7-3827-4b0e-9267-362e0d59e88b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042181885s
    Nov 15 05:54:18.699: INFO: Pod "downwardapi-volume-ebd258e7-3827-4b0e-9267-362e0d59e88b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059752909s
    Nov 15 05:54:20.680: INFO: Pod "downwardapi-volume-ebd258e7-3827-4b0e-9267-362e0d59e88b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040647631s
    STEP: Saw pod success 11/15/23 05:54:20.68
    Nov 15 05:54:20.680: INFO: Pod "downwardapi-volume-ebd258e7-3827-4b0e-9267-362e0d59e88b" satisfied condition "Succeeded or Failed"
    Nov 15 05:54:20.698: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-ebd258e7-3827-4b0e-9267-362e0d59e88b container client-container: <nil>
    STEP: delete the pod 11/15/23 05:54:20.771
    Nov 15 05:54:20.819: INFO: Waiting for pod downwardapi-volume-ebd258e7-3827-4b0e-9267-362e0d59e88b to disappear
    Nov 15 05:54:20.843: INFO: Pod downwardapi-volume-ebd258e7-3827-4b0e-9267-362e0d59e88b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:54:20.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3053" for this suite. 11/15/23 05:54:20.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:54:20.904
Nov 15 05:54:20.904: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename csiinlinevolumes 11/15/23 05:54:20.905
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:54:20.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:54:20.97
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 11/15/23 05:54:20.981
STEP: getting 11/15/23 05:54:21.079
STEP: listing 11/15/23 05:54:21.118
STEP: deleting 11/15/23 05:54:21.135
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Nov 15 05:54:21.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-9752" for this suite. 11/15/23 05:54:21.26
------------------------------
â€¢ [0.382 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:54:20.904
    Nov 15 05:54:20.904: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename csiinlinevolumes 11/15/23 05:54:20.905
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:54:20.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:54:20.97
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 11/15/23 05:54:20.981
    STEP: getting 11/15/23 05:54:21.079
    STEP: listing 11/15/23 05:54:21.118
    STEP: deleting 11/15/23 05:54:21.135
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:54:21.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-9752" for this suite. 11/15/23 05:54:21.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:54:21.288
Nov 15 05:54:21.289: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename services 11/15/23 05:54:21.289
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:54:21.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:54:21.371
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2121 11/15/23 05:54:21.384
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 11/15/23 05:54:21.448
STEP: creating service externalsvc in namespace services-2121 11/15/23 05:54:21.448
STEP: creating replication controller externalsvc in namespace services-2121 11/15/23 05:54:21.492
I1115 05:54:21.510956      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2121, replica count: 2
I1115 05:54:24.561775      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 11/15/23 05:54:24.588
Nov 15 05:54:24.667: INFO: Creating new exec pod
Nov 15 05:54:24.696: INFO: Waiting up to 5m0s for pod "execpod94w4k" in namespace "services-2121" to be "running"
Nov 15 05:54:24.746: INFO: Pod "execpod94w4k": Phase="Pending", Reason="", readiness=false. Elapsed: 50.031254ms
Nov 15 05:54:26.765: INFO: Pod "execpod94w4k": Phase="Running", Reason="", readiness=true. Elapsed: 2.069261626s
Nov 15 05:54:26.765: INFO: Pod "execpod94w4k" satisfied condition "running"
Nov 15 05:54:26.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2121 exec execpod94w4k -- /bin/sh -x -c nslookup nodeport-service.services-2121.svc.cluster.local'
Nov 15 05:54:27.107: INFO: stderr: "+ nslookup nodeport-service.services-2121.svc.cluster.local\n"
Nov 15 05:54:27.107: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-2121.svc.cluster.local\tcanonical name = externalsvc.services-2121.svc.cluster.local.\nName:\texternalsvc.services-2121.svc.cluster.local\nAddress: 172.21.123.6\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2121, will wait for the garbage collector to delete the pods 11/15/23 05:54:27.107
Nov 15 05:54:27.200: INFO: Deleting ReplicationController externalsvc took: 26.359912ms
Nov 15 05:54:27.301: INFO: Terminating ReplicationController externalsvc pods took: 100.561387ms
Nov 15 05:54:29.868: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 05:54:29.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2121" for this suite. 11/15/23 05:54:29.961
------------------------------
â€¢ [SLOW TEST] [8.697 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:54:21.288
    Nov 15 05:54:21.289: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename services 11/15/23 05:54:21.289
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:54:21.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:54:21.371
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-2121 11/15/23 05:54:21.384
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 11/15/23 05:54:21.448
    STEP: creating service externalsvc in namespace services-2121 11/15/23 05:54:21.448
    STEP: creating replication controller externalsvc in namespace services-2121 11/15/23 05:54:21.492
    I1115 05:54:21.510956      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2121, replica count: 2
    I1115 05:54:24.561775      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 11/15/23 05:54:24.588
    Nov 15 05:54:24.667: INFO: Creating new exec pod
    Nov 15 05:54:24.696: INFO: Waiting up to 5m0s for pod "execpod94w4k" in namespace "services-2121" to be "running"
    Nov 15 05:54:24.746: INFO: Pod "execpod94w4k": Phase="Pending", Reason="", readiness=false. Elapsed: 50.031254ms
    Nov 15 05:54:26.765: INFO: Pod "execpod94w4k": Phase="Running", Reason="", readiness=true. Elapsed: 2.069261626s
    Nov 15 05:54:26.765: INFO: Pod "execpod94w4k" satisfied condition "running"
    Nov 15 05:54:26.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2121 exec execpod94w4k -- /bin/sh -x -c nslookup nodeport-service.services-2121.svc.cluster.local'
    Nov 15 05:54:27.107: INFO: stderr: "+ nslookup nodeport-service.services-2121.svc.cluster.local\n"
    Nov 15 05:54:27.107: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-2121.svc.cluster.local\tcanonical name = externalsvc.services-2121.svc.cluster.local.\nName:\texternalsvc.services-2121.svc.cluster.local\nAddress: 172.21.123.6\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-2121, will wait for the garbage collector to delete the pods 11/15/23 05:54:27.107
    Nov 15 05:54:27.200: INFO: Deleting ReplicationController externalsvc took: 26.359912ms
    Nov 15 05:54:27.301: INFO: Terminating ReplicationController externalsvc pods took: 100.561387ms
    Nov 15 05:54:29.868: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:54:29.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2121" for this suite. 11/15/23 05:54:29.961
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:54:29.986
Nov 15 05:54:29.987: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubectl 11/15/23 05:54:29.987
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:54:30.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:54:30.051
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 11/15/23 05:54:30.062
Nov 15 05:54:30.063: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Nov 15 05:54:30.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 create -f -'
Nov 15 05:54:31.837: INFO: stderr: ""
Nov 15 05:54:31.837: INFO: stdout: "service/agnhost-replica created\n"
Nov 15 05:54:31.837: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Nov 15 05:54:31.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 create -f -'
Nov 15 05:54:33.557: INFO: stderr: ""
Nov 15 05:54:33.557: INFO: stdout: "service/agnhost-primary created\n"
Nov 15 05:54:33.558: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov 15 05:54:33.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 create -f -'
Nov 15 05:54:34.069: INFO: stderr: ""
Nov 15 05:54:34.069: INFO: stdout: "service/frontend created\n"
Nov 15 05:54:34.069: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Nov 15 05:54:34.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 create -f -'
Nov 15 05:54:35.603: INFO: stderr: ""
Nov 15 05:54:35.603: INFO: stdout: "deployment.apps/frontend created\n"
Nov 15 05:54:35.603: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 15 05:54:35.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 create -f -'
Nov 15 05:54:36.115: INFO: stderr: ""
Nov 15 05:54:36.115: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Nov 15 05:54:36.115: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 15 05:54:36.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 create -f -'
Nov 15 05:54:36.621: INFO: stderr: ""
Nov 15 05:54:36.621: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 11/15/23 05:54:36.621
Nov 15 05:54:36.621: INFO: Waiting for all frontend pods to be Running.
Nov 15 05:54:41.675: INFO: Waiting for frontend to serve content.
Nov 15 05:54:41.755: INFO: Trying to add a new entry to the guestbook.
Nov 15 05:54:41.818: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 11/15/23 05:54:41.884
Nov 15 05:54:41.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 delete --grace-period=0 --force -f -'
Nov 15 05:54:42.032: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 05:54:42.032: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 11/15/23 05:54:42.032
Nov 15 05:54:42.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 delete --grace-period=0 --force -f -'
Nov 15 05:54:42.284: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 05:54:42.284: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 11/15/23 05:54:42.285
Nov 15 05:54:42.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 delete --grace-period=0 --force -f -'
Nov 15 05:54:42.452: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 05:54:42.452: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 11/15/23 05:54:42.452
Nov 15 05:54:42.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 delete --grace-period=0 --force -f -'
Nov 15 05:54:42.563: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 05:54:42.563: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 11/15/23 05:54:42.563
Nov 15 05:54:42.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 delete --grace-period=0 --force -f -'
Nov 15 05:54:42.691: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 05:54:42.691: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 11/15/23 05:54:42.691
Nov 15 05:54:42.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 delete --grace-period=0 --force -f -'
Nov 15 05:54:42.807: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 05:54:42.807: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 05:54:42.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4813" for this suite. 11/15/23 05:54:42.837
------------------------------
â€¢ [SLOW TEST] [12.884 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:54:29.986
    Nov 15 05:54:29.987: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubectl 11/15/23 05:54:29.987
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:54:30.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:54:30.051
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 11/15/23 05:54:30.062
    Nov 15 05:54:30.063: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Nov 15 05:54:30.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 create -f -'
    Nov 15 05:54:31.837: INFO: stderr: ""
    Nov 15 05:54:31.837: INFO: stdout: "service/agnhost-replica created\n"
    Nov 15 05:54:31.837: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Nov 15 05:54:31.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 create -f -'
    Nov 15 05:54:33.557: INFO: stderr: ""
    Nov 15 05:54:33.557: INFO: stdout: "service/agnhost-primary created\n"
    Nov 15 05:54:33.558: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Nov 15 05:54:33.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 create -f -'
    Nov 15 05:54:34.069: INFO: stderr: ""
    Nov 15 05:54:34.069: INFO: stdout: "service/frontend created\n"
    Nov 15 05:54:34.069: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Nov 15 05:54:34.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 create -f -'
    Nov 15 05:54:35.603: INFO: stderr: ""
    Nov 15 05:54:35.603: INFO: stdout: "deployment.apps/frontend created\n"
    Nov 15 05:54:35.603: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Nov 15 05:54:35.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 create -f -'
    Nov 15 05:54:36.115: INFO: stderr: ""
    Nov 15 05:54:36.115: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Nov 15 05:54:36.115: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Nov 15 05:54:36.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 create -f -'
    Nov 15 05:54:36.621: INFO: stderr: ""
    Nov 15 05:54:36.621: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 11/15/23 05:54:36.621
    Nov 15 05:54:36.621: INFO: Waiting for all frontend pods to be Running.
    Nov 15 05:54:41.675: INFO: Waiting for frontend to serve content.
    Nov 15 05:54:41.755: INFO: Trying to add a new entry to the guestbook.
    Nov 15 05:54:41.818: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 11/15/23 05:54:41.884
    Nov 15 05:54:41.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 delete --grace-period=0 --force -f -'
    Nov 15 05:54:42.032: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 15 05:54:42.032: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 11/15/23 05:54:42.032
    Nov 15 05:54:42.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 delete --grace-period=0 --force -f -'
    Nov 15 05:54:42.284: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 15 05:54:42.284: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 11/15/23 05:54:42.285
    Nov 15 05:54:42.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 delete --grace-period=0 --force -f -'
    Nov 15 05:54:42.452: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 15 05:54:42.452: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 11/15/23 05:54:42.452
    Nov 15 05:54:42.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 delete --grace-period=0 --force -f -'
    Nov 15 05:54:42.563: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 15 05:54:42.563: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 11/15/23 05:54:42.563
    Nov 15 05:54:42.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 delete --grace-period=0 --force -f -'
    Nov 15 05:54:42.691: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 15 05:54:42.691: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 11/15/23 05:54:42.691
    Nov 15 05:54:42.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4813 delete --grace-period=0 --force -f -'
    Nov 15 05:54:42.807: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 15 05:54:42.807: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:54:42.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4813" for this suite. 11/15/23 05:54:42.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:54:42.872
Nov 15 05:54:42.872: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename secrets 11/15/23 05:54:42.874
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:54:42.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:54:42.93
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-5a9b8636-b2dc-407d-9649-c201f90026fb 11/15/23 05:54:42.941
STEP: Creating a pod to test consume secrets 11/15/23 05:54:42.958
Nov 15 05:54:42.992: INFO: Waiting up to 5m0s for pod "pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243" in namespace "secrets-2121" to be "Succeeded or Failed"
Nov 15 05:54:43.015: INFO: Pod "pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243": Phase="Pending", Reason="", readiness=false. Elapsed: 23.393151ms
Nov 15 05:54:45.036: INFO: Pod "pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044342982s
Nov 15 05:54:47.035: INFO: Pod "pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043415517s
Nov 15 05:54:49.035: INFO: Pod "pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043016656s
Nov 15 05:54:51.038: INFO: Pod "pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.046000254s
STEP: Saw pod success 11/15/23 05:54:51.038
Nov 15 05:54:51.038: INFO: Pod "pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243" satisfied condition "Succeeded or Failed"
Nov 15 05:54:51.056: INFO: Trying to get logs from node 10.72.152.81 pod pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243 container secret-volume-test: <nil>
STEP: delete the pod 11/15/23 05:54:51.121
Nov 15 05:54:51.188: INFO: Waiting for pod pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243 to disappear
Nov 15 05:54:51.206: INFO: Pod pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 05:54:51.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2121" for this suite. 11/15/23 05:54:51.233
------------------------------
â€¢ [SLOW TEST] [8.389 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:54:42.872
    Nov 15 05:54:42.872: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename secrets 11/15/23 05:54:42.874
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:54:42.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:54:42.93
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-5a9b8636-b2dc-407d-9649-c201f90026fb 11/15/23 05:54:42.941
    STEP: Creating a pod to test consume secrets 11/15/23 05:54:42.958
    Nov 15 05:54:42.992: INFO: Waiting up to 5m0s for pod "pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243" in namespace "secrets-2121" to be "Succeeded or Failed"
    Nov 15 05:54:43.015: INFO: Pod "pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243": Phase="Pending", Reason="", readiness=false. Elapsed: 23.393151ms
    Nov 15 05:54:45.036: INFO: Pod "pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044342982s
    Nov 15 05:54:47.035: INFO: Pod "pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043415517s
    Nov 15 05:54:49.035: INFO: Pod "pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043016656s
    Nov 15 05:54:51.038: INFO: Pod "pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.046000254s
    STEP: Saw pod success 11/15/23 05:54:51.038
    Nov 15 05:54:51.038: INFO: Pod "pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243" satisfied condition "Succeeded or Failed"
    Nov 15 05:54:51.056: INFO: Trying to get logs from node 10.72.152.81 pod pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243 container secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 05:54:51.121
    Nov 15 05:54:51.188: INFO: Waiting for pod pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243 to disappear
    Nov 15 05:54:51.206: INFO: Pod pod-secrets-89db1c74-d21c-4d9b-8a29-2536f9ae5243 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:54:51.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2121" for this suite. 11/15/23 05:54:51.233
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:54:51.261
Nov 15 05:54:51.261: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename security-context-test 11/15/23 05:54:51.262
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:54:51.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:54:51.342
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Nov 15 05:54:51.396: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-4ebc743f-2a54-402b-b506-50cb17b98804" in namespace "security-context-test-8347" to be "Succeeded or Failed"
Nov 15 05:54:51.413: INFO: Pod "alpine-nnp-false-4ebc743f-2a54-402b-b506-50cb17b98804": Phase="Pending", Reason="", readiness=false. Elapsed: 17.185653ms
Nov 15 05:54:53.433: INFO: Pod "alpine-nnp-false-4ebc743f-2a54-402b-b506-50cb17b98804": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037061394s
Nov 15 05:54:55.435: INFO: Pod "alpine-nnp-false-4ebc743f-2a54-402b-b506-50cb17b98804": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03893564s
Nov 15 05:54:57.460: INFO: Pod "alpine-nnp-false-4ebc743f-2a54-402b-b506-50cb17b98804": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064327411s
Nov 15 05:54:57.461: INFO: Pod "alpine-nnp-false-4ebc743f-2a54-402b-b506-50cb17b98804" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Nov 15 05:54:57.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8347" for this suite. 11/15/23 05:54:57.54
------------------------------
â€¢ [SLOW TEST] [6.306 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:54:51.261
    Nov 15 05:54:51.261: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename security-context-test 11/15/23 05:54:51.262
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:54:51.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:54:51.342
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Nov 15 05:54:51.396: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-4ebc743f-2a54-402b-b506-50cb17b98804" in namespace "security-context-test-8347" to be "Succeeded or Failed"
    Nov 15 05:54:51.413: INFO: Pod "alpine-nnp-false-4ebc743f-2a54-402b-b506-50cb17b98804": Phase="Pending", Reason="", readiness=false. Elapsed: 17.185653ms
    Nov 15 05:54:53.433: INFO: Pod "alpine-nnp-false-4ebc743f-2a54-402b-b506-50cb17b98804": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037061394s
    Nov 15 05:54:55.435: INFO: Pod "alpine-nnp-false-4ebc743f-2a54-402b-b506-50cb17b98804": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03893564s
    Nov 15 05:54:57.460: INFO: Pod "alpine-nnp-false-4ebc743f-2a54-402b-b506-50cb17b98804": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064327411s
    Nov 15 05:54:57.461: INFO: Pod "alpine-nnp-false-4ebc743f-2a54-402b-b506-50cb17b98804" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:54:57.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8347" for this suite. 11/15/23 05:54:57.54
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:54:57.567
Nov 15 05:54:57.567: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename sched-preemption 11/15/23 05:54:57.569
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:54:57.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:54:57.634
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Nov 15 05:54:57.708: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 15 05:55:57.932: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:55:57.961
Nov 15 05:55:57.961: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename sched-preemption-path 11/15/23 05:55:57.962
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:55:58.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:55:58.036
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 11/15/23 05:55:58.049
STEP: Trying to launch a pod without a label to get a node which can launch it. 11/15/23 05:55:58.049
Nov 15 05:55:58.086: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-9670" to be "running"
Nov 15 05:55:58.105: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 18.67669ms
Nov 15 05:56:00.123: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03663598s
Nov 15 05:56:02.133: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.046412091s
Nov 15 05:56:02.133: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 11/15/23 05:56:02.149
Nov 15 05:56:02.211: INFO: found a healthy node: 10.72.152.81
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Nov 15 05:56:08.570: INFO: pods created so far: [1 1 1]
Nov 15 05:56:08.570: INFO: length of pods created so far: 3
Nov 15 05:56:12.624: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Nov 15 05:56:19.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 05:56:19.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-9670" for this suite. 11/15/23 05:56:20.015
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7014" for this suite. 11/15/23 05:56:20.042
------------------------------
â€¢ [SLOW TEST] [82.503 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:54:57.567
    Nov 15 05:54:57.567: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename sched-preemption 11/15/23 05:54:57.569
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:54:57.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:54:57.634
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Nov 15 05:54:57.708: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 15 05:55:57.932: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:55:57.961
    Nov 15 05:55:57.961: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename sched-preemption-path 11/15/23 05:55:57.962
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:55:58.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:55:58.036
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 11/15/23 05:55:58.049
    STEP: Trying to launch a pod without a label to get a node which can launch it. 11/15/23 05:55:58.049
    Nov 15 05:55:58.086: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-9670" to be "running"
    Nov 15 05:55:58.105: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 18.67669ms
    Nov 15 05:56:00.123: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03663598s
    Nov 15 05:56:02.133: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.046412091s
    Nov 15 05:56:02.133: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 11/15/23 05:56:02.149
    Nov 15 05:56:02.211: INFO: found a healthy node: 10.72.152.81
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Nov 15 05:56:08.570: INFO: pods created so far: [1 1 1]
    Nov 15 05:56:08.570: INFO: length of pods created so far: 3
    Nov 15 05:56:12.624: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:56:19.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:56:19.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-9670" for this suite. 11/15/23 05:56:20.015
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7014" for this suite. 11/15/23 05:56:20.042
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:56:20.072
Nov 15 05:56:20.072: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubectl 11/15/23 05:56:20.073
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:56:20.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:56:20.156
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 11/15/23 05:56:20.179
Nov 15 05:56:20.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2867 create -f -'
Nov 15 05:56:20.664: INFO: stderr: ""
Nov 15 05:56:20.664: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 11/15/23 05:56:20.664
Nov 15 05:56:21.694: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 05:56:21.694: INFO: Found 0 / 1
Nov 15 05:56:22.683: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 05:56:22.683: INFO: Found 0 / 1
Nov 15 05:56:23.684: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 05:56:23.684: INFO: Found 1 / 1
Nov 15 05:56:23.684: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 11/15/23 05:56:23.684
Nov 15 05:56:23.730: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 05:56:23.730: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 15 05:56:23.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2867 patch pod agnhost-primary-lc2hz -p {"metadata":{"annotations":{"x":"y"}}}'
Nov 15 05:56:23.936: INFO: stderr: ""
Nov 15 05:56:23.936: INFO: stdout: "pod/agnhost-primary-lc2hz patched\n"
STEP: checking annotations 11/15/23 05:56:23.936
Nov 15 05:56:23.972: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 05:56:23.972: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 05:56:23.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2867" for this suite. 11/15/23 05:56:24.002
------------------------------
â€¢ [3.968 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:56:20.072
    Nov 15 05:56:20.072: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubectl 11/15/23 05:56:20.073
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:56:20.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:56:20.156
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 11/15/23 05:56:20.179
    Nov 15 05:56:20.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2867 create -f -'
    Nov 15 05:56:20.664: INFO: stderr: ""
    Nov 15 05:56:20.664: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 11/15/23 05:56:20.664
    Nov 15 05:56:21.694: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 05:56:21.694: INFO: Found 0 / 1
    Nov 15 05:56:22.683: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 05:56:22.683: INFO: Found 0 / 1
    Nov 15 05:56:23.684: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 05:56:23.684: INFO: Found 1 / 1
    Nov 15 05:56:23.684: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 11/15/23 05:56:23.684
    Nov 15 05:56:23.730: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 05:56:23.730: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Nov 15 05:56:23.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2867 patch pod agnhost-primary-lc2hz -p {"metadata":{"annotations":{"x":"y"}}}'
    Nov 15 05:56:23.936: INFO: stderr: ""
    Nov 15 05:56:23.936: INFO: stdout: "pod/agnhost-primary-lc2hz patched\n"
    STEP: checking annotations 11/15/23 05:56:23.936
    Nov 15 05:56:23.972: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 05:56:23.972: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:56:23.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2867" for this suite. 11/15/23 05:56:24.002
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:56:24.04
Nov 15 05:56:24.040: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename gc 11/15/23 05:56:24.041
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:56:24.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:56:24.097
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 11/15/23 05:56:24.137
STEP: delete the rc 11/15/23 05:56:29.169
STEP: wait for the rc to be deleted 11/15/23 05:56:29.207
Nov 15 05:56:30.298: INFO: 0 pods remaining
Nov 15 05:56:30.298: INFO: 0 pods has nil DeletionTimestamp
Nov 15 05:56:30.298: INFO: 
STEP: Gathering metrics 11/15/23 05:56:31.249
W1115 05:56:31.281924      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 15 05:56:31.281: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 15 05:56:31.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2866" for this suite. 11/15/23 05:56:31.339
------------------------------
â€¢ [SLOW TEST] [7.329 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:56:24.04
    Nov 15 05:56:24.040: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename gc 11/15/23 05:56:24.041
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:56:24.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:56:24.097
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 11/15/23 05:56:24.137
    STEP: delete the rc 11/15/23 05:56:29.169
    STEP: wait for the rc to be deleted 11/15/23 05:56:29.207
    Nov 15 05:56:30.298: INFO: 0 pods remaining
    Nov 15 05:56:30.298: INFO: 0 pods has nil DeletionTimestamp
    Nov 15 05:56:30.298: INFO: 
    STEP: Gathering metrics 11/15/23 05:56:31.249
    W1115 05:56:31.281924      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Nov 15 05:56:31.281: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:56:31.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2866" for this suite. 11/15/23 05:56:31.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:56:31.379
Nov 15 05:56:31.379: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 05:56:31.39
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:56:31.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:56:31.504
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 11/15/23 05:56:31.533
Nov 15 05:56:31.574: INFO: Waiting up to 5m0s for pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960" in namespace "projected-4896" to be "Succeeded or Failed"
Nov 15 05:56:31.610: INFO: Pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960": Phase="Pending", Reason="", readiness=false. Elapsed: 36.135104ms
Nov 15 05:56:33.634: INFO: Pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059912985s
Nov 15 05:56:35.633: INFO: Pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05892457s
Nov 15 05:56:37.638: INFO: Pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063597129s
Nov 15 05:56:39.633: INFO: Pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960": Phase="Pending", Reason="", readiness=false. Elapsed: 8.059088131s
Nov 15 05:56:41.628: INFO: Pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960": Phase="Pending", Reason="", readiness=false. Elapsed: 10.053996604s
Nov 15 05:56:43.629: INFO: Pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960": Phase="Pending", Reason="", readiness=false. Elapsed: 12.054651031s
Nov 15 05:56:45.634: INFO: Pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.059536121s
STEP: Saw pod success 11/15/23 05:56:45.634
Nov 15 05:56:45.634: INFO: Pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960" satisfied condition "Succeeded or Failed"
Nov 15 05:56:45.657: INFO: Trying to get logs from node 10.72.152.88 pod downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960 container client-container: <nil>
STEP: delete the pod 11/15/23 05:56:45.743
Nov 15 05:56:45.804: INFO: Waiting for pod downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960 to disappear
Nov 15 05:56:45.826: INFO: Pod downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 05:56:45.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4896" for this suite. 11/15/23 05:56:45.866
------------------------------
â€¢ [SLOW TEST] [14.524 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:56:31.379
    Nov 15 05:56:31.379: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 05:56:31.39
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:56:31.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:56:31.504
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 11/15/23 05:56:31.533
    Nov 15 05:56:31.574: INFO: Waiting up to 5m0s for pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960" in namespace "projected-4896" to be "Succeeded or Failed"
    Nov 15 05:56:31.610: INFO: Pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960": Phase="Pending", Reason="", readiness=false. Elapsed: 36.135104ms
    Nov 15 05:56:33.634: INFO: Pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059912985s
    Nov 15 05:56:35.633: INFO: Pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05892457s
    Nov 15 05:56:37.638: INFO: Pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063597129s
    Nov 15 05:56:39.633: INFO: Pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960": Phase="Pending", Reason="", readiness=false. Elapsed: 8.059088131s
    Nov 15 05:56:41.628: INFO: Pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960": Phase="Pending", Reason="", readiness=false. Elapsed: 10.053996604s
    Nov 15 05:56:43.629: INFO: Pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960": Phase="Pending", Reason="", readiness=false. Elapsed: 12.054651031s
    Nov 15 05:56:45.634: INFO: Pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.059536121s
    STEP: Saw pod success 11/15/23 05:56:45.634
    Nov 15 05:56:45.634: INFO: Pod "downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960" satisfied condition "Succeeded or Failed"
    Nov 15 05:56:45.657: INFO: Trying to get logs from node 10.72.152.88 pod downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960 container client-container: <nil>
    STEP: delete the pod 11/15/23 05:56:45.743
    Nov 15 05:56:45.804: INFO: Waiting for pod downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960 to disappear
    Nov 15 05:56:45.826: INFO: Pod downwardapi-volume-081063bd-22a1-41af-8194-788d5f903960 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:56:45.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4896" for this suite. 11/15/23 05:56:45.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:56:45.897
Nov 15 05:56:45.897: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename daemonsets 11/15/23 05:56:45.898
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:56:45.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:56:45.967
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
STEP: Creating a simple DaemonSet "daemon-set" 11/15/23 05:56:46.14
STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 05:56:46.157
Nov 15 05:56:46.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 05:56:46.203: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 05:56:47.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 05:56:47.256: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 05:56:48.279: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 05:56:48.279: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 05:56:49.259: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 15 05:56:49.259: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 11/15/23 05:56:49.269
Nov 15 05:56:49.364: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 05:56:49.364: INFO: Node 10.72.152.86 is running 0 daemon pod, expected 1
Nov 15 05:56:50.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 05:56:50.417: INFO: Node 10.72.152.86 is running 0 daemon pod, expected 1
Nov 15 05:56:51.410: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 05:56:51.410: INFO: Node 10.72.152.86 is running 0 daemon pod, expected 1
Nov 15 05:56:52.414: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 15 05:56:52.414: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 11/15/23 05:56:52.414
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 11/15/23 05:56:52.443
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7535, will wait for the garbage collector to delete the pods 11/15/23 05:56:52.443
Nov 15 05:56:52.526: INFO: Deleting DaemonSet.extensions daemon-set took: 21.788677ms
Nov 15 05:56:52.727: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.983933ms
Nov 15 05:56:55.048: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 05:56:55.048: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 15 05:56:55.060: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"65087"},"items":null}

Nov 15 05:56:55.076: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"65087"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 05:56:55.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7535" for this suite. 11/15/23 05:56:55.217
------------------------------
â€¢ [SLOW TEST] [9.347 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:56:45.897
    Nov 15 05:56:45.897: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename daemonsets 11/15/23 05:56:45.898
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:56:45.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:56:45.967
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:305
    STEP: Creating a simple DaemonSet "daemon-set" 11/15/23 05:56:46.14
    STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 05:56:46.157
    Nov 15 05:56:46.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 05:56:46.203: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 05:56:47.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 05:56:47.256: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 05:56:48.279: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 05:56:48.279: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 05:56:49.259: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 15 05:56:49.259: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 11/15/23 05:56:49.269
    Nov 15 05:56:49.364: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 05:56:49.364: INFO: Node 10.72.152.86 is running 0 daemon pod, expected 1
    Nov 15 05:56:50.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 05:56:50.417: INFO: Node 10.72.152.86 is running 0 daemon pod, expected 1
    Nov 15 05:56:51.410: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 05:56:51.410: INFO: Node 10.72.152.86 is running 0 daemon pod, expected 1
    Nov 15 05:56:52.414: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 15 05:56:52.414: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 11/15/23 05:56:52.414
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 11/15/23 05:56:52.443
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7535, will wait for the garbage collector to delete the pods 11/15/23 05:56:52.443
    Nov 15 05:56:52.526: INFO: Deleting DaemonSet.extensions daemon-set took: 21.788677ms
    Nov 15 05:56:52.727: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.983933ms
    Nov 15 05:56:55.048: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 05:56:55.048: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 15 05:56:55.060: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"65087"},"items":null}

    Nov 15 05:56:55.076: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"65087"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:56:55.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7535" for this suite. 11/15/23 05:56:55.217
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:56:55.244
Nov 15 05:56:55.244: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename emptydir 11/15/23 05:56:55.245
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:56:55.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:56:55.31
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 11/15/23 05:56:55.324
Nov 15 05:56:55.376: INFO: Waiting up to 5m0s for pod "pod-a5db3b53-7df5-4c76-a3aa-2d4fe872dd2c" in namespace "emptydir-3810" to be "Succeeded or Failed"
Nov 15 05:56:55.394: INFO: Pod "pod-a5db3b53-7df5-4c76-a3aa-2d4fe872dd2c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.32908ms
Nov 15 05:56:57.413: INFO: Pod "pod-a5db3b53-7df5-4c76-a3aa-2d4fe872dd2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037025317s
Nov 15 05:56:59.415: INFO: Pod "pod-a5db3b53-7df5-4c76-a3aa-2d4fe872dd2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038818086s
STEP: Saw pod success 11/15/23 05:56:59.415
Nov 15 05:56:59.415: INFO: Pod "pod-a5db3b53-7df5-4c76-a3aa-2d4fe872dd2c" satisfied condition "Succeeded or Failed"
Nov 15 05:56:59.433: INFO: Trying to get logs from node 10.72.152.88 pod pod-a5db3b53-7df5-4c76-a3aa-2d4fe872dd2c container test-container: <nil>
STEP: delete the pod 11/15/23 05:56:59.474
Nov 15 05:56:59.536: INFO: Waiting for pod pod-a5db3b53-7df5-4c76-a3aa-2d4fe872dd2c to disappear
Nov 15 05:56:59.553: INFO: Pod pod-a5db3b53-7df5-4c76-a3aa-2d4fe872dd2c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 05:56:59.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3810" for this suite. 11/15/23 05:56:59.582
------------------------------
â€¢ [4.365 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:56:55.244
    Nov 15 05:56:55.244: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename emptydir 11/15/23 05:56:55.245
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:56:55.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:56:55.31
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 11/15/23 05:56:55.324
    Nov 15 05:56:55.376: INFO: Waiting up to 5m0s for pod "pod-a5db3b53-7df5-4c76-a3aa-2d4fe872dd2c" in namespace "emptydir-3810" to be "Succeeded or Failed"
    Nov 15 05:56:55.394: INFO: Pod "pod-a5db3b53-7df5-4c76-a3aa-2d4fe872dd2c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.32908ms
    Nov 15 05:56:57.413: INFO: Pod "pod-a5db3b53-7df5-4c76-a3aa-2d4fe872dd2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037025317s
    Nov 15 05:56:59.415: INFO: Pod "pod-a5db3b53-7df5-4c76-a3aa-2d4fe872dd2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038818086s
    STEP: Saw pod success 11/15/23 05:56:59.415
    Nov 15 05:56:59.415: INFO: Pod "pod-a5db3b53-7df5-4c76-a3aa-2d4fe872dd2c" satisfied condition "Succeeded or Failed"
    Nov 15 05:56:59.433: INFO: Trying to get logs from node 10.72.152.88 pod pod-a5db3b53-7df5-4c76-a3aa-2d4fe872dd2c container test-container: <nil>
    STEP: delete the pod 11/15/23 05:56:59.474
    Nov 15 05:56:59.536: INFO: Waiting for pod pod-a5db3b53-7df5-4c76-a3aa-2d4fe872dd2c to disappear
    Nov 15 05:56:59.553: INFO: Pod pod-a5db3b53-7df5-4c76-a3aa-2d4fe872dd2c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:56:59.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3810" for this suite. 11/15/23 05:56:59.582
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:56:59.61
Nov 15 05:56:59.610: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename cronjob 11/15/23 05:56:59.611
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:56:59.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:56:59.678
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 11/15/23 05:56:59.691
W1115 05:56:59.714580      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring more than one job is running at a time 11/15/23 05:56:59.714
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 11/15/23 05:58:01.736
STEP: Removing cronjob 11/15/23 05:58:01.75
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Nov 15 05:58:01.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8665" for this suite. 11/15/23 05:58:01.86
------------------------------
â€¢ [SLOW TEST] [62.275 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:56:59.61
    Nov 15 05:56:59.610: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename cronjob 11/15/23 05:56:59.611
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:56:59.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:56:59.678
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 11/15/23 05:56:59.691
    W1115 05:56:59.714580      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring more than one job is running at a time 11/15/23 05:56:59.714
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 11/15/23 05:58:01.736
    STEP: Removing cronjob 11/15/23 05:58:01.75
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:58:01.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8665" for this suite. 11/15/23 05:58:01.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:58:01.886
Nov 15 05:58:01.886: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename hostport 11/15/23 05:58:01.886
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:01.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:02.001
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 11/15/23 05:58:02.082
Nov 15 05:58:02.118: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-6840" to be "running and ready"
Nov 15 05:58:02.135: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.258926ms
Nov 15 05:58:02.135: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 05:58:04.158: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039394551s
Nov 15 05:58:04.158: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 05:58:06.154: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.036216799s
Nov 15 05:58:06.154: INFO: The phase of Pod pod1 is Running (Ready = true)
Nov 15 05:58:06.154: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.72.152.81 on the node which pod1 resides and expect scheduled 11/15/23 05:58:06.155
Nov 15 05:58:06.189: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-6840" to be "running and ready"
Nov 15 05:58:06.206: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.553144ms
Nov 15 05:58:06.206: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 05:58:08.226: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036039736s
Nov 15 05:58:08.226: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 05:58:10.222: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.032766892s
Nov 15 05:58:10.222: INFO: The phase of Pod pod2 is Running (Ready = true)
Nov 15 05:58:10.222: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.72.152.81 but use UDP protocol on the node which pod2 resides 11/15/23 05:58:10.222
Nov 15 05:58:10.249: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-6840" to be "running and ready"
Nov 15 05:58:10.274: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 24.196272ms
Nov 15 05:58:10.274: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 05:58:12.348: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.098269239s
Nov 15 05:58:12.348: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 05:58:14.291: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.042046441s
Nov 15 05:58:14.291: INFO: The phase of Pod pod3 is Running (Ready = true)
Nov 15 05:58:14.291: INFO: Pod "pod3" satisfied condition "running and ready"
Nov 15 05:58:14.320: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-6840" to be "running and ready"
Nov 15 05:58:14.339: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 19.475046ms
Nov 15 05:58:14.339: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Nov 15 05:58:16.358: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.037843477s
Nov 15 05:58:16.358: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Nov 15 05:58:16.358: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 11/15/23 05:58:16.376
Nov 15 05:58:16.376: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.72.152.81 http://127.0.0.1:54323/hostname] Namespace:hostport-6840 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 05:58:16.377: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 05:58:16.377: INFO: ExecWithOptions: Clientset creation
Nov 15 05:58:16.377: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-6840/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.72.152.81+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.72.152.81, port: 54323 11/15/23 05:58:16.591
Nov 15 05:58:16.591: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.72.152.81:54323/hostname] Namespace:hostport-6840 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 05:58:16.591: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 05:58:16.592: INFO: ExecWithOptions: Clientset creation
Nov 15 05:58:16.592: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-6840/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.72.152.81%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.72.152.81, port: 54323 UDP 11/15/23 05:58:16.803
Nov 15 05:58:16.803: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.72.152.81 54323] Namespace:hostport-6840 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 05:58:16.803: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 05:58:16.804: INFO: ExecWithOptions: Clientset creation
Nov 15 05:58:16.804: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-6840/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.72.152.81+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Nov 15 05:58:22.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-6840" for this suite. 11/15/23 05:58:22.065
------------------------------
â€¢ [SLOW TEST] [20.206 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:58:01.886
    Nov 15 05:58:01.886: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename hostport 11/15/23 05:58:01.886
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:01.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:02.001
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 11/15/23 05:58:02.082
    Nov 15 05:58:02.118: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-6840" to be "running and ready"
    Nov 15 05:58:02.135: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.258926ms
    Nov 15 05:58:02.135: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 05:58:04.158: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039394551s
    Nov 15 05:58:04.158: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 05:58:06.154: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.036216799s
    Nov 15 05:58:06.154: INFO: The phase of Pod pod1 is Running (Ready = true)
    Nov 15 05:58:06.154: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.72.152.81 on the node which pod1 resides and expect scheduled 11/15/23 05:58:06.155
    Nov 15 05:58:06.189: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-6840" to be "running and ready"
    Nov 15 05:58:06.206: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.553144ms
    Nov 15 05:58:06.206: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 05:58:08.226: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036039736s
    Nov 15 05:58:08.226: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 05:58:10.222: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.032766892s
    Nov 15 05:58:10.222: INFO: The phase of Pod pod2 is Running (Ready = true)
    Nov 15 05:58:10.222: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.72.152.81 but use UDP protocol on the node which pod2 resides 11/15/23 05:58:10.222
    Nov 15 05:58:10.249: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-6840" to be "running and ready"
    Nov 15 05:58:10.274: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 24.196272ms
    Nov 15 05:58:10.274: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 05:58:12.348: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.098269239s
    Nov 15 05:58:12.348: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 05:58:14.291: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.042046441s
    Nov 15 05:58:14.291: INFO: The phase of Pod pod3 is Running (Ready = true)
    Nov 15 05:58:14.291: INFO: Pod "pod3" satisfied condition "running and ready"
    Nov 15 05:58:14.320: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-6840" to be "running and ready"
    Nov 15 05:58:14.339: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 19.475046ms
    Nov 15 05:58:14.339: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 05:58:16.358: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.037843477s
    Nov 15 05:58:16.358: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Nov 15 05:58:16.358: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 11/15/23 05:58:16.376
    Nov 15 05:58:16.376: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.72.152.81 http://127.0.0.1:54323/hostname] Namespace:hostport-6840 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 05:58:16.377: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 05:58:16.377: INFO: ExecWithOptions: Clientset creation
    Nov 15 05:58:16.377: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-6840/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.72.152.81+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.72.152.81, port: 54323 11/15/23 05:58:16.591
    Nov 15 05:58:16.591: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.72.152.81:54323/hostname] Namespace:hostport-6840 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 05:58:16.591: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 05:58:16.592: INFO: ExecWithOptions: Clientset creation
    Nov 15 05:58:16.592: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-6840/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.72.152.81%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.72.152.81, port: 54323 UDP 11/15/23 05:58:16.803
    Nov 15 05:58:16.803: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.72.152.81 54323] Namespace:hostport-6840 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 05:58:16.803: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 05:58:16.804: INFO: ExecWithOptions: Clientset creation
    Nov 15 05:58:16.804: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-6840/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.72.152.81+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:58:22.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-6840" for this suite. 11/15/23 05:58:22.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:58:22.094
Nov 15 05:58:22.094: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename emptydir 11/15/23 05:58:22.095
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:22.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:22.167
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 11/15/23 05:58:22.181
Nov 15 05:58:22.216: INFO: Waiting up to 5m0s for pod "pod-2df897fb-c80d-42f0-93d3-b8f5e6c5f63a" in namespace "emptydir-7772" to be "Succeeded or Failed"
Nov 15 05:58:22.237: INFO: Pod "pod-2df897fb-c80d-42f0-93d3-b8f5e6c5f63a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.824283ms
Nov 15 05:58:24.282: INFO: Pod "pod-2df897fb-c80d-42f0-93d3-b8f5e6c5f63a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066455231s
Nov 15 05:58:26.263: INFO: Pod "pod-2df897fb-c80d-42f0-93d3-b8f5e6c5f63a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047478789s
Nov 15 05:58:28.258: INFO: Pod "pod-2df897fb-c80d-42f0-93d3-b8f5e6c5f63a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04224837s
STEP: Saw pod success 11/15/23 05:58:28.258
Nov 15 05:58:28.258: INFO: Pod "pod-2df897fb-c80d-42f0-93d3-b8f5e6c5f63a" satisfied condition "Succeeded or Failed"
Nov 15 05:58:28.279: INFO: Trying to get logs from node 10.72.152.88 pod pod-2df897fb-c80d-42f0-93d3-b8f5e6c5f63a container test-container: <nil>
STEP: delete the pod 11/15/23 05:58:28.321
Nov 15 05:58:28.376: INFO: Waiting for pod pod-2df897fb-c80d-42f0-93d3-b8f5e6c5f63a to disappear
Nov 15 05:58:28.393: INFO: Pod pod-2df897fb-c80d-42f0-93d3-b8f5e6c5f63a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 05:58:28.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7772" for this suite. 11/15/23 05:58:28.422
------------------------------
â€¢ [SLOW TEST] [6.363 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:58:22.094
    Nov 15 05:58:22.094: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename emptydir 11/15/23 05:58:22.095
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:22.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:22.167
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 11/15/23 05:58:22.181
    Nov 15 05:58:22.216: INFO: Waiting up to 5m0s for pod "pod-2df897fb-c80d-42f0-93d3-b8f5e6c5f63a" in namespace "emptydir-7772" to be "Succeeded or Failed"
    Nov 15 05:58:22.237: INFO: Pod "pod-2df897fb-c80d-42f0-93d3-b8f5e6c5f63a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.824283ms
    Nov 15 05:58:24.282: INFO: Pod "pod-2df897fb-c80d-42f0-93d3-b8f5e6c5f63a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066455231s
    Nov 15 05:58:26.263: INFO: Pod "pod-2df897fb-c80d-42f0-93d3-b8f5e6c5f63a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047478789s
    Nov 15 05:58:28.258: INFO: Pod "pod-2df897fb-c80d-42f0-93d3-b8f5e6c5f63a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04224837s
    STEP: Saw pod success 11/15/23 05:58:28.258
    Nov 15 05:58:28.258: INFO: Pod "pod-2df897fb-c80d-42f0-93d3-b8f5e6c5f63a" satisfied condition "Succeeded or Failed"
    Nov 15 05:58:28.279: INFO: Trying to get logs from node 10.72.152.88 pod pod-2df897fb-c80d-42f0-93d3-b8f5e6c5f63a container test-container: <nil>
    STEP: delete the pod 11/15/23 05:58:28.321
    Nov 15 05:58:28.376: INFO: Waiting for pod pod-2df897fb-c80d-42f0-93d3-b8f5e6c5f63a to disappear
    Nov 15 05:58:28.393: INFO: Pod pod-2df897fb-c80d-42f0-93d3-b8f5e6c5f63a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:58:28.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7772" for this suite. 11/15/23 05:58:28.422
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:58:28.457
Nov 15 05:58:28.457: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename daemonsets 11/15/23 05:58:28.458
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:28.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:28.544
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
STEP: Creating simple DaemonSet "daemon-set" 11/15/23 05:58:28.68
STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 05:58:28.699
Nov 15 05:58:28.751: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 05:58:28.751: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 05:58:29.819: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 05:58:29.819: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 05:58:30.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 05:58:30.809: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 05:58:31.799: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 15 05:58:31.799: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 11/15/23 05:58:31.81
Nov 15 05:58:31.821: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 11/15/23 05:58:31.821
Nov 15 05:58:31.851: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 11/15/23 05:58:31.851
Nov 15 05:58:31.858: INFO: Observed &DaemonSet event: ADDED
Nov 15 05:58:31.858: INFO: Observed &DaemonSet event: MODIFIED
Nov 15 05:58:31.859: INFO: Observed &DaemonSet event: MODIFIED
Nov 15 05:58:31.859: INFO: Observed &DaemonSet event: MODIFIED
Nov 15 05:58:31.859: INFO: Observed &DaemonSet event: MODIFIED
Nov 15 05:58:31.859: INFO: Found daemon set daemon-set in namespace daemonsets-6974 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 15 05:58:31.859: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 11/15/23 05:58:31.859
STEP: watching for the daemon set status to be patched 11/15/23 05:58:31.878
Nov 15 05:58:31.885: INFO: Observed &DaemonSet event: ADDED
Nov 15 05:58:31.886: INFO: Observed &DaemonSet event: MODIFIED
Nov 15 05:58:31.886: INFO: Observed &DaemonSet event: MODIFIED
Nov 15 05:58:31.886: INFO: Observed &DaemonSet event: MODIFIED
Nov 15 05:58:31.886: INFO: Observed &DaemonSet event: MODIFIED
Nov 15 05:58:31.886: INFO: Observed daemon set daemon-set in namespace daemonsets-6974 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 15 05:58:31.886: INFO: Observed &DaemonSet event: MODIFIED
Nov 15 05:58:31.886: INFO: Found daemon set daemon-set in namespace daemonsets-6974 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Nov 15 05:58:31.886: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 11/15/23 05:58:31.898
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6974, will wait for the garbage collector to delete the pods 11/15/23 05:58:31.898
Nov 15 05:58:31.982: INFO: Deleting DaemonSet.extensions daemon-set took: 21.842185ms
Nov 15 05:58:32.183: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.77914ms
Nov 15 05:58:35.001: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 05:58:35.001: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 15 05:58:35.012: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"66096"},"items":null}

Nov 15 05:58:35.029: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"66097"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 05:58:35.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6974" for this suite. 11/15/23 05:58:35.149
------------------------------
â€¢ [SLOW TEST] [6.717 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:58:28.457
    Nov 15 05:58:28.457: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename daemonsets 11/15/23 05:58:28.458
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:28.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:28.544
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:873
    STEP: Creating simple DaemonSet "daemon-set" 11/15/23 05:58:28.68
    STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 05:58:28.699
    Nov 15 05:58:28.751: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 05:58:28.751: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 05:58:29.819: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 05:58:29.819: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 05:58:30.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 05:58:30.809: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 05:58:31.799: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 15 05:58:31.799: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 11/15/23 05:58:31.81
    Nov 15 05:58:31.821: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 11/15/23 05:58:31.821
    Nov 15 05:58:31.851: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 11/15/23 05:58:31.851
    Nov 15 05:58:31.858: INFO: Observed &DaemonSet event: ADDED
    Nov 15 05:58:31.858: INFO: Observed &DaemonSet event: MODIFIED
    Nov 15 05:58:31.859: INFO: Observed &DaemonSet event: MODIFIED
    Nov 15 05:58:31.859: INFO: Observed &DaemonSet event: MODIFIED
    Nov 15 05:58:31.859: INFO: Observed &DaemonSet event: MODIFIED
    Nov 15 05:58:31.859: INFO: Found daemon set daemon-set in namespace daemonsets-6974 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Nov 15 05:58:31.859: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 11/15/23 05:58:31.859
    STEP: watching for the daemon set status to be patched 11/15/23 05:58:31.878
    Nov 15 05:58:31.885: INFO: Observed &DaemonSet event: ADDED
    Nov 15 05:58:31.886: INFO: Observed &DaemonSet event: MODIFIED
    Nov 15 05:58:31.886: INFO: Observed &DaemonSet event: MODIFIED
    Nov 15 05:58:31.886: INFO: Observed &DaemonSet event: MODIFIED
    Nov 15 05:58:31.886: INFO: Observed &DaemonSet event: MODIFIED
    Nov 15 05:58:31.886: INFO: Observed daemon set daemon-set in namespace daemonsets-6974 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Nov 15 05:58:31.886: INFO: Observed &DaemonSet event: MODIFIED
    Nov 15 05:58:31.886: INFO: Found daemon set daemon-set in namespace daemonsets-6974 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Nov 15 05:58:31.886: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 11/15/23 05:58:31.898
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6974, will wait for the garbage collector to delete the pods 11/15/23 05:58:31.898
    Nov 15 05:58:31.982: INFO: Deleting DaemonSet.extensions daemon-set took: 21.842185ms
    Nov 15 05:58:32.183: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.77914ms
    Nov 15 05:58:35.001: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 05:58:35.001: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 15 05:58:35.012: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"66096"},"items":null}

    Nov 15 05:58:35.029: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"66097"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:58:35.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6974" for this suite. 11/15/23 05:58:35.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:58:35.176
Nov 15 05:58:35.176: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename configmap 11/15/23 05:58:35.177
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:35.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:35.242
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-1360/configmap-test-46facf7b-ef5d-47e2-8884-d40a4df72581 11/15/23 05:58:35.255
STEP: Creating a pod to test consume configMaps 11/15/23 05:58:35.281
Nov 15 05:58:35.320: INFO: Waiting up to 5m0s for pod "pod-configmaps-e49f2d4d-a9db-4549-9c1f-73897e2927ad" in namespace "configmap-1360" to be "Succeeded or Failed"
Nov 15 05:58:35.346: INFO: Pod "pod-configmaps-e49f2d4d-a9db-4549-9c1f-73897e2927ad": Phase="Pending", Reason="", readiness=false. Elapsed: 25.42621ms
Nov 15 05:58:37.370: INFO: Pod "pod-configmaps-e49f2d4d-a9db-4549-9c1f-73897e2927ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05004925s
Nov 15 05:58:39.371: INFO: Pod "pod-configmaps-e49f2d4d-a9db-4549-9c1f-73897e2927ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050803853s
STEP: Saw pod success 11/15/23 05:58:39.371
Nov 15 05:58:39.372: INFO: Pod "pod-configmaps-e49f2d4d-a9db-4549-9c1f-73897e2927ad" satisfied condition "Succeeded or Failed"
Nov 15 05:58:39.389: INFO: Trying to get logs from node 10.72.152.81 pod pod-configmaps-e49f2d4d-a9db-4549-9c1f-73897e2927ad container env-test: <nil>
STEP: delete the pod 11/15/23 05:58:39.486
Nov 15 05:58:39.546: INFO: Waiting for pod pod-configmaps-e49f2d4d-a9db-4549-9c1f-73897e2927ad to disappear
Nov 15 05:58:39.563: INFO: Pod pod-configmaps-e49f2d4d-a9db-4549-9c1f-73897e2927ad no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 05:58:39.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1360" for this suite. 11/15/23 05:58:39.594
------------------------------
â€¢ [4.445 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:58:35.176
    Nov 15 05:58:35.176: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename configmap 11/15/23 05:58:35.177
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:35.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:35.242
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-1360/configmap-test-46facf7b-ef5d-47e2-8884-d40a4df72581 11/15/23 05:58:35.255
    STEP: Creating a pod to test consume configMaps 11/15/23 05:58:35.281
    Nov 15 05:58:35.320: INFO: Waiting up to 5m0s for pod "pod-configmaps-e49f2d4d-a9db-4549-9c1f-73897e2927ad" in namespace "configmap-1360" to be "Succeeded or Failed"
    Nov 15 05:58:35.346: INFO: Pod "pod-configmaps-e49f2d4d-a9db-4549-9c1f-73897e2927ad": Phase="Pending", Reason="", readiness=false. Elapsed: 25.42621ms
    Nov 15 05:58:37.370: INFO: Pod "pod-configmaps-e49f2d4d-a9db-4549-9c1f-73897e2927ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05004925s
    Nov 15 05:58:39.371: INFO: Pod "pod-configmaps-e49f2d4d-a9db-4549-9c1f-73897e2927ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050803853s
    STEP: Saw pod success 11/15/23 05:58:39.371
    Nov 15 05:58:39.372: INFO: Pod "pod-configmaps-e49f2d4d-a9db-4549-9c1f-73897e2927ad" satisfied condition "Succeeded or Failed"
    Nov 15 05:58:39.389: INFO: Trying to get logs from node 10.72.152.81 pod pod-configmaps-e49f2d4d-a9db-4549-9c1f-73897e2927ad container env-test: <nil>
    STEP: delete the pod 11/15/23 05:58:39.486
    Nov 15 05:58:39.546: INFO: Waiting for pod pod-configmaps-e49f2d4d-a9db-4549-9c1f-73897e2927ad to disappear
    Nov 15 05:58:39.563: INFO: Pod pod-configmaps-e49f2d4d-a9db-4549-9c1f-73897e2927ad no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:58:39.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1360" for this suite. 11/15/23 05:58:39.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:58:39.621
Nov 15 05:58:39.621: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename secrets 11/15/23 05:58:39.622
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:39.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:39.699
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-f3a88773-529a-4a52-9f10-1fb6ed27127e 11/15/23 05:58:39.711
STEP: Creating a pod to test consume secrets 11/15/23 05:58:39.729
Nov 15 05:58:39.773: INFO: Waiting up to 5m0s for pod "pod-secrets-bf2124e9-e7da-42bb-8fe5-f1433a9a2766" in namespace "secrets-526" to be "Succeeded or Failed"
Nov 15 05:58:39.791: INFO: Pod "pod-secrets-bf2124e9-e7da-42bb-8fe5-f1433a9a2766": Phase="Pending", Reason="", readiness=false. Elapsed: 17.278433ms
Nov 15 05:58:41.811: INFO: Pod "pod-secrets-bf2124e9-e7da-42bb-8fe5-f1433a9a2766": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037720223s
Nov 15 05:58:43.810: INFO: Pod "pod-secrets-bf2124e9-e7da-42bb-8fe5-f1433a9a2766": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036757288s
Nov 15 05:58:45.809: INFO: Pod "pod-secrets-bf2124e9-e7da-42bb-8fe5-f1433a9a2766": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035856623s
STEP: Saw pod success 11/15/23 05:58:45.809
Nov 15 05:58:45.809: INFO: Pod "pod-secrets-bf2124e9-e7da-42bb-8fe5-f1433a9a2766" satisfied condition "Succeeded or Failed"
Nov 15 05:58:45.827: INFO: Trying to get logs from node 10.72.152.81 pod pod-secrets-bf2124e9-e7da-42bb-8fe5-f1433a9a2766 container secret-env-test: <nil>
STEP: delete the pod 11/15/23 05:58:45.9
Nov 15 05:58:45.982: INFO: Waiting for pod pod-secrets-bf2124e9-e7da-42bb-8fe5-f1433a9a2766 to disappear
Nov 15 05:58:45.998: INFO: Pod pod-secrets-bf2124e9-e7da-42bb-8fe5-f1433a9a2766 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 05:58:45.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-526" for this suite. 11/15/23 05:58:46.029
------------------------------
â€¢ [SLOW TEST] [6.433 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:58:39.621
    Nov 15 05:58:39.621: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename secrets 11/15/23 05:58:39.622
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:39.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:39.699
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-f3a88773-529a-4a52-9f10-1fb6ed27127e 11/15/23 05:58:39.711
    STEP: Creating a pod to test consume secrets 11/15/23 05:58:39.729
    Nov 15 05:58:39.773: INFO: Waiting up to 5m0s for pod "pod-secrets-bf2124e9-e7da-42bb-8fe5-f1433a9a2766" in namespace "secrets-526" to be "Succeeded or Failed"
    Nov 15 05:58:39.791: INFO: Pod "pod-secrets-bf2124e9-e7da-42bb-8fe5-f1433a9a2766": Phase="Pending", Reason="", readiness=false. Elapsed: 17.278433ms
    Nov 15 05:58:41.811: INFO: Pod "pod-secrets-bf2124e9-e7da-42bb-8fe5-f1433a9a2766": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037720223s
    Nov 15 05:58:43.810: INFO: Pod "pod-secrets-bf2124e9-e7da-42bb-8fe5-f1433a9a2766": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036757288s
    Nov 15 05:58:45.809: INFO: Pod "pod-secrets-bf2124e9-e7da-42bb-8fe5-f1433a9a2766": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035856623s
    STEP: Saw pod success 11/15/23 05:58:45.809
    Nov 15 05:58:45.809: INFO: Pod "pod-secrets-bf2124e9-e7da-42bb-8fe5-f1433a9a2766" satisfied condition "Succeeded or Failed"
    Nov 15 05:58:45.827: INFO: Trying to get logs from node 10.72.152.81 pod pod-secrets-bf2124e9-e7da-42bb-8fe5-f1433a9a2766 container secret-env-test: <nil>
    STEP: delete the pod 11/15/23 05:58:45.9
    Nov 15 05:58:45.982: INFO: Waiting for pod pod-secrets-bf2124e9-e7da-42bb-8fe5-f1433a9a2766 to disappear
    Nov 15 05:58:45.998: INFO: Pod pod-secrets-bf2124e9-e7da-42bb-8fe5-f1433a9a2766 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:58:45.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-526" for this suite. 11/15/23 05:58:46.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:58:46.056
Nov 15 05:58:46.056: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubectl 11/15/23 05:58:46.057
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:46.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:46.122
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 11/15/23 05:58:46.138
Nov 15 05:58:46.139: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2871 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 11/15/23 05:58:46.192
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 05:58:46.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2871" for this suite. 11/15/23 05:58:46.242
------------------------------
â€¢ [0.216 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:58:46.056
    Nov 15 05:58:46.056: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubectl 11/15/23 05:58:46.057
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:46.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:46.122
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 11/15/23 05:58:46.138
    Nov 15 05:58:46.139: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2871 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 11/15/23 05:58:46.192
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:58:46.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2871" for this suite. 11/15/23 05:58:46.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:58:46.272
Nov 15 05:58:46.272: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename cronjob 11/15/23 05:58:46.273
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:46.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:46.34
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 11/15/23 05:58:46.353
STEP: creating 11/15/23 05:58:46.353
W1115 05:58:46.375554      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: getting 11/15/23 05:58:46.375
STEP: listing 11/15/23 05:58:46.406
STEP: watching 11/15/23 05:58:46.433
Nov 15 05:58:46.433: INFO: starting watch
STEP: cluster-wide listing 11/15/23 05:58:46.439
STEP: cluster-wide watching 11/15/23 05:58:46.455
Nov 15 05:58:46.455: INFO: starting watch
STEP: patching 11/15/23 05:58:46.462
STEP: updating 11/15/23 05:58:46.491
Nov 15 05:58:46.549: INFO: waiting for watch events with expected annotations
Nov 15 05:58:46.549: INFO: saw patched and updated annotations
STEP: patching /status 11/15/23 05:58:46.549
STEP: updating /status 11/15/23 05:58:46.575
STEP: get /status 11/15/23 05:58:46.639
STEP: deleting 11/15/23 05:58:46.656
STEP: deleting a collection 11/15/23 05:58:46.734
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Nov 15 05:58:46.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1198" for this suite. 11/15/23 05:58:46.819
------------------------------
â€¢ [0.578 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:58:46.272
    Nov 15 05:58:46.272: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename cronjob 11/15/23 05:58:46.273
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:46.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:46.34
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 11/15/23 05:58:46.353
    STEP: creating 11/15/23 05:58:46.353
    W1115 05:58:46.375554      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: getting 11/15/23 05:58:46.375
    STEP: listing 11/15/23 05:58:46.406
    STEP: watching 11/15/23 05:58:46.433
    Nov 15 05:58:46.433: INFO: starting watch
    STEP: cluster-wide listing 11/15/23 05:58:46.439
    STEP: cluster-wide watching 11/15/23 05:58:46.455
    Nov 15 05:58:46.455: INFO: starting watch
    STEP: patching 11/15/23 05:58:46.462
    STEP: updating 11/15/23 05:58:46.491
    Nov 15 05:58:46.549: INFO: waiting for watch events with expected annotations
    Nov 15 05:58:46.549: INFO: saw patched and updated annotations
    STEP: patching /status 11/15/23 05:58:46.549
    STEP: updating /status 11/15/23 05:58:46.575
    STEP: get /status 11/15/23 05:58:46.639
    STEP: deleting 11/15/23 05:58:46.656
    STEP: deleting a collection 11/15/23 05:58:46.734
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:58:46.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1198" for this suite. 11/15/23 05:58:46.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:58:46.851
Nov 15 05:58:46.851: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 05:58:46.852
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:46.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:46.921
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 11/15/23 05:58:46.933
Nov 15 05:58:46.960: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5eef2fd-1bb5-4e2d-983a-c267e9ecc6e5" in namespace "projected-407" to be "Succeeded or Failed"
Nov 15 05:58:46.985: INFO: Pod "downwardapi-volume-c5eef2fd-1bb5-4e2d-983a-c267e9ecc6e5": Phase="Pending", Reason="", readiness=false. Elapsed: 25.322852ms
Nov 15 05:58:49.005: INFO: Pod "downwardapi-volume-c5eef2fd-1bb5-4e2d-983a-c267e9ecc6e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044677302s
Nov 15 05:58:51.004: INFO: Pod "downwardapi-volume-c5eef2fd-1bb5-4e2d-983a-c267e9ecc6e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043464752s
STEP: Saw pod success 11/15/23 05:58:51.004
Nov 15 05:58:51.004: INFO: Pod "downwardapi-volume-c5eef2fd-1bb5-4e2d-983a-c267e9ecc6e5" satisfied condition "Succeeded or Failed"
Nov 15 05:58:51.031: INFO: Trying to get logs from node 10.72.152.88 pod downwardapi-volume-c5eef2fd-1bb5-4e2d-983a-c267e9ecc6e5 container client-container: <nil>
STEP: delete the pod 11/15/23 05:58:51.077
Nov 15 05:58:51.141: INFO: Waiting for pod downwardapi-volume-c5eef2fd-1bb5-4e2d-983a-c267e9ecc6e5 to disappear
Nov 15 05:58:51.161: INFO: Pod downwardapi-volume-c5eef2fd-1bb5-4e2d-983a-c267e9ecc6e5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 05:58:51.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-407" for this suite. 11/15/23 05:58:51.191
------------------------------
â€¢ [4.373 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:58:46.851
    Nov 15 05:58:46.851: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 05:58:46.852
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:46.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:46.921
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 11/15/23 05:58:46.933
    Nov 15 05:58:46.960: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5eef2fd-1bb5-4e2d-983a-c267e9ecc6e5" in namespace "projected-407" to be "Succeeded or Failed"
    Nov 15 05:58:46.985: INFO: Pod "downwardapi-volume-c5eef2fd-1bb5-4e2d-983a-c267e9ecc6e5": Phase="Pending", Reason="", readiness=false. Elapsed: 25.322852ms
    Nov 15 05:58:49.005: INFO: Pod "downwardapi-volume-c5eef2fd-1bb5-4e2d-983a-c267e9ecc6e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044677302s
    Nov 15 05:58:51.004: INFO: Pod "downwardapi-volume-c5eef2fd-1bb5-4e2d-983a-c267e9ecc6e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043464752s
    STEP: Saw pod success 11/15/23 05:58:51.004
    Nov 15 05:58:51.004: INFO: Pod "downwardapi-volume-c5eef2fd-1bb5-4e2d-983a-c267e9ecc6e5" satisfied condition "Succeeded or Failed"
    Nov 15 05:58:51.031: INFO: Trying to get logs from node 10.72.152.88 pod downwardapi-volume-c5eef2fd-1bb5-4e2d-983a-c267e9ecc6e5 container client-container: <nil>
    STEP: delete the pod 11/15/23 05:58:51.077
    Nov 15 05:58:51.141: INFO: Waiting for pod downwardapi-volume-c5eef2fd-1bb5-4e2d-983a-c267e9ecc6e5 to disappear
    Nov 15 05:58:51.161: INFO: Pod downwardapi-volume-c5eef2fd-1bb5-4e2d-983a-c267e9ecc6e5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:58:51.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-407" for this suite. 11/15/23 05:58:51.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:58:51.225
Nov 15 05:58:51.226: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename replicaset 11/15/23 05:58:51.227
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:51.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:51.291
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 11/15/23 05:58:51.306
W1115 05:58:51.324287      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Nov 15 05:58:51.355: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 15 05:58:56.382: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/15/23 05:58:56.383
STEP: getting scale subresource 11/15/23 05:58:56.383
STEP: updating a scale subresource 11/15/23 05:58:56.406
STEP: verifying the replicaset Spec.Replicas was modified 11/15/23 05:58:56.425
STEP: Patch a scale subresource 11/15/23 05:58:56.441
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Nov 15 05:58:56.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9676" for this suite. 11/15/23 05:58:56.521
------------------------------
â€¢ [SLOW TEST] [5.326 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:58:51.225
    Nov 15 05:58:51.226: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename replicaset 11/15/23 05:58:51.227
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:51.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:51.291
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 11/15/23 05:58:51.306
    W1115 05:58:51.324287      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Nov 15 05:58:51.355: INFO: Pod name sample-pod: Found 0 pods out of 1
    Nov 15 05:58:56.382: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/15/23 05:58:56.383
    STEP: getting scale subresource 11/15/23 05:58:56.383
    STEP: updating a scale subresource 11/15/23 05:58:56.406
    STEP: verifying the replicaset Spec.Replicas was modified 11/15/23 05:58:56.425
    STEP: Patch a scale subresource 11/15/23 05:58:56.441
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:58:56.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9676" for this suite. 11/15/23 05:58:56.521
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:58:56.552
Nov 15 05:58:56.552: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename deployment 11/15/23 05:58:56.553
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:56.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:56.619
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Nov 15 05:58:56.638: INFO: Creating simple deployment test-new-deployment
W1115 05:58:56.657796      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Nov 15 05:58:56.707: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 11/15/23 05:58:58.766
STEP: updating a scale subresource 11/15/23 05:58:58.777
STEP: verifying the deployment Spec.Replicas was modified 11/15/23 05:58:58.795
STEP: Patch a scale subresource 11/15/23 05:58:58.81
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 15 05:58:58.866: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-2071  b44ba700-b000-43dc-8678-e3b66850718d 66695 3 2023-11-15 05:58:56 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-11-15 05:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 05:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a65948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-11-15 05:58:58 +0000 UTC,LastTransitionTime:2023-11-15 05:58:56 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-11-15 05:58:58 +0000 UTC,LastTransitionTime:2023-11-15 05:58:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 15 05:58:58.881: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-2071  1f5a9623-771d-4b9d-a997-6afcd90e6f18 66700 2 2023-11-15 05:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment b44ba700-b000-43dc-8678-e3b66850718d 0xc003a65d67 0xc003a65d68}] [] [{kube-controller-manager Update apps/v1 2023-11-15 05:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b44ba700-b000-43dc-8678-e3b66850718d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 05:58:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a65df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 15 05:58:58.899: INFO: Pod "test-new-deployment-7f5969cbc7-42sjv" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-42sjv test-new-deployment-7f5969cbc7- deployment-2071  b6a9caa8-cf8f-4cc8-8b9a-c01a9cba507d 66681 0 2023-11-15 05:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:69229180e6f118314dc9dfbd0c674ee30516c926b8705826996eb5e3a7177980 cni.projectcalico.org/podIP:172.30.213.154/32 cni.projectcalico.org/podIPs:172.30.213.154/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.213.154"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 1f5a9623-771d-4b9d-a997-6afcd90e6f18 0xc004864197 0xc004864198}] [] [{kube-controller-manager Update v1 2023-11-15 05:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f5a9623-771d-4b9d-a997-6afcd90e6f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 05:58:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 05:58:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 05:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tvpw7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tvpw7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:172.30.213.154,StartTime:2023-11-15 05:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 05:58:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://9ab77a34fe886e31fbdba405507f105c396ae898ea23e44c78ac4594c98f23af,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.213.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 05:58:58.900: INFO: Pod "test-new-deployment-7f5969cbc7-gf58p" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-gf58p test-new-deployment-7f5969cbc7- deployment-2071  243152da-66f1-4ef2-85bc-5e2ac6f04d6d 66701 0 2023-11-15 05:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 1f5a9623-771d-4b9d-a997-6afcd90e6f18 0xc004864407 0xc004864408}] [] [{kube-controller-manager Update v1 2023-11-15 05:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f5a9623-771d-4b9d-a997-6afcd90e6f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 05:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qvqvm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvqvm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-g7m9d,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:,StartTime:2023-11-15 05:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 15 05:58:58.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2071" for this suite. 11/15/23 05:58:58.934
------------------------------
â€¢ [2.405 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:58:56.552
    Nov 15 05:58:56.552: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename deployment 11/15/23 05:58:56.553
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:56.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:56.619
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Nov 15 05:58:56.638: INFO: Creating simple deployment test-new-deployment
    W1115 05:58:56.657796      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Nov 15 05:58:56.707: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 11/15/23 05:58:58.766
    STEP: updating a scale subresource 11/15/23 05:58:58.777
    STEP: verifying the deployment Spec.Replicas was modified 11/15/23 05:58:58.795
    STEP: Patch a scale subresource 11/15/23 05:58:58.81
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 15 05:58:58.866: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-2071  b44ba700-b000-43dc-8678-e3b66850718d 66695 3 2023-11-15 05:58:56 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-11-15 05:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 05:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a65948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-11-15 05:58:58 +0000 UTC,LastTransitionTime:2023-11-15 05:58:56 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-11-15 05:58:58 +0000 UTC,LastTransitionTime:2023-11-15 05:58:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Nov 15 05:58:58.881: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-2071  1f5a9623-771d-4b9d-a997-6afcd90e6f18 66700 2 2023-11-15 05:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment b44ba700-b000-43dc-8678-e3b66850718d 0xc003a65d67 0xc003a65d68}] [] [{kube-controller-manager Update apps/v1 2023-11-15 05:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b44ba700-b000-43dc-8678-e3b66850718d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 05:58:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a65df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 05:58:58.899: INFO: Pod "test-new-deployment-7f5969cbc7-42sjv" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-42sjv test-new-deployment-7f5969cbc7- deployment-2071  b6a9caa8-cf8f-4cc8-8b9a-c01a9cba507d 66681 0 2023-11-15 05:58:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:69229180e6f118314dc9dfbd0c674ee30516c926b8705826996eb5e3a7177980 cni.projectcalico.org/podIP:172.30.213.154/32 cni.projectcalico.org/podIPs:172.30.213.154/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.213.154"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 1f5a9623-771d-4b9d-a997-6afcd90e6f18 0xc004864197 0xc004864198}] [] [{kube-controller-manager Update v1 2023-11-15 05:58:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f5a9623-771d-4b9d-a997-6afcd90e6f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 05:58:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 05:58:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 05:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tvpw7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tvpw7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:172.30.213.154,StartTime:2023-11-15 05:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 05:58:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://9ab77a34fe886e31fbdba405507f105c396ae898ea23e44c78ac4594c98f23af,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.213.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 05:58:58.900: INFO: Pod "test-new-deployment-7f5969cbc7-gf58p" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-gf58p test-new-deployment-7f5969cbc7- deployment-2071  243152da-66f1-4ef2-85bc-5e2ac6f04d6d 66701 0 2023-11-15 05:58:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 1f5a9623-771d-4b9d-a997-6afcd90e6f18 0xc004864407 0xc004864408}] [] [{kube-controller-manager Update v1 2023-11-15 05:58:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f5a9623-771d-4b9d-a997-6afcd90e6f18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 05:58:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qvqvm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvqvm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-g7m9d,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:,StartTime:2023-11-15 05:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:58:58.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2071" for this suite. 11/15/23 05:58:58.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:58:58.961
Nov 15 05:58:58.961: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename deployment 11/15/23 05:58:58.962
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:59.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:59.018
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 11/15/23 05:58:59.049
Nov 15 05:58:59.049: INFO: Creating simple deployment test-deployment-hvj9z
Nov 15 05:58:59.103: INFO: deployment "test-deployment-hvj9z" doesn't have the required revision set
Nov 15 05:59:01.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 5, 58, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 5, 58, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 5, 58, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 5, 58, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-hvj9z-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 11/15/23 05:59:03.182
Nov 15 05:59:03.195: INFO: Deployment test-deployment-hvj9z has Conditions: [{Available True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:59:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:58:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hvj9z-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 11/15/23 05:59:03.195
Nov 15 05:59:03.232: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 5, 59, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 5, 59, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 5, 59, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 5, 58, 59, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-hvj9z-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 11/15/23 05:59:03.232
Nov 15 05:59:03.241: INFO: Observed &Deployment event: ADDED
Nov 15 05:59:03.241: INFO: Observed Deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hvj9z-54bc444df"}
Nov 15 05:59:03.241: INFO: Observed &Deployment event: MODIFIED
Nov 15 05:59:03.241: INFO: Observed Deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hvj9z-54bc444df"}
Nov 15 05:59:03.241: INFO: Observed Deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 15 05:59:03.241: INFO: Observed &Deployment event: MODIFIED
Nov 15 05:59:03.241: INFO: Observed Deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 15 05:59:03.241: INFO: Observed Deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-hvj9z-54bc444df" is progressing.}
Nov 15 05:59:03.241: INFO: Observed &Deployment event: MODIFIED
Nov 15 05:59:03.241: INFO: Observed Deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:59:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 15 05:59:03.242: INFO: Observed Deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:58:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hvj9z-54bc444df" has successfully progressed.}
Nov 15 05:59:03.242: INFO: Observed &Deployment event: MODIFIED
Nov 15 05:59:03.242: INFO: Observed Deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:59:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 15 05:59:03.242: INFO: Observed Deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:58:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hvj9z-54bc444df" has successfully progressed.}
Nov 15 05:59:03.242: INFO: Found Deployment test-deployment-hvj9z in namespace deployment-2174 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 15 05:59:03.242: INFO: Deployment test-deployment-hvj9z has an updated status
STEP: patching the Statefulset Status 11/15/23 05:59:03.242
Nov 15 05:59:03.242: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 15 05:59:03.263: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 11/15/23 05:59:03.264
Nov 15 05:59:03.277: INFO: Observed &Deployment event: ADDED
Nov 15 05:59:03.277: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hvj9z-54bc444df"}
Nov 15 05:59:03.277: INFO: Observed &Deployment event: MODIFIED
Nov 15 05:59:03.277: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hvj9z-54bc444df"}
Nov 15 05:59:03.277: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 15 05:59:03.278: INFO: Observed &Deployment event: MODIFIED
Nov 15 05:59:03.278: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 15 05:59:03.278: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-hvj9z-54bc444df" is progressing.}
Nov 15 05:59:03.278: INFO: Observed &Deployment event: MODIFIED
Nov 15 05:59:03.278: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:59:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 15 05:59:03.278: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:58:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hvj9z-54bc444df" has successfully progressed.}
Nov 15 05:59:03.278: INFO: Observed &Deployment event: MODIFIED
Nov 15 05:59:03.278: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:59:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 15 05:59:03.278: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:58:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hvj9z-54bc444df" has successfully progressed.}
Nov 15 05:59:03.278: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 15 05:59:03.278: INFO: Observed &Deployment event: MODIFIED
Nov 15 05:59:03.278: INFO: Found deployment test-deployment-hvj9z in namespace deployment-2174 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Nov 15 05:59:03.278: INFO: Deployment test-deployment-hvj9z has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 15 05:59:03.292: INFO: Deployment "test-deployment-hvj9z":
&Deployment{ObjectMeta:{test-deployment-hvj9z  deployment-2174  e8628189-f84b-4efb-88ed-dad624463649 66823 1 2023-11-15 05:58:59 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-11-15 05:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-11-15 05:59:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-11-15 05:59:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00457e8a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-hvj9z-54bc444df",LastUpdateTime:2023-11-15 05:59:03 +0000 UTC,LastTransitionTime:2023-11-15 05:59:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 15 05:59:03.308: INFO: New ReplicaSet "test-deployment-hvj9z-54bc444df" of Deployment "test-deployment-hvj9z":
&ReplicaSet{ObjectMeta:{test-deployment-hvj9z-54bc444df  deployment-2174  af5c9b22-f0c4-45f3-b48a-d41b47783fa9 66780 1 2023-11-15 05:58:59 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-hvj9z e8628189-f84b-4efb-88ed-dad624463649 0xc00457ec90 0xc00457ec91}] [] [{kube-controller-manager Update apps/v1 2023-11-15 05:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e8628189-f84b-4efb-88ed-dad624463649\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 05:59:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00457ed38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 15 05:59:03.337: INFO: Pod "test-deployment-hvj9z-54bc444df-rzd4c" is available:
&Pod{ObjectMeta:{test-deployment-hvj9z-54bc444df-rzd4c test-deployment-hvj9z-54bc444df- deployment-2174  931fe80d-93c3-4d16-a06f-bf8871761223 66779 0 2023-11-15 05:58:59 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:fc9d43d23699df481f7f7b24254b5a609566ae3c1a97ae8c8582a04eeddecb82 cni.projectcalico.org/podIP:172.30.213.151/32 cni.projectcalico.org/podIPs:172.30.213.151/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.213.151"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-hvj9z-54bc444df af5c9b22-f0c4-45f3-b48a-d41b47783fa9 0xc00412dee7 0xc00412dee8}] [] [{calico Update v1 2023-11-15 05:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-11-15 05:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af5c9b22-f0c4-45f3-b48a-d41b47783fa9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-11-15 05:59:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 05:59:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8t8vm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8t8vm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c36,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:59:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:59:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:172.30.213.151,StartTime:2023-11-15 05:58:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 05:59:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://8853f7d89a6a45ab29ef8e8824173f70389d99d6f92c6767444ceb707ec879c7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.213.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 15 05:59:03.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2174" for this suite. 11/15/23 05:59:03.383
------------------------------
â€¢ [4.449 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:58:58.961
    Nov 15 05:58:58.961: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename deployment 11/15/23 05:58:58.962
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:58:59.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:58:59.018
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 11/15/23 05:58:59.049
    Nov 15 05:58:59.049: INFO: Creating simple deployment test-deployment-hvj9z
    Nov 15 05:58:59.103: INFO: deployment "test-deployment-hvj9z" doesn't have the required revision set
    Nov 15 05:59:01.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 5, 58, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 5, 58, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 5, 58, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 5, 58, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-hvj9z-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 11/15/23 05:59:03.182
    Nov 15 05:59:03.195: INFO: Deployment test-deployment-hvj9z has Conditions: [{Available True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:59:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:58:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hvj9z-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 11/15/23 05:59:03.195
    Nov 15 05:59:03.232: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 5, 59, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 5, 59, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 5, 59, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 5, 58, 59, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-hvj9z-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 11/15/23 05:59:03.232
    Nov 15 05:59:03.241: INFO: Observed &Deployment event: ADDED
    Nov 15 05:59:03.241: INFO: Observed Deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hvj9z-54bc444df"}
    Nov 15 05:59:03.241: INFO: Observed &Deployment event: MODIFIED
    Nov 15 05:59:03.241: INFO: Observed Deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hvj9z-54bc444df"}
    Nov 15 05:59:03.241: INFO: Observed Deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Nov 15 05:59:03.241: INFO: Observed &Deployment event: MODIFIED
    Nov 15 05:59:03.241: INFO: Observed Deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Nov 15 05:59:03.241: INFO: Observed Deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-hvj9z-54bc444df" is progressing.}
    Nov 15 05:59:03.241: INFO: Observed &Deployment event: MODIFIED
    Nov 15 05:59:03.241: INFO: Observed Deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:59:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Nov 15 05:59:03.242: INFO: Observed Deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:58:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hvj9z-54bc444df" has successfully progressed.}
    Nov 15 05:59:03.242: INFO: Observed &Deployment event: MODIFIED
    Nov 15 05:59:03.242: INFO: Observed Deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:59:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Nov 15 05:59:03.242: INFO: Observed Deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:58:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hvj9z-54bc444df" has successfully progressed.}
    Nov 15 05:59:03.242: INFO: Found Deployment test-deployment-hvj9z in namespace deployment-2174 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Nov 15 05:59:03.242: INFO: Deployment test-deployment-hvj9z has an updated status
    STEP: patching the Statefulset Status 11/15/23 05:59:03.242
    Nov 15 05:59:03.242: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Nov 15 05:59:03.263: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 11/15/23 05:59:03.264
    Nov 15 05:59:03.277: INFO: Observed &Deployment event: ADDED
    Nov 15 05:59:03.277: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hvj9z-54bc444df"}
    Nov 15 05:59:03.277: INFO: Observed &Deployment event: MODIFIED
    Nov 15 05:59:03.277: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hvj9z-54bc444df"}
    Nov 15 05:59:03.277: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Nov 15 05:59:03.278: INFO: Observed &Deployment event: MODIFIED
    Nov 15 05:59:03.278: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Nov 15 05:59:03.278: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:58:59 +0000 UTC 2023-11-15 05:58:59 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-hvj9z-54bc444df" is progressing.}
    Nov 15 05:59:03.278: INFO: Observed &Deployment event: MODIFIED
    Nov 15 05:59:03.278: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:59:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Nov 15 05:59:03.278: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:58:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hvj9z-54bc444df" has successfully progressed.}
    Nov 15 05:59:03.278: INFO: Observed &Deployment event: MODIFIED
    Nov 15 05:59:03.278: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:59:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Nov 15 05:59:03.278: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 05:59:01 +0000 UTC 2023-11-15 05:58:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hvj9z-54bc444df" has successfully progressed.}
    Nov 15 05:59:03.278: INFO: Observed deployment test-deployment-hvj9z in namespace deployment-2174 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Nov 15 05:59:03.278: INFO: Observed &Deployment event: MODIFIED
    Nov 15 05:59:03.278: INFO: Found deployment test-deployment-hvj9z in namespace deployment-2174 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Nov 15 05:59:03.278: INFO: Deployment test-deployment-hvj9z has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 15 05:59:03.292: INFO: Deployment "test-deployment-hvj9z":
    &Deployment{ObjectMeta:{test-deployment-hvj9z  deployment-2174  e8628189-f84b-4efb-88ed-dad624463649 66823 1 2023-11-15 05:58:59 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-11-15 05:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-11-15 05:59:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-11-15 05:59:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00457e8a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-hvj9z-54bc444df",LastUpdateTime:2023-11-15 05:59:03 +0000 UTC,LastTransitionTime:2023-11-15 05:59:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Nov 15 05:59:03.308: INFO: New ReplicaSet "test-deployment-hvj9z-54bc444df" of Deployment "test-deployment-hvj9z":
    &ReplicaSet{ObjectMeta:{test-deployment-hvj9z-54bc444df  deployment-2174  af5c9b22-f0c4-45f3-b48a-d41b47783fa9 66780 1 2023-11-15 05:58:59 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-hvj9z e8628189-f84b-4efb-88ed-dad624463649 0xc00457ec90 0xc00457ec91}] [] [{kube-controller-manager Update apps/v1 2023-11-15 05:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e8628189-f84b-4efb-88ed-dad624463649\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 05:59:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00457ed38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 05:59:03.337: INFO: Pod "test-deployment-hvj9z-54bc444df-rzd4c" is available:
    &Pod{ObjectMeta:{test-deployment-hvj9z-54bc444df-rzd4c test-deployment-hvj9z-54bc444df- deployment-2174  931fe80d-93c3-4d16-a06f-bf8871761223 66779 0 2023-11-15 05:58:59 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:fc9d43d23699df481f7f7b24254b5a609566ae3c1a97ae8c8582a04eeddecb82 cni.projectcalico.org/podIP:172.30.213.151/32 cni.projectcalico.org/podIPs:172.30.213.151/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.213.151"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-hvj9z-54bc444df af5c9b22-f0c4-45f3-b48a-d41b47783fa9 0xc00412dee7 0xc00412dee8}] [] [{calico Update v1 2023-11-15 05:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-11-15 05:58:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af5c9b22-f0c4-45f3-b48a-d41b47783fa9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-11-15 05:59:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 05:59:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8t8vm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8t8vm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c36,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:59:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:59:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 05:58:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:172.30.213.151,StartTime:2023-11-15 05:58:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 05:59:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://8853f7d89a6a45ab29ef8e8824173f70389d99d6f92c6767444ceb707ec879c7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.213.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:59:03.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2174" for this suite. 11/15/23 05:59:03.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:59:03.412
Nov 15 05:59:03.412: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubectl 11/15/23 05:59:03.413
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:59:03.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:59:03.491
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Nov 15 05:59:03.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4132 version'
Nov 15 05:59:03.580: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Nov 15 05:59:03.580: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.9\", GitCommit:\"d1483fdf7a0578c83523bc1e2212a606a44fd71d\", GitTreeState:\"clean\", BuildDate:\"2023-09-13T11:32:41Z\", GoVersion:\"go1.20.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.9+636f2be\", GitCommit:\"e782f8ba0e57d260867ea108b671c94844780ef2\", GitTreeState:\"clean\", BuildDate:\"2023-10-20T19:28:29Z\", GoVersion:\"go1.19.13 X:strictfipsruntime\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 05:59:03.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4132" for this suite. 11/15/23 05:59:03.623
------------------------------
â€¢ [0.237 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:59:03.412
    Nov 15 05:59:03.412: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubectl 11/15/23 05:59:03.413
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:59:03.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:59:03.491
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Nov 15 05:59:03.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-4132 version'
    Nov 15 05:59:03.580: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Nov 15 05:59:03.580: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.9\", GitCommit:\"d1483fdf7a0578c83523bc1e2212a606a44fd71d\", GitTreeState:\"clean\", BuildDate:\"2023-09-13T11:32:41Z\", GoVersion:\"go1.20.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.9+636f2be\", GitCommit:\"e782f8ba0e57d260867ea108b671c94844780ef2\", GitTreeState:\"clean\", BuildDate:\"2023-10-20T19:28:29Z\", GoVersion:\"go1.19.13 X:strictfipsruntime\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:59:03.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4132" for this suite. 11/15/23 05:59:03.623
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:59:03.649
Nov 15 05:59:03.650: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename discovery 11/15/23 05:59:03.65
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:59:03.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:59:03.713
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 11/15/23 05:59:03.737
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Nov 15 05:59:03.997: INFO: Checking APIGroup: apiregistration.k8s.io
Nov 15 05:59:04.002: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Nov 15 05:59:04.002: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Nov 15 05:59:04.002: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Nov 15 05:59:04.002: INFO: Checking APIGroup: apps
Nov 15 05:59:04.008: INFO: PreferredVersion.GroupVersion: apps/v1
Nov 15 05:59:04.008: INFO: Versions found [{apps/v1 v1}]
Nov 15 05:59:04.008: INFO: apps/v1 matches apps/v1
Nov 15 05:59:04.008: INFO: Checking APIGroup: events.k8s.io
Nov 15 05:59:04.013: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Nov 15 05:59:04.013: INFO: Versions found [{events.k8s.io/v1 v1}]
Nov 15 05:59:04.013: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Nov 15 05:59:04.013: INFO: Checking APIGroup: authentication.k8s.io
Nov 15 05:59:04.018: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Nov 15 05:59:04.018: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Nov 15 05:59:04.018: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Nov 15 05:59:04.018: INFO: Checking APIGroup: authorization.k8s.io
Nov 15 05:59:04.023: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Nov 15 05:59:04.023: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Nov 15 05:59:04.023: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Nov 15 05:59:04.023: INFO: Checking APIGroup: autoscaling
Nov 15 05:59:04.030: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Nov 15 05:59:04.030: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Nov 15 05:59:04.030: INFO: autoscaling/v2 matches autoscaling/v2
Nov 15 05:59:04.030: INFO: Checking APIGroup: batch
Nov 15 05:59:04.037: INFO: PreferredVersion.GroupVersion: batch/v1
Nov 15 05:59:04.037: INFO: Versions found [{batch/v1 v1}]
Nov 15 05:59:04.037: INFO: batch/v1 matches batch/v1
Nov 15 05:59:04.037: INFO: Checking APIGroup: certificates.k8s.io
Nov 15 05:59:04.043: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Nov 15 05:59:04.043: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Nov 15 05:59:04.043: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Nov 15 05:59:04.043: INFO: Checking APIGroup: networking.k8s.io
Nov 15 05:59:04.048: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Nov 15 05:59:04.048: INFO: Versions found [{networking.k8s.io/v1 v1}]
Nov 15 05:59:04.048: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Nov 15 05:59:04.048: INFO: Checking APIGroup: policy
Nov 15 05:59:04.053: INFO: PreferredVersion.GroupVersion: policy/v1
Nov 15 05:59:04.054: INFO: Versions found [{policy/v1 v1}]
Nov 15 05:59:04.054: INFO: policy/v1 matches policy/v1
Nov 15 05:59:04.054: INFO: Checking APIGroup: rbac.authorization.k8s.io
Nov 15 05:59:04.059: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Nov 15 05:59:04.059: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Nov 15 05:59:04.059: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Nov 15 05:59:04.059: INFO: Checking APIGroup: storage.k8s.io
Nov 15 05:59:04.064: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Nov 15 05:59:04.064: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Nov 15 05:59:04.064: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Nov 15 05:59:04.064: INFO: Checking APIGroup: admissionregistration.k8s.io
Nov 15 05:59:04.073: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Nov 15 05:59:04.073: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Nov 15 05:59:04.073: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Nov 15 05:59:04.073: INFO: Checking APIGroup: apiextensions.k8s.io
Nov 15 05:59:04.080: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Nov 15 05:59:04.080: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Nov 15 05:59:04.080: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Nov 15 05:59:04.080: INFO: Checking APIGroup: scheduling.k8s.io
Nov 15 05:59:04.086: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Nov 15 05:59:04.086: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Nov 15 05:59:04.086: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Nov 15 05:59:04.086: INFO: Checking APIGroup: coordination.k8s.io
Nov 15 05:59:04.095: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Nov 15 05:59:04.095: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Nov 15 05:59:04.095: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Nov 15 05:59:04.095: INFO: Checking APIGroup: node.k8s.io
Nov 15 05:59:04.100: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Nov 15 05:59:04.100: INFO: Versions found [{node.k8s.io/v1 v1}]
Nov 15 05:59:04.100: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Nov 15 05:59:04.100: INFO: Checking APIGroup: discovery.k8s.io
Nov 15 05:59:04.105: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Nov 15 05:59:04.105: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Nov 15 05:59:04.105: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Nov 15 05:59:04.105: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Nov 15 05:59:04.112: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Nov 15 05:59:04.112: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Nov 15 05:59:04.112: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Nov 15 05:59:04.112: INFO: Checking APIGroup: apps.openshift.io
Nov 15 05:59:04.116: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Nov 15 05:59:04.117: INFO: Versions found [{apps.openshift.io/v1 v1}]
Nov 15 05:59:04.117: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Nov 15 05:59:04.117: INFO: Checking APIGroup: authorization.openshift.io
Nov 15 05:59:04.122: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Nov 15 05:59:04.122: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Nov 15 05:59:04.122: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Nov 15 05:59:04.122: INFO: Checking APIGroup: build.openshift.io
Nov 15 05:59:04.127: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Nov 15 05:59:04.128: INFO: Versions found [{build.openshift.io/v1 v1}]
Nov 15 05:59:04.128: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Nov 15 05:59:04.128: INFO: Checking APIGroup: image.openshift.io
Nov 15 05:59:04.136: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Nov 15 05:59:04.136: INFO: Versions found [{image.openshift.io/v1 v1}]
Nov 15 05:59:04.136: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Nov 15 05:59:04.136: INFO: Checking APIGroup: oauth.openshift.io
Nov 15 05:59:04.142: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Nov 15 05:59:04.142: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Nov 15 05:59:04.142: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Nov 15 05:59:04.142: INFO: Checking APIGroup: project.openshift.io
Nov 15 05:59:04.148: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Nov 15 05:59:04.148: INFO: Versions found [{project.openshift.io/v1 v1}]
Nov 15 05:59:04.148: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Nov 15 05:59:04.148: INFO: Checking APIGroup: quota.openshift.io
Nov 15 05:59:04.153: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Nov 15 05:59:04.153: INFO: Versions found [{quota.openshift.io/v1 v1}]
Nov 15 05:59:04.153: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Nov 15 05:59:04.153: INFO: Checking APIGroup: route.openshift.io
Nov 15 05:59:04.162: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Nov 15 05:59:04.162: INFO: Versions found [{route.openshift.io/v1 v1}]
Nov 15 05:59:04.162: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Nov 15 05:59:04.162: INFO: Checking APIGroup: security.openshift.io
Nov 15 05:59:04.170: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Nov 15 05:59:04.170: INFO: Versions found [{security.openshift.io/v1 v1}]
Nov 15 05:59:04.170: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Nov 15 05:59:04.170: INFO: Checking APIGroup: template.openshift.io
Nov 15 05:59:04.176: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Nov 15 05:59:04.176: INFO: Versions found [{template.openshift.io/v1 v1}]
Nov 15 05:59:04.176: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Nov 15 05:59:04.176: INFO: Checking APIGroup: user.openshift.io
Nov 15 05:59:04.181: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Nov 15 05:59:04.181: INFO: Versions found [{user.openshift.io/v1 v1}]
Nov 15 05:59:04.181: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Nov 15 05:59:04.181: INFO: Checking APIGroup: packages.operators.coreos.com
Nov 15 05:59:04.186: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Nov 15 05:59:04.186: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Nov 15 05:59:04.186: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Nov 15 05:59:04.186: INFO: Checking APIGroup: config.openshift.io
Nov 15 05:59:04.192: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Nov 15 05:59:04.192: INFO: Versions found [{config.openshift.io/v1 v1}]
Nov 15 05:59:04.192: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Nov 15 05:59:04.192: INFO: Checking APIGroup: operator.openshift.io
Nov 15 05:59:04.198: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Nov 15 05:59:04.198: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Nov 15 05:59:04.198: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Nov 15 05:59:04.198: INFO: Checking APIGroup: apiserver.openshift.io
Nov 15 05:59:04.204: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
Nov 15 05:59:04.204: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
Nov 15 05:59:04.204: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
Nov 15 05:59:04.204: INFO: Checking APIGroup: cloudcredential.openshift.io
Nov 15 05:59:04.209: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Nov 15 05:59:04.209: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Nov 15 05:59:04.209: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Nov 15 05:59:04.209: INFO: Checking APIGroup: console.openshift.io
Nov 15 05:59:04.214: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Nov 15 05:59:04.214: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Nov 15 05:59:04.214: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Nov 15 05:59:04.214: INFO: Checking APIGroup: crd.projectcalico.org
Nov 15 05:59:04.219: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Nov 15 05:59:04.219: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Nov 15 05:59:04.219: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Nov 15 05:59:04.219: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Nov 15 05:59:04.224: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Nov 15 05:59:04.224: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Nov 15 05:59:04.224: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Nov 15 05:59:04.224: INFO: Checking APIGroup: ingress.operator.openshift.io
Nov 15 05:59:04.230: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Nov 15 05:59:04.230: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Nov 15 05:59:04.230: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Nov 15 05:59:04.230: INFO: Checking APIGroup: k8s.cni.cncf.io
Nov 15 05:59:04.240: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Nov 15 05:59:04.240: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Nov 15 05:59:04.240: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Nov 15 05:59:04.240: INFO: Checking APIGroup: machineconfiguration.openshift.io
Nov 15 05:59:04.246: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Nov 15 05:59:04.246: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Nov 15 05:59:04.246: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Nov 15 05:59:04.246: INFO: Checking APIGroup: monitoring.coreos.com
Nov 15 05:59:04.251: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Nov 15 05:59:04.251: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Nov 15 05:59:04.251: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Nov 15 05:59:04.251: INFO: Checking APIGroup: network.operator.openshift.io
Nov 15 05:59:04.268: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Nov 15 05:59:04.268: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Nov 15 05:59:04.268: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Nov 15 05:59:04.268: INFO: Checking APIGroup: operator.tigera.io
Nov 15 05:59:04.275: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Nov 15 05:59:04.275: INFO: Versions found [{operator.tigera.io/v1 v1}]
Nov 15 05:59:04.275: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Nov 15 05:59:04.275: INFO: Checking APIGroup: operators.coreos.com
Nov 15 05:59:04.285: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
Nov 15 05:59:04.285: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Nov 15 05:59:04.285: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
Nov 15 05:59:04.285: INFO: Checking APIGroup: performance.openshift.io
Nov 15 05:59:04.293: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
Nov 15 05:59:04.293: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
Nov 15 05:59:04.293: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
Nov 15 05:59:04.293: INFO: Checking APIGroup: samples.operator.openshift.io
Nov 15 05:59:04.300: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Nov 15 05:59:04.300: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Nov 15 05:59:04.300: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Nov 15 05:59:04.300: INFO: Checking APIGroup: security.internal.openshift.io
Nov 15 05:59:04.307: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Nov 15 05:59:04.307: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Nov 15 05:59:04.307: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Nov 15 05:59:04.307: INFO: Checking APIGroup: snapshot.storage.k8s.io
Nov 15 05:59:04.313: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Nov 15 05:59:04.313: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Nov 15 05:59:04.314: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Nov 15 05:59:04.314: INFO: Checking APIGroup: tuned.openshift.io
Nov 15 05:59:04.326: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Nov 15 05:59:04.326: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Nov 15 05:59:04.326: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Nov 15 05:59:04.326: INFO: Checking APIGroup: controlplane.operator.openshift.io
Nov 15 05:59:04.331: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Nov 15 05:59:04.331: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Nov 15 05:59:04.331: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Nov 15 05:59:04.331: INFO: Checking APIGroup: ibm.com
Nov 15 05:59:04.337: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Nov 15 05:59:04.337: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Nov 15 05:59:04.337: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Nov 15 05:59:04.337: INFO: Checking APIGroup: migration.k8s.io
Nov 15 05:59:04.342: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Nov 15 05:59:04.342: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Nov 15 05:59:04.342: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Nov 15 05:59:04.342: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Nov 15 05:59:04.347: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Nov 15 05:59:04.347: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Nov 15 05:59:04.347: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Nov 15 05:59:04.347: INFO: Checking APIGroup: helm.openshift.io
Nov 15 05:59:04.352: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Nov 15 05:59:04.352: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Nov 15 05:59:04.352: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Nov 15 05:59:04.353: INFO: Checking APIGroup: metrics.k8s.io
Nov 15 05:59:04.358: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Nov 15 05:59:04.358: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Nov 15 05:59:04.358: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Nov 15 05:59:04.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-935" for this suite. 11/15/23 05:59:04.427
------------------------------
â€¢ [0.821 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:59:03.649
    Nov 15 05:59:03.650: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename discovery 11/15/23 05:59:03.65
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:59:03.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:59:03.713
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 11/15/23 05:59:03.737
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Nov 15 05:59:03.997: INFO: Checking APIGroup: apiregistration.k8s.io
    Nov 15 05:59:04.002: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Nov 15 05:59:04.002: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Nov 15 05:59:04.002: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Nov 15 05:59:04.002: INFO: Checking APIGroup: apps
    Nov 15 05:59:04.008: INFO: PreferredVersion.GroupVersion: apps/v1
    Nov 15 05:59:04.008: INFO: Versions found [{apps/v1 v1}]
    Nov 15 05:59:04.008: INFO: apps/v1 matches apps/v1
    Nov 15 05:59:04.008: INFO: Checking APIGroup: events.k8s.io
    Nov 15 05:59:04.013: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Nov 15 05:59:04.013: INFO: Versions found [{events.k8s.io/v1 v1}]
    Nov 15 05:59:04.013: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Nov 15 05:59:04.013: INFO: Checking APIGroup: authentication.k8s.io
    Nov 15 05:59:04.018: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Nov 15 05:59:04.018: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Nov 15 05:59:04.018: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Nov 15 05:59:04.018: INFO: Checking APIGroup: authorization.k8s.io
    Nov 15 05:59:04.023: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Nov 15 05:59:04.023: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Nov 15 05:59:04.023: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Nov 15 05:59:04.023: INFO: Checking APIGroup: autoscaling
    Nov 15 05:59:04.030: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Nov 15 05:59:04.030: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Nov 15 05:59:04.030: INFO: autoscaling/v2 matches autoscaling/v2
    Nov 15 05:59:04.030: INFO: Checking APIGroup: batch
    Nov 15 05:59:04.037: INFO: PreferredVersion.GroupVersion: batch/v1
    Nov 15 05:59:04.037: INFO: Versions found [{batch/v1 v1}]
    Nov 15 05:59:04.037: INFO: batch/v1 matches batch/v1
    Nov 15 05:59:04.037: INFO: Checking APIGroup: certificates.k8s.io
    Nov 15 05:59:04.043: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Nov 15 05:59:04.043: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Nov 15 05:59:04.043: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Nov 15 05:59:04.043: INFO: Checking APIGroup: networking.k8s.io
    Nov 15 05:59:04.048: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Nov 15 05:59:04.048: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Nov 15 05:59:04.048: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Nov 15 05:59:04.048: INFO: Checking APIGroup: policy
    Nov 15 05:59:04.053: INFO: PreferredVersion.GroupVersion: policy/v1
    Nov 15 05:59:04.054: INFO: Versions found [{policy/v1 v1}]
    Nov 15 05:59:04.054: INFO: policy/v1 matches policy/v1
    Nov 15 05:59:04.054: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Nov 15 05:59:04.059: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Nov 15 05:59:04.059: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Nov 15 05:59:04.059: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Nov 15 05:59:04.059: INFO: Checking APIGroup: storage.k8s.io
    Nov 15 05:59:04.064: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Nov 15 05:59:04.064: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Nov 15 05:59:04.064: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Nov 15 05:59:04.064: INFO: Checking APIGroup: admissionregistration.k8s.io
    Nov 15 05:59:04.073: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Nov 15 05:59:04.073: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Nov 15 05:59:04.073: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Nov 15 05:59:04.073: INFO: Checking APIGroup: apiextensions.k8s.io
    Nov 15 05:59:04.080: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Nov 15 05:59:04.080: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Nov 15 05:59:04.080: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Nov 15 05:59:04.080: INFO: Checking APIGroup: scheduling.k8s.io
    Nov 15 05:59:04.086: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Nov 15 05:59:04.086: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Nov 15 05:59:04.086: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Nov 15 05:59:04.086: INFO: Checking APIGroup: coordination.k8s.io
    Nov 15 05:59:04.095: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Nov 15 05:59:04.095: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Nov 15 05:59:04.095: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Nov 15 05:59:04.095: INFO: Checking APIGroup: node.k8s.io
    Nov 15 05:59:04.100: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Nov 15 05:59:04.100: INFO: Versions found [{node.k8s.io/v1 v1}]
    Nov 15 05:59:04.100: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Nov 15 05:59:04.100: INFO: Checking APIGroup: discovery.k8s.io
    Nov 15 05:59:04.105: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Nov 15 05:59:04.105: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Nov 15 05:59:04.105: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Nov 15 05:59:04.105: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Nov 15 05:59:04.112: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Nov 15 05:59:04.112: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Nov 15 05:59:04.112: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Nov 15 05:59:04.112: INFO: Checking APIGroup: apps.openshift.io
    Nov 15 05:59:04.116: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
    Nov 15 05:59:04.117: INFO: Versions found [{apps.openshift.io/v1 v1}]
    Nov 15 05:59:04.117: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
    Nov 15 05:59:04.117: INFO: Checking APIGroup: authorization.openshift.io
    Nov 15 05:59:04.122: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
    Nov 15 05:59:04.122: INFO: Versions found [{authorization.openshift.io/v1 v1}]
    Nov 15 05:59:04.122: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
    Nov 15 05:59:04.122: INFO: Checking APIGroup: build.openshift.io
    Nov 15 05:59:04.127: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
    Nov 15 05:59:04.128: INFO: Versions found [{build.openshift.io/v1 v1}]
    Nov 15 05:59:04.128: INFO: build.openshift.io/v1 matches build.openshift.io/v1
    Nov 15 05:59:04.128: INFO: Checking APIGroup: image.openshift.io
    Nov 15 05:59:04.136: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
    Nov 15 05:59:04.136: INFO: Versions found [{image.openshift.io/v1 v1}]
    Nov 15 05:59:04.136: INFO: image.openshift.io/v1 matches image.openshift.io/v1
    Nov 15 05:59:04.136: INFO: Checking APIGroup: oauth.openshift.io
    Nov 15 05:59:04.142: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
    Nov 15 05:59:04.142: INFO: Versions found [{oauth.openshift.io/v1 v1}]
    Nov 15 05:59:04.142: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
    Nov 15 05:59:04.142: INFO: Checking APIGroup: project.openshift.io
    Nov 15 05:59:04.148: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
    Nov 15 05:59:04.148: INFO: Versions found [{project.openshift.io/v1 v1}]
    Nov 15 05:59:04.148: INFO: project.openshift.io/v1 matches project.openshift.io/v1
    Nov 15 05:59:04.148: INFO: Checking APIGroup: quota.openshift.io
    Nov 15 05:59:04.153: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
    Nov 15 05:59:04.153: INFO: Versions found [{quota.openshift.io/v1 v1}]
    Nov 15 05:59:04.153: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
    Nov 15 05:59:04.153: INFO: Checking APIGroup: route.openshift.io
    Nov 15 05:59:04.162: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
    Nov 15 05:59:04.162: INFO: Versions found [{route.openshift.io/v1 v1}]
    Nov 15 05:59:04.162: INFO: route.openshift.io/v1 matches route.openshift.io/v1
    Nov 15 05:59:04.162: INFO: Checking APIGroup: security.openshift.io
    Nov 15 05:59:04.170: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
    Nov 15 05:59:04.170: INFO: Versions found [{security.openshift.io/v1 v1}]
    Nov 15 05:59:04.170: INFO: security.openshift.io/v1 matches security.openshift.io/v1
    Nov 15 05:59:04.170: INFO: Checking APIGroup: template.openshift.io
    Nov 15 05:59:04.176: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
    Nov 15 05:59:04.176: INFO: Versions found [{template.openshift.io/v1 v1}]
    Nov 15 05:59:04.176: INFO: template.openshift.io/v1 matches template.openshift.io/v1
    Nov 15 05:59:04.176: INFO: Checking APIGroup: user.openshift.io
    Nov 15 05:59:04.181: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
    Nov 15 05:59:04.181: INFO: Versions found [{user.openshift.io/v1 v1}]
    Nov 15 05:59:04.181: INFO: user.openshift.io/v1 matches user.openshift.io/v1
    Nov 15 05:59:04.181: INFO: Checking APIGroup: packages.operators.coreos.com
    Nov 15 05:59:04.186: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
    Nov 15 05:59:04.186: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
    Nov 15 05:59:04.186: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
    Nov 15 05:59:04.186: INFO: Checking APIGroup: config.openshift.io
    Nov 15 05:59:04.192: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
    Nov 15 05:59:04.192: INFO: Versions found [{config.openshift.io/v1 v1}]
    Nov 15 05:59:04.192: INFO: config.openshift.io/v1 matches config.openshift.io/v1
    Nov 15 05:59:04.192: INFO: Checking APIGroup: operator.openshift.io
    Nov 15 05:59:04.198: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
    Nov 15 05:59:04.198: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
    Nov 15 05:59:04.198: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
    Nov 15 05:59:04.198: INFO: Checking APIGroup: apiserver.openshift.io
    Nov 15 05:59:04.204: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
    Nov 15 05:59:04.204: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
    Nov 15 05:59:04.204: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
    Nov 15 05:59:04.204: INFO: Checking APIGroup: cloudcredential.openshift.io
    Nov 15 05:59:04.209: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
    Nov 15 05:59:04.209: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
    Nov 15 05:59:04.209: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
    Nov 15 05:59:04.209: INFO: Checking APIGroup: console.openshift.io
    Nov 15 05:59:04.214: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
    Nov 15 05:59:04.214: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
    Nov 15 05:59:04.214: INFO: console.openshift.io/v1 matches console.openshift.io/v1
    Nov 15 05:59:04.214: INFO: Checking APIGroup: crd.projectcalico.org
    Nov 15 05:59:04.219: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Nov 15 05:59:04.219: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Nov 15 05:59:04.219: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Nov 15 05:59:04.219: INFO: Checking APIGroup: imageregistry.operator.openshift.io
    Nov 15 05:59:04.224: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
    Nov 15 05:59:04.224: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
    Nov 15 05:59:04.224: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
    Nov 15 05:59:04.224: INFO: Checking APIGroup: ingress.operator.openshift.io
    Nov 15 05:59:04.230: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
    Nov 15 05:59:04.230: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
    Nov 15 05:59:04.230: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
    Nov 15 05:59:04.230: INFO: Checking APIGroup: k8s.cni.cncf.io
    Nov 15 05:59:04.240: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
    Nov 15 05:59:04.240: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
    Nov 15 05:59:04.240: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
    Nov 15 05:59:04.240: INFO: Checking APIGroup: machineconfiguration.openshift.io
    Nov 15 05:59:04.246: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
    Nov 15 05:59:04.246: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
    Nov 15 05:59:04.246: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
    Nov 15 05:59:04.246: INFO: Checking APIGroup: monitoring.coreos.com
    Nov 15 05:59:04.251: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    Nov 15 05:59:04.251: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    Nov 15 05:59:04.251: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    Nov 15 05:59:04.251: INFO: Checking APIGroup: network.operator.openshift.io
    Nov 15 05:59:04.268: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
    Nov 15 05:59:04.268: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
    Nov 15 05:59:04.268: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
    Nov 15 05:59:04.268: INFO: Checking APIGroup: operator.tigera.io
    Nov 15 05:59:04.275: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
    Nov 15 05:59:04.275: INFO: Versions found [{operator.tigera.io/v1 v1}]
    Nov 15 05:59:04.275: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
    Nov 15 05:59:04.275: INFO: Checking APIGroup: operators.coreos.com
    Nov 15 05:59:04.285: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
    Nov 15 05:59:04.285: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
    Nov 15 05:59:04.285: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
    Nov 15 05:59:04.285: INFO: Checking APIGroup: performance.openshift.io
    Nov 15 05:59:04.293: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
    Nov 15 05:59:04.293: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
    Nov 15 05:59:04.293: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
    Nov 15 05:59:04.293: INFO: Checking APIGroup: samples.operator.openshift.io
    Nov 15 05:59:04.300: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
    Nov 15 05:59:04.300: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
    Nov 15 05:59:04.300: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
    Nov 15 05:59:04.300: INFO: Checking APIGroup: security.internal.openshift.io
    Nov 15 05:59:04.307: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
    Nov 15 05:59:04.307: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
    Nov 15 05:59:04.307: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
    Nov 15 05:59:04.307: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Nov 15 05:59:04.313: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Nov 15 05:59:04.313: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Nov 15 05:59:04.314: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Nov 15 05:59:04.314: INFO: Checking APIGroup: tuned.openshift.io
    Nov 15 05:59:04.326: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
    Nov 15 05:59:04.326: INFO: Versions found [{tuned.openshift.io/v1 v1}]
    Nov 15 05:59:04.326: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
    Nov 15 05:59:04.326: INFO: Checking APIGroup: controlplane.operator.openshift.io
    Nov 15 05:59:04.331: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
    Nov 15 05:59:04.331: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
    Nov 15 05:59:04.331: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
    Nov 15 05:59:04.331: INFO: Checking APIGroup: ibm.com
    Nov 15 05:59:04.337: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
    Nov 15 05:59:04.337: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
    Nov 15 05:59:04.337: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
    Nov 15 05:59:04.337: INFO: Checking APIGroup: migration.k8s.io
    Nov 15 05:59:04.342: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
    Nov 15 05:59:04.342: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
    Nov 15 05:59:04.342: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
    Nov 15 05:59:04.342: INFO: Checking APIGroup: whereabouts.cni.cncf.io
    Nov 15 05:59:04.347: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
    Nov 15 05:59:04.347: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
    Nov 15 05:59:04.347: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
    Nov 15 05:59:04.347: INFO: Checking APIGroup: helm.openshift.io
    Nov 15 05:59:04.352: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
    Nov 15 05:59:04.352: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
    Nov 15 05:59:04.352: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
    Nov 15 05:59:04.353: INFO: Checking APIGroup: metrics.k8s.io
    Nov 15 05:59:04.358: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Nov 15 05:59:04.358: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Nov 15 05:59:04.358: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:59:04.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-935" for this suite. 11/15/23 05:59:04.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:59:04.471
Nov 15 05:59:04.471: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename container-lifecycle-hook 11/15/23 05:59:04.472
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:59:04.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:59:04.554
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 11/15/23 05:59:04.593
Nov 15 05:59:04.627: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6893" to be "running and ready"
Nov 15 05:59:04.653: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 25.366952ms
Nov 15 05:59:04.653: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 15 05:59:06.676: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048282181s
Nov 15 05:59:06.676: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 15 05:59:08.672: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.044292559s
Nov 15 05:59:08.672: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Nov 15 05:59:08.672: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 11/15/23 05:59:08.697
Nov 15 05:59:08.733: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6893" to be "running and ready"
Nov 15 05:59:08.751: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 18.836148ms
Nov 15 05:59:08.751: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 15 05:59:10.770: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037463773s
Nov 15 05:59:10.770: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 15 05:59:12.775: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.042516409s
Nov 15 05:59:12.775: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Nov 15 05:59:12.775: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 11/15/23 05:59:12.793
STEP: delete the pod with lifecycle hook 11/15/23 05:59:12.835
Nov 15 05:59:12.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 15 05:59:12.900: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 15 05:59:14.901: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 15 05:59:14.935: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Nov 15 05:59:14.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6893" for this suite. 11/15/23 05:59:14.966
------------------------------
â€¢ [SLOW TEST] [10.547 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:59:04.471
    Nov 15 05:59:04.471: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename container-lifecycle-hook 11/15/23 05:59:04.472
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:59:04.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:59:04.554
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 11/15/23 05:59:04.593
    Nov 15 05:59:04.627: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6893" to be "running and ready"
    Nov 15 05:59:04.653: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 25.366952ms
    Nov 15 05:59:04.653: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 05:59:06.676: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048282181s
    Nov 15 05:59:06.676: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 05:59:08.672: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.044292559s
    Nov 15 05:59:08.672: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Nov 15 05:59:08.672: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 11/15/23 05:59:08.697
    Nov 15 05:59:08.733: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6893" to be "running and ready"
    Nov 15 05:59:08.751: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 18.836148ms
    Nov 15 05:59:08.751: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 05:59:10.770: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037463773s
    Nov 15 05:59:10.770: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 05:59:12.775: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.042516409s
    Nov 15 05:59:12.775: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Nov 15 05:59:12.775: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 11/15/23 05:59:12.793
    STEP: delete the pod with lifecycle hook 11/15/23 05:59:12.835
    Nov 15 05:59:12.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Nov 15 05:59:12.900: INFO: Pod pod-with-poststart-exec-hook still exists
    Nov 15 05:59:14.901: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Nov 15 05:59:14.935: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Nov 15 05:59:14.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6893" for this suite. 11/15/23 05:59:14.966
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 05:59:15.02
Nov 15 05:59:15.020: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename sched-preemption 11/15/23 05:59:15.021
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:59:15.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:59:15.128
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Nov 15 05:59:15.212: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 15 06:00:15.467: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:00:15.497
Nov 15 06:00:15.497: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename sched-preemption-path 11/15/23 06:00:15.498
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:00:15.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:00:15.567
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Nov 15 06:00:15.634: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Nov 15 06:00:15.648: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Nov 15 06:00:15.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:00:15.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-6188" for this suite. 11/15/23 06:00:15.946
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7570" for this suite. 11/15/23 06:00:15.975
------------------------------
â€¢ [SLOW TEST] [60.989 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 05:59:15.02
    Nov 15 05:59:15.020: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename sched-preemption 11/15/23 05:59:15.021
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 05:59:15.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 05:59:15.128
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Nov 15 05:59:15.212: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 15 06:00:15.467: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:00:15.497
    Nov 15 06:00:15.497: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename sched-preemption-path 11/15/23 06:00:15.498
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:00:15.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:00:15.567
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Nov 15 06:00:15.634: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Nov 15 06:00:15.648: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:00:15.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:00:15.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-6188" for this suite. 11/15/23 06:00:15.946
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7570" for this suite. 11/15/23 06:00:15.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:00:16.012
Nov 15 06:00:16.012: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename dns 11/15/23 06:00:16.013
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:00:16.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:00:16.083
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 11/15/23 06:00:16.095
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3897.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3897.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 11/15/23 06:00:16.124
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3897.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3897.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 11/15/23 06:00:16.124
STEP: creating a pod to probe DNS 11/15/23 06:00:16.124
STEP: submitting the pod to kubernetes 11/15/23 06:00:16.124
Nov 15 06:00:16.161: INFO: Waiting up to 15m0s for pod "dns-test-5f2642cd-1183-45d4-81fb-fe35613a2563" in namespace "dns-3897" to be "running"
Nov 15 06:00:16.180: INFO: Pod "dns-test-5f2642cd-1183-45d4-81fb-fe35613a2563": Phase="Pending", Reason="", readiness=false. Elapsed: 19.511766ms
Nov 15 06:00:18.199: INFO: Pod "dns-test-5f2642cd-1183-45d4-81fb-fe35613a2563": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038387427s
Nov 15 06:00:20.206: INFO: Pod "dns-test-5f2642cd-1183-45d4-81fb-fe35613a2563": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045418219s
Nov 15 06:00:22.200: INFO: Pod "dns-test-5f2642cd-1183-45d4-81fb-fe35613a2563": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038950933s
Nov 15 06:00:24.204: INFO: Pod "dns-test-5f2642cd-1183-45d4-81fb-fe35613a2563": Phase="Running", Reason="", readiness=true. Elapsed: 8.043615657s
Nov 15 06:00:24.205: INFO: Pod "dns-test-5f2642cd-1183-45d4-81fb-fe35613a2563" satisfied condition "running"
STEP: retrieving the pod 11/15/23 06:00:24.205
STEP: looking for the results for each expected name from probers 11/15/23 06:00:24.224
Nov 15 06:00:24.334: INFO: DNS probes using dns-3897/dns-test-5f2642cd-1183-45d4-81fb-fe35613a2563 succeeded

STEP: deleting the pod 11/15/23 06:00:24.334
STEP: deleting the test headless service 11/15/23 06:00:24.379
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 15 06:00:24.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3897" for this suite. 11/15/23 06:00:24.468
------------------------------
â€¢ [SLOW TEST] [8.480 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:00:16.012
    Nov 15 06:00:16.012: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename dns 11/15/23 06:00:16.013
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:00:16.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:00:16.083
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 11/15/23 06:00:16.095
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3897.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3897.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     11/15/23 06:00:16.124
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3897.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3897.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     11/15/23 06:00:16.124
    STEP: creating a pod to probe DNS 11/15/23 06:00:16.124
    STEP: submitting the pod to kubernetes 11/15/23 06:00:16.124
    Nov 15 06:00:16.161: INFO: Waiting up to 15m0s for pod "dns-test-5f2642cd-1183-45d4-81fb-fe35613a2563" in namespace "dns-3897" to be "running"
    Nov 15 06:00:16.180: INFO: Pod "dns-test-5f2642cd-1183-45d4-81fb-fe35613a2563": Phase="Pending", Reason="", readiness=false. Elapsed: 19.511766ms
    Nov 15 06:00:18.199: INFO: Pod "dns-test-5f2642cd-1183-45d4-81fb-fe35613a2563": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038387427s
    Nov 15 06:00:20.206: INFO: Pod "dns-test-5f2642cd-1183-45d4-81fb-fe35613a2563": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045418219s
    Nov 15 06:00:22.200: INFO: Pod "dns-test-5f2642cd-1183-45d4-81fb-fe35613a2563": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038950933s
    Nov 15 06:00:24.204: INFO: Pod "dns-test-5f2642cd-1183-45d4-81fb-fe35613a2563": Phase="Running", Reason="", readiness=true. Elapsed: 8.043615657s
    Nov 15 06:00:24.205: INFO: Pod "dns-test-5f2642cd-1183-45d4-81fb-fe35613a2563" satisfied condition "running"
    STEP: retrieving the pod 11/15/23 06:00:24.205
    STEP: looking for the results for each expected name from probers 11/15/23 06:00:24.224
    Nov 15 06:00:24.334: INFO: DNS probes using dns-3897/dns-test-5f2642cd-1183-45d4-81fb-fe35613a2563 succeeded

    STEP: deleting the pod 11/15/23 06:00:24.334
    STEP: deleting the test headless service 11/15/23 06:00:24.379
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:00:24.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3897" for this suite. 11/15/23 06:00:24.468
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:00:24.494
Nov 15 06:00:24.494: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubectl 11/15/23 06:00:24.495
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:00:24.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:00:24.566
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/15/23 06:00:24.578
Nov 15 06:00:24.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9752 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Nov 15 06:00:24.703: INFO: stderr: ""
Nov 15 06:00:24.703: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 11/15/23 06:00:24.703
Nov 15 06:00:24.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9752 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Nov 15 06:00:25.155: INFO: stderr: ""
Nov 15 06:00:25.155: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/15/23 06:00:25.155
Nov 15 06:00:25.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9752 delete pods e2e-test-httpd-pod'
Nov 15 06:00:28.265: INFO: stderr: ""
Nov 15 06:00:28.265: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 06:00:28.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9752" for this suite. 11/15/23 06:00:28.297
------------------------------
â€¢ [3.835 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:00:24.494
    Nov 15 06:00:24.494: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubectl 11/15/23 06:00:24.495
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:00:24.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:00:24.566
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/15/23 06:00:24.578
    Nov 15 06:00:24.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9752 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Nov 15 06:00:24.703: INFO: stderr: ""
    Nov 15 06:00:24.703: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 11/15/23 06:00:24.703
    Nov 15 06:00:24.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9752 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Nov 15 06:00:25.155: INFO: stderr: ""
    Nov 15 06:00:25.155: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/15/23 06:00:25.155
    Nov 15 06:00:25.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9752 delete pods e2e-test-httpd-pod'
    Nov 15 06:00:28.265: INFO: stderr: ""
    Nov 15 06:00:28.265: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:00:28.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9752" for this suite. 11/15/23 06:00:28.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:00:28.329
Nov 15 06:00:28.329: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename pods 11/15/23 06:00:28.341
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:00:28.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:00:28.409
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 11/15/23 06:00:28.421
STEP: submitting the pod to kubernetes 11/15/23 06:00:28.422
Nov 15 06:00:28.467: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a" in namespace "pods-5847" to be "running and ready"
Nov 15 06:00:28.491: INFO: Pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a": Phase="Pending", Reason="", readiness=false. Elapsed: 23.32924ms
Nov 15 06:00:28.491: INFO: The phase of Pod pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:00:30.517: INFO: Pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049455022s
Nov 15 06:00:30.517: INFO: The phase of Pod pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:00:32.521: INFO: Pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a": Phase="Running", Reason="", readiness=true. Elapsed: 4.053735042s
Nov 15 06:00:32.521: INFO: The phase of Pod pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a is Running (Ready = true)
Nov 15 06:00:32.521: INFO: Pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 11/15/23 06:00:32.541
STEP: updating the pod 11/15/23 06:00:32.559
Nov 15 06:00:33.111: INFO: Successfully updated pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a"
Nov 15 06:00:33.111: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a" in namespace "pods-5847" to be "terminated with reason DeadlineExceeded"
Nov 15 06:00:33.129: INFO: Pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a": Phase="Running", Reason="", readiness=true. Elapsed: 17.784216ms
Nov 15 06:00:35.154: INFO: Pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.043442167s
Nov 15 06:00:35.154: INFO: Pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 06:00:35.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5847" for this suite. 11/15/23 06:00:35.23
------------------------------
â€¢ [SLOW TEST] [6.930 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:00:28.329
    Nov 15 06:00:28.329: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename pods 11/15/23 06:00:28.341
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:00:28.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:00:28.409
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 11/15/23 06:00:28.421
    STEP: submitting the pod to kubernetes 11/15/23 06:00:28.422
    Nov 15 06:00:28.467: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a" in namespace "pods-5847" to be "running and ready"
    Nov 15 06:00:28.491: INFO: Pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a": Phase="Pending", Reason="", readiness=false. Elapsed: 23.32924ms
    Nov 15 06:00:28.491: INFO: The phase of Pod pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:00:30.517: INFO: Pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049455022s
    Nov 15 06:00:30.517: INFO: The phase of Pod pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:00:32.521: INFO: Pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a": Phase="Running", Reason="", readiness=true. Elapsed: 4.053735042s
    Nov 15 06:00:32.521: INFO: The phase of Pod pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a is Running (Ready = true)
    Nov 15 06:00:32.521: INFO: Pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 11/15/23 06:00:32.541
    STEP: updating the pod 11/15/23 06:00:32.559
    Nov 15 06:00:33.111: INFO: Successfully updated pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a"
    Nov 15 06:00:33.111: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a" in namespace "pods-5847" to be "terminated with reason DeadlineExceeded"
    Nov 15 06:00:33.129: INFO: Pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a": Phase="Running", Reason="", readiness=true. Elapsed: 17.784216ms
    Nov 15 06:00:35.154: INFO: Pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.043442167s
    Nov 15 06:00:35.154: INFO: Pod "pod-update-activedeadlineseconds-05443b87-52b7-4f7f-8074-07cce676b83a" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:00:35.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5847" for this suite. 11/15/23 06:00:35.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:00:35.259
Nov 15 06:00:35.259: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename replicaset 11/15/23 06:00:35.26
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:00:35.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:00:35.336
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Nov 15 06:00:35.355: INFO: Creating ReplicaSet my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10
Nov 15 06:00:35.402: INFO: Pod name my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10: Found 0 pods out of 1
Nov 15 06:00:40.422: INFO: Pod name my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10: Found 1 pods out of 1
Nov 15 06:00:40.422: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10" is running
Nov 15 06:00:40.422: INFO: Waiting up to 5m0s for pod "my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10-rh9dg" in namespace "replicaset-1537" to be "running"
Nov 15 06:00:40.440: INFO: Pod "my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10-rh9dg": Phase="Running", Reason="", readiness=true. Elapsed: 17.733155ms
Nov 15 06:00:40.440: INFO: Pod "my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10-rh9dg" satisfied condition "running"
Nov 15 06:00:40.440: INFO: Pod "my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10-rh9dg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 06:00:35 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 06:00:36 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 06:00:36 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 06:00:35 +0000 UTC Reason: Message:}])
Nov 15 06:00:40.440: INFO: Trying to dial the pod
Nov 15 06:00:45.518: INFO: Controller my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10: Got expected result from replica 1 [my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10-rh9dg]: "my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10-rh9dg", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Nov 15 06:00:45.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1537" for this suite. 11/15/23 06:00:45.553
------------------------------
â€¢ [SLOW TEST] [10.324 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:00:35.259
    Nov 15 06:00:35.259: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename replicaset 11/15/23 06:00:35.26
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:00:35.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:00:35.336
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Nov 15 06:00:35.355: INFO: Creating ReplicaSet my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10
    Nov 15 06:00:35.402: INFO: Pod name my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10: Found 0 pods out of 1
    Nov 15 06:00:40.422: INFO: Pod name my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10: Found 1 pods out of 1
    Nov 15 06:00:40.422: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10" is running
    Nov 15 06:00:40.422: INFO: Waiting up to 5m0s for pod "my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10-rh9dg" in namespace "replicaset-1537" to be "running"
    Nov 15 06:00:40.440: INFO: Pod "my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10-rh9dg": Phase="Running", Reason="", readiness=true. Elapsed: 17.733155ms
    Nov 15 06:00:40.440: INFO: Pod "my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10-rh9dg" satisfied condition "running"
    Nov 15 06:00:40.440: INFO: Pod "my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10-rh9dg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 06:00:35 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 06:00:36 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 06:00:36 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 06:00:35 +0000 UTC Reason: Message:}])
    Nov 15 06:00:40.440: INFO: Trying to dial the pod
    Nov 15 06:00:45.518: INFO: Controller my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10: Got expected result from replica 1 [my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10-rh9dg]: "my-hostname-basic-72720016-781c-4fb6-a291-2cbf0d1d8c10-rh9dg", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:00:45.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1537" for this suite. 11/15/23 06:00:45.553
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:00:45.584
Nov 15 06:00:45.584: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename deployment 11/15/23 06:00:45.585
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:00:45.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:00:45.654
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Nov 15 06:00:45.669: INFO: Creating deployment "webserver-deployment"
W1115 06:00:45.688821      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Nov 15 06:00:45.688: INFO: Waiting for observed generation 1
Nov 15 06:00:47.711: INFO: Waiting for all required pods to come up
Nov 15 06:00:47.737: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 11/15/23 06:00:47.737
Nov 15 06:00:47.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vxbw8" in namespace "deployment-6989" to be "running"
Nov 15 06:00:47.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4q6dq" in namespace "deployment-6989" to be "running"
Nov 15 06:00:47.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-pcxdw" in namespace "deployment-6989" to be "running"
Nov 15 06:00:47.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-969s2" in namespace "deployment-6989" to be "running"
Nov 15 06:00:47.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rgwxj" in namespace "deployment-6989" to be "running"
Nov 15 06:00:47.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-svg8c" in namespace "deployment-6989" to be "running"
Nov 15 06:00:47.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-tczrv" in namespace "deployment-6989" to be "running"
Nov 15 06:00:47.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5zdp6" in namespace "deployment-6989" to be "running"
Nov 15 06:00:47.758: INFO: Pod "webserver-deployment-7f5969cbc7-pcxdw": Phase="Pending", Reason="", readiness=false. Elapsed: 20.535177ms
Nov 15 06:00:47.766: INFO: Pod "webserver-deployment-7f5969cbc7-969s2": Phase="Pending", Reason="", readiness=false. Elapsed: 28.947258ms
Nov 15 06:00:47.766: INFO: Pod "webserver-deployment-7f5969cbc7-vxbw8": Phase="Pending", Reason="", readiness=false. Elapsed: 29.443145ms
Nov 15 06:00:47.767: INFO: Pod "webserver-deployment-7f5969cbc7-4q6dq": Phase="Pending", Reason="", readiness=false. Elapsed: 29.805493ms
Nov 15 06:00:47.767: INFO: Pod "webserver-deployment-7f5969cbc7-rgwxj": Phase="Pending", Reason="", readiness=false. Elapsed: 29.553763ms
Nov 15 06:00:47.767: INFO: Pod "webserver-deployment-7f5969cbc7-svg8c": Phase="Pending", Reason="", readiness=false. Elapsed: 29.493568ms
Nov 15 06:00:47.767: INFO: Pod "webserver-deployment-7f5969cbc7-tczrv": Phase="Running", Reason="", readiness=true. Elapsed: 29.401769ms
Nov 15 06:00:47.767: INFO: Pod "webserver-deployment-7f5969cbc7-tczrv" satisfied condition "running"
Nov 15 06:00:47.767: INFO: Pod "webserver-deployment-7f5969cbc7-5zdp6": Phase="Pending", Reason="", readiness=false. Elapsed: 29.108815ms
Nov 15 06:00:49.780: INFO: Pod "webserver-deployment-7f5969cbc7-pcxdw": Phase="Running", Reason="", readiness=true. Elapsed: 2.043400355s
Nov 15 06:00:49.780: INFO: Pod "webserver-deployment-7f5969cbc7-pcxdw" satisfied condition "running"
Nov 15 06:00:49.787: INFO: Pod "webserver-deployment-7f5969cbc7-5zdp6": Phase="Running", Reason="", readiness=true. Elapsed: 2.04887914s
Nov 15 06:00:49.787: INFO: Pod "webserver-deployment-7f5969cbc7-5zdp6" satisfied condition "running"
Nov 15 06:00:49.790: INFO: Pod "webserver-deployment-7f5969cbc7-rgwxj": Phase="Running", Reason="", readiness=true. Elapsed: 2.053159772s
Nov 15 06:00:49.790: INFO: Pod "webserver-deployment-7f5969cbc7-rgwxj" satisfied condition "running"
Nov 15 06:00:49.792: INFO: Pod "webserver-deployment-7f5969cbc7-969s2": Phase="Running", Reason="", readiness=true. Elapsed: 2.055369606s
Nov 15 06:00:49.793: INFO: Pod "webserver-deployment-7f5969cbc7-969s2" satisfied condition "running"
Nov 15 06:00:49.794: INFO: Pod "webserver-deployment-7f5969cbc7-4q6dq": Phase="Running", Reason="", readiness=true. Elapsed: 2.05664205s
Nov 15 06:00:49.794: INFO: Pod "webserver-deployment-7f5969cbc7-4q6dq" satisfied condition "running"
Nov 15 06:00:49.794: INFO: Pod "webserver-deployment-7f5969cbc7-vxbw8": Phase="Running", Reason="", readiness=true. Elapsed: 2.05683613s
Nov 15 06:00:49.794: INFO: Pod "webserver-deployment-7f5969cbc7-vxbw8" satisfied condition "running"
Nov 15 06:00:49.794: INFO: Pod "webserver-deployment-7f5969cbc7-svg8c": Phase="Running", Reason="", readiness=true. Elapsed: 2.056899677s
Nov 15 06:00:49.794: INFO: Pod "webserver-deployment-7f5969cbc7-svg8c" satisfied condition "running"
Nov 15 06:00:49.794: INFO: Waiting for deployment "webserver-deployment" to complete
Nov 15 06:00:49.878: INFO: Updating deployment "webserver-deployment" with a non-existent image
Nov 15 06:00:50.035: INFO: Updating deployment webserver-deployment
Nov 15 06:00:50.035: INFO: Waiting for observed generation 2
Nov 15 06:00:52.062: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Nov 15 06:00:52.098: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Nov 15 06:00:52.119: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 15 06:00:52.180: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Nov 15 06:00:52.180: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Nov 15 06:00:52.192: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 15 06:00:52.223: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Nov 15 06:00:52.223: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Nov 15 06:00:52.307: INFO: Updating deployment webserver-deployment
Nov 15 06:00:52.307: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Nov 15 06:00:52.349: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Nov 15 06:00:54.385: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 15 06:00:54.420: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6989  c340b09c-9bb9-4254-8905-b4134c6dc0d4 68584 3 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000dc0708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:9,UnavailableReplicas:24,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-11-15 06:00:52 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-11-15 06:00:54 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,},},ReadyReplicas:9,CollisionCount:nil,},}

Nov 15 06:00:54.438: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-6989  3dab3845-fa44-48fc-b151-da9feb4cb53c 68458 3 2023-11-15 06:00:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment c340b09c-9bb9-4254-8905-b4134c6dc0d4 0xc000dc0d57 0xc000dc0d58}] [] [{kube-controller-manager Update apps/v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c340b09c-9bb9-4254-8905-b4134c6dc0d4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000dc0df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 15 06:00:54.438: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Nov 15 06:00:54.439: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-6989  1a37ca15-d9ca-406a-91b0-7defbc493f5a 68582 3 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment c340b09c-9bb9-4254-8905-b4134c6dc0d4 0xc000dc0c47 0xc000dc0c48}] [] [{kube-controller-manager Update apps/v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c340b09c-9bb9-4254-8905-b4134c6dc0d4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000dc0cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[]ReplicaSetCondition{},},}
Nov 15 06:00:54.500: INFO: Pod "webserver-deployment-7f5969cbc7-4q6dq" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4q6dq webserver-deployment-7f5969cbc7- deployment-6989  241fc823-9646-4547-98a9-3fb17debe44b 68256 0 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7993096ac8df75c707534f3f7f0698f7b9bccb19acb3b04f3b85fbec41bc7f29 cni.projectcalico.org/podIP:172.30.10.172/32 cni.projectcalico.org/podIPs:172.30.10.172/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.10.172"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc000b03867 0xc000b03868}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.10.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lrz48,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lrz48,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:172.30.10.172,StartTime:2023-11-15 06:00:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://118651cd32cbbe080bb786f76c82829548fa9da0297dd32149a5fa1a9508c1d9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.10.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.500: INFO: Pod "webserver-deployment-7f5969cbc7-4q978" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4q978 webserver-deployment-7f5969cbc7- deployment-6989  079d0df9-9f6f-4294-8e99-afdba86efee0 68580 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:58e573128c1cff22e8f860a07c749f676e1fd88d900e451c326735a7878d1285 cni.projectcalico.org/podIP:172.30.213.164/32 cni.projectcalico.org/podIPs:172.30.213.164/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.213.164"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc000b03af7 0xc000b03af8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.164\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5s49p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5s49p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:172.30.213.164,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:00:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://9fe2bca9a8c23fb765f8352a91d706887ece817f9a986f53f504580e75b7401e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.213.164,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.500: INFO: Pod "webserver-deployment-7f5969cbc7-5md7g" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5md7g webserver-deployment-7f5969cbc7- deployment-6989  74d9a24e-b97e-477d-a44b-3c68000bdb28 68565 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ba135f48fb59f52c029a8ae10761c99d8c77ca07ccb45e0dc492505bc44c159b cni.projectcalico.org/podIP:172.30.10.162/32 cni.projectcalico.org/podIPs:172.30.10.162/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.10.162"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc000b03d87 0xc000b03d88}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9xbrv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9xbrv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.500: INFO: Pod "webserver-deployment-7f5969cbc7-5q8vp" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5q8vp webserver-deployment-7f5969cbc7- deployment-6989  37459934-6a53-468b-8c83-978bf93b7126 68212 0 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:31b23cbd4e0a60138debd226478803da93314ccff81dcd795720359e85d20896 cni.projectcalico.org/podIP:172.30.10.177/32 cni.projectcalico.org/podIPs:172.30.10.177/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.10.177"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc000b03ff7 0xc000b03ff8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.10.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-smbqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-smbqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:172.30.10.177,StartTime:2023-11-15 06:00:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://38b08c2985a58c8156ce87e1b9b5d38dc5d64a96743c0f1138983f12b3225b13,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.10.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.501: INFO: Pod "webserver-deployment-7f5969cbc7-5zdp6" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5zdp6 webserver-deployment-7f5969cbc7- deployment-6989  286bf520-0ceb-4b04-aa80-4e86196b6599 68263 0 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2606dd84e8b60cc1f6406a60715e68c664eec92e0561d5340a3b9b775d8d743f cni.projectcalico.org/podIP:172.30.214.182/32 cni.projectcalico.org/podIPs:172.30.214.182/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.214.182"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0012dd787 0xc0012dd788}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.214.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w5gwp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w5gwp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:172.30.214.182,StartTime:2023-11-15 06:00:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://75cab773189c47c876ec0f32205f20e3ea1e41bccbed68b606f36806f7abff34,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.214.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.501: INFO: Pod "webserver-deployment-7f5969cbc7-6xcpr" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6xcpr webserver-deployment-7f5969cbc7- deployment-6989  d29a2515-916d-4dd1-9d4a-a21200108b1c 68460 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0028d2167 0xc0028d2168}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jf2mn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jf2mn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.512: INFO: Pod "webserver-deployment-7f5969cbc7-7zj2w" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7zj2w webserver-deployment-7f5969cbc7- deployment-6989  ce86ae0f-9600-4adb-957d-7a3f87180d57 68590 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:21e1a1e8eb91a59d6b14e0fd2f2d19e081a109c6c13d2c7a98bc8c3b375e50d4 cni.projectcalico.org/podIP:172.30.10.183/32 cni.projectcalico.org/podIPs:172.30.10.183/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.10.183"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0028d2397 0xc0028d2398}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-snw92,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-snw92,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.513: INFO: Pod "webserver-deployment-7f5969cbc7-8tsft" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8tsft webserver-deployment-7f5969cbc7- deployment-6989  57371a9f-5011-4a09-a60a-d62e3f039372 68506 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0028d25e7 0xc0028d25e8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qsvzx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qsvzx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.513: INFO: Pod "webserver-deployment-7f5969cbc7-bw6hj" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bw6hj webserver-deployment-7f5969cbc7- deployment-6989  1f566d11-08b7-413b-be27-c7d71691ebe0 68606 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a71696eaf0d75bf6ae7a8513fc9e5b61fe397b69011bcccc2d4af4c2d0f5a8f4 cni.projectcalico.org/podIP:172.30.214.174/32 cni.projectcalico.org/podIPs:172.30.214.174/32 openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0028d27f7 0xc0028d27f8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m692f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m692f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.514: INFO: Pod "webserver-deployment-7f5969cbc7-d7497" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d7497 webserver-deployment-7f5969cbc7- deployment-6989  48b0e661-b0db-4ddd-b917-cad2746690df 68588 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:da6734ece49e7c9f687576f83798d8eb4484c1177ee23b1cdf3f444d09068530 cni.projectcalico.org/podIP:172.30.214.181/32 cni.projectcalico.org/podIPs:172.30.214.181/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.214.181"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0028d2a37 0xc0028d2a38}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hwdnk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hwdnk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.514: INFO: Pod "webserver-deployment-7f5969cbc7-d82q8" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d82q8 webserver-deployment-7f5969cbc7- deployment-6989  024d2e42-6e54-48ae-815e-2a53e99253ee 68566 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:813fe35938e1ea8159180eb3b18b49d3c18540b172cdebe5f98ea0714abfb548 cni.projectcalico.org/podIP:172.30.214.183/32 cni.projectcalico.org/podIPs:172.30.214.183/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.214.183"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0028d3517 0xc0028d3518}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bjxzn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bjxzn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.515: INFO: Pod "webserver-deployment-7f5969cbc7-ggtf8" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ggtf8 webserver-deployment-7f5969cbc7- deployment-6989  98caac8e-7f08-421d-914f-80c3062e3a2c 68537 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:dd78e18f51ee570fda91765e0fbfe035d8182503d1233ac42bbb54ddb01b5dd3 cni.projectcalico.org/podIP:172.30.10.165/32 cni.projectcalico.org/podIPs:172.30.10.165/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.10.165"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0028d3927 0xc0028d3928}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gh66w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gh66w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.515: INFO: Pod "webserver-deployment-7f5969cbc7-ghq7g" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ghq7g webserver-deployment-7f5969cbc7- deployment-6989  a1713c70-9942-4914-a690-45ffefa43905 68564 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4be01577f43a0253af0f7a7c73da1db406bac204a678e11a9561af7dfe1ee890 cni.projectcalico.org/podIP:172.30.213.165/32 cni.projectcalico.org/podIPs:172.30.213.165/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.213.165"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0028d3b77 0xc0028d3b78}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5llb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5llb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.516: INFO: Pod "webserver-deployment-7f5969cbc7-gw7ss" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gw7ss webserver-deployment-7f5969cbc7- deployment-6989  2473a83c-9492-49cc-99c2-73bd0d7f1c2c 68228 0 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3acb4ed325e45ad33065a86447f291234c79796ccca0a43cba25da8c8416a79f cni.projectcalico.org/podIP:172.30.214.171/32 cni.projectcalico.org/podIPs:172.30.214.171/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.214.171"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0028d3dc7 0xc0028d3dc8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.214.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4xtz2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4xtz2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:172.30.214.171,StartTime:2023-11-15 06:00:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://6fe8e6e1e246909ac916b2f16a646b309f55baec0db0b075bc96927afc6571d6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.214.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.516: INFO: Pod "webserver-deployment-7f5969cbc7-pcxdw" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pcxdw webserver-deployment-7f5969cbc7- deployment-6989  44f8aaef-7303-4dca-8312-f1b1f385a8d4 68251 0 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4854dca387d61b3209da3bdf6532f19ec1940ae125d3be01b682115f0ec8a57e cni.projectcalico.org/podIP:172.30.213.190/32 cni.projectcalico.org/podIPs:172.30.213.190/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.213.190"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0046d4087 0xc0046d4088}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pjvfr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pjvfr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:172.30.213.190,StartTime:2023-11-15 06:00:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://1f23b9fa6493ec5b2cdec617b22f458c28e995f55d315fa374b09ef48120654c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.213.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.517: INFO: Pod "webserver-deployment-7f5969cbc7-plrqb" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-plrqb webserver-deployment-7f5969cbc7- deployment-6989  474eb985-6249-4540-8c47-102b3a408323 68594 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:306029895db4c476aac149f8152e85bcee65a41ac0e5e91c2c5437ee7762ab25 cni.projectcalico.org/podIP:172.30.213.155/32 cni.projectcalico.org/podIPs:172.30.213.155/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.213.155"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0046d46d7 0xc0046d46d8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87vg8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87vg8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.517: INFO: Pod "webserver-deployment-7f5969cbc7-rgwxj" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rgwxj webserver-deployment-7f5969cbc7- deployment-6989  17dc3ffc-539f-4986-a76a-733d5425753f 68242 0 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cde74ea345621483e060883fb678b8cb2b8834be919afd1871966f6e9cd00327 cni.projectcalico.org/podIP:172.30.213.188/32 cni.projectcalico.org/podIPs:172.30.213.188/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.213.188"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0046d4957 0xc0046d4958}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qckn4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qckn4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:172.30.213.188,StartTime:2023-11-15 06:00:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://b1ce0ae1d2abf023c4b503092b2c35bcae9042329b3129a245120a49382a0826,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.213.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.520: INFO: Pod "webserver-deployment-7f5969cbc7-svg8c" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-svg8c webserver-deployment-7f5969cbc7- deployment-6989  d9a59da0-fcf6-46fd-b1fd-930d409f6e58 68259 0 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5cfa957b812237cd0fd9b4e889e4ccaccb0570a91729beec051e0c5efcea0542 cni.projectcalico.org/podIP:172.30.10.176/32 cni.projectcalico.org/podIPs:172.30.10.176/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.10.176"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0046d4be7 0xc0046d4be8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.10.176\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2tg2g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2tg2g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:172.30.10.176,StartTime:2023-11-15 06:00:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://31ec4c5ab2d4952cfc8c625866f1c2f99535d9775c80fb2d42ee0d41eb767a34,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.10.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.532: INFO: Pod "webserver-deployment-7f5969cbc7-tczrv" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tczrv webserver-deployment-7f5969cbc7- deployment-6989  bcbb252f-4c1a-4cc1-abb8-7a0877e67e67 68233 0 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ff905dd728f26c46fa09c9dda8512c2f7cfb729939e16aefbead34c39b05cc02 cni.projectcalico.org/podIP:172.30.214.169/32 cni.projectcalico.org/podIPs:172.30.214.169/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.214.169"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0046d4e57 0xc0046d4e58}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.214.169\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p4z95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p4z95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:172.30.214.169,StartTime:2023-11-15 06:00:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://917b66c1115fb816cee62f9902d7df3621be070fc5f4a206ccdc44960a149ee8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.214.169,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.532: INFO: Pod "webserver-deployment-7f5969cbc7-txxlz" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-txxlz webserver-deployment-7f5969cbc7- deployment-6989  e1d1d9c5-deee-4df0-9273-b40838b68da6 68485 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0046d50c7 0xc0046d50c8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-42qkz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-42qkz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.533: INFO: Pod "webserver-deployment-d9f79cb5-4sgmn" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4sgmn webserver-deployment-d9f79cb5- deployment-6989  06d81b37-6ab9-4366-abc8-27f1fa0b3fd1 68607 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ec9103db4cd063ee2f7f53ec8aea80702f3a44c78a3e21591c20b0988cf59da5 cni.projectcalico.org/podIP:172.30.213.163/32 cni.projectcalico.org/podIPs:172.30.213.163/32 openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0046d52d7 0xc0046d52d8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ljr8d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ljr8d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.533: INFO: Pod "webserver-deployment-d9f79cb5-4ths2" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4ths2 webserver-deployment-d9f79cb5- deployment-6989  6af97a6e-173a-4b7a-ba6e-97570048ccd7 68548 0 2023-11-15 06:00:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6e8fc2c85ee2adadd3deeb4874dd85dfb710df2d18602b0b9738c3be7b12dd11 cni.projectcalico.org/podIP:172.30.214.189/32 cni.projectcalico.org/podIPs:172.30.214.189/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.214.189"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0046d5537 0xc0046d5538}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.214.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2rd8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2rd8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:172.30.214.189,StartTime:2023-11-15 06:00:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.214.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.534: INFO: Pod "webserver-deployment-d9f79cb5-b84nn" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-b84nn webserver-deployment-d9f79cb5- deployment-6989  e26c10c9-199e-4e16-9bde-4d15d7e92356 68446 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0046d57d7 0xc0046d57d8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qzbjr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qzbjr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.534: INFO: Pod "webserver-deployment-d9f79cb5-bqtcj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bqtcj webserver-deployment-d9f79cb5- deployment-6989  d249878b-b2af-4492-b88f-fd28b12f5adc 68493 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0046d5a07 0xc0046d5a08}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mggn9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mggn9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.535: INFO: Pod "webserver-deployment-d9f79cb5-j2wcm" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j2wcm webserver-deployment-d9f79cb5- deployment-6989  b78afd5a-548a-48bd-98ac-cb5521489d57 68529 0 2023-11-15 06:00:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:66b5d34cb17b8af62b25d40b4fdc330b6b2490c23ff0022dc378d530755ded1b cni.projectcalico.org/podIP:172.30.10.168/32 cni.projectcalico.org/podIPs:172.30.10.168/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.10.168"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0046d5c57 0xc0046d5c58}] [] [{calico Update v1 2023-11-15 06:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-11-15 06:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-11-15 06:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.10.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wx5xz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wx5xz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:172.30.10.168,StartTime:2023-11-15 06:00:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.10.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.535: INFO: Pod "webserver-deployment-d9f79cb5-khww8" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-khww8 webserver-deployment-d9f79cb5- deployment-6989  cbb862c7-b402-4264-90ef-b62296b37e14 68482 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0046d5ef7 0xc0046d5ef8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-89t55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-89t55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.536: INFO: Pod "webserver-deployment-d9f79cb5-rhftw" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rhftw webserver-deployment-d9f79cb5- deployment-6989  39295167-052b-4906-8e4e-e04ca078940a 68608 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ead2d8e74df82addddac3a6138a091151d081e6e75477bac2d7e1c43c3937c36 cni.projectcalico.org/podIP:172.30.10.164/32 cni.projectcalico.org/podIPs:172.30.10.164/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.10.164"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0045a2557 0xc0045a2558}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hd4k8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hd4k8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.536: INFO: Pod "webserver-deployment-d9f79cb5-rhn8d" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rhn8d webserver-deployment-d9f79cb5- deployment-6989  820ab70c-1a3d-418e-bbb8-5f52eb59bd66 68555 0 2023-11-15 06:00:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:25f2a78f79eff8e12c34a3570e9fdf837609c6b75216febf60c6abb60dc8336b cni.projectcalico.org/podIP:172.30.214.190/32 cni.projectcalico.org/podIPs:172.30.214.190/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.214.190"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0045a27d7 0xc0045a27d8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.214.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j427g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j427g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:172.30.214.190,StartTime:2023-11-15 06:00:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.214.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.539: INFO: Pod "webserver-deployment-d9f79cb5-rxhj4" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rxhj4 webserver-deployment-d9f79cb5- deployment-6989  87a06c8a-8fb7-44a3-82f9-5a3f70e653b5 68495 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0045a2a77 0xc0045a2a78}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6lcrr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6lcrr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.539: INFO: Pod "webserver-deployment-d9f79cb5-sc2gk" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sc2gk webserver-deployment-d9f79cb5- deployment-6989  84930f03-7495-411d-9305-79fd16bc1702 68539 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2c25531d5c6b1d2b0ac399cd82ff74dfff95f50549ba63bb95ebd2b7785961af cni.projectcalico.org/podIP:172.30.214.184/32 cni.projectcalico.org/podIPs:172.30.214.184/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.214.184"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0045a2ca7 0xc0045a2ca8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-99mdw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-99mdw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.540: INFO: Pod "webserver-deployment-d9f79cb5-t44zc" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-t44zc webserver-deployment-d9f79cb5- deployment-6989  3c836042-d5a4-4397-b1e7-3f0d7f96a89f 68483 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0045a36e7 0xc0045a36e8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6v92h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6v92h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.540: INFO: Pod "webserver-deployment-d9f79cb5-zwxq2" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zwxq2 webserver-deployment-d9f79cb5- deployment-6989  4e05bab2-6b5f-405a-99fe-26793d66c9cf 68600 0 2023-11-15 06:00:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f494af07a1914440d895c0844dcfbbd94da97410ab84a97382b1f416b97df972 cni.projectcalico.org/podIP:172.30.213.160/32 cni.projectcalico.org/podIPs:172.30.213.160/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.213.160"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0045a39e7 0xc0045a39e8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-crrpg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-crrpg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:172.30.213.160,StartTime:2023-11-15 06:00:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.213.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 06:00:54.541: INFO: Pod "webserver-deployment-d9f79cb5-zz77q" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zz77q webserver-deployment-d9f79cb5- deployment-6989  fd671aad-259e-4222-8d5b-19bb6c8f1287 68592 0 2023-11-15 06:00:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:763525e54e7cd5e4d5ff0acd1758809a1d8ebdbe2f4d43ec14574757b5f845fd cni.projectcalico.org/podIP:172.30.213.162/32 cni.projectcalico.org/podIPs:172.30.213.162/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.213.162"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0045a3c87 0xc0045a3c88}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-47dlx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47dlx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:172.30.213.162,StartTime:2023-11-15 06:00:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.213.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 15 06:00:54.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6989" for this suite. 11/15/23 06:00:54.574
------------------------------
â€¢ [SLOW TEST] [9.020 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:00:45.584
    Nov 15 06:00:45.584: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename deployment 11/15/23 06:00:45.585
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:00:45.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:00:45.654
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Nov 15 06:00:45.669: INFO: Creating deployment "webserver-deployment"
    W1115 06:00:45.688821      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Nov 15 06:00:45.688: INFO: Waiting for observed generation 1
    Nov 15 06:00:47.711: INFO: Waiting for all required pods to come up
    Nov 15 06:00:47.737: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 11/15/23 06:00:47.737
    Nov 15 06:00:47.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vxbw8" in namespace "deployment-6989" to be "running"
    Nov 15 06:00:47.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4q6dq" in namespace "deployment-6989" to be "running"
    Nov 15 06:00:47.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-pcxdw" in namespace "deployment-6989" to be "running"
    Nov 15 06:00:47.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-969s2" in namespace "deployment-6989" to be "running"
    Nov 15 06:00:47.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rgwxj" in namespace "deployment-6989" to be "running"
    Nov 15 06:00:47.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-svg8c" in namespace "deployment-6989" to be "running"
    Nov 15 06:00:47.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-tczrv" in namespace "deployment-6989" to be "running"
    Nov 15 06:00:47.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5zdp6" in namespace "deployment-6989" to be "running"
    Nov 15 06:00:47.758: INFO: Pod "webserver-deployment-7f5969cbc7-pcxdw": Phase="Pending", Reason="", readiness=false. Elapsed: 20.535177ms
    Nov 15 06:00:47.766: INFO: Pod "webserver-deployment-7f5969cbc7-969s2": Phase="Pending", Reason="", readiness=false. Elapsed: 28.947258ms
    Nov 15 06:00:47.766: INFO: Pod "webserver-deployment-7f5969cbc7-vxbw8": Phase="Pending", Reason="", readiness=false. Elapsed: 29.443145ms
    Nov 15 06:00:47.767: INFO: Pod "webserver-deployment-7f5969cbc7-4q6dq": Phase="Pending", Reason="", readiness=false. Elapsed: 29.805493ms
    Nov 15 06:00:47.767: INFO: Pod "webserver-deployment-7f5969cbc7-rgwxj": Phase="Pending", Reason="", readiness=false. Elapsed: 29.553763ms
    Nov 15 06:00:47.767: INFO: Pod "webserver-deployment-7f5969cbc7-svg8c": Phase="Pending", Reason="", readiness=false. Elapsed: 29.493568ms
    Nov 15 06:00:47.767: INFO: Pod "webserver-deployment-7f5969cbc7-tczrv": Phase="Running", Reason="", readiness=true. Elapsed: 29.401769ms
    Nov 15 06:00:47.767: INFO: Pod "webserver-deployment-7f5969cbc7-tczrv" satisfied condition "running"
    Nov 15 06:00:47.767: INFO: Pod "webserver-deployment-7f5969cbc7-5zdp6": Phase="Pending", Reason="", readiness=false. Elapsed: 29.108815ms
    Nov 15 06:00:49.780: INFO: Pod "webserver-deployment-7f5969cbc7-pcxdw": Phase="Running", Reason="", readiness=true. Elapsed: 2.043400355s
    Nov 15 06:00:49.780: INFO: Pod "webserver-deployment-7f5969cbc7-pcxdw" satisfied condition "running"
    Nov 15 06:00:49.787: INFO: Pod "webserver-deployment-7f5969cbc7-5zdp6": Phase="Running", Reason="", readiness=true. Elapsed: 2.04887914s
    Nov 15 06:00:49.787: INFO: Pod "webserver-deployment-7f5969cbc7-5zdp6" satisfied condition "running"
    Nov 15 06:00:49.790: INFO: Pod "webserver-deployment-7f5969cbc7-rgwxj": Phase="Running", Reason="", readiness=true. Elapsed: 2.053159772s
    Nov 15 06:00:49.790: INFO: Pod "webserver-deployment-7f5969cbc7-rgwxj" satisfied condition "running"
    Nov 15 06:00:49.792: INFO: Pod "webserver-deployment-7f5969cbc7-969s2": Phase="Running", Reason="", readiness=true. Elapsed: 2.055369606s
    Nov 15 06:00:49.793: INFO: Pod "webserver-deployment-7f5969cbc7-969s2" satisfied condition "running"
    Nov 15 06:00:49.794: INFO: Pod "webserver-deployment-7f5969cbc7-4q6dq": Phase="Running", Reason="", readiness=true. Elapsed: 2.05664205s
    Nov 15 06:00:49.794: INFO: Pod "webserver-deployment-7f5969cbc7-4q6dq" satisfied condition "running"
    Nov 15 06:00:49.794: INFO: Pod "webserver-deployment-7f5969cbc7-vxbw8": Phase="Running", Reason="", readiness=true. Elapsed: 2.05683613s
    Nov 15 06:00:49.794: INFO: Pod "webserver-deployment-7f5969cbc7-vxbw8" satisfied condition "running"
    Nov 15 06:00:49.794: INFO: Pod "webserver-deployment-7f5969cbc7-svg8c": Phase="Running", Reason="", readiness=true. Elapsed: 2.056899677s
    Nov 15 06:00:49.794: INFO: Pod "webserver-deployment-7f5969cbc7-svg8c" satisfied condition "running"
    Nov 15 06:00:49.794: INFO: Waiting for deployment "webserver-deployment" to complete
    Nov 15 06:00:49.878: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Nov 15 06:00:50.035: INFO: Updating deployment webserver-deployment
    Nov 15 06:00:50.035: INFO: Waiting for observed generation 2
    Nov 15 06:00:52.062: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Nov 15 06:00:52.098: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Nov 15 06:00:52.119: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Nov 15 06:00:52.180: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Nov 15 06:00:52.180: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Nov 15 06:00:52.192: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Nov 15 06:00:52.223: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Nov 15 06:00:52.223: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Nov 15 06:00:52.307: INFO: Updating deployment webserver-deployment
    Nov 15 06:00:52.307: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Nov 15 06:00:52.349: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Nov 15 06:00:54.385: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 15 06:00:54.420: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-6989  c340b09c-9bb9-4254-8905-b4134c6dc0d4 68584 3 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000dc0708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:9,UnavailableReplicas:24,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-11-15 06:00:52 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-11-15 06:00:54 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,},},ReadyReplicas:9,CollisionCount:nil,},}

    Nov 15 06:00:54.438: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-6989  3dab3845-fa44-48fc-b151-da9feb4cb53c 68458 3 2023-11-15 06:00:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment c340b09c-9bb9-4254-8905-b4134c6dc0d4 0xc000dc0d57 0xc000dc0d58}] [] [{kube-controller-manager Update apps/v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c340b09c-9bb9-4254-8905-b4134c6dc0d4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000dc0df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 06:00:54.438: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Nov 15 06:00:54.439: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-6989  1a37ca15-d9ca-406a-91b0-7defbc493f5a 68582 3 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment c340b09c-9bb9-4254-8905-b4134c6dc0d4 0xc000dc0c47 0xc000dc0c48}] [] [{kube-controller-manager Update apps/v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c340b09c-9bb9-4254-8905-b4134c6dc0d4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000dc0cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 06:00:54.500: INFO: Pod "webserver-deployment-7f5969cbc7-4q6dq" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4q6dq webserver-deployment-7f5969cbc7- deployment-6989  241fc823-9646-4547-98a9-3fb17debe44b 68256 0 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7993096ac8df75c707534f3f7f0698f7b9bccb19acb3b04f3b85fbec41bc7f29 cni.projectcalico.org/podIP:172.30.10.172/32 cni.projectcalico.org/podIPs:172.30.10.172/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.10.172"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc000b03867 0xc000b03868}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.10.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lrz48,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lrz48,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:172.30.10.172,StartTime:2023-11-15 06:00:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://118651cd32cbbe080bb786f76c82829548fa9da0297dd32149a5fa1a9508c1d9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.10.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.500: INFO: Pod "webserver-deployment-7f5969cbc7-4q978" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4q978 webserver-deployment-7f5969cbc7- deployment-6989  079d0df9-9f6f-4294-8e99-afdba86efee0 68580 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:58e573128c1cff22e8f860a07c749f676e1fd88d900e451c326735a7878d1285 cni.projectcalico.org/podIP:172.30.213.164/32 cni.projectcalico.org/podIPs:172.30.213.164/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.213.164"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc000b03af7 0xc000b03af8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.164\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5s49p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5s49p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:172.30.213.164,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:00:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://9fe2bca9a8c23fb765f8352a91d706887ece817f9a986f53f504580e75b7401e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.213.164,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.500: INFO: Pod "webserver-deployment-7f5969cbc7-5md7g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5md7g webserver-deployment-7f5969cbc7- deployment-6989  74d9a24e-b97e-477d-a44b-3c68000bdb28 68565 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ba135f48fb59f52c029a8ae10761c99d8c77ca07ccb45e0dc492505bc44c159b cni.projectcalico.org/podIP:172.30.10.162/32 cni.projectcalico.org/podIPs:172.30.10.162/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.10.162"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc000b03d87 0xc000b03d88}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9xbrv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9xbrv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.500: INFO: Pod "webserver-deployment-7f5969cbc7-5q8vp" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5q8vp webserver-deployment-7f5969cbc7- deployment-6989  37459934-6a53-468b-8c83-978bf93b7126 68212 0 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:31b23cbd4e0a60138debd226478803da93314ccff81dcd795720359e85d20896 cni.projectcalico.org/podIP:172.30.10.177/32 cni.projectcalico.org/podIPs:172.30.10.177/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.10.177"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc000b03ff7 0xc000b03ff8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.10.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-smbqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-smbqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:172.30.10.177,StartTime:2023-11-15 06:00:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://38b08c2985a58c8156ce87e1b9b5d38dc5d64a96743c0f1138983f12b3225b13,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.10.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.501: INFO: Pod "webserver-deployment-7f5969cbc7-5zdp6" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5zdp6 webserver-deployment-7f5969cbc7- deployment-6989  286bf520-0ceb-4b04-aa80-4e86196b6599 68263 0 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2606dd84e8b60cc1f6406a60715e68c664eec92e0561d5340a3b9b775d8d743f cni.projectcalico.org/podIP:172.30.214.182/32 cni.projectcalico.org/podIPs:172.30.214.182/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.214.182"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0012dd787 0xc0012dd788}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.214.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w5gwp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w5gwp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:172.30.214.182,StartTime:2023-11-15 06:00:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://75cab773189c47c876ec0f32205f20e3ea1e41bccbed68b606f36806f7abff34,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.214.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.501: INFO: Pod "webserver-deployment-7f5969cbc7-6xcpr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6xcpr webserver-deployment-7f5969cbc7- deployment-6989  d29a2515-916d-4dd1-9d4a-a21200108b1c 68460 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0028d2167 0xc0028d2168}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jf2mn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jf2mn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.512: INFO: Pod "webserver-deployment-7f5969cbc7-7zj2w" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7zj2w webserver-deployment-7f5969cbc7- deployment-6989  ce86ae0f-9600-4adb-957d-7a3f87180d57 68590 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:21e1a1e8eb91a59d6b14e0fd2f2d19e081a109c6c13d2c7a98bc8c3b375e50d4 cni.projectcalico.org/podIP:172.30.10.183/32 cni.projectcalico.org/podIPs:172.30.10.183/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.10.183"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0028d2397 0xc0028d2398}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-snw92,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-snw92,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.513: INFO: Pod "webserver-deployment-7f5969cbc7-8tsft" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8tsft webserver-deployment-7f5969cbc7- deployment-6989  57371a9f-5011-4a09-a60a-d62e3f039372 68506 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0028d25e7 0xc0028d25e8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qsvzx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qsvzx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.513: INFO: Pod "webserver-deployment-7f5969cbc7-bw6hj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bw6hj webserver-deployment-7f5969cbc7- deployment-6989  1f566d11-08b7-413b-be27-c7d71691ebe0 68606 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a71696eaf0d75bf6ae7a8513fc9e5b61fe397b69011bcccc2d4af4c2d0f5a8f4 cni.projectcalico.org/podIP:172.30.214.174/32 cni.projectcalico.org/podIPs:172.30.214.174/32 openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0028d27f7 0xc0028d27f8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m692f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m692f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.514: INFO: Pod "webserver-deployment-7f5969cbc7-d7497" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d7497 webserver-deployment-7f5969cbc7- deployment-6989  48b0e661-b0db-4ddd-b917-cad2746690df 68588 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:da6734ece49e7c9f687576f83798d8eb4484c1177ee23b1cdf3f444d09068530 cni.projectcalico.org/podIP:172.30.214.181/32 cni.projectcalico.org/podIPs:172.30.214.181/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.214.181"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0028d2a37 0xc0028d2a38}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hwdnk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hwdnk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.514: INFO: Pod "webserver-deployment-7f5969cbc7-d82q8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d82q8 webserver-deployment-7f5969cbc7- deployment-6989  024d2e42-6e54-48ae-815e-2a53e99253ee 68566 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:813fe35938e1ea8159180eb3b18b49d3c18540b172cdebe5f98ea0714abfb548 cni.projectcalico.org/podIP:172.30.214.183/32 cni.projectcalico.org/podIPs:172.30.214.183/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.214.183"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0028d3517 0xc0028d3518}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bjxzn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bjxzn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.515: INFO: Pod "webserver-deployment-7f5969cbc7-ggtf8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ggtf8 webserver-deployment-7f5969cbc7- deployment-6989  98caac8e-7f08-421d-914f-80c3062e3a2c 68537 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:dd78e18f51ee570fda91765e0fbfe035d8182503d1233ac42bbb54ddb01b5dd3 cni.projectcalico.org/podIP:172.30.10.165/32 cni.projectcalico.org/podIPs:172.30.10.165/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.10.165"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0028d3927 0xc0028d3928}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gh66w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gh66w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.515: INFO: Pod "webserver-deployment-7f5969cbc7-ghq7g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ghq7g webserver-deployment-7f5969cbc7- deployment-6989  a1713c70-9942-4914-a690-45ffefa43905 68564 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4be01577f43a0253af0f7a7c73da1db406bac204a678e11a9561af7dfe1ee890 cni.projectcalico.org/podIP:172.30.213.165/32 cni.projectcalico.org/podIPs:172.30.213.165/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.213.165"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0028d3b77 0xc0028d3b78}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5llb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5llb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.516: INFO: Pod "webserver-deployment-7f5969cbc7-gw7ss" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gw7ss webserver-deployment-7f5969cbc7- deployment-6989  2473a83c-9492-49cc-99c2-73bd0d7f1c2c 68228 0 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3acb4ed325e45ad33065a86447f291234c79796ccca0a43cba25da8c8416a79f cni.projectcalico.org/podIP:172.30.214.171/32 cni.projectcalico.org/podIPs:172.30.214.171/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.214.171"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0028d3dc7 0xc0028d3dc8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.214.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4xtz2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4xtz2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:172.30.214.171,StartTime:2023-11-15 06:00:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://6fe8e6e1e246909ac916b2f16a646b309f55baec0db0b075bc96927afc6571d6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.214.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.516: INFO: Pod "webserver-deployment-7f5969cbc7-pcxdw" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pcxdw webserver-deployment-7f5969cbc7- deployment-6989  44f8aaef-7303-4dca-8312-f1b1f385a8d4 68251 0 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4854dca387d61b3209da3bdf6532f19ec1940ae125d3be01b682115f0ec8a57e cni.projectcalico.org/podIP:172.30.213.190/32 cni.projectcalico.org/podIPs:172.30.213.190/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.213.190"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0046d4087 0xc0046d4088}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pjvfr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pjvfr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:172.30.213.190,StartTime:2023-11-15 06:00:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://1f23b9fa6493ec5b2cdec617b22f458c28e995f55d315fa374b09ef48120654c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.213.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.517: INFO: Pod "webserver-deployment-7f5969cbc7-plrqb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-plrqb webserver-deployment-7f5969cbc7- deployment-6989  474eb985-6249-4540-8c47-102b3a408323 68594 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:306029895db4c476aac149f8152e85bcee65a41ac0e5e91c2c5437ee7762ab25 cni.projectcalico.org/podIP:172.30.213.155/32 cni.projectcalico.org/podIPs:172.30.213.155/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.213.155"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0046d46d7 0xc0046d46d8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87vg8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87vg8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.517: INFO: Pod "webserver-deployment-7f5969cbc7-rgwxj" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rgwxj webserver-deployment-7f5969cbc7- deployment-6989  17dc3ffc-539f-4986-a76a-733d5425753f 68242 0 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cde74ea345621483e060883fb678b8cb2b8834be919afd1871966f6e9cd00327 cni.projectcalico.org/podIP:172.30.213.188/32 cni.projectcalico.org/podIPs:172.30.213.188/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.213.188"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0046d4957 0xc0046d4958}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qckn4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qckn4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:172.30.213.188,StartTime:2023-11-15 06:00:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://b1ce0ae1d2abf023c4b503092b2c35bcae9042329b3129a245120a49382a0826,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.213.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.520: INFO: Pod "webserver-deployment-7f5969cbc7-svg8c" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-svg8c webserver-deployment-7f5969cbc7- deployment-6989  d9a59da0-fcf6-46fd-b1fd-930d409f6e58 68259 0 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5cfa957b812237cd0fd9b4e889e4ccaccb0570a91729beec051e0c5efcea0542 cni.projectcalico.org/podIP:172.30.10.176/32 cni.projectcalico.org/podIPs:172.30.10.176/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.10.176"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0046d4be7 0xc0046d4be8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.10.176\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2tg2g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2tg2g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:172.30.10.176,StartTime:2023-11-15 06:00:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://31ec4c5ab2d4952cfc8c625866f1c2f99535d9775c80fb2d42ee0d41eb767a34,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.10.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.532: INFO: Pod "webserver-deployment-7f5969cbc7-tczrv" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tczrv webserver-deployment-7f5969cbc7- deployment-6989  bcbb252f-4c1a-4cc1-abb8-7a0877e67e67 68233 0 2023-11-15 06:00:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ff905dd728f26c46fa09c9dda8512c2f7cfb729939e16aefbead34c39b05cc02 cni.projectcalico.org/podIP:172.30.214.169/32 cni.projectcalico.org/podIPs:172.30.214.169/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.214.169"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0046d4e57 0xc0046d4e58}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.214.169\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p4z95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p4z95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:172.30.214.169,StartTime:2023-11-15 06:00:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://917b66c1115fb816cee62f9902d7df3621be070fc5f4a206ccdc44960a149ee8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.214.169,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.532: INFO: Pod "webserver-deployment-7f5969cbc7-txxlz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-txxlz webserver-deployment-7f5969cbc7- deployment-6989  e1d1d9c5-deee-4df0-9273-b40838b68da6 68485 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1a37ca15-d9ca-406a-91b0-7defbc493f5a 0xc0046d50c7 0xc0046d50c8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a37ca15-d9ca-406a-91b0-7defbc493f5a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-42qkz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-42qkz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.533: INFO: Pod "webserver-deployment-d9f79cb5-4sgmn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4sgmn webserver-deployment-d9f79cb5- deployment-6989  06d81b37-6ab9-4366-abc8-27f1fa0b3fd1 68607 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ec9103db4cd063ee2f7f53ec8aea80702f3a44c78a3e21591c20b0988cf59da5 cni.projectcalico.org/podIP:172.30.213.163/32 cni.projectcalico.org/podIPs:172.30.213.163/32 openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0046d52d7 0xc0046d52d8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ljr8d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ljr8d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.533: INFO: Pod "webserver-deployment-d9f79cb5-4ths2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4ths2 webserver-deployment-d9f79cb5- deployment-6989  6af97a6e-173a-4b7a-ba6e-97570048ccd7 68548 0 2023-11-15 06:00:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6e8fc2c85ee2adadd3deeb4874dd85dfb710df2d18602b0b9738c3be7b12dd11 cni.projectcalico.org/podIP:172.30.214.189/32 cni.projectcalico.org/podIPs:172.30.214.189/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.214.189"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0046d5537 0xc0046d5538}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.214.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2rd8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2rd8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:172.30.214.189,StartTime:2023-11-15 06:00:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.214.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.534: INFO: Pod "webserver-deployment-d9f79cb5-b84nn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-b84nn webserver-deployment-d9f79cb5- deployment-6989  e26c10c9-199e-4e16-9bde-4d15d7e92356 68446 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0046d57d7 0xc0046d57d8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qzbjr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qzbjr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.534: INFO: Pod "webserver-deployment-d9f79cb5-bqtcj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bqtcj webserver-deployment-d9f79cb5- deployment-6989  d249878b-b2af-4492-b88f-fd28b12f5adc 68493 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0046d5a07 0xc0046d5a08}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mggn9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mggn9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.535: INFO: Pod "webserver-deployment-d9f79cb5-j2wcm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j2wcm webserver-deployment-d9f79cb5- deployment-6989  b78afd5a-548a-48bd-98ac-cb5521489d57 68529 0 2023-11-15 06:00:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:66b5d34cb17b8af62b25d40b4fdc330b6b2490c23ff0022dc378d530755ded1b cni.projectcalico.org/podIP:172.30.10.168/32 cni.projectcalico.org/podIPs:172.30.10.168/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.10.168"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0046d5c57 0xc0046d5c58}] [] [{calico Update v1 2023-11-15 06:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-11-15 06:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-11-15 06:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.10.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wx5xz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wx5xz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:172.30.10.168,StartTime:2023-11-15 06:00:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.10.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.535: INFO: Pod "webserver-deployment-d9f79cb5-khww8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-khww8 webserver-deployment-d9f79cb5- deployment-6989  cbb862c7-b402-4264-90ef-b62296b37e14 68482 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0046d5ef7 0xc0046d5ef8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-89t55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-89t55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.536: INFO: Pod "webserver-deployment-d9f79cb5-rhftw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rhftw webserver-deployment-d9f79cb5- deployment-6989  39295167-052b-4906-8e4e-e04ca078940a 68608 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ead2d8e74df82addddac3a6138a091151d081e6e75477bac2d7e1c43c3937c36 cni.projectcalico.org/podIP:172.30.10.164/32 cni.projectcalico.org/podIPs:172.30.10.164/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.10.164"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0045a2557 0xc0045a2558}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hd4k8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hd4k8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.536: INFO: Pod "webserver-deployment-d9f79cb5-rhn8d" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rhn8d webserver-deployment-d9f79cb5- deployment-6989  820ab70c-1a3d-418e-bbb8-5f52eb59bd66 68555 0 2023-11-15 06:00:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:25f2a78f79eff8e12c34a3570e9fdf837609c6b75216febf60c6abb60dc8336b cni.projectcalico.org/podIP:172.30.214.190/32 cni.projectcalico.org/podIPs:172.30.214.190/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.214.190"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0045a27d7 0xc0045a27d8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.214.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j427g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j427g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:172.30.214.190,StartTime:2023-11-15 06:00:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.214.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.539: INFO: Pod "webserver-deployment-d9f79cb5-rxhj4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rxhj4 webserver-deployment-d9f79cb5- deployment-6989  87a06c8a-8fb7-44a3-82f9-5a3f70e653b5 68495 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0045a2a77 0xc0045a2a78}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6lcrr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6lcrr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.539: INFO: Pod "webserver-deployment-d9f79cb5-sc2gk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sc2gk webserver-deployment-d9f79cb5- deployment-6989  84930f03-7495-411d-9305-79fd16bc1702 68539 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2c25531d5c6b1d2b0ac399cd82ff74dfff95f50549ba63bb95ebd2b7785961af cni.projectcalico.org/podIP:172.30.214.184/32 cni.projectcalico.org/podIPs:172.30.214.184/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.214.184"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0045a2ca7 0xc0045a2ca8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-99mdw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-99mdw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.540: INFO: Pod "webserver-deployment-d9f79cb5-t44zc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-t44zc webserver-deployment-d9f79cb5- deployment-6989  3c836042-d5a4-4397-b1e7-3f0d7f96a89f 68483 0 2023-11-15 06:00:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0045a36e7 0xc0045a36e8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6v92h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6v92h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:,StartTime:2023-11-15 06:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.540: INFO: Pod "webserver-deployment-d9f79cb5-zwxq2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zwxq2 webserver-deployment-d9f79cb5- deployment-6989  4e05bab2-6b5f-405a-99fe-26793d66c9cf 68600 0 2023-11-15 06:00:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f494af07a1914440d895c0844dcfbbd94da97410ab84a97382b1f416b97df972 cni.projectcalico.org/podIP:172.30.213.160/32 cni.projectcalico.org/podIPs:172.30.213.160/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.213.160"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0045a39e7 0xc0045a39e8}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-crrpg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-crrpg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:172.30.213.160,StartTime:2023-11-15 06:00:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.213.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 06:00:54.541: INFO: Pod "webserver-deployment-d9f79cb5-zz77q" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zz77q webserver-deployment-d9f79cb5- deployment-6989  fd671aad-259e-4222-8d5b-19bb6c8f1287 68592 0 2023-11-15 06:00:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:763525e54e7cd5e4d5ff0acd1758809a1d8ebdbe2f4d43ec14574757b5f845fd cni.projectcalico.org/podIP:172.30.213.162/32 cni.projectcalico.org/podIPs:172.30.213.162/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.213.162"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3dab3845-fa44-48fc-b151-da9feb4cb53c 0xc0045a3c87 0xc0045a3c88}] [] [{kube-controller-manager Update v1 2023-11-15 06:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dab3845-fa44-48fc-b151-da9feb4cb53c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:00:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-47dlx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47dlx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-r5th5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:00:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:172.30.213.162,StartTime:2023-11-15 06:00:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.213.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:00:54.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6989" for this suite. 11/15/23 06:00:54.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:00:54.607
Nov 15 06:00:54.607: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename runtimeclass 11/15/23 06:00:54.609
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:00:54.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:00:54.73
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Nov 15 06:00:54.817: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2151 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Nov 15 06:00:54.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2151" for this suite. 11/15/23 06:00:54.89
------------------------------
â€¢ [0.316 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:00:54.607
    Nov 15 06:00:54.607: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename runtimeclass 11/15/23 06:00:54.609
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:00:54.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:00:54.73
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Nov 15 06:00:54.817: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2151 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:00:54.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2151" for this suite. 11/15/23 06:00:54.89
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:00:54.924
Nov 15 06:00:54.924: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename secrets 11/15/23 06:00:54.925
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:00:54.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:00:54.994
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 06:00:55.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7869" for this suite. 11/15/23 06:00:55.182
------------------------------
â€¢ [0.295 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:00:54.924
    Nov 15 06:00:54.924: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename secrets 11/15/23 06:00:54.925
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:00:54.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:00:54.994
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:00:55.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7869" for this suite. 11/15/23 06:00:55.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:00:55.227
Nov 15 06:00:55.227: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename deployment 11/15/23 06:00:55.228
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:00:55.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:00:55.309
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 11/15/23 06:00:55.35
STEP: waiting for Deployment to be created 11/15/23 06:00:55.389
STEP: waiting for all Replicas to be Ready 11/15/23 06:00:55.396
Nov 15 06:00:55.402: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 15 06:00:55.402: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 15 06:00:55.403: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 15 06:00:55.403: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 15 06:00:55.431: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 15 06:00:55.431: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 15 06:00:55.487: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 15 06:00:55.487: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 15 06:00:57.349: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Nov 15 06:00:57.349: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Nov 15 06:00:57.695: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 11/15/23 06:00:57.695
W1115 06:00:57.712412      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Nov 15 06:00:57.724: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 11/15/23 06:00:57.724
Nov 15 06:00:57.731: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0
Nov 15 06:00:57.731: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0
Nov 15 06:00:57.731: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0
Nov 15 06:00:57.731: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0
Nov 15 06:00:57.732: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0
Nov 15 06:00:57.732: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0
Nov 15 06:00:57.732: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0
Nov 15 06:00:57.732: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0
Nov 15 06:00:57.733: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
Nov 15 06:00:57.733: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
Nov 15 06:00:57.733: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
Nov 15 06:00:57.733: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
Nov 15 06:00:57.733: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
Nov 15 06:00:57.733: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
Nov 15 06:00:57.748: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
Nov 15 06:00:57.748: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
Nov 15 06:00:57.780: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
Nov 15 06:00:57.780: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
Nov 15 06:00:57.829: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
Nov 15 06:00:57.829: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
Nov 15 06:00:57.842: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
Nov 15 06:00:57.842: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
Nov 15 06:00:59.883: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
Nov 15 06:00:59.883: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
Nov 15 06:00:59.943: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
STEP: listing Deployments 11/15/23 06:00:59.943
Nov 15 06:00:59.964: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 11/15/23 06:00:59.964
Nov 15 06:00:59.983: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 11/15/23 06:00:59.983
Nov 15 06:01:00.010: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 06:01:00.011: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 06:01:00.092: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 06:01:00.092: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 06:01:00.092: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 06:01:00.112: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 06:01:03.445: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 06:01:03.485: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 06:01:03.554: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 06:01:03.581: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 06:01:05.992: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 11/15/23 06:01:06.065
STEP: fetching the DeploymentStatus 11/15/23 06:01:06.099
Nov 15 06:01:06.123: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
Nov 15 06:01:06.123: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
Nov 15 06:01:06.123: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
Nov 15 06:01:06.123: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
Nov 15 06:01:06.123: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
Nov 15 06:01:06.123: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
Nov 15 06:01:06.123: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
Nov 15 06:01:06.123: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
Nov 15 06:01:06.123: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
Nov 15 06:01:06.124: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
Nov 15 06:01:06.124: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 3
STEP: deleting the Deployment 11/15/23 06:01:06.124
Nov 15 06:01:06.170: INFO: observed event type MODIFIED
Nov 15 06:01:06.170: INFO: observed event type MODIFIED
Nov 15 06:01:06.170: INFO: observed event type MODIFIED
Nov 15 06:01:06.170: INFO: observed event type MODIFIED
Nov 15 06:01:06.170: INFO: observed event type MODIFIED
Nov 15 06:01:06.170: INFO: observed event type MODIFIED
Nov 15 06:01:06.170: INFO: observed event type MODIFIED
Nov 15 06:01:06.170: INFO: observed event type MODIFIED
Nov 15 06:01:06.171: INFO: observed event type MODIFIED
Nov 15 06:01:06.171: INFO: observed event type MODIFIED
Nov 15 06:01:06.171: INFO: observed event type MODIFIED
Nov 15 06:01:06.171: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 15 06:01:06.183: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 15 06:01:06.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4642" for this suite. 11/15/23 06:01:06.228
------------------------------
â€¢ [SLOW TEST] [11.030 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:00:55.227
    Nov 15 06:00:55.227: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename deployment 11/15/23 06:00:55.228
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:00:55.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:00:55.309
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 11/15/23 06:00:55.35
    STEP: waiting for Deployment to be created 11/15/23 06:00:55.389
    STEP: waiting for all Replicas to be Ready 11/15/23 06:00:55.396
    Nov 15 06:00:55.402: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 15 06:00:55.402: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 15 06:00:55.403: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 15 06:00:55.403: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 15 06:00:55.431: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 15 06:00:55.431: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 15 06:00:55.487: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 15 06:00:55.487: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 15 06:00:57.349: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Nov 15 06:00:57.349: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Nov 15 06:00:57.695: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 11/15/23 06:00:57.695
    W1115 06:00:57.712412      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Nov 15 06:00:57.724: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 11/15/23 06:00:57.724
    Nov 15 06:00:57.731: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0
    Nov 15 06:00:57.731: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0
    Nov 15 06:00:57.731: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0
    Nov 15 06:00:57.731: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0
    Nov 15 06:00:57.732: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0
    Nov 15 06:00:57.732: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0
    Nov 15 06:00:57.732: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0
    Nov 15 06:00:57.732: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 0
    Nov 15 06:00:57.733: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
    Nov 15 06:00:57.733: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
    Nov 15 06:00:57.733: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
    Nov 15 06:00:57.733: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
    Nov 15 06:00:57.733: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
    Nov 15 06:00:57.733: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
    Nov 15 06:00:57.748: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
    Nov 15 06:00:57.748: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
    Nov 15 06:00:57.780: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
    Nov 15 06:00:57.780: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
    Nov 15 06:00:57.829: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
    Nov 15 06:00:57.829: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
    Nov 15 06:00:57.842: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
    Nov 15 06:00:57.842: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
    Nov 15 06:00:59.883: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
    Nov 15 06:00:59.883: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
    Nov 15 06:00:59.943: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
    STEP: listing Deployments 11/15/23 06:00:59.943
    Nov 15 06:00:59.964: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 11/15/23 06:00:59.964
    Nov 15 06:00:59.983: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 11/15/23 06:00:59.983
    Nov 15 06:01:00.010: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 06:01:00.011: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 06:01:00.092: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 06:01:00.092: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 06:01:00.092: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 06:01:00.112: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 06:01:03.445: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 06:01:03.485: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 06:01:03.554: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 06:01:03.581: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 06:01:05.992: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 11/15/23 06:01:06.065
    STEP: fetching the DeploymentStatus 11/15/23 06:01:06.099
    Nov 15 06:01:06.123: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
    Nov 15 06:01:06.123: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
    Nov 15 06:01:06.123: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
    Nov 15 06:01:06.123: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
    Nov 15 06:01:06.123: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
    Nov 15 06:01:06.123: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 1
    Nov 15 06:01:06.123: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
    Nov 15 06:01:06.123: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
    Nov 15 06:01:06.123: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
    Nov 15 06:01:06.124: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 2
    Nov 15 06:01:06.124: INFO: observed Deployment test-deployment in namespace deployment-4642 with ReadyReplicas 3
    STEP: deleting the Deployment 11/15/23 06:01:06.124
    Nov 15 06:01:06.170: INFO: observed event type MODIFIED
    Nov 15 06:01:06.170: INFO: observed event type MODIFIED
    Nov 15 06:01:06.170: INFO: observed event type MODIFIED
    Nov 15 06:01:06.170: INFO: observed event type MODIFIED
    Nov 15 06:01:06.170: INFO: observed event type MODIFIED
    Nov 15 06:01:06.170: INFO: observed event type MODIFIED
    Nov 15 06:01:06.170: INFO: observed event type MODIFIED
    Nov 15 06:01:06.170: INFO: observed event type MODIFIED
    Nov 15 06:01:06.171: INFO: observed event type MODIFIED
    Nov 15 06:01:06.171: INFO: observed event type MODIFIED
    Nov 15 06:01:06.171: INFO: observed event type MODIFIED
    Nov 15 06:01:06.171: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 15 06:01:06.183: INFO: Log out all the ReplicaSets if there is no deployment created
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:01:06.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4642" for this suite. 11/15/23 06:01:06.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:01:06.263
Nov 15 06:01:06.263: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 06:01:06.264
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:06.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:06.332
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
Nov 15 06:01:06.373: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-6450cb17-fba4-4076-ae5c-9c16907ab0e3 11/15/23 06:01:06.373
STEP: Creating configMap with name cm-test-opt-upd-fd81b791-cf4e-4263-b3d6-9200d91162f0 11/15/23 06:01:06.391
STEP: Creating the pod 11/15/23 06:01:06.411
Nov 15 06:01:06.447: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0f17ed74-b581-487b-b853-d01a007fb6de" in namespace "projected-4710" to be "running and ready"
Nov 15 06:01:06.470: INFO: Pod "pod-projected-configmaps-0f17ed74-b581-487b-b853-d01a007fb6de": Phase="Pending", Reason="", readiness=false. Elapsed: 23.242416ms
Nov 15 06:01:06.471: INFO: The phase of Pod pod-projected-configmaps-0f17ed74-b581-487b-b853-d01a007fb6de is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:01:08.491: INFO: Pod "pod-projected-configmaps-0f17ed74-b581-487b-b853-d01a007fb6de": Phase="Running", Reason="", readiness=true. Elapsed: 2.044334785s
Nov 15 06:01:08.492: INFO: The phase of Pod pod-projected-configmaps-0f17ed74-b581-487b-b853-d01a007fb6de is Running (Ready = true)
Nov 15 06:01:08.492: INFO: Pod "pod-projected-configmaps-0f17ed74-b581-487b-b853-d01a007fb6de" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-6450cb17-fba4-4076-ae5c-9c16907ab0e3 11/15/23 06:01:08.695
STEP: Updating configmap cm-test-opt-upd-fd81b791-cf4e-4263-b3d6-9200d91162f0 11/15/23 06:01:08.722
STEP: Creating configMap with name cm-test-opt-create-65a369da-192f-4fe8-a40b-982b59208d94 11/15/23 06:01:08.742
STEP: waiting to observe update in volume 11/15/23 06:01:08.763
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:01:10.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4710" for this suite. 11/15/23 06:01:10.977
------------------------------
â€¢ [4.745 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:01:06.263
    Nov 15 06:01:06.263: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 06:01:06.264
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:06.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:06.332
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    Nov 15 06:01:06.373: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name cm-test-opt-del-6450cb17-fba4-4076-ae5c-9c16907ab0e3 11/15/23 06:01:06.373
    STEP: Creating configMap with name cm-test-opt-upd-fd81b791-cf4e-4263-b3d6-9200d91162f0 11/15/23 06:01:06.391
    STEP: Creating the pod 11/15/23 06:01:06.411
    Nov 15 06:01:06.447: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0f17ed74-b581-487b-b853-d01a007fb6de" in namespace "projected-4710" to be "running and ready"
    Nov 15 06:01:06.470: INFO: Pod "pod-projected-configmaps-0f17ed74-b581-487b-b853-d01a007fb6de": Phase="Pending", Reason="", readiness=false. Elapsed: 23.242416ms
    Nov 15 06:01:06.471: INFO: The phase of Pod pod-projected-configmaps-0f17ed74-b581-487b-b853-d01a007fb6de is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:01:08.491: INFO: Pod "pod-projected-configmaps-0f17ed74-b581-487b-b853-d01a007fb6de": Phase="Running", Reason="", readiness=true. Elapsed: 2.044334785s
    Nov 15 06:01:08.492: INFO: The phase of Pod pod-projected-configmaps-0f17ed74-b581-487b-b853-d01a007fb6de is Running (Ready = true)
    Nov 15 06:01:08.492: INFO: Pod "pod-projected-configmaps-0f17ed74-b581-487b-b853-d01a007fb6de" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-6450cb17-fba4-4076-ae5c-9c16907ab0e3 11/15/23 06:01:08.695
    STEP: Updating configmap cm-test-opt-upd-fd81b791-cf4e-4263-b3d6-9200d91162f0 11/15/23 06:01:08.722
    STEP: Creating configMap with name cm-test-opt-create-65a369da-192f-4fe8-a40b-982b59208d94 11/15/23 06:01:08.742
    STEP: waiting to observe update in volume 11/15/23 06:01:08.763
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:01:10.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4710" for this suite. 11/15/23 06:01:10.977
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:01:11.009
Nov 15 06:01:11.010: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename runtimeclass 11/15/23 06:01:11.011
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:11.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:11.178
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-1485-delete-me 11/15/23 06:01:11.231
STEP: Waiting for the RuntimeClass to disappear 11/15/23 06:01:11.271
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Nov 15 06:01:11.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1485" for this suite. 11/15/23 06:01:11.355
------------------------------
â€¢ [0.377 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:01:11.009
    Nov 15 06:01:11.010: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename runtimeclass 11/15/23 06:01:11.011
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:11.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:11.178
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-1485-delete-me 11/15/23 06:01:11.231
    STEP: Waiting for the RuntimeClass to disappear 11/15/23 06:01:11.271
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:01:11.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1485" for this suite. 11/15/23 06:01:11.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:01:11.388
Nov 15 06:01:11.388: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename configmap 11/15/23 06:01:11.39
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:11.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:11.475
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:01:11.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9189" for this suite. 11/15/23 06:01:11.748
------------------------------
â€¢ [0.406 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:01:11.388
    Nov 15 06:01:11.388: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename configmap 11/15/23 06:01:11.39
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:11.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:11.475
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:01:11.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9189" for this suite. 11/15/23 06:01:11.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:01:11.797
Nov 15 06:01:11.797: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename replicaset 11/15/23 06:01:11.798
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:11.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:11.874
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 11/15/23 06:01:11.884
W1115 06:01:11.904189      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Verify that the required pods have come up 11/15/23 06:01:11.904
Nov 15 06:01:11.957: INFO: Pod name sample-pod: Found 0 pods out of 3
Nov 15 06:01:16.979: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 11/15/23 06:01:16.979
Nov 15 06:01:16.994: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 11/15/23 06:01:16.994
STEP: DeleteCollection of the ReplicaSets 11/15/23 06:01:17.055
STEP: After DeleteCollection verify that ReplicaSets have been deleted 11/15/23 06:01:17.092
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Nov 15 06:01:17.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3466" for this suite. 11/15/23 06:01:17.184
------------------------------
â€¢ [SLOW TEST] [5.414 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:01:11.797
    Nov 15 06:01:11.797: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename replicaset 11/15/23 06:01:11.798
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:11.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:11.874
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 11/15/23 06:01:11.884
    W1115 06:01:11.904189      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Verify that the required pods have come up 11/15/23 06:01:11.904
    Nov 15 06:01:11.957: INFO: Pod name sample-pod: Found 0 pods out of 3
    Nov 15 06:01:16.979: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 11/15/23 06:01:16.979
    Nov 15 06:01:16.994: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 11/15/23 06:01:16.994
    STEP: DeleteCollection of the ReplicaSets 11/15/23 06:01:17.055
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 11/15/23 06:01:17.092
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:01:17.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3466" for this suite. 11/15/23 06:01:17.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:01:17.217
Nov 15 06:01:17.217: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename container-lifecycle-hook 11/15/23 06:01:17.219
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:17.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:17.293
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 11/15/23 06:01:17.37
Nov 15 06:01:17.403: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6207" to be "running and ready"
Nov 15 06:01:17.435: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 32.568538ms
Nov 15 06:01:17.435: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:01:19.456: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053303288s
Nov 15 06:01:19.456: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:01:21.460: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.0574263s
Nov 15 06:01:21.460: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Nov 15 06:01:21.460: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 11/15/23 06:01:21.479
Nov 15 06:01:21.509: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6207" to be "running and ready"
Nov 15 06:01:21.533: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 24.285896ms
Nov 15 06:01:21.533: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:01:23.553: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044330582s
Nov 15 06:01:23.553: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:01:25.556: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.046895627s
Nov 15 06:01:25.558: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Nov 15 06:01:25.558: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 11/15/23 06:01:25.575
Nov 15 06:01:25.605: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 15 06:01:25.623: INFO: Pod pod-with-prestop-http-hook still exists
Nov 15 06:01:27.624: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 15 06:01:27.650: INFO: Pod pod-with-prestop-http-hook still exists
Nov 15 06:01:29.624: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 15 06:01:29.661: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 11/15/23 06:01:29.661
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Nov 15 06:01:29.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6207" for this suite. 11/15/23 06:01:29.873
------------------------------
â€¢ [SLOW TEST] [12.693 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:01:17.217
    Nov 15 06:01:17.217: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename container-lifecycle-hook 11/15/23 06:01:17.219
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:17.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:17.293
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 11/15/23 06:01:17.37
    Nov 15 06:01:17.403: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6207" to be "running and ready"
    Nov 15 06:01:17.435: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 32.568538ms
    Nov 15 06:01:17.435: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:01:19.456: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053303288s
    Nov 15 06:01:19.456: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:01:21.460: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.0574263s
    Nov 15 06:01:21.460: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Nov 15 06:01:21.460: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 11/15/23 06:01:21.479
    Nov 15 06:01:21.509: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6207" to be "running and ready"
    Nov 15 06:01:21.533: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 24.285896ms
    Nov 15 06:01:21.533: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:01:23.553: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044330582s
    Nov 15 06:01:23.553: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:01:25.556: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.046895627s
    Nov 15 06:01:25.558: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Nov 15 06:01:25.558: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 11/15/23 06:01:25.575
    Nov 15 06:01:25.605: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Nov 15 06:01:25.623: INFO: Pod pod-with-prestop-http-hook still exists
    Nov 15 06:01:27.624: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Nov 15 06:01:27.650: INFO: Pod pod-with-prestop-http-hook still exists
    Nov 15 06:01:29.624: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Nov 15 06:01:29.661: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 11/15/23 06:01:29.661
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:01:29.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6207" for this suite. 11/15/23 06:01:29.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:01:29.913
Nov 15 06:01:29.913: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 06:01:29.915
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:29.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:30.06
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
Nov 15 06:01:30.134: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-0540af38-a3fd-4327-b492-09e41e7ed994 11/15/23 06:01:30.134
STEP: Creating the pod 11/15/23 06:01:30.154
Nov 15 06:01:30.190: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-102876e2-26b2-4ff8-8023-5074d5ba751f" in namespace "projected-1053" to be "running and ready"
Nov 15 06:01:30.241: INFO: Pod "pod-projected-configmaps-102876e2-26b2-4ff8-8023-5074d5ba751f": Phase="Pending", Reason="", readiness=false. Elapsed: 51.314151ms
Nov 15 06:01:30.241: INFO: The phase of Pod pod-projected-configmaps-102876e2-26b2-4ff8-8023-5074d5ba751f is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:01:32.260: INFO: Pod "pod-projected-configmaps-102876e2-26b2-4ff8-8023-5074d5ba751f": Phase="Running", Reason="", readiness=true. Elapsed: 2.069780475s
Nov 15 06:01:32.260: INFO: The phase of Pod pod-projected-configmaps-102876e2-26b2-4ff8-8023-5074d5ba751f is Running (Ready = true)
Nov 15 06:01:32.260: INFO: Pod "pod-projected-configmaps-102876e2-26b2-4ff8-8023-5074d5ba751f" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-0540af38-a3fd-4327-b492-09e41e7ed994 11/15/23 06:01:32.342
STEP: waiting to observe update in volume 11/15/23 06:01:32.365
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:01:34.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1053" for this suite. 11/15/23 06:01:34.549
------------------------------
â€¢ [4.662 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:01:29.913
    Nov 15 06:01:29.913: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 06:01:29.915
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:29.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:30.06
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    Nov 15 06:01:30.134: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-0540af38-a3fd-4327-b492-09e41e7ed994 11/15/23 06:01:30.134
    STEP: Creating the pod 11/15/23 06:01:30.154
    Nov 15 06:01:30.190: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-102876e2-26b2-4ff8-8023-5074d5ba751f" in namespace "projected-1053" to be "running and ready"
    Nov 15 06:01:30.241: INFO: Pod "pod-projected-configmaps-102876e2-26b2-4ff8-8023-5074d5ba751f": Phase="Pending", Reason="", readiness=false. Elapsed: 51.314151ms
    Nov 15 06:01:30.241: INFO: The phase of Pod pod-projected-configmaps-102876e2-26b2-4ff8-8023-5074d5ba751f is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:01:32.260: INFO: Pod "pod-projected-configmaps-102876e2-26b2-4ff8-8023-5074d5ba751f": Phase="Running", Reason="", readiness=true. Elapsed: 2.069780475s
    Nov 15 06:01:32.260: INFO: The phase of Pod pod-projected-configmaps-102876e2-26b2-4ff8-8023-5074d5ba751f is Running (Ready = true)
    Nov 15 06:01:32.260: INFO: Pod "pod-projected-configmaps-102876e2-26b2-4ff8-8023-5074d5ba751f" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-0540af38-a3fd-4327-b492-09e41e7ed994 11/15/23 06:01:32.342
    STEP: waiting to observe update in volume 11/15/23 06:01:32.365
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:01:34.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1053" for this suite. 11/15/23 06:01:34.549
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:01:34.577
Nov 15 06:01:34.577: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename webhook 11/15/23 06:01:34.578
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:34.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:34.636
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 06:01:34.751
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:01:35.462
STEP: Deploying the webhook pod 11/15/23 06:01:35.513
STEP: Wait for the deployment to be ready 11/15/23 06:01:35.552
Nov 15 06:01:35.586: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 06:01:37.622: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 6, 1, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 1, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 1, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 1, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 06:01:39.664
STEP: Verifying the service has paired with the endpoint 11/15/23 06:01:39.714
Nov 15 06:01:40.715: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 11/15/23 06:01:40.734
STEP: create a configmap that should be updated by the webhook 11/15/23 06:01:40.79
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:01:40.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4364" for this suite. 11/15/23 06:01:41.07
STEP: Destroying namespace "webhook-4364-markers" for this suite. 11/15/23 06:01:41.094
------------------------------
â€¢ [SLOW TEST] [6.543 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:01:34.577
    Nov 15 06:01:34.577: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename webhook 11/15/23 06:01:34.578
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:34.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:34.636
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 06:01:34.751
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:01:35.462
    STEP: Deploying the webhook pod 11/15/23 06:01:35.513
    STEP: Wait for the deployment to be ready 11/15/23 06:01:35.552
    Nov 15 06:01:35.586: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 06:01:37.622: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 6, 1, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 1, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 1, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 1, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 06:01:39.664
    STEP: Verifying the service has paired with the endpoint 11/15/23 06:01:39.714
    Nov 15 06:01:40.715: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 11/15/23 06:01:40.734
    STEP: create a configmap that should be updated by the webhook 11/15/23 06:01:40.79
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:01:40.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4364" for this suite. 11/15/23 06:01:41.07
    STEP: Destroying namespace "webhook-4364-markers" for this suite. 11/15/23 06:01:41.094
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:01:41.121
Nov 15 06:01:41.121: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename emptydir 11/15/23 06:01:41.122
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:41.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:41.187
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 11/15/23 06:01:41.2
Nov 15 06:01:41.240: INFO: Waiting up to 5m0s for pod "pod-7b9d6481-553c-4d6e-9c9e-edbbfc13c4c5" in namespace "emptydir-8131" to be "Succeeded or Failed"
Nov 15 06:01:41.261: INFO: Pod "pod-7b9d6481-553c-4d6e-9c9e-edbbfc13c4c5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.316226ms
Nov 15 06:01:43.280: INFO: Pod "pod-7b9d6481-553c-4d6e-9c9e-edbbfc13c4c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.039661335s
Nov 15 06:01:45.277: INFO: Pod "pod-7b9d6481-553c-4d6e-9c9e-edbbfc13c4c5": Phase="Running", Reason="", readiness=false. Elapsed: 4.036890984s
Nov 15 06:01:47.287: INFO: Pod "pod-7b9d6481-553c-4d6e-9c9e-edbbfc13c4c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046969438s
STEP: Saw pod success 11/15/23 06:01:47.287
Nov 15 06:01:47.288: INFO: Pod "pod-7b9d6481-553c-4d6e-9c9e-edbbfc13c4c5" satisfied condition "Succeeded or Failed"
Nov 15 06:01:47.305: INFO: Trying to get logs from node 10.72.152.86 pod pod-7b9d6481-553c-4d6e-9c9e-edbbfc13c4c5 container test-container: <nil>
STEP: delete the pod 11/15/23 06:01:47.354
Nov 15 06:01:47.408: INFO: Waiting for pod pod-7b9d6481-553c-4d6e-9c9e-edbbfc13c4c5 to disappear
Nov 15 06:01:47.428: INFO: Pod pod-7b9d6481-553c-4d6e-9c9e-edbbfc13c4c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 06:01:47.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8131" for this suite. 11/15/23 06:01:47.456
------------------------------
â€¢ [SLOW TEST] [6.362 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:01:41.121
    Nov 15 06:01:41.121: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename emptydir 11/15/23 06:01:41.122
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:41.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:41.187
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 11/15/23 06:01:41.2
    Nov 15 06:01:41.240: INFO: Waiting up to 5m0s for pod "pod-7b9d6481-553c-4d6e-9c9e-edbbfc13c4c5" in namespace "emptydir-8131" to be "Succeeded or Failed"
    Nov 15 06:01:41.261: INFO: Pod "pod-7b9d6481-553c-4d6e-9c9e-edbbfc13c4c5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.316226ms
    Nov 15 06:01:43.280: INFO: Pod "pod-7b9d6481-553c-4d6e-9c9e-edbbfc13c4c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.039661335s
    Nov 15 06:01:45.277: INFO: Pod "pod-7b9d6481-553c-4d6e-9c9e-edbbfc13c4c5": Phase="Running", Reason="", readiness=false. Elapsed: 4.036890984s
    Nov 15 06:01:47.287: INFO: Pod "pod-7b9d6481-553c-4d6e-9c9e-edbbfc13c4c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046969438s
    STEP: Saw pod success 11/15/23 06:01:47.287
    Nov 15 06:01:47.288: INFO: Pod "pod-7b9d6481-553c-4d6e-9c9e-edbbfc13c4c5" satisfied condition "Succeeded or Failed"
    Nov 15 06:01:47.305: INFO: Trying to get logs from node 10.72.152.86 pod pod-7b9d6481-553c-4d6e-9c9e-edbbfc13c4c5 container test-container: <nil>
    STEP: delete the pod 11/15/23 06:01:47.354
    Nov 15 06:01:47.408: INFO: Waiting for pod pod-7b9d6481-553c-4d6e-9c9e-edbbfc13c4c5 to disappear
    Nov 15 06:01:47.428: INFO: Pod pod-7b9d6481-553c-4d6e-9c9e-edbbfc13c4c5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:01:47.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8131" for this suite. 11/15/23 06:01:47.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:01:47.483
Nov 15 06:01:47.483: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename downward-api 11/15/23 06:01:47.484
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:47.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:47.559
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 11/15/23 06:01:47.569
W1115 06:01:47.606691      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Nov 15 06:01:47.606: INFO: Waiting up to 5m0s for pod "downwardapi-volume-177baee1-1cd7-4a97-a974-f67fea27607d" in namespace "downward-api-7263" to be "Succeeded or Failed"
Nov 15 06:01:47.632: INFO: Pod "downwardapi-volume-177baee1-1cd7-4a97-a974-f67fea27607d": Phase="Pending", Reason="", readiness=false. Elapsed: 25.979821ms
Nov 15 06:01:49.650: INFO: Pod "downwardapi-volume-177baee1-1cd7-4a97-a974-f67fea27607d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043731993s
Nov 15 06:01:51.653: INFO: Pod "downwardapi-volume-177baee1-1cd7-4a97-a974-f67fea27607d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046598514s
Nov 15 06:01:53.660: INFO: Pod "downwardapi-volume-177baee1-1cd7-4a97-a974-f67fea27607d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053112133s
STEP: Saw pod success 11/15/23 06:01:53.66
Nov 15 06:01:53.660: INFO: Pod "downwardapi-volume-177baee1-1cd7-4a97-a974-f67fea27607d" satisfied condition "Succeeded or Failed"
Nov 15 06:01:53.694: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-177baee1-1cd7-4a97-a974-f67fea27607d container client-container: <nil>
STEP: delete the pod 11/15/23 06:01:53.756
Nov 15 06:01:53.819: INFO: Waiting for pod downwardapi-volume-177baee1-1cd7-4a97-a974-f67fea27607d to disappear
Nov 15 06:01:53.841: INFO: Pod downwardapi-volume-177baee1-1cd7-4a97-a974-f67fea27607d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 06:01:53.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7263" for this suite. 11/15/23 06:01:53.868
------------------------------
â€¢ [SLOW TEST] [6.414 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:01:47.483
    Nov 15 06:01:47.483: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename downward-api 11/15/23 06:01:47.484
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:47.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:47.559
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 11/15/23 06:01:47.569
    W1115 06:01:47.606691      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Nov 15 06:01:47.606: INFO: Waiting up to 5m0s for pod "downwardapi-volume-177baee1-1cd7-4a97-a974-f67fea27607d" in namespace "downward-api-7263" to be "Succeeded or Failed"
    Nov 15 06:01:47.632: INFO: Pod "downwardapi-volume-177baee1-1cd7-4a97-a974-f67fea27607d": Phase="Pending", Reason="", readiness=false. Elapsed: 25.979821ms
    Nov 15 06:01:49.650: INFO: Pod "downwardapi-volume-177baee1-1cd7-4a97-a974-f67fea27607d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043731993s
    Nov 15 06:01:51.653: INFO: Pod "downwardapi-volume-177baee1-1cd7-4a97-a974-f67fea27607d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046598514s
    Nov 15 06:01:53.660: INFO: Pod "downwardapi-volume-177baee1-1cd7-4a97-a974-f67fea27607d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053112133s
    STEP: Saw pod success 11/15/23 06:01:53.66
    Nov 15 06:01:53.660: INFO: Pod "downwardapi-volume-177baee1-1cd7-4a97-a974-f67fea27607d" satisfied condition "Succeeded or Failed"
    Nov 15 06:01:53.694: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-177baee1-1cd7-4a97-a974-f67fea27607d container client-container: <nil>
    STEP: delete the pod 11/15/23 06:01:53.756
    Nov 15 06:01:53.819: INFO: Waiting for pod downwardapi-volume-177baee1-1cd7-4a97-a974-f67fea27607d to disappear
    Nov 15 06:01:53.841: INFO: Pod downwardapi-volume-177baee1-1cd7-4a97-a974-f67fea27607d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:01:53.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7263" for this suite. 11/15/23 06:01:53.868
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:01:53.899
Nov 15 06:01:53.899: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename job 11/15/23 06:01:53.9
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:53.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:53.974
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 11/15/23 06:01:53.986
W1115 06:01:54.009261      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism 11/15/23 06:01:54.009
STEP: delete a job 11/15/23 06:01:58.032
STEP: deleting Job.batch foo in namespace job-6939, will wait for the garbage collector to delete the pods 11/15/23 06:01:58.033
Nov 15 06:01:58.127: INFO: Deleting Job.batch foo took: 26.184254ms
Nov 15 06:01:58.228: INFO: Terminating Job.batch foo pods took: 100.524493ms
STEP: Ensuring job was deleted 11/15/23 06:02:30.729
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Nov 15 06:02:30.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6939" for this suite. 11/15/23 06:02:30.791
------------------------------
â€¢ [SLOW TEST] [36.919 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:01:53.899
    Nov 15 06:01:53.899: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename job 11/15/23 06:01:53.9
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:01:53.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:01:53.974
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 11/15/23 06:01:53.986
    W1115 06:01:54.009261      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring active pods == parallelism 11/15/23 06:01:54.009
    STEP: delete a job 11/15/23 06:01:58.032
    STEP: deleting Job.batch foo in namespace job-6939, will wait for the garbage collector to delete the pods 11/15/23 06:01:58.033
    Nov 15 06:01:58.127: INFO: Deleting Job.batch foo took: 26.184254ms
    Nov 15 06:01:58.228: INFO: Terminating Job.batch foo pods took: 100.524493ms
    STEP: Ensuring job was deleted 11/15/23 06:02:30.729
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:02:30.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6939" for this suite. 11/15/23 06:02:30.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:02:30.819
Nov 15 06:02:30.819: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename container-probe 11/15/23 06:02:30.821
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:02:30.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:02:30.898
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 15 06:03:30.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2850" for this suite. 11/15/23 06:03:30.997
------------------------------
â€¢ [SLOW TEST] [60.212 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:02:30.819
    Nov 15 06:02:30.819: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename container-probe 11/15/23 06:02:30.821
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:02:30.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:02:30.898
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:03:30.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2850" for this suite. 11/15/23 06:03:30.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:03:31.032
Nov 15 06:03:31.032: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename webhook 11/15/23 06:03:31.033
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:03:31.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:03:31.097
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 06:03:31.246
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:03:31.942
STEP: Deploying the webhook pod 11/15/23 06:03:32.007
STEP: Wait for the deployment to be ready 11/15/23 06:03:32.063
Nov 15 06:03:32.095: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 06:03:34.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 6, 3, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 3, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 3, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 3, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 06:03:36.156
STEP: Verifying the service has paired with the endpoint 11/15/23 06:03:36.193
Nov 15 06:03:37.194: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 11/15/23 06:03:37.217
STEP: create a pod that should be updated by the webhook 11/15/23 06:03:37.277
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:03:37.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2403" for this suite. 11/15/23 06:03:37.687
STEP: Destroying namespace "webhook-2403-markers" for this suite. 11/15/23 06:03:37.711
------------------------------
â€¢ [SLOW TEST] [6.712 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:03:31.032
    Nov 15 06:03:31.032: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename webhook 11/15/23 06:03:31.033
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:03:31.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:03:31.097
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 06:03:31.246
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:03:31.942
    STEP: Deploying the webhook pod 11/15/23 06:03:32.007
    STEP: Wait for the deployment to be ready 11/15/23 06:03:32.063
    Nov 15 06:03:32.095: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 06:03:34.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 6, 3, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 3, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 3, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 3, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 06:03:36.156
    STEP: Verifying the service has paired with the endpoint 11/15/23 06:03:36.193
    Nov 15 06:03:37.194: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 11/15/23 06:03:37.217
    STEP: create a pod that should be updated by the webhook 11/15/23 06:03:37.277
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:03:37.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2403" for this suite. 11/15/23 06:03:37.687
    STEP: Destroying namespace "webhook-2403-markers" for this suite. 11/15/23 06:03:37.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:03:37.748
Nov 15 06:03:37.748: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename var-expansion 11/15/23 06:03:37.748
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:03:37.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:03:37.818
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 11/15/23 06:03:37.83
Nov 15 06:03:37.864: INFO: Waiting up to 5m0s for pod "var-expansion-95047b9e-7c2c-44a2-8ae3-c26d83d1f82b" in namespace "var-expansion-8931" to be "Succeeded or Failed"
Nov 15 06:03:37.886: INFO: Pod "var-expansion-95047b9e-7c2c-44a2-8ae3-c26d83d1f82b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.897598ms
Nov 15 06:03:39.907: INFO: Pod "var-expansion-95047b9e-7c2c-44a2-8ae3-c26d83d1f82b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043254232s
Nov 15 06:03:41.911: INFO: Pod "var-expansion-95047b9e-7c2c-44a2-8ae3-c26d83d1f82b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047227633s
Nov 15 06:03:43.906: INFO: Pod "var-expansion-95047b9e-7c2c-44a2-8ae3-c26d83d1f82b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04214199s
STEP: Saw pod success 11/15/23 06:03:43.906
Nov 15 06:03:43.907: INFO: Pod "var-expansion-95047b9e-7c2c-44a2-8ae3-c26d83d1f82b" satisfied condition "Succeeded or Failed"
Nov 15 06:03:43.931: INFO: Trying to get logs from node 10.72.152.81 pod var-expansion-95047b9e-7c2c-44a2-8ae3-c26d83d1f82b container dapi-container: <nil>
STEP: delete the pod 11/15/23 06:03:44.029
Nov 15 06:03:44.093: INFO: Waiting for pod var-expansion-95047b9e-7c2c-44a2-8ae3-c26d83d1f82b to disappear
Nov 15 06:03:44.111: INFO: Pod var-expansion-95047b9e-7c2c-44a2-8ae3-c26d83d1f82b no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 15 06:03:44.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8931" for this suite. 11/15/23 06:03:44.158
------------------------------
â€¢ [SLOW TEST] [6.449 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:03:37.748
    Nov 15 06:03:37.748: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename var-expansion 11/15/23 06:03:37.748
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:03:37.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:03:37.818
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 11/15/23 06:03:37.83
    Nov 15 06:03:37.864: INFO: Waiting up to 5m0s for pod "var-expansion-95047b9e-7c2c-44a2-8ae3-c26d83d1f82b" in namespace "var-expansion-8931" to be "Succeeded or Failed"
    Nov 15 06:03:37.886: INFO: Pod "var-expansion-95047b9e-7c2c-44a2-8ae3-c26d83d1f82b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.897598ms
    Nov 15 06:03:39.907: INFO: Pod "var-expansion-95047b9e-7c2c-44a2-8ae3-c26d83d1f82b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043254232s
    Nov 15 06:03:41.911: INFO: Pod "var-expansion-95047b9e-7c2c-44a2-8ae3-c26d83d1f82b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047227633s
    Nov 15 06:03:43.906: INFO: Pod "var-expansion-95047b9e-7c2c-44a2-8ae3-c26d83d1f82b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04214199s
    STEP: Saw pod success 11/15/23 06:03:43.906
    Nov 15 06:03:43.907: INFO: Pod "var-expansion-95047b9e-7c2c-44a2-8ae3-c26d83d1f82b" satisfied condition "Succeeded or Failed"
    Nov 15 06:03:43.931: INFO: Trying to get logs from node 10.72.152.81 pod var-expansion-95047b9e-7c2c-44a2-8ae3-c26d83d1f82b container dapi-container: <nil>
    STEP: delete the pod 11/15/23 06:03:44.029
    Nov 15 06:03:44.093: INFO: Waiting for pod var-expansion-95047b9e-7c2c-44a2-8ae3-c26d83d1f82b to disappear
    Nov 15 06:03:44.111: INFO: Pod var-expansion-95047b9e-7c2c-44a2-8ae3-c26d83d1f82b no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:03:44.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8931" for this suite. 11/15/23 06:03:44.158
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:03:44.197
Nov 15 06:03:44.197: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 06:03:44.198
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:03:44.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:03:44.264
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Nov 15 06:03:44.279: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/15/23 06:03:49.505
Nov 15 06:03:49.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-2195 --namespace=crd-publish-openapi-2195 create -f -'
Nov 15 06:03:51.228: INFO: stderr: ""
Nov 15 06:03:51.228: INFO: stdout: "e2e-test-crd-publish-openapi-9083-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 15 06:03:51.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-2195 --namespace=crd-publish-openapi-2195 delete e2e-test-crd-publish-openapi-9083-crds test-cr'
Nov 15 06:03:51.366: INFO: stderr: ""
Nov 15 06:03:51.366: INFO: stdout: "e2e-test-crd-publish-openapi-9083-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Nov 15 06:03:51.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-2195 --namespace=crd-publish-openapi-2195 apply -f -'
Nov 15 06:03:53.157: INFO: stderr: ""
Nov 15 06:03:53.157: INFO: stdout: "e2e-test-crd-publish-openapi-9083-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 15 06:03:53.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-2195 --namespace=crd-publish-openapi-2195 delete e2e-test-crd-publish-openapi-9083-crds test-cr'
Nov 15 06:03:53.276: INFO: stderr: ""
Nov 15 06:03:53.276: INFO: stdout: "e2e-test-crd-publish-openapi-9083-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 11/15/23 06:03:53.276
Nov 15 06:03:53.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-2195 explain e2e-test-crd-publish-openapi-9083-crds'
Nov 15 06:03:53.730: INFO: stderr: ""
Nov 15 06:03:53.730: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9083-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:03:57.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2195" for this suite. 11/15/23 06:03:57.84
------------------------------
â€¢ [SLOW TEST] [13.669 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:03:44.197
    Nov 15 06:03:44.197: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 06:03:44.198
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:03:44.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:03:44.264
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Nov 15 06:03:44.279: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/15/23 06:03:49.505
    Nov 15 06:03:49.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-2195 --namespace=crd-publish-openapi-2195 create -f -'
    Nov 15 06:03:51.228: INFO: stderr: ""
    Nov 15 06:03:51.228: INFO: stdout: "e2e-test-crd-publish-openapi-9083-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Nov 15 06:03:51.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-2195 --namespace=crd-publish-openapi-2195 delete e2e-test-crd-publish-openapi-9083-crds test-cr'
    Nov 15 06:03:51.366: INFO: stderr: ""
    Nov 15 06:03:51.366: INFO: stdout: "e2e-test-crd-publish-openapi-9083-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Nov 15 06:03:51.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-2195 --namespace=crd-publish-openapi-2195 apply -f -'
    Nov 15 06:03:53.157: INFO: stderr: ""
    Nov 15 06:03:53.157: INFO: stdout: "e2e-test-crd-publish-openapi-9083-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Nov 15 06:03:53.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-2195 --namespace=crd-publish-openapi-2195 delete e2e-test-crd-publish-openapi-9083-crds test-cr'
    Nov 15 06:03:53.276: INFO: stderr: ""
    Nov 15 06:03:53.276: INFO: stdout: "e2e-test-crd-publish-openapi-9083-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 11/15/23 06:03:53.276
    Nov 15 06:03:53.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-2195 explain e2e-test-crd-publish-openapi-9083-crds'
    Nov 15 06:03:53.730: INFO: stderr: ""
    Nov 15 06:03:53.730: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9083-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:03:57.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2195" for this suite. 11/15/23 06:03:57.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:03:57.867
Nov 15 06:03:57.868: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename podtemplate 11/15/23 06:03:57.868
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:03:57.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:03:57.939
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 11/15/23 06:03:57.951
W1115 06:03:57.970193      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Replace a pod template 11/15/23 06:03:57.97
Nov 15 06:03:58.008: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Nov 15 06:03:58.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8822" for this suite. 11/15/23 06:03:58.031
------------------------------
â€¢ [0.185 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:03:57.867
    Nov 15 06:03:57.868: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename podtemplate 11/15/23 06:03:57.868
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:03:57.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:03:57.939
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 11/15/23 06:03:57.951
    W1115 06:03:57.970193      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Replace a pod template 11/15/23 06:03:57.97
    Nov 15 06:03:58.008: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:03:58.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8822" for this suite. 11/15/23 06:03:58.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:03:58.054
Nov 15 06:03:58.054: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 06:03:58.055
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:03:58.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:03:58.125
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-f87cba56-00ec-4d3c-a8a2-6b181f620468 11/15/23 06:03:58.139
STEP: Creating a pod to test consume configMaps 11/15/23 06:03:58.161
Nov 15 06:03:58.196: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a2cefee6-d73f-4c40-aa80-2adb225948c2" in namespace "projected-7052" to be "Succeeded or Failed"
Nov 15 06:03:58.218: INFO: Pod "pod-projected-configmaps-a2cefee6-d73f-4c40-aa80-2adb225948c2": Phase="Pending", Reason="", readiness=false. Elapsed: 21.159624ms
Nov 15 06:04:00.234: INFO: Pod "pod-projected-configmaps-a2cefee6-d73f-4c40-aa80-2adb225948c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037793303s
Nov 15 06:04:02.232: INFO: Pod "pod-projected-configmaps-a2cefee6-d73f-4c40-aa80-2adb225948c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035468546s
Nov 15 06:04:04.239: INFO: Pod "pod-projected-configmaps-a2cefee6-d73f-4c40-aa80-2adb225948c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042666897s
STEP: Saw pod success 11/15/23 06:04:04.239
Nov 15 06:04:04.239: INFO: Pod "pod-projected-configmaps-a2cefee6-d73f-4c40-aa80-2adb225948c2" satisfied condition "Succeeded or Failed"
Nov 15 06:04:04.262: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-configmaps-a2cefee6-d73f-4c40-aa80-2adb225948c2 container agnhost-container: <nil>
STEP: delete the pod 11/15/23 06:04:04.328
Nov 15 06:04:04.397: INFO: Waiting for pod pod-projected-configmaps-a2cefee6-d73f-4c40-aa80-2adb225948c2 to disappear
Nov 15 06:04:04.413: INFO: Pod pod-projected-configmaps-a2cefee6-d73f-4c40-aa80-2adb225948c2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:04:04.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7052" for this suite. 11/15/23 06:04:04.437
------------------------------
â€¢ [SLOW TEST] [6.406 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:03:58.054
    Nov 15 06:03:58.054: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 06:03:58.055
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:03:58.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:03:58.125
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-f87cba56-00ec-4d3c-a8a2-6b181f620468 11/15/23 06:03:58.139
    STEP: Creating a pod to test consume configMaps 11/15/23 06:03:58.161
    Nov 15 06:03:58.196: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a2cefee6-d73f-4c40-aa80-2adb225948c2" in namespace "projected-7052" to be "Succeeded or Failed"
    Nov 15 06:03:58.218: INFO: Pod "pod-projected-configmaps-a2cefee6-d73f-4c40-aa80-2adb225948c2": Phase="Pending", Reason="", readiness=false. Elapsed: 21.159624ms
    Nov 15 06:04:00.234: INFO: Pod "pod-projected-configmaps-a2cefee6-d73f-4c40-aa80-2adb225948c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037793303s
    Nov 15 06:04:02.232: INFO: Pod "pod-projected-configmaps-a2cefee6-d73f-4c40-aa80-2adb225948c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035468546s
    Nov 15 06:04:04.239: INFO: Pod "pod-projected-configmaps-a2cefee6-d73f-4c40-aa80-2adb225948c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042666897s
    STEP: Saw pod success 11/15/23 06:04:04.239
    Nov 15 06:04:04.239: INFO: Pod "pod-projected-configmaps-a2cefee6-d73f-4c40-aa80-2adb225948c2" satisfied condition "Succeeded or Failed"
    Nov 15 06:04:04.262: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-configmaps-a2cefee6-d73f-4c40-aa80-2adb225948c2 container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 06:04:04.328
    Nov 15 06:04:04.397: INFO: Waiting for pod pod-projected-configmaps-a2cefee6-d73f-4c40-aa80-2adb225948c2 to disappear
    Nov 15 06:04:04.413: INFO: Pod pod-projected-configmaps-a2cefee6-d73f-4c40-aa80-2adb225948c2 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:04:04.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7052" for this suite. 11/15/23 06:04:04.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:04:04.461
Nov 15 06:04:04.461: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 06:04:04.462
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:04:04.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:04:04.549
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Nov 15 06:04:04.558: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:04:05.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9882" for this suite. 11/15/23 06:04:05.679
------------------------------
â€¢ [1.245 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:04:04.461
    Nov 15 06:04:04.461: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 06:04:04.462
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:04:04.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:04:04.549
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Nov 15 06:04:04.558: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:04:05.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9882" for this suite. 11/15/23 06:04:05.679
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:04:05.707
Nov 15 06:04:05.707: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename deployment 11/15/23 06:04:05.708
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:04:05.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:04:05.821
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Nov 15 06:04:05.875: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Nov 15 06:04:10.895: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/15/23 06:04:10.895
Nov 15 06:04:10.895: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 11/15/23 06:04:10.941
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 15 06:04:10.984: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5163  062e532f-78fd-4ed4-8eb6-bcef1110d52a 72013 1 2023-11-15 06:04:10 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-11-15 06:04:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004637418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Nov 15 06:04:10.999: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-5163  00ae3a07-3519-49db-8cde-ad2ad3ef2eab 72015 1 2023-11-15 06:04:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 062e532f-78fd-4ed4-8eb6-bcef1110d52a 0xc004637897 0xc004637898}] [] [{kube-controller-manager Update apps/v1 2023-11-15 06:04:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"062e532f-78fd-4ed4-8eb6-bcef1110d52a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004637928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 15 06:04:10.999: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Nov 15 06:04:10.999: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5163  3afbdb3b-5335-4510-b680-7e6c7e6f0268 72014 1 2023-11-15 06:04:05 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 062e532f-78fd-4ed4-8eb6-bcef1110d52a 0xc004637767 0xc004637768}] [] [{e2e.test Update apps/v1 2023-11-15 06:04:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:04:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-11-15 06:04:10 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"062e532f-78fd-4ed4-8eb6-bcef1110d52a\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004637828 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 15 06:04:11.035: INFO: Pod "test-cleanup-controller-bfvhq" is available:
&Pod{ObjectMeta:{test-cleanup-controller-bfvhq test-cleanup-controller- deployment-5163  f08b5255-c8d6-4e65-964b-e758c6f39377 71968 0 2023-11-15 06:04:05 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:ca589778aa16b37879673676f4ac165d4a5ee5ca379736c8516b46f8117052e8 cni.projectcalico.org/podIP:172.30.213.181/32 cni.projectcalico.org/podIPs:172.30.213.181/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.213.181"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller 3afbdb3b-5335-4510-b680-7e6c7e6f0268 0xc004f6d787 0xc004f6d788}] [] [{kube-controller-manager Update v1 2023-11-15 06:04:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3afbdb3b-5335-4510-b680-7e6c7e6f0268\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:04:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ckfrw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ckfrw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c40,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:04:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:172.30.213.181,StartTime:2023-11-15 06:04:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:04:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://4eebe99c2d70212b01c332633b229a7dfce3cf585484df8a27011a653cd5a441,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.213.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 15 06:04:11.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5163" for this suite. 11/15/23 06:04:11.077
------------------------------
â€¢ [SLOW TEST] [5.393 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:04:05.707
    Nov 15 06:04:05.707: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename deployment 11/15/23 06:04:05.708
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:04:05.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:04:05.821
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Nov 15 06:04:05.875: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Nov 15 06:04:10.895: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/15/23 06:04:10.895
    Nov 15 06:04:10.895: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 11/15/23 06:04:10.941
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 15 06:04:10.984: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5163  062e532f-78fd-4ed4-8eb6-bcef1110d52a 72013 1 2023-11-15 06:04:10 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-11-15 06:04:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004637418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Nov 15 06:04:10.999: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-5163  00ae3a07-3519-49db-8cde-ad2ad3ef2eab 72015 1 2023-11-15 06:04:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 062e532f-78fd-4ed4-8eb6-bcef1110d52a 0xc004637897 0xc004637898}] [] [{kube-controller-manager Update apps/v1 2023-11-15 06:04:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"062e532f-78fd-4ed4-8eb6-bcef1110d52a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004637928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 06:04:10.999: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Nov 15 06:04:10.999: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5163  3afbdb3b-5335-4510-b680-7e6c7e6f0268 72014 1 2023-11-15 06:04:05 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 062e532f-78fd-4ed4-8eb6-bcef1110d52a 0xc004637767 0xc004637768}] [] [{e2e.test Update apps/v1 2023-11-15 06:04:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:04:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-11-15 06:04:10 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"062e532f-78fd-4ed4-8eb6-bcef1110d52a\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004637828 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 06:04:11.035: INFO: Pod "test-cleanup-controller-bfvhq" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-bfvhq test-cleanup-controller- deployment-5163  f08b5255-c8d6-4e65-964b-e758c6f39377 71968 0 2023-11-15 06:04:05 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:ca589778aa16b37879673676f4ac165d4a5ee5ca379736c8516b46f8117052e8 cni.projectcalico.org/podIP:172.30.213.181/32 cni.projectcalico.org/podIPs:172.30.213.181/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.213.181"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller 3afbdb3b-5335-4510-b680-7e6c7e6f0268 0xc004f6d787 0xc004f6d788}] [] [{kube-controller-manager Update v1 2023-11-15 06:04:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3afbdb3b-5335-4510-b680-7e6c7e6f0268\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 06:04:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 06:04:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ckfrw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ckfrw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c40,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:04:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:04:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:04:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:172.30.213.181,StartTime:2023-11-15 06:04:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:04:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://4eebe99c2d70212b01c332633b229a7dfce3cf585484df8a27011a653cd5a441,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.213.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:04:11.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5163" for this suite. 11/15/23 06:04:11.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:04:11.101
Nov 15 06:04:11.101: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename daemonsets 11/15/23 06:04:11.102
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:04:11.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:04:11.184
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
Nov 15 06:04:11.322: INFO: Create a RollingUpdate DaemonSet
Nov 15 06:04:11.342: INFO: Check that daemon pods launch on every node of the cluster
Nov 15 06:04:11.388: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 06:04:11.388: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 06:04:12.436: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 06:04:12.436: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 06:04:13.444: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 15 06:04:13.444: INFO: Node 10.72.152.86 is running 0 daemon pod, expected 1
Nov 15 06:04:14.429: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 15 06:04:14.429: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Nov 15 06:04:14.429: INFO: Update the DaemonSet to trigger a rollout
Nov 15 06:04:14.516: INFO: Updating DaemonSet daemon-set
Nov 15 06:04:16.593: INFO: Roll back the DaemonSet before rollout is complete
Nov 15 06:04:16.643: INFO: Updating DaemonSet daemon-set
Nov 15 06:04:16.643: INFO: Make sure DaemonSet rollback is complete
Nov 15 06:04:16.658: INFO: Wrong image for pod: daemon-set-f55fp. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Nov 15 06:04:16.658: INFO: Pod daemon-set-f55fp is not available
Nov 15 06:04:20.697: INFO: Pod daemon-set-f6hh7 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 11/15/23 06:04:20.752
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1114, will wait for the garbage collector to delete the pods 11/15/23 06:04:20.752
Nov 15 06:04:20.843: INFO: Deleting DaemonSet.extensions daemon-set took: 25.242913ms
Nov 15 06:04:21.043: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.264966ms
Nov 15 06:04:24.469: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 06:04:24.469: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 15 06:04:24.502: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"72302"},"items":null}

Nov 15 06:04:24.538: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"72302"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:04:24.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1114" for this suite. 11/15/23 06:04:24.765
------------------------------
â€¢ [SLOW TEST] [13.725 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:04:11.101
    Nov 15 06:04:11.101: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename daemonsets 11/15/23 06:04:11.102
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:04:11.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:04:11.184
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:443
    Nov 15 06:04:11.322: INFO: Create a RollingUpdate DaemonSet
    Nov 15 06:04:11.342: INFO: Check that daemon pods launch on every node of the cluster
    Nov 15 06:04:11.388: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 06:04:11.388: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 06:04:12.436: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 06:04:12.436: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 06:04:13.444: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 15 06:04:13.444: INFO: Node 10.72.152.86 is running 0 daemon pod, expected 1
    Nov 15 06:04:14.429: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 15 06:04:14.429: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Nov 15 06:04:14.429: INFO: Update the DaemonSet to trigger a rollout
    Nov 15 06:04:14.516: INFO: Updating DaemonSet daemon-set
    Nov 15 06:04:16.593: INFO: Roll back the DaemonSet before rollout is complete
    Nov 15 06:04:16.643: INFO: Updating DaemonSet daemon-set
    Nov 15 06:04:16.643: INFO: Make sure DaemonSet rollback is complete
    Nov 15 06:04:16.658: INFO: Wrong image for pod: daemon-set-f55fp. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Nov 15 06:04:16.658: INFO: Pod daemon-set-f55fp is not available
    Nov 15 06:04:20.697: INFO: Pod daemon-set-f6hh7 is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 11/15/23 06:04:20.752
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1114, will wait for the garbage collector to delete the pods 11/15/23 06:04:20.752
    Nov 15 06:04:20.843: INFO: Deleting DaemonSet.extensions daemon-set took: 25.242913ms
    Nov 15 06:04:21.043: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.264966ms
    Nov 15 06:04:24.469: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 06:04:24.469: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 15 06:04:24.502: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"72302"},"items":null}

    Nov 15 06:04:24.538: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"72302"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:04:24.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1114" for this suite. 11/15/23 06:04:24.765
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:04:24.827
Nov 15 06:04:24.827: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename webhook 11/15/23 06:04:24.828
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:04:24.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:04:24.905
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 06:04:24.97
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:04:25.372
STEP: Deploying the webhook pod 11/15/23 06:04:25.42
STEP: Wait for the deployment to be ready 11/15/23 06:04:25.462
Nov 15 06:04:25.492: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 06:04:27.536: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 6, 4, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 4, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 4, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 4, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 06:04:29.553
STEP: Verifying the service has paired with the endpoint 11/15/23 06:04:29.594
Nov 15 06:04:30.594: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 11/15/23 06:04:30.81
STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 06:04:30.898
STEP: Deleting the collection of validation webhooks 11/15/23 06:04:30.947
STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 06:04:31.134
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:04:31.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-795" for this suite. 11/15/23 06:04:31.338
STEP: Destroying namespace "webhook-795-markers" for this suite. 11/15/23 06:04:31.36
------------------------------
â€¢ [SLOW TEST] [6.555 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:04:24.827
    Nov 15 06:04:24.827: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename webhook 11/15/23 06:04:24.828
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:04:24.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:04:24.905
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 06:04:24.97
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:04:25.372
    STEP: Deploying the webhook pod 11/15/23 06:04:25.42
    STEP: Wait for the deployment to be ready 11/15/23 06:04:25.462
    Nov 15 06:04:25.492: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 06:04:27.536: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 6, 4, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 4, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 4, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 4, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 06:04:29.553
    STEP: Verifying the service has paired with the endpoint 11/15/23 06:04:29.594
    Nov 15 06:04:30.594: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 11/15/23 06:04:30.81
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 06:04:30.898
    STEP: Deleting the collection of validation webhooks 11/15/23 06:04:30.947
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 06:04:31.134
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:04:31.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-795" for this suite. 11/15/23 06:04:31.338
    STEP: Destroying namespace "webhook-795-markers" for this suite. 11/15/23 06:04:31.36
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:04:31.382
Nov 15 06:04:31.383: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename webhook 11/15/23 06:04:31.383
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:04:31.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:04:31.467
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 06:04:31.545
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:04:32.271
STEP: Deploying the webhook pod 11/15/23 06:04:32.302
STEP: Wait for the deployment to be ready 11/15/23 06:04:32.359
Nov 15 06:04:32.387: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/15/23 06:04:34.425
STEP: Verifying the service has paired with the endpoint 11/15/23 06:04:34.464
Nov 15 06:04:35.465: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 11/15/23 06:04:35.479
STEP: create a pod 11/15/23 06:04:35.634
Nov 15 06:04:35.667: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1132" to be "running"
Nov 15 06:04:35.681: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.083326ms
Nov 15 06:04:37.704: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036752431s
Nov 15 06:04:39.697: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.030152523s
Nov 15 06:04:39.697: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 11/15/23 06:04:39.697
Nov 15 06:04:39.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=webhook-1132 attach --namespace=webhook-1132 to-be-attached-pod -i -c=container1'
Nov 15 06:04:40.001: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:04:40.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1132" for this suite. 11/15/23 06:04:40.181
STEP: Destroying namespace "webhook-1132-markers" for this suite. 11/15/23 06:04:40.204
------------------------------
â€¢ [SLOW TEST] [8.853 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:04:31.382
    Nov 15 06:04:31.383: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename webhook 11/15/23 06:04:31.383
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:04:31.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:04:31.467
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 06:04:31.545
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:04:32.271
    STEP: Deploying the webhook pod 11/15/23 06:04:32.302
    STEP: Wait for the deployment to be ready 11/15/23 06:04:32.359
    Nov 15 06:04:32.387: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/15/23 06:04:34.425
    STEP: Verifying the service has paired with the endpoint 11/15/23 06:04:34.464
    Nov 15 06:04:35.465: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 11/15/23 06:04:35.479
    STEP: create a pod 11/15/23 06:04:35.634
    Nov 15 06:04:35.667: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1132" to be "running"
    Nov 15 06:04:35.681: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.083326ms
    Nov 15 06:04:37.704: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036752431s
    Nov 15 06:04:39.697: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.030152523s
    Nov 15 06:04:39.697: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 11/15/23 06:04:39.697
    Nov 15 06:04:39.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=webhook-1132 attach --namespace=webhook-1132 to-be-attached-pod -i -c=container1'
    Nov 15 06:04:40.001: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:04:40.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1132" for this suite. 11/15/23 06:04:40.181
    STEP: Destroying namespace "webhook-1132-markers" for this suite. 11/15/23 06:04:40.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:04:40.236
Nov 15 06:04:40.237: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 06:04:40.238
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:04:40.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:04:40.319
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-8065ad5a-98b8-434c-a6c5-2b474705e9ac 11/15/23 06:04:40.329
STEP: Creating secret with name secret-projected-all-test-volume-d1e35438-cd30-4308-a9f9-cc41e661b687 11/15/23 06:04:40.349
STEP: Creating a pod to test Check all projections for projected volume plugin 11/15/23 06:04:40.369
Nov 15 06:04:40.404: INFO: Waiting up to 5m0s for pod "projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3" in namespace "projected-9664" to be "Succeeded or Failed"
Nov 15 06:04:40.423: INFO: Pod "projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3": Phase="Pending", Reason="", readiness=false. Elapsed: 19.180935ms
Nov 15 06:04:42.444: INFO: Pod "projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040316468s
Nov 15 06:04:44.442: INFO: Pod "projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038338966s
Nov 15 06:04:46.447: INFO: Pod "projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042829695s
Nov 15 06:04:48.440: INFO: Pod "projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.035996637s
STEP: Saw pod success 11/15/23 06:04:48.441
Nov 15 06:04:48.441: INFO: Pod "projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3" satisfied condition "Succeeded or Failed"
Nov 15 06:04:48.454: INFO: Trying to get logs from node 10.72.152.81 pod projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3 container projected-all-volume-test: <nil>
STEP: delete the pod 11/15/23 06:04:48.528
Nov 15 06:04:48.577: INFO: Waiting for pod projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3 to disappear
Nov 15 06:04:48.597: INFO: Pod projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Nov 15 06:04:48.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9664" for this suite. 11/15/23 06:04:48.615
------------------------------
â€¢ [SLOW TEST] [8.401 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:04:40.236
    Nov 15 06:04:40.237: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 06:04:40.238
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:04:40.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:04:40.319
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-8065ad5a-98b8-434c-a6c5-2b474705e9ac 11/15/23 06:04:40.329
    STEP: Creating secret with name secret-projected-all-test-volume-d1e35438-cd30-4308-a9f9-cc41e661b687 11/15/23 06:04:40.349
    STEP: Creating a pod to test Check all projections for projected volume plugin 11/15/23 06:04:40.369
    Nov 15 06:04:40.404: INFO: Waiting up to 5m0s for pod "projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3" in namespace "projected-9664" to be "Succeeded or Failed"
    Nov 15 06:04:40.423: INFO: Pod "projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3": Phase="Pending", Reason="", readiness=false. Elapsed: 19.180935ms
    Nov 15 06:04:42.444: INFO: Pod "projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040316468s
    Nov 15 06:04:44.442: INFO: Pod "projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038338966s
    Nov 15 06:04:46.447: INFO: Pod "projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042829695s
    Nov 15 06:04:48.440: INFO: Pod "projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.035996637s
    STEP: Saw pod success 11/15/23 06:04:48.441
    Nov 15 06:04:48.441: INFO: Pod "projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3" satisfied condition "Succeeded or Failed"
    Nov 15 06:04:48.454: INFO: Trying to get logs from node 10.72.152.81 pod projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3 container projected-all-volume-test: <nil>
    STEP: delete the pod 11/15/23 06:04:48.528
    Nov 15 06:04:48.577: INFO: Waiting for pod projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3 to disappear
    Nov 15 06:04:48.597: INFO: Pod projected-volume-95b56509-87e5-4b6a-a0b3-6a05c0ddadd3 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:04:48.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9664" for this suite. 11/15/23 06:04:48.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:04:48.64
Nov 15 06:04:48.640: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 06:04:48.641
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:04:48.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:04:48.715
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-a9b00279-d471-4b09-82b8-5ee677209fdf 11/15/23 06:04:48.724
STEP: Creating a pod to test consume configMaps 11/15/23 06:04:48.746
Nov 15 06:04:48.777: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ff4e6eb8-d1b4-4605-8b66-e82f26e92b1d" in namespace "projected-9730" to be "Succeeded or Failed"
Nov 15 06:04:48.799: INFO: Pod "pod-projected-configmaps-ff4e6eb8-d1b4-4605-8b66-e82f26e92b1d": Phase="Pending", Reason="", readiness=false. Elapsed: 21.573296ms
Nov 15 06:04:50.813: INFO: Pod "pod-projected-configmaps-ff4e6eb8-d1b4-4605-8b66-e82f26e92b1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036330279s
Nov 15 06:04:52.833: INFO: Pod "pod-projected-configmaps-ff4e6eb8-d1b4-4605-8b66-e82f26e92b1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055511508s
STEP: Saw pod success 11/15/23 06:04:52.833
Nov 15 06:04:52.833: INFO: Pod "pod-projected-configmaps-ff4e6eb8-d1b4-4605-8b66-e82f26e92b1d" satisfied condition "Succeeded or Failed"
Nov 15 06:04:52.848: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-configmaps-ff4e6eb8-d1b4-4605-8b66-e82f26e92b1d container agnhost-container: <nil>
STEP: delete the pod 11/15/23 06:04:52.899
Nov 15 06:04:52.946: INFO: Waiting for pod pod-projected-configmaps-ff4e6eb8-d1b4-4605-8b66-e82f26e92b1d to disappear
Nov 15 06:04:52.962: INFO: Pod pod-projected-configmaps-ff4e6eb8-d1b4-4605-8b66-e82f26e92b1d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:04:52.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9730" for this suite. 11/15/23 06:04:52.987
------------------------------
â€¢ [4.372 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:04:48.64
    Nov 15 06:04:48.640: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 06:04:48.641
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:04:48.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:04:48.715
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-a9b00279-d471-4b09-82b8-5ee677209fdf 11/15/23 06:04:48.724
    STEP: Creating a pod to test consume configMaps 11/15/23 06:04:48.746
    Nov 15 06:04:48.777: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ff4e6eb8-d1b4-4605-8b66-e82f26e92b1d" in namespace "projected-9730" to be "Succeeded or Failed"
    Nov 15 06:04:48.799: INFO: Pod "pod-projected-configmaps-ff4e6eb8-d1b4-4605-8b66-e82f26e92b1d": Phase="Pending", Reason="", readiness=false. Elapsed: 21.573296ms
    Nov 15 06:04:50.813: INFO: Pod "pod-projected-configmaps-ff4e6eb8-d1b4-4605-8b66-e82f26e92b1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036330279s
    Nov 15 06:04:52.833: INFO: Pod "pod-projected-configmaps-ff4e6eb8-d1b4-4605-8b66-e82f26e92b1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055511508s
    STEP: Saw pod success 11/15/23 06:04:52.833
    Nov 15 06:04:52.833: INFO: Pod "pod-projected-configmaps-ff4e6eb8-d1b4-4605-8b66-e82f26e92b1d" satisfied condition "Succeeded or Failed"
    Nov 15 06:04:52.848: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-configmaps-ff4e6eb8-d1b4-4605-8b66-e82f26e92b1d container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 06:04:52.899
    Nov 15 06:04:52.946: INFO: Waiting for pod pod-projected-configmaps-ff4e6eb8-d1b4-4605-8b66-e82f26e92b1d to disappear
    Nov 15 06:04:52.962: INFO: Pod pod-projected-configmaps-ff4e6eb8-d1b4-4605-8b66-e82f26e92b1d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:04:52.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9730" for this suite. 11/15/23 06:04:52.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:04:53.014
Nov 15 06:04:53.014: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename daemonsets 11/15/23 06:04:53.015
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:04:53.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:04:53.091
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
Nov 15 06:04:53.236: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 06:04:53.258
Nov 15 06:04:53.302: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 06:04:53.302: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 06:04:54.368: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 06:04:54.368: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 06:04:55.351: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 06:04:55.351: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 06:04:56.344: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 15 06:04:56.344: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 11/15/23 06:04:56.404
STEP: Check that daemon pods images are updated. 11/15/23 06:04:56.445
Nov 15 06:04:56.459: INFO: Wrong image for pod: daemon-set-bvgkt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 06:04:56.459: INFO: Wrong image for pod: daemon-set-k6gd6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 06:04:56.459: INFO: Wrong image for pod: daemon-set-kxd6w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 06:04:57.502: INFO: Wrong image for pod: daemon-set-bvgkt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 06:04:57.502: INFO: Wrong image for pod: daemon-set-k6gd6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 06:04:58.495: INFO: Wrong image for pod: daemon-set-bvgkt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 06:04:58.495: INFO: Wrong image for pod: daemon-set-k6gd6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 06:04:58.495: INFO: Pod daemon-set-sjmwv is not available
Nov 15 06:04:59.498: INFO: Wrong image for pod: daemon-set-bvgkt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 06:04:59.498: INFO: Wrong image for pod: daemon-set-k6gd6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 06:04:59.498: INFO: Pod daemon-set-sjmwv is not available
Nov 15 06:05:00.499: INFO: Wrong image for pod: daemon-set-k6gd6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 06:05:01.497: INFO: Wrong image for pod: daemon-set-k6gd6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 06:05:02.500: INFO: Pod daemon-set-5zx54 is not available
Nov 15 06:05:02.500: INFO: Wrong image for pod: daemon-set-k6gd6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 06:05:03.497: INFO: Pod daemon-set-5zx54 is not available
Nov 15 06:05:03.497: INFO: Wrong image for pod: daemon-set-k6gd6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 06:05:05.497: INFO: Pod daemon-set-72mmv is not available
STEP: Check that daemon pods are still running on every node of the cluster. 11/15/23 06:05:05.521
Nov 15 06:05:05.554: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 06:05:05.554: INFO: Node 10.72.152.86 is running 0 daemon pod, expected 1
Nov 15 06:05:06.600: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 06:05:06.600: INFO: Node 10.72.152.86 is running 0 daemon pod, expected 1
Nov 15 06:05:07.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 15 06:05:07.656: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 11/15/23 06:05:07.858
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6269, will wait for the garbage collector to delete the pods 11/15/23 06:05:07.859
Nov 15 06:05:07.962: INFO: Deleting DaemonSet.extensions daemon-set took: 34.811767ms
Nov 15 06:05:08.065: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.880418ms
Nov 15 06:05:10.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 06:05:10.280: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 15 06:05:10.310: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"73316"},"items":null}

Nov 15 06:05:10.340: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"73316"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:05:10.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6269" for this suite. 11/15/23 06:05:10.461
------------------------------
â€¢ [SLOW TEST] [17.469 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:04:53.014
    Nov 15 06:04:53.014: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename daemonsets 11/15/23 06:04:53.015
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:04:53.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:04:53.091
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:385
    Nov 15 06:04:53.236: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 06:04:53.258
    Nov 15 06:04:53.302: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 06:04:53.302: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 06:04:54.368: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 06:04:54.368: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 06:04:55.351: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 06:04:55.351: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 06:04:56.344: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 15 06:04:56.344: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 11/15/23 06:04:56.404
    STEP: Check that daemon pods images are updated. 11/15/23 06:04:56.445
    Nov 15 06:04:56.459: INFO: Wrong image for pod: daemon-set-bvgkt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 06:04:56.459: INFO: Wrong image for pod: daemon-set-k6gd6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 06:04:56.459: INFO: Wrong image for pod: daemon-set-kxd6w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 06:04:57.502: INFO: Wrong image for pod: daemon-set-bvgkt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 06:04:57.502: INFO: Wrong image for pod: daemon-set-k6gd6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 06:04:58.495: INFO: Wrong image for pod: daemon-set-bvgkt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 06:04:58.495: INFO: Wrong image for pod: daemon-set-k6gd6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 06:04:58.495: INFO: Pod daemon-set-sjmwv is not available
    Nov 15 06:04:59.498: INFO: Wrong image for pod: daemon-set-bvgkt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 06:04:59.498: INFO: Wrong image for pod: daemon-set-k6gd6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 06:04:59.498: INFO: Pod daemon-set-sjmwv is not available
    Nov 15 06:05:00.499: INFO: Wrong image for pod: daemon-set-k6gd6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 06:05:01.497: INFO: Wrong image for pod: daemon-set-k6gd6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 06:05:02.500: INFO: Pod daemon-set-5zx54 is not available
    Nov 15 06:05:02.500: INFO: Wrong image for pod: daemon-set-k6gd6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 06:05:03.497: INFO: Pod daemon-set-5zx54 is not available
    Nov 15 06:05:03.497: INFO: Wrong image for pod: daemon-set-k6gd6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 06:05:05.497: INFO: Pod daemon-set-72mmv is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 11/15/23 06:05:05.521
    Nov 15 06:05:05.554: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 06:05:05.554: INFO: Node 10.72.152.86 is running 0 daemon pod, expected 1
    Nov 15 06:05:06.600: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 06:05:06.600: INFO: Node 10.72.152.86 is running 0 daemon pod, expected 1
    Nov 15 06:05:07.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 15 06:05:07.656: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 11/15/23 06:05:07.858
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6269, will wait for the garbage collector to delete the pods 11/15/23 06:05:07.859
    Nov 15 06:05:07.962: INFO: Deleting DaemonSet.extensions daemon-set took: 34.811767ms
    Nov 15 06:05:08.065: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.880418ms
    Nov 15 06:05:10.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 06:05:10.280: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 15 06:05:10.310: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"73316"},"items":null}

    Nov 15 06:05:10.340: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"73316"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:05:10.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6269" for this suite. 11/15/23 06:05:10.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:05:10.486
Nov 15 06:05:10.486: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename services 11/15/23 06:05:10.487
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:10.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:10.563
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9813 11/15/23 06:05:10.575
STEP: changing the ExternalName service to type=ClusterIP 11/15/23 06:05:10.593
STEP: creating replication controller externalname-service in namespace services-9813 11/15/23 06:05:10.654
I1115 06:05:10.675661      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9813, replica count: 2
I1115 06:05:13.726976      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 06:05:13.727: INFO: Creating new exec pod
Nov 15 06:05:13.765: INFO: Waiting up to 5m0s for pod "execpodt5sfm" in namespace "services-9813" to be "running"
Nov 15 06:05:13.780: INFO: Pod "execpodt5sfm": Phase="Pending", Reason="", readiness=false. Elapsed: 14.611155ms
Nov 15 06:05:15.796: INFO: Pod "execpodt5sfm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030660063s
Nov 15 06:05:17.809: INFO: Pod "execpodt5sfm": Phase="Running", Reason="", readiness=true. Elapsed: 4.044122863s
Nov 15 06:05:17.809: INFO: Pod "execpodt5sfm" satisfied condition "running"
Nov 15 06:05:18.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-9813 exec execpodt5sfm -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Nov 15 06:05:19.114: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 15 06:05:19.114: INFO: stdout: ""
Nov 15 06:05:19.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-9813 exec execpodt5sfm -- /bin/sh -x -c nc -v -z -w 2 172.21.18.58 80'
Nov 15 06:05:19.415: INFO: stderr: "+ nc -v -z -w 2 172.21.18.58 80\nConnection to 172.21.18.58 80 port [tcp/http] succeeded!\n"
Nov 15 06:05:19.415: INFO: stdout: ""
Nov 15 06:05:19.415: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 06:05:19.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9813" for this suite. 11/15/23 06:05:19.524
------------------------------
â€¢ [SLOW TEST] [9.060 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:05:10.486
    Nov 15 06:05:10.486: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename services 11/15/23 06:05:10.487
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:10.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:10.563
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9813 11/15/23 06:05:10.575
    STEP: changing the ExternalName service to type=ClusterIP 11/15/23 06:05:10.593
    STEP: creating replication controller externalname-service in namespace services-9813 11/15/23 06:05:10.654
    I1115 06:05:10.675661      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9813, replica count: 2
    I1115 06:05:13.726976      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 15 06:05:13.727: INFO: Creating new exec pod
    Nov 15 06:05:13.765: INFO: Waiting up to 5m0s for pod "execpodt5sfm" in namespace "services-9813" to be "running"
    Nov 15 06:05:13.780: INFO: Pod "execpodt5sfm": Phase="Pending", Reason="", readiness=false. Elapsed: 14.611155ms
    Nov 15 06:05:15.796: INFO: Pod "execpodt5sfm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030660063s
    Nov 15 06:05:17.809: INFO: Pod "execpodt5sfm": Phase="Running", Reason="", readiness=true. Elapsed: 4.044122863s
    Nov 15 06:05:17.809: INFO: Pod "execpodt5sfm" satisfied condition "running"
    Nov 15 06:05:18.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-9813 exec execpodt5sfm -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Nov 15 06:05:19.114: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Nov 15 06:05:19.114: INFO: stdout: ""
    Nov 15 06:05:19.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-9813 exec execpodt5sfm -- /bin/sh -x -c nc -v -z -w 2 172.21.18.58 80'
    Nov 15 06:05:19.415: INFO: stderr: "+ nc -v -z -w 2 172.21.18.58 80\nConnection to 172.21.18.58 80 port [tcp/http] succeeded!\n"
    Nov 15 06:05:19.415: INFO: stdout: ""
    Nov 15 06:05:19.415: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:05:19.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9813" for this suite. 11/15/23 06:05:19.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:05:19.548
Nov 15 06:05:19.548: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename init-container 11/15/23 06:05:19.549
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:19.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:19.633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 11/15/23 06:05:19.642
Nov 15 06:05:19.643: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:05:23.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7897" for this suite. 11/15/23 06:05:23.819
------------------------------
â€¢ [4.303 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:05:19.548
    Nov 15 06:05:19.548: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename init-container 11/15/23 06:05:19.549
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:19.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:19.633
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 11/15/23 06:05:19.642
    Nov 15 06:05:19.643: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:05:23.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7897" for this suite. 11/15/23 06:05:23.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:05:23.857
Nov 15 06:05:23.857: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 06:05:23.859
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:23.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:23.936
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
Nov 15 06:05:23.964: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-1241b0ec-0c5e-4747-8446-a7e17b9a0fc5 11/15/23 06:05:23.965
STEP: Creating secret with name s-test-opt-upd-2dcccfe9-fc28-41d5-8b0b-28949f32afad 11/15/23 06:05:23.99
STEP: Creating the pod 11/15/23 06:05:24.007
Nov 15 06:05:24.044: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b7d1d464-5d0a-42b6-9f81-ebc525bce7c1" in namespace "projected-3344" to be "running and ready"
Nov 15 06:05:24.065: INFO: Pod "pod-projected-secrets-b7d1d464-5d0a-42b6-9f81-ebc525bce7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.836728ms
Nov 15 06:05:24.065: INFO: The phase of Pod pod-projected-secrets-b7d1d464-5d0a-42b6-9f81-ebc525bce7c1 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:05:26.082: INFO: Pod "pod-projected-secrets-b7d1d464-5d0a-42b6-9f81-ebc525bce7c1": Phase="Running", Reason="", readiness=true. Elapsed: 2.038111233s
Nov 15 06:05:26.082: INFO: The phase of Pod pod-projected-secrets-b7d1d464-5d0a-42b6-9f81-ebc525bce7c1 is Running (Ready = true)
Nov 15 06:05:26.082: INFO: Pod "pod-projected-secrets-b7d1d464-5d0a-42b6-9f81-ebc525bce7c1" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-1241b0ec-0c5e-4747-8446-a7e17b9a0fc5 11/15/23 06:05:26.236
STEP: Updating secret s-test-opt-upd-2dcccfe9-fc28-41d5-8b0b-28949f32afad 11/15/23 06:05:26.258
STEP: Creating secret with name s-test-opt-create-78ae275a-6e1d-4f6c-b615-890c6b69bdc9 11/15/23 06:05:26.276
STEP: waiting to observe update in volume 11/15/23 06:05:26.295
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 15 06:05:28.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3344" for this suite. 11/15/23 06:05:28.454
------------------------------
â€¢ [4.622 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:05:23.857
    Nov 15 06:05:23.857: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 06:05:23.859
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:23.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:23.936
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    Nov 15 06:05:23.964: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating secret with name s-test-opt-del-1241b0ec-0c5e-4747-8446-a7e17b9a0fc5 11/15/23 06:05:23.965
    STEP: Creating secret with name s-test-opt-upd-2dcccfe9-fc28-41d5-8b0b-28949f32afad 11/15/23 06:05:23.99
    STEP: Creating the pod 11/15/23 06:05:24.007
    Nov 15 06:05:24.044: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b7d1d464-5d0a-42b6-9f81-ebc525bce7c1" in namespace "projected-3344" to be "running and ready"
    Nov 15 06:05:24.065: INFO: Pod "pod-projected-secrets-b7d1d464-5d0a-42b6-9f81-ebc525bce7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.836728ms
    Nov 15 06:05:24.065: INFO: The phase of Pod pod-projected-secrets-b7d1d464-5d0a-42b6-9f81-ebc525bce7c1 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:05:26.082: INFO: Pod "pod-projected-secrets-b7d1d464-5d0a-42b6-9f81-ebc525bce7c1": Phase="Running", Reason="", readiness=true. Elapsed: 2.038111233s
    Nov 15 06:05:26.082: INFO: The phase of Pod pod-projected-secrets-b7d1d464-5d0a-42b6-9f81-ebc525bce7c1 is Running (Ready = true)
    Nov 15 06:05:26.082: INFO: Pod "pod-projected-secrets-b7d1d464-5d0a-42b6-9f81-ebc525bce7c1" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-1241b0ec-0c5e-4747-8446-a7e17b9a0fc5 11/15/23 06:05:26.236
    STEP: Updating secret s-test-opt-upd-2dcccfe9-fc28-41d5-8b0b-28949f32afad 11/15/23 06:05:26.258
    STEP: Creating secret with name s-test-opt-create-78ae275a-6e1d-4f6c-b615-890c6b69bdc9 11/15/23 06:05:26.276
    STEP: waiting to observe update in volume 11/15/23 06:05:26.295
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:05:28.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3344" for this suite. 11/15/23 06:05:28.454
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:05:28.479
Nov 15 06:05:28.479: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename csiinlinevolumes 11/15/23 06:05:28.479
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:28.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:28.568
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 11/15/23 06:05:28.576
STEP: getting 11/15/23 06:05:28.655
STEP: listing in namespace 11/15/23 06:05:28.674
STEP: patching 11/15/23 06:05:28.692
STEP: deleting 11/15/23 06:05:28.717
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Nov 15 06:05:28.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-2066" for this suite. 11/15/23 06:05:28.779
------------------------------
â€¢ [0.325 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:05:28.479
    Nov 15 06:05:28.479: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename csiinlinevolumes 11/15/23 06:05:28.479
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:28.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:28.568
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 11/15/23 06:05:28.576
    STEP: getting 11/15/23 06:05:28.655
    STEP: listing in namespace 11/15/23 06:05:28.674
    STEP: patching 11/15/23 06:05:28.692
    STEP: deleting 11/15/23 06:05:28.717
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:05:28.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-2066" for this suite. 11/15/23 06:05:28.779
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:05:28.804
Nov 15 06:05:28.804: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 06:05:28.805
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:28.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:28.88
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 11/15/23 06:05:28.891
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 11/15/23 06:05:28.896
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 11/15/23 06:05:28.896
STEP: fetching the /apis/apiextensions.k8s.io discovery document 11/15/23 06:05:28.896
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 11/15/23 06:05:28.901
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 11/15/23 06:05:28.901
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 11/15/23 06:05:28.905
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:05:28.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8809" for this suite. 11/15/23 06:05:28.937
------------------------------
â€¢ [0.155 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:05:28.804
    Nov 15 06:05:28.804: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 06:05:28.805
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:28.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:28.88
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 11/15/23 06:05:28.891
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 11/15/23 06:05:28.896
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 11/15/23 06:05:28.896
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 11/15/23 06:05:28.896
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 11/15/23 06:05:28.901
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 11/15/23 06:05:28.901
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 11/15/23 06:05:28.905
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:05:28.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8809" for this suite. 11/15/23 06:05:28.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:05:28.962
Nov 15 06:05:28.962: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename conformance-tests 11/15/23 06:05:28.963
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:29.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:29.04
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 11/15/23 06:05:29.052
Nov 15 06:05:29.052: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Nov 15 06:05:29.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-7294" for this suite. 11/15/23 06:05:29.103
------------------------------
â€¢ [0.167 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:05:28.962
    Nov 15 06:05:28.962: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename conformance-tests 11/15/23 06:05:28.963
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:29.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:29.04
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 11/15/23 06:05:29.052
    Nov 15 06:05:29.052: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:05:29.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-7294" for this suite. 11/15/23 06:05:29.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:05:29.132
Nov 15 06:05:29.132: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename proxy 11/15/23 06:05:29.133
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:29.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:29.26
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Nov 15 06:05:29.301: INFO: Creating pod...
Nov 15 06:05:29.332: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4628" to be "running"
Nov 15 06:05:29.377: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 45.115422ms
Nov 15 06:05:31.394: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062513276s
Nov 15 06:05:33.395: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.063046563s
Nov 15 06:05:33.395: INFO: Pod "agnhost" satisfied condition "running"
Nov 15 06:05:33.395: INFO: Creating service...
Nov 15 06:05:33.434: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/pods/agnhost/proxy?method=DELETE
Nov 15 06:05:33.468: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 15 06:05:33.468: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/pods/agnhost/proxy?method=OPTIONS
Nov 15 06:05:33.488: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 15 06:05:33.488: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/pods/agnhost/proxy?method=PATCH
Nov 15 06:05:33.509: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 15 06:05:33.509: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/pods/agnhost/proxy?method=POST
Nov 15 06:05:33.536: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 15 06:05:33.537: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/pods/agnhost/proxy?method=PUT
Nov 15 06:05:33.564: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Nov 15 06:05:33.564: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/services/e2e-proxy-test-service/proxy?method=DELETE
Nov 15 06:05:33.596: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 15 06:05:33.596: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/services/e2e-proxy-test-service/proxy?method=OPTIONS
Nov 15 06:05:33.628: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 15 06:05:33.628: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/services/e2e-proxy-test-service/proxy?method=PATCH
Nov 15 06:05:33.659: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 15 06:05:33.659: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/services/e2e-proxy-test-service/proxy?method=POST
Nov 15 06:05:33.690: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 15 06:05:33.690: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/services/e2e-proxy-test-service/proxy?method=PUT
Nov 15 06:05:33.720: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Nov 15 06:05:33.720: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/pods/agnhost/proxy?method=GET
Nov 15 06:05:33.738: INFO: http.Client request:GET StatusCode:301
Nov 15 06:05:33.738: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/services/e2e-proxy-test-service/proxy?method=GET
Nov 15 06:05:33.770: INFO: http.Client request:GET StatusCode:301
Nov 15 06:05:33.770: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/pods/agnhost/proxy?method=HEAD
Nov 15 06:05:33.785: INFO: http.Client request:HEAD StatusCode:301
Nov 15 06:05:33.786: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/services/e2e-proxy-test-service/proxy?method=HEAD
Nov 15 06:05:33.815: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Nov 15 06:05:33.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-4628" for this suite. 11/15/23 06:05:33.847
------------------------------
â€¢ [4.738 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:05:29.132
    Nov 15 06:05:29.132: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename proxy 11/15/23 06:05:29.133
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:29.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:29.26
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Nov 15 06:05:29.301: INFO: Creating pod...
    Nov 15 06:05:29.332: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4628" to be "running"
    Nov 15 06:05:29.377: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 45.115422ms
    Nov 15 06:05:31.394: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062513276s
    Nov 15 06:05:33.395: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.063046563s
    Nov 15 06:05:33.395: INFO: Pod "agnhost" satisfied condition "running"
    Nov 15 06:05:33.395: INFO: Creating service...
    Nov 15 06:05:33.434: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/pods/agnhost/proxy?method=DELETE
    Nov 15 06:05:33.468: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Nov 15 06:05:33.468: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/pods/agnhost/proxy?method=OPTIONS
    Nov 15 06:05:33.488: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Nov 15 06:05:33.488: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/pods/agnhost/proxy?method=PATCH
    Nov 15 06:05:33.509: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Nov 15 06:05:33.509: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/pods/agnhost/proxy?method=POST
    Nov 15 06:05:33.536: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Nov 15 06:05:33.537: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/pods/agnhost/proxy?method=PUT
    Nov 15 06:05:33.564: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Nov 15 06:05:33.564: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/services/e2e-proxy-test-service/proxy?method=DELETE
    Nov 15 06:05:33.596: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Nov 15 06:05:33.596: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Nov 15 06:05:33.628: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Nov 15 06:05:33.628: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/services/e2e-proxy-test-service/proxy?method=PATCH
    Nov 15 06:05:33.659: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Nov 15 06:05:33.659: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/services/e2e-proxy-test-service/proxy?method=POST
    Nov 15 06:05:33.690: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Nov 15 06:05:33.690: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/services/e2e-proxy-test-service/proxy?method=PUT
    Nov 15 06:05:33.720: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Nov 15 06:05:33.720: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/pods/agnhost/proxy?method=GET
    Nov 15 06:05:33.738: INFO: http.Client request:GET StatusCode:301
    Nov 15 06:05:33.738: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/services/e2e-proxy-test-service/proxy?method=GET
    Nov 15 06:05:33.770: INFO: http.Client request:GET StatusCode:301
    Nov 15 06:05:33.770: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/pods/agnhost/proxy?method=HEAD
    Nov 15 06:05:33.785: INFO: http.Client request:HEAD StatusCode:301
    Nov 15 06:05:33.786: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4628/services/e2e-proxy-test-service/proxy?method=HEAD
    Nov 15 06:05:33.815: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:05:33.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-4628" for this suite. 11/15/23 06:05:33.847
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:05:33.876
Nov 15 06:05:33.876: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename daemonsets 11/15/23 06:05:33.877
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:33.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:33.96
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
STEP: Creating simple DaemonSet "daemon-set" 11/15/23 06:05:34.093
STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 06:05:34.113
Nov 15 06:05:34.172: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 06:05:34.172: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 06:05:35.213: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 06:05:35.213: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 06:05:36.215: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 15 06:05:36.215: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 11/15/23 06:05:36.231
Nov 15 06:05:36.319: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 06:05:36.319: INFO: Node 10.72.152.88 is running 0 daemon pod, expected 1
Nov 15 06:05:37.396: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 06:05:37.396: INFO: Node 10.72.152.88 is running 0 daemon pod, expected 1
Nov 15 06:05:38.365: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 06:05:38.365: INFO: Node 10.72.152.88 is running 0 daemon pod, expected 1
Nov 15 06:05:39.378: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 06:05:39.378: INFO: Node 10.72.152.88 is running 0 daemon pod, expected 1
Nov 15 06:05:40.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 06:05:40.363: INFO: Node 10.72.152.88 is running 0 daemon pod, expected 1
Nov 15 06:05:41.359: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 15 06:05:41.360: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 11/15/23 06:05:41.374
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8545, will wait for the garbage collector to delete the pods 11/15/23 06:05:41.375
Nov 15 06:05:41.466: INFO: Deleting DaemonSet.extensions daemon-set took: 25.510533ms
Nov 15 06:05:41.571: INFO: Terminating DaemonSet.extensions daemon-set pods took: 104.669644ms
Nov 15 06:05:44.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 06:05:44.486: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 15 06:05:44.500: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"74302"},"items":null}

Nov 15 06:05:44.514: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"74303"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:05:44.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8545" for this suite. 11/15/23 06:05:44.605
------------------------------
â€¢ [SLOW TEST] [10.753 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:05:33.876
    Nov 15 06:05:33.876: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename daemonsets 11/15/23 06:05:33.877
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:33.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:33.96
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:177
    STEP: Creating simple DaemonSet "daemon-set" 11/15/23 06:05:34.093
    STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 06:05:34.113
    Nov 15 06:05:34.172: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 06:05:34.172: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 06:05:35.213: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 06:05:35.213: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 06:05:36.215: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 15 06:05:36.215: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 11/15/23 06:05:36.231
    Nov 15 06:05:36.319: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 06:05:36.319: INFO: Node 10.72.152.88 is running 0 daemon pod, expected 1
    Nov 15 06:05:37.396: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 06:05:37.396: INFO: Node 10.72.152.88 is running 0 daemon pod, expected 1
    Nov 15 06:05:38.365: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 06:05:38.365: INFO: Node 10.72.152.88 is running 0 daemon pod, expected 1
    Nov 15 06:05:39.378: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 06:05:39.378: INFO: Node 10.72.152.88 is running 0 daemon pod, expected 1
    Nov 15 06:05:40.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 06:05:40.363: INFO: Node 10.72.152.88 is running 0 daemon pod, expected 1
    Nov 15 06:05:41.359: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 15 06:05:41.360: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 11/15/23 06:05:41.374
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8545, will wait for the garbage collector to delete the pods 11/15/23 06:05:41.375
    Nov 15 06:05:41.466: INFO: Deleting DaemonSet.extensions daemon-set took: 25.510533ms
    Nov 15 06:05:41.571: INFO: Terminating DaemonSet.extensions daemon-set pods took: 104.669644ms
    Nov 15 06:05:44.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 06:05:44.486: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 15 06:05:44.500: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"74302"},"items":null}

    Nov 15 06:05:44.514: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"74303"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:05:44.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8545" for this suite. 11/15/23 06:05:44.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:05:44.632
Nov 15 06:05:44.632: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename init-container 11/15/23 06:05:44.633
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:44.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:44.715
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 11/15/23 06:05:44.726
Nov 15 06:05:44.726: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:05:51.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1863" for this suite. 11/15/23 06:05:51.277
------------------------------
â€¢ [SLOW TEST] [6.669 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:05:44.632
    Nov 15 06:05:44.632: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename init-container 11/15/23 06:05:44.633
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:44.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:44.715
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 11/15/23 06:05:44.726
    Nov 15 06:05:44.726: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:05:51.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1863" for this suite. 11/15/23 06:05:51.277
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:05:51.301
Nov 15 06:05:51.301: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename statefulset 11/15/23 06:05:51.304
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:51.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:51.381
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6023 11/15/23 06:05:51.393
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 11/15/23 06:05:51.411
STEP: Creating pod with conflicting port in namespace statefulset-6023 11/15/23 06:05:51.452
STEP: Waiting until pod test-pod will start running in namespace statefulset-6023 11/15/23 06:05:51.488
Nov 15 06:05:51.489: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-6023" to be "running"
Nov 15 06:05:51.513: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006409ms
Nov 15 06:05:53.531: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=false. Elapsed: 2.042781924s
Nov 15 06:05:53.531: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-6023 11/15/23 06:05:53.531
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6023 11/15/23 06:05:53.552
Nov 15 06:05:53.608: INFO: Observed stateful pod in namespace: statefulset-6023, name: ss-0, uid: f4c3d707-4b0e-4286-b2a7-1ef74ab38de9, status phase: Pending. Waiting for statefulset controller to delete.
Nov 15 06:05:53.648: INFO: Observed stateful pod in namespace: statefulset-6023, name: ss-0, uid: f4c3d707-4b0e-4286-b2a7-1ef74ab38de9, status phase: Failed. Waiting for statefulset controller to delete.
Nov 15 06:05:53.678: INFO: Observed stateful pod in namespace: statefulset-6023, name: ss-0, uid: f4c3d707-4b0e-4286-b2a7-1ef74ab38de9, status phase: Failed. Waiting for statefulset controller to delete.
Nov 15 06:05:53.707: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6023
STEP: Removing pod with conflicting port in namespace statefulset-6023 11/15/23 06:05:53.707
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6023 and will be in running state 11/15/23 06:05:53.751
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 15 06:05:57.815: INFO: Deleting all statefulset in ns statefulset-6023
Nov 15 06:05:57.827: INFO: Scaling statefulset ss to 0
Nov 15 06:06:07.893: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 06:06:07.907: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 15 06:06:07.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6023" for this suite. 11/15/23 06:06:07.984
------------------------------
â€¢ [SLOW TEST] [16.707 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:05:51.301
    Nov 15 06:05:51.301: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename statefulset 11/15/23 06:05:51.304
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:05:51.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:05:51.381
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6023 11/15/23 06:05:51.393
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 11/15/23 06:05:51.411
    STEP: Creating pod with conflicting port in namespace statefulset-6023 11/15/23 06:05:51.452
    STEP: Waiting until pod test-pod will start running in namespace statefulset-6023 11/15/23 06:05:51.488
    Nov 15 06:05:51.489: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-6023" to be "running"
    Nov 15 06:05:51.513: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006409ms
    Nov 15 06:05:53.531: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=false. Elapsed: 2.042781924s
    Nov 15 06:05:53.531: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-6023 11/15/23 06:05:53.531
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6023 11/15/23 06:05:53.552
    Nov 15 06:05:53.608: INFO: Observed stateful pod in namespace: statefulset-6023, name: ss-0, uid: f4c3d707-4b0e-4286-b2a7-1ef74ab38de9, status phase: Pending. Waiting for statefulset controller to delete.
    Nov 15 06:05:53.648: INFO: Observed stateful pod in namespace: statefulset-6023, name: ss-0, uid: f4c3d707-4b0e-4286-b2a7-1ef74ab38de9, status phase: Failed. Waiting for statefulset controller to delete.
    Nov 15 06:05:53.678: INFO: Observed stateful pod in namespace: statefulset-6023, name: ss-0, uid: f4c3d707-4b0e-4286-b2a7-1ef74ab38de9, status phase: Failed. Waiting for statefulset controller to delete.
    Nov 15 06:05:53.707: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6023
    STEP: Removing pod with conflicting port in namespace statefulset-6023 11/15/23 06:05:53.707
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6023 and will be in running state 11/15/23 06:05:53.751
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 15 06:05:57.815: INFO: Deleting all statefulset in ns statefulset-6023
    Nov 15 06:05:57.827: INFO: Scaling statefulset ss to 0
    Nov 15 06:06:07.893: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 06:06:07.907: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:06:07.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6023" for this suite. 11/15/23 06:06:07.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:06:08.011
Nov 15 06:06:08.011: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename secrets 11/15/23 06:06:08.012
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:06:08.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:06:08.09
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-fc7b2c06-1038-44b9-92b2-cb1ac5fb0030 11/15/23 06:06:08.18
STEP: Creating a pod to test consume secrets 11/15/23 06:06:08.193
Nov 15 06:06:08.220: INFO: Waiting up to 5m0s for pod "pod-secrets-ea89c966-18f5-4076-b643-0966c91f20df" in namespace "secrets-3625" to be "Succeeded or Failed"
Nov 15 06:06:08.237: INFO: Pod "pod-secrets-ea89c966-18f5-4076-b643-0966c91f20df": Phase="Pending", Reason="", readiness=false. Elapsed: 17.475304ms
Nov 15 06:06:10.253: INFO: Pod "pod-secrets-ea89c966-18f5-4076-b643-0966c91f20df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033462205s
Nov 15 06:06:12.288: INFO: Pod "pod-secrets-ea89c966-18f5-4076-b643-0966c91f20df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067872167s
Nov 15 06:06:14.254: INFO: Pod "pod-secrets-ea89c966-18f5-4076-b643-0966c91f20df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033818237s
STEP: Saw pod success 11/15/23 06:06:14.254
Nov 15 06:06:14.254: INFO: Pod "pod-secrets-ea89c966-18f5-4076-b643-0966c91f20df" satisfied condition "Succeeded or Failed"
Nov 15 06:06:14.269: INFO: Trying to get logs from node 10.72.152.88 pod pod-secrets-ea89c966-18f5-4076-b643-0966c91f20df container secret-volume-test: <nil>
STEP: delete the pod 11/15/23 06:06:14.308
Nov 15 06:06:14.345: INFO: Waiting for pod pod-secrets-ea89c966-18f5-4076-b643-0966c91f20df to disappear
Nov 15 06:06:14.365: INFO: Pod pod-secrets-ea89c966-18f5-4076-b643-0966c91f20df no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 06:06:14.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3625" for this suite. 11/15/23 06:06:14.389
STEP: Destroying namespace "secret-namespace-8860" for this suite. 11/15/23 06:06:14.414
------------------------------
â€¢ [SLOW TEST] [6.426 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:06:08.011
    Nov 15 06:06:08.011: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename secrets 11/15/23 06:06:08.012
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:06:08.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:06:08.09
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-fc7b2c06-1038-44b9-92b2-cb1ac5fb0030 11/15/23 06:06:08.18
    STEP: Creating a pod to test consume secrets 11/15/23 06:06:08.193
    Nov 15 06:06:08.220: INFO: Waiting up to 5m0s for pod "pod-secrets-ea89c966-18f5-4076-b643-0966c91f20df" in namespace "secrets-3625" to be "Succeeded or Failed"
    Nov 15 06:06:08.237: INFO: Pod "pod-secrets-ea89c966-18f5-4076-b643-0966c91f20df": Phase="Pending", Reason="", readiness=false. Elapsed: 17.475304ms
    Nov 15 06:06:10.253: INFO: Pod "pod-secrets-ea89c966-18f5-4076-b643-0966c91f20df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033462205s
    Nov 15 06:06:12.288: INFO: Pod "pod-secrets-ea89c966-18f5-4076-b643-0966c91f20df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067872167s
    Nov 15 06:06:14.254: INFO: Pod "pod-secrets-ea89c966-18f5-4076-b643-0966c91f20df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033818237s
    STEP: Saw pod success 11/15/23 06:06:14.254
    Nov 15 06:06:14.254: INFO: Pod "pod-secrets-ea89c966-18f5-4076-b643-0966c91f20df" satisfied condition "Succeeded or Failed"
    Nov 15 06:06:14.269: INFO: Trying to get logs from node 10.72.152.88 pod pod-secrets-ea89c966-18f5-4076-b643-0966c91f20df container secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 06:06:14.308
    Nov 15 06:06:14.345: INFO: Waiting for pod pod-secrets-ea89c966-18f5-4076-b643-0966c91f20df to disappear
    Nov 15 06:06:14.365: INFO: Pod pod-secrets-ea89c966-18f5-4076-b643-0966c91f20df no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:06:14.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3625" for this suite. 11/15/23 06:06:14.389
    STEP: Destroying namespace "secret-namespace-8860" for this suite. 11/15/23 06:06:14.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:06:14.438
Nov 15 06:06:14.438: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename gc 11/15/23 06:06:14.439
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:06:14.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:06:14.516
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Nov 15 06:06:14.647: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"7f456904-0684-40ac-9a2e-a71368ad3875", Controller:(*bool)(0xc0051f1882), BlockOwnerDeletion:(*bool)(0xc0051f1883)}}
Nov 15 06:06:14.669: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"af47f0f7-0a65-42e6-946c-b8e9da2a679f", Controller:(*bool)(0xc0045a2796), BlockOwnerDeletion:(*bool)(0xc0045a2797)}}
Nov 15 06:06:14.692: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"4b994982-c968-410a-a806-915a9e726d43", Controller:(*bool)(0xc0051f1b82), BlockOwnerDeletion:(*bool)(0xc0051f1b83)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 15 06:06:19.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8985" for this suite. 11/15/23 06:06:19.776
------------------------------
â€¢ [SLOW TEST] [5.365 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:06:14.438
    Nov 15 06:06:14.438: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename gc 11/15/23 06:06:14.439
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:06:14.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:06:14.516
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Nov 15 06:06:14.647: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"7f456904-0684-40ac-9a2e-a71368ad3875", Controller:(*bool)(0xc0051f1882), BlockOwnerDeletion:(*bool)(0xc0051f1883)}}
    Nov 15 06:06:14.669: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"af47f0f7-0a65-42e6-946c-b8e9da2a679f", Controller:(*bool)(0xc0045a2796), BlockOwnerDeletion:(*bool)(0xc0045a2797)}}
    Nov 15 06:06:14.692: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"4b994982-c968-410a-a806-915a9e726d43", Controller:(*bool)(0xc0051f1b82), BlockOwnerDeletion:(*bool)(0xc0051f1b83)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:06:19.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8985" for this suite. 11/15/23 06:06:19.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:06:19.804
Nov 15 06:06:19.804: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename webhook 11/15/23 06:06:19.806
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:06:19.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:06:19.875
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 06:06:19.936
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:06:20.288
STEP: Deploying the webhook pod 11/15/23 06:06:20.338
STEP: Wait for the deployment to be ready 11/15/23 06:06:20.373
Nov 15 06:06:20.409: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/15/23 06:06:22.481
STEP: Verifying the service has paired with the endpoint 11/15/23 06:06:22.524
Nov 15 06:06:23.524: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 11/15/23 06:06:23.544
STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 06:06:23.601
STEP: Updating a validating webhook configuration's rules to not include the create operation 11/15/23 06:06:23.636
STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 06:06:23.677
STEP: Patching a validating webhook configuration's rules to include the create operation 11/15/23 06:06:23.738
STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 06:06:23.761
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:06:23.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3024" for this suite. 11/15/23 06:06:24.022
STEP: Destroying namespace "webhook-3024-markers" for this suite. 11/15/23 06:06:24.045
------------------------------
â€¢ [4.265 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:06:19.804
    Nov 15 06:06:19.804: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename webhook 11/15/23 06:06:19.806
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:06:19.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:06:19.875
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 06:06:19.936
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:06:20.288
    STEP: Deploying the webhook pod 11/15/23 06:06:20.338
    STEP: Wait for the deployment to be ready 11/15/23 06:06:20.373
    Nov 15 06:06:20.409: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/15/23 06:06:22.481
    STEP: Verifying the service has paired with the endpoint 11/15/23 06:06:22.524
    Nov 15 06:06:23.524: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 11/15/23 06:06:23.544
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 06:06:23.601
    STEP: Updating a validating webhook configuration's rules to not include the create operation 11/15/23 06:06:23.636
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 06:06:23.677
    STEP: Patching a validating webhook configuration's rules to include the create operation 11/15/23 06:06:23.738
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 06:06:23.761
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:06:23.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3024" for this suite. 11/15/23 06:06:24.022
    STEP: Destroying namespace "webhook-3024-markers" for this suite. 11/15/23 06:06:24.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:06:24.07
Nov 15 06:06:24.070: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename downward-api 11/15/23 06:06:24.07
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:06:24.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:06:24.154
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 11/15/23 06:06:24.163
Nov 15 06:06:24.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-60810cb6-acb0-4003-8d12-33ab2ac70e72" in namespace "downward-api-2594" to be "Succeeded or Failed"
Nov 15 06:06:24.225: INFO: Pod "downwardapi-volume-60810cb6-acb0-4003-8d12-33ab2ac70e72": Phase="Pending", Reason="", readiness=false. Elapsed: 20.844687ms
Nov 15 06:06:26.244: INFO: Pod "downwardapi-volume-60810cb6-acb0-4003-8d12-33ab2ac70e72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039447131s
Nov 15 06:06:28.251: INFO: Pod "downwardapi-volume-60810cb6-acb0-4003-8d12-33ab2ac70e72": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046029998s
Nov 15 06:06:30.244: INFO: Pod "downwardapi-volume-60810cb6-acb0-4003-8d12-33ab2ac70e72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039421592s
STEP: Saw pod success 11/15/23 06:06:30.244
Nov 15 06:06:30.244: INFO: Pod "downwardapi-volume-60810cb6-acb0-4003-8d12-33ab2ac70e72" satisfied condition "Succeeded or Failed"
Nov 15 06:06:30.260: INFO: Trying to get logs from node 10.72.152.88 pod downwardapi-volume-60810cb6-acb0-4003-8d12-33ab2ac70e72 container client-container: <nil>
STEP: delete the pod 11/15/23 06:06:30.299
Nov 15 06:06:30.348: INFO: Waiting for pod downwardapi-volume-60810cb6-acb0-4003-8d12-33ab2ac70e72 to disappear
Nov 15 06:06:30.367: INFO: Pod downwardapi-volume-60810cb6-acb0-4003-8d12-33ab2ac70e72 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 06:06:30.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2594" for this suite. 11/15/23 06:06:30.389
------------------------------
â€¢ [SLOW TEST] [6.343 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:06:24.07
    Nov 15 06:06:24.070: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename downward-api 11/15/23 06:06:24.07
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:06:24.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:06:24.154
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 11/15/23 06:06:24.163
    Nov 15 06:06:24.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-60810cb6-acb0-4003-8d12-33ab2ac70e72" in namespace "downward-api-2594" to be "Succeeded or Failed"
    Nov 15 06:06:24.225: INFO: Pod "downwardapi-volume-60810cb6-acb0-4003-8d12-33ab2ac70e72": Phase="Pending", Reason="", readiness=false. Elapsed: 20.844687ms
    Nov 15 06:06:26.244: INFO: Pod "downwardapi-volume-60810cb6-acb0-4003-8d12-33ab2ac70e72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039447131s
    Nov 15 06:06:28.251: INFO: Pod "downwardapi-volume-60810cb6-acb0-4003-8d12-33ab2ac70e72": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046029998s
    Nov 15 06:06:30.244: INFO: Pod "downwardapi-volume-60810cb6-acb0-4003-8d12-33ab2ac70e72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039421592s
    STEP: Saw pod success 11/15/23 06:06:30.244
    Nov 15 06:06:30.244: INFO: Pod "downwardapi-volume-60810cb6-acb0-4003-8d12-33ab2ac70e72" satisfied condition "Succeeded or Failed"
    Nov 15 06:06:30.260: INFO: Trying to get logs from node 10.72.152.88 pod downwardapi-volume-60810cb6-acb0-4003-8d12-33ab2ac70e72 container client-container: <nil>
    STEP: delete the pod 11/15/23 06:06:30.299
    Nov 15 06:06:30.348: INFO: Waiting for pod downwardapi-volume-60810cb6-acb0-4003-8d12-33ab2ac70e72 to disappear
    Nov 15 06:06:30.367: INFO: Pod downwardapi-volume-60810cb6-acb0-4003-8d12-33ab2ac70e72 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:06:30.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2594" for this suite. 11/15/23 06:06:30.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:06:30.415
Nov 15 06:06:30.415: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename resourcequota 11/15/23 06:06:30.416
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:06:30.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:06:30.512
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-hz5rd" 11/15/23 06:06:30.548
Nov 15 06:06:30.591: INFO: Resource quota "e2e-rq-status-hz5rd" reports spec: hard cpu limit of 500m
Nov 15 06:06:30.591: INFO: Resource quota "e2e-rq-status-hz5rd" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-hz5rd" /status 11/15/23 06:06:30.591
STEP: Confirm /status for "e2e-rq-status-hz5rd" resourceQuota via watch 11/15/23 06:06:30.628
Nov 15 06:06:30.637: INFO: observed resourceQuota "e2e-rq-status-hz5rd" in namespace "resourcequota-4846" with hard status: v1.ResourceList(nil)
Nov 15 06:06:30.637: INFO: Found resourceQuota "e2e-rq-status-hz5rd" in namespace "resourcequota-4846" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Nov 15 06:06:30.638: INFO: ResourceQuota "e2e-rq-status-hz5rd" /status was updated
STEP: Patching hard spec values for cpu & memory 11/15/23 06:06:30.655
Nov 15 06:06:30.680: INFO: Resource quota "e2e-rq-status-hz5rd" reports spec: hard cpu limit of 1
Nov 15 06:06:30.680: INFO: Resource quota "e2e-rq-status-hz5rd" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-hz5rd" /status 11/15/23 06:06:30.68
STEP: Confirm /status for "e2e-rq-status-hz5rd" resourceQuota via watch 11/15/23 06:06:30.705
Nov 15 06:06:30.710: INFO: observed resourceQuota "e2e-rq-status-hz5rd" in namespace "resourcequota-4846" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Nov 15 06:06:30.711: INFO: Found resourceQuota "e2e-rq-status-hz5rd" in namespace "resourcequota-4846" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Nov 15 06:06:30.711: INFO: ResourceQuota "e2e-rq-status-hz5rd" /status was patched
STEP: Get "e2e-rq-status-hz5rd" /status 11/15/23 06:06:30.711
Nov 15 06:06:30.732: INFO: Resourcequota "e2e-rq-status-hz5rd" reports status: hard cpu of 1
Nov 15 06:06:30.732: INFO: Resourcequota "e2e-rq-status-hz5rd" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-hz5rd" /status before checking Spec is unchanged 11/15/23 06:06:30.748
Nov 15 06:06:30.769: INFO: Resourcequota "e2e-rq-status-hz5rd" reports status: hard cpu of 2
Nov 15 06:06:30.769: INFO: Resourcequota "e2e-rq-status-hz5rd" reports status: hard memory of 2Gi
Nov 15 06:06:30.774: INFO: Found resourceQuota "e2e-rq-status-hz5rd" in namespace "resourcequota-4846" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Nov 15 06:09:35.804: INFO: ResourceQuota "e2e-rq-status-hz5rd" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 06:09:35.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4846" for this suite. 11/15/23 06:09:35.832
------------------------------
â€¢ [SLOW TEST] [185.440 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:06:30.415
    Nov 15 06:06:30.415: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename resourcequota 11/15/23 06:06:30.416
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:06:30.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:06:30.512
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-hz5rd" 11/15/23 06:06:30.548
    Nov 15 06:06:30.591: INFO: Resource quota "e2e-rq-status-hz5rd" reports spec: hard cpu limit of 500m
    Nov 15 06:06:30.591: INFO: Resource quota "e2e-rq-status-hz5rd" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-hz5rd" /status 11/15/23 06:06:30.591
    STEP: Confirm /status for "e2e-rq-status-hz5rd" resourceQuota via watch 11/15/23 06:06:30.628
    Nov 15 06:06:30.637: INFO: observed resourceQuota "e2e-rq-status-hz5rd" in namespace "resourcequota-4846" with hard status: v1.ResourceList(nil)
    Nov 15 06:06:30.637: INFO: Found resourceQuota "e2e-rq-status-hz5rd" in namespace "resourcequota-4846" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Nov 15 06:06:30.638: INFO: ResourceQuota "e2e-rq-status-hz5rd" /status was updated
    STEP: Patching hard spec values for cpu & memory 11/15/23 06:06:30.655
    Nov 15 06:06:30.680: INFO: Resource quota "e2e-rq-status-hz5rd" reports spec: hard cpu limit of 1
    Nov 15 06:06:30.680: INFO: Resource quota "e2e-rq-status-hz5rd" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-hz5rd" /status 11/15/23 06:06:30.68
    STEP: Confirm /status for "e2e-rq-status-hz5rd" resourceQuota via watch 11/15/23 06:06:30.705
    Nov 15 06:06:30.710: INFO: observed resourceQuota "e2e-rq-status-hz5rd" in namespace "resourcequota-4846" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Nov 15 06:06:30.711: INFO: Found resourceQuota "e2e-rq-status-hz5rd" in namespace "resourcequota-4846" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Nov 15 06:06:30.711: INFO: ResourceQuota "e2e-rq-status-hz5rd" /status was patched
    STEP: Get "e2e-rq-status-hz5rd" /status 11/15/23 06:06:30.711
    Nov 15 06:06:30.732: INFO: Resourcequota "e2e-rq-status-hz5rd" reports status: hard cpu of 1
    Nov 15 06:06:30.732: INFO: Resourcequota "e2e-rq-status-hz5rd" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-hz5rd" /status before checking Spec is unchanged 11/15/23 06:06:30.748
    Nov 15 06:06:30.769: INFO: Resourcequota "e2e-rq-status-hz5rd" reports status: hard cpu of 2
    Nov 15 06:06:30.769: INFO: Resourcequota "e2e-rq-status-hz5rd" reports status: hard memory of 2Gi
    Nov 15 06:06:30.774: INFO: Found resourceQuota "e2e-rq-status-hz5rd" in namespace "resourcequota-4846" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Nov 15 06:09:35.804: INFO: ResourceQuota "e2e-rq-status-hz5rd" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:09:35.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4846" for this suite. 11/15/23 06:09:35.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:09:35.857
Nov 15 06:09:35.857: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename dns 11/15/23 06:09:35.858
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:09:35.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:09:35.93
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 11/15/23 06:09:35.939
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 11/15/23 06:09:35.939
STEP: creating a pod to probe DNS 11/15/23 06:09:35.939
STEP: submitting the pod to kubernetes 11/15/23 06:09:35.939
Nov 15 06:09:35.986: INFO: Waiting up to 15m0s for pod "dns-test-54d34207-2c6f-4bd0-b017-3ff5ad4fc3cf" in namespace "dns-8957" to be "running"
Nov 15 06:09:36.025: INFO: Pod "dns-test-54d34207-2c6f-4bd0-b017-3ff5ad4fc3cf": Phase="Pending", Reason="", readiness=false. Elapsed: 38.816105ms
Nov 15 06:09:38.042: INFO: Pod "dns-test-54d34207-2c6f-4bd0-b017-3ff5ad4fc3cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056018217s
Nov 15 06:09:40.078: INFO: Pod "dns-test-54d34207-2c6f-4bd0-b017-3ff5ad4fc3cf": Phase="Running", Reason="", readiness=true. Elapsed: 4.092028073s
Nov 15 06:09:40.078: INFO: Pod "dns-test-54d34207-2c6f-4bd0-b017-3ff5ad4fc3cf" satisfied condition "running"
STEP: retrieving the pod 11/15/23 06:09:40.078
STEP: looking for the results for each expected name from probers 11/15/23 06:09:40.121
Nov 15 06:09:40.240: INFO: DNS probes using dns-8957/dns-test-54d34207-2c6f-4bd0-b017-3ff5ad4fc3cf succeeded

STEP: deleting the pod 11/15/23 06:09:40.24
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 15 06:09:40.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8957" for this suite. 11/15/23 06:09:40.315
------------------------------
â€¢ [4.483 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:09:35.857
    Nov 15 06:09:35.857: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename dns 11/15/23 06:09:35.858
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:09:35.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:09:35.93
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     11/15/23 06:09:35.939
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     11/15/23 06:09:35.939
    STEP: creating a pod to probe DNS 11/15/23 06:09:35.939
    STEP: submitting the pod to kubernetes 11/15/23 06:09:35.939
    Nov 15 06:09:35.986: INFO: Waiting up to 15m0s for pod "dns-test-54d34207-2c6f-4bd0-b017-3ff5ad4fc3cf" in namespace "dns-8957" to be "running"
    Nov 15 06:09:36.025: INFO: Pod "dns-test-54d34207-2c6f-4bd0-b017-3ff5ad4fc3cf": Phase="Pending", Reason="", readiness=false. Elapsed: 38.816105ms
    Nov 15 06:09:38.042: INFO: Pod "dns-test-54d34207-2c6f-4bd0-b017-3ff5ad4fc3cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056018217s
    Nov 15 06:09:40.078: INFO: Pod "dns-test-54d34207-2c6f-4bd0-b017-3ff5ad4fc3cf": Phase="Running", Reason="", readiness=true. Elapsed: 4.092028073s
    Nov 15 06:09:40.078: INFO: Pod "dns-test-54d34207-2c6f-4bd0-b017-3ff5ad4fc3cf" satisfied condition "running"
    STEP: retrieving the pod 11/15/23 06:09:40.078
    STEP: looking for the results for each expected name from probers 11/15/23 06:09:40.121
    Nov 15 06:09:40.240: INFO: DNS probes using dns-8957/dns-test-54d34207-2c6f-4bd0-b017-3ff5ad4fc3cf succeeded

    STEP: deleting the pod 11/15/23 06:09:40.24
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:09:40.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8957" for this suite. 11/15/23 06:09:40.315
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:09:40.341
Nov 15 06:09:40.341: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename watch 11/15/23 06:09:40.342
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:09:40.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:09:40.423
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 11/15/23 06:09:40.433
STEP: creating a new configmap 11/15/23 06:09:40.437
STEP: modifying the configmap once 11/15/23 06:09:40.459
STEP: closing the watch once it receives two notifications 11/15/23 06:09:40.505
Nov 15 06:09:40.505: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8798  927bb724-4299-4f41-8619-346b3c768f29 76292 0 2023-11-15 06:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-15 06:09:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 06:09:40.506: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8798  927bb724-4299-4f41-8619-346b3c768f29 76299 0 2023-11-15 06:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-15 06:09:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 11/15/23 06:09:40.506
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 11/15/23 06:09:40.553
STEP: deleting the configmap 11/15/23 06:09:40.559
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 11/15/23 06:09:40.59
Nov 15 06:09:40.590: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8798  927bb724-4299-4f41-8619-346b3c768f29 76303 0 2023-11-15 06:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-15 06:09:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 06:09:40.590: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8798  927bb724-4299-4f41-8619-346b3c768f29 76304 0 2023-11-15 06:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-15 06:09:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Nov 15 06:09:40.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8798" for this suite. 11/15/23 06:09:40.613
------------------------------
â€¢ [0.298 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:09:40.341
    Nov 15 06:09:40.341: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename watch 11/15/23 06:09:40.342
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:09:40.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:09:40.423
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 11/15/23 06:09:40.433
    STEP: creating a new configmap 11/15/23 06:09:40.437
    STEP: modifying the configmap once 11/15/23 06:09:40.459
    STEP: closing the watch once it receives two notifications 11/15/23 06:09:40.505
    Nov 15 06:09:40.505: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8798  927bb724-4299-4f41-8619-346b3c768f29 76292 0 2023-11-15 06:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-15 06:09:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 06:09:40.506: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8798  927bb724-4299-4f41-8619-346b3c768f29 76299 0 2023-11-15 06:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-15 06:09:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 11/15/23 06:09:40.506
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 11/15/23 06:09:40.553
    STEP: deleting the configmap 11/15/23 06:09:40.559
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 11/15/23 06:09:40.59
    Nov 15 06:09:40.590: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8798  927bb724-4299-4f41-8619-346b3c768f29 76303 0 2023-11-15 06:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-15 06:09:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 06:09:40.590: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8798  927bb724-4299-4f41-8619-346b3c768f29 76304 0 2023-11-15 06:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-15 06:09:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:09:40.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8798" for this suite. 11/15/23 06:09:40.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:09:40.644
Nov 15 06:09:40.644: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename subpath 11/15/23 06:09:40.645
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:09:40.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:09:40.73
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/15/23 06:09:40.742
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-mfd4 11/15/23 06:09:40.782
STEP: Creating a pod to test atomic-volume-subpath 11/15/23 06:09:40.782
Nov 15 06:09:40.818: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mfd4" in namespace "subpath-3453" to be "Succeeded or Failed"
Nov 15 06:09:40.837: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.449417ms
Nov 15 06:09:42.851: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03291538s
Nov 15 06:09:44.854: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=true. Elapsed: 4.035516284s
Nov 15 06:09:46.853: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=true. Elapsed: 6.034304407s
Nov 15 06:09:48.857: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=true. Elapsed: 8.038228083s
Nov 15 06:09:50.855: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=true. Elapsed: 10.036839559s
Nov 15 06:09:52.852: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=true. Elapsed: 12.03403397s
Nov 15 06:09:54.852: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=true. Elapsed: 14.033519662s
Nov 15 06:09:56.853: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=true. Elapsed: 16.0345288s
Nov 15 06:09:58.854: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=true. Elapsed: 18.035747765s
Nov 15 06:10:00.854: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=true. Elapsed: 20.035587482s
Nov 15 06:10:02.853: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=false. Elapsed: 22.034064313s
Nov 15 06:10:04.857: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.038558671s
STEP: Saw pod success 11/15/23 06:10:04.857
Nov 15 06:10:04.857: INFO: Pod "pod-subpath-test-downwardapi-mfd4" satisfied condition "Succeeded or Failed"
Nov 15 06:10:04.875: INFO: Trying to get logs from node 10.72.152.86 pod pod-subpath-test-downwardapi-mfd4 container test-container-subpath-downwardapi-mfd4: <nil>
STEP: delete the pod 11/15/23 06:10:04.989
Nov 15 06:10:05.054: INFO: Waiting for pod pod-subpath-test-downwardapi-mfd4 to disappear
Nov 15 06:10:05.070: INFO: Pod pod-subpath-test-downwardapi-mfd4 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-mfd4 11/15/23 06:10:05.07
Nov 15 06:10:05.070: INFO: Deleting pod "pod-subpath-test-downwardapi-mfd4" in namespace "subpath-3453"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Nov 15 06:10:05.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3453" for this suite. 11/15/23 06:10:05.131
------------------------------
â€¢ [SLOW TEST] [24.525 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:09:40.644
    Nov 15 06:09:40.644: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename subpath 11/15/23 06:09:40.645
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:09:40.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:09:40.73
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/15/23 06:09:40.742
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-mfd4 11/15/23 06:09:40.782
    STEP: Creating a pod to test atomic-volume-subpath 11/15/23 06:09:40.782
    Nov 15 06:09:40.818: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mfd4" in namespace "subpath-3453" to be "Succeeded or Failed"
    Nov 15 06:09:40.837: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.449417ms
    Nov 15 06:09:42.851: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03291538s
    Nov 15 06:09:44.854: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=true. Elapsed: 4.035516284s
    Nov 15 06:09:46.853: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=true. Elapsed: 6.034304407s
    Nov 15 06:09:48.857: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=true. Elapsed: 8.038228083s
    Nov 15 06:09:50.855: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=true. Elapsed: 10.036839559s
    Nov 15 06:09:52.852: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=true. Elapsed: 12.03403397s
    Nov 15 06:09:54.852: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=true. Elapsed: 14.033519662s
    Nov 15 06:09:56.853: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=true. Elapsed: 16.0345288s
    Nov 15 06:09:58.854: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=true. Elapsed: 18.035747765s
    Nov 15 06:10:00.854: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=true. Elapsed: 20.035587482s
    Nov 15 06:10:02.853: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Running", Reason="", readiness=false. Elapsed: 22.034064313s
    Nov 15 06:10:04.857: INFO: Pod "pod-subpath-test-downwardapi-mfd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.038558671s
    STEP: Saw pod success 11/15/23 06:10:04.857
    Nov 15 06:10:04.857: INFO: Pod "pod-subpath-test-downwardapi-mfd4" satisfied condition "Succeeded or Failed"
    Nov 15 06:10:04.875: INFO: Trying to get logs from node 10.72.152.86 pod pod-subpath-test-downwardapi-mfd4 container test-container-subpath-downwardapi-mfd4: <nil>
    STEP: delete the pod 11/15/23 06:10:04.989
    Nov 15 06:10:05.054: INFO: Waiting for pod pod-subpath-test-downwardapi-mfd4 to disappear
    Nov 15 06:10:05.070: INFO: Pod pod-subpath-test-downwardapi-mfd4 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-mfd4 11/15/23 06:10:05.07
    Nov 15 06:10:05.070: INFO: Deleting pod "pod-subpath-test-downwardapi-mfd4" in namespace "subpath-3453"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:10:05.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3453" for this suite. 11/15/23 06:10:05.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:10:05.18
Nov 15 06:10:05.180: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename containers 11/15/23 06:10:05.181
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:05.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:05.284
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Nov 15 06:10:05.332: INFO: Waiting up to 5m0s for pod "client-containers-3a9d7ba0-8e4f-4d4b-a0ed-fb3b37f733ad" in namespace "containers-6910" to be "running"
Nov 15 06:10:05.349: INFO: Pod "client-containers-3a9d7ba0-8e4f-4d4b-a0ed-fb3b37f733ad": Phase="Pending", Reason="", readiness=false. Elapsed: 16.620698ms
Nov 15 06:10:07.386: INFO: Pod "client-containers-3a9d7ba0-8e4f-4d4b-a0ed-fb3b37f733ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053929758s
Nov 15 06:10:09.368: INFO: Pod "client-containers-3a9d7ba0-8e4f-4d4b-a0ed-fb3b37f733ad": Phase="Running", Reason="", readiness=true. Elapsed: 4.035322497s
Nov 15 06:10:09.368: INFO: Pod "client-containers-3a9d7ba0-8e4f-4d4b-a0ed-fb3b37f733ad" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Nov 15 06:10:09.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6910" for this suite. 11/15/23 06:10:09.427
------------------------------
â€¢ [4.270 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:10:05.18
    Nov 15 06:10:05.180: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename containers 11/15/23 06:10:05.181
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:05.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:05.284
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Nov 15 06:10:05.332: INFO: Waiting up to 5m0s for pod "client-containers-3a9d7ba0-8e4f-4d4b-a0ed-fb3b37f733ad" in namespace "containers-6910" to be "running"
    Nov 15 06:10:05.349: INFO: Pod "client-containers-3a9d7ba0-8e4f-4d4b-a0ed-fb3b37f733ad": Phase="Pending", Reason="", readiness=false. Elapsed: 16.620698ms
    Nov 15 06:10:07.386: INFO: Pod "client-containers-3a9d7ba0-8e4f-4d4b-a0ed-fb3b37f733ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053929758s
    Nov 15 06:10:09.368: INFO: Pod "client-containers-3a9d7ba0-8e4f-4d4b-a0ed-fb3b37f733ad": Phase="Running", Reason="", readiness=true. Elapsed: 4.035322497s
    Nov 15 06:10:09.368: INFO: Pod "client-containers-3a9d7ba0-8e4f-4d4b-a0ed-fb3b37f733ad" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:10:09.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6910" for this suite. 11/15/23 06:10:09.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:10:09.453
Nov 15 06:10:09.453: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubelet-test 11/15/23 06:10:09.454
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:09.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:09.557
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
W1115 06:10:09.612146      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "bin-falsed945dbed-f2ae-4cdc-b14e-dd54285a4b12" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "bin-falsed945dbed-f2ae-4cdc-b14e-dd54285a4b12" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "bin-falsed945dbed-f2ae-4cdc-b14e-dd54285a4b12" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "bin-falsed945dbed-f2ae-4cdc-b14e-dd54285a4b12" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Nov 15 06:10:13.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2805" for this suite. 11/15/23 06:10:13.666
------------------------------
â€¢ [4.237 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:10:09.453
    Nov 15 06:10:09.453: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubelet-test 11/15/23 06:10:09.454
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:09.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:09.557
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    W1115 06:10:09.612146      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "bin-falsed945dbed-f2ae-4cdc-b14e-dd54285a4b12" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "bin-falsed945dbed-f2ae-4cdc-b14e-dd54285a4b12" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "bin-falsed945dbed-f2ae-4cdc-b14e-dd54285a4b12" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "bin-falsed945dbed-f2ae-4cdc-b14e-dd54285a4b12" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:10:13.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2805" for this suite. 11/15/23 06:10:13.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:10:13.694
Nov 15 06:10:13.694: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 06:10:13.694
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:13.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:13.769
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Nov 15 06:10:13.780: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:10:14.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6266" for this suite. 11/15/23 06:10:14.503
------------------------------
â€¢ [0.856 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:10:13.694
    Nov 15 06:10:13.694: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 06:10:13.694
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:13.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:13.769
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Nov 15 06:10:13.780: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:10:14.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6266" for this suite. 11/15/23 06:10:14.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:10:14.551
Nov 15 06:10:14.551: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename security-context-test 11/15/23 06:10:14.555
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:14.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:14.699
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
W1115 06:10:14.783562      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-user-65534-63e25f2b-fe0b-4ad9-b5b0-af135677e11b" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-user-65534-63e25f2b-fe0b-4ad9-b5b0-af135677e11b" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "busybox-user-65534-63e25f2b-fe0b-4ad9-b5b0-af135677e11b" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Nov 15 06:10:14.784: INFO: Waiting up to 5m0s for pod "busybox-user-65534-63e25f2b-fe0b-4ad9-b5b0-af135677e11b" in namespace "security-context-test-217" to be "Succeeded or Failed"
Nov 15 06:10:14.816: INFO: Pod "busybox-user-65534-63e25f2b-fe0b-4ad9-b5b0-af135677e11b": Phase="Pending", Reason="", readiness=false. Elapsed: 32.3016ms
Nov 15 06:10:16.835: INFO: Pod "busybox-user-65534-63e25f2b-fe0b-4ad9-b5b0-af135677e11b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051353709s
Nov 15 06:10:18.835: INFO: Pod "busybox-user-65534-63e25f2b-fe0b-4ad9-b5b0-af135677e11b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050613775s
Nov 15 06:10:18.835: INFO: Pod "busybox-user-65534-63e25f2b-fe0b-4ad9-b5b0-af135677e11b" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Nov 15 06:10:18.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-217" for this suite. 11/15/23 06:10:18.866
------------------------------
â€¢ [4.378 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:10:14.551
    Nov 15 06:10:14.551: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename security-context-test 11/15/23 06:10:14.555
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:14.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:14.699
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    W1115 06:10:14.783562      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-user-65534-63e25f2b-fe0b-4ad9-b5b0-af135677e11b" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-user-65534-63e25f2b-fe0b-4ad9-b5b0-af135677e11b" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "busybox-user-65534-63e25f2b-fe0b-4ad9-b5b0-af135677e11b" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Nov 15 06:10:14.784: INFO: Waiting up to 5m0s for pod "busybox-user-65534-63e25f2b-fe0b-4ad9-b5b0-af135677e11b" in namespace "security-context-test-217" to be "Succeeded or Failed"
    Nov 15 06:10:14.816: INFO: Pod "busybox-user-65534-63e25f2b-fe0b-4ad9-b5b0-af135677e11b": Phase="Pending", Reason="", readiness=false. Elapsed: 32.3016ms
    Nov 15 06:10:16.835: INFO: Pod "busybox-user-65534-63e25f2b-fe0b-4ad9-b5b0-af135677e11b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051353709s
    Nov 15 06:10:18.835: INFO: Pod "busybox-user-65534-63e25f2b-fe0b-4ad9-b5b0-af135677e11b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050613775s
    Nov 15 06:10:18.835: INFO: Pod "busybox-user-65534-63e25f2b-fe0b-4ad9-b5b0-af135677e11b" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:10:18.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-217" for this suite. 11/15/23 06:10:18.866
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:10:18.929
Nov 15 06:10:18.929: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 06:10:18.931
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:18.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:19.013
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-15e9ce24-9621-4ce3-9282-8f68cc66e631 11/15/23 06:10:19.022
STEP: Creating a pod to test consume configMaps 11/15/23 06:10:19.04
Nov 15 06:10:19.077: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-149d1ba1-5dbc-4592-99f5-1439c64c46c6" in namespace "projected-6310" to be "Succeeded or Failed"
Nov 15 06:10:19.099: INFO: Pod "pod-projected-configmaps-149d1ba1-5dbc-4592-99f5-1439c64c46c6": Phase="Pending", Reason="", readiness=false. Elapsed: 22.180577ms
Nov 15 06:10:21.114: INFO: Pod "pod-projected-configmaps-149d1ba1-5dbc-4592-99f5-1439c64c46c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0367951s
Nov 15 06:10:23.115: INFO: Pod "pod-projected-configmaps-149d1ba1-5dbc-4592-99f5-1439c64c46c6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037380005s
Nov 15 06:10:25.115: INFO: Pod "pod-projected-configmaps-149d1ba1-5dbc-4592-99f5-1439c64c46c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037547135s
STEP: Saw pod success 11/15/23 06:10:25.115
Nov 15 06:10:25.115: INFO: Pod "pod-projected-configmaps-149d1ba1-5dbc-4592-99f5-1439c64c46c6" satisfied condition "Succeeded or Failed"
Nov 15 06:10:25.130: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-configmaps-149d1ba1-5dbc-4592-99f5-1439c64c46c6 container projected-configmap-volume-test: <nil>
STEP: delete the pod 11/15/23 06:10:25.163
Nov 15 06:10:25.229: INFO: Waiting for pod pod-projected-configmaps-149d1ba1-5dbc-4592-99f5-1439c64c46c6 to disappear
Nov 15 06:10:25.243: INFO: Pod pod-projected-configmaps-149d1ba1-5dbc-4592-99f5-1439c64c46c6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:10:25.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6310" for this suite. 11/15/23 06:10:25.263
------------------------------
â€¢ [SLOW TEST] [6.356 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:10:18.929
    Nov 15 06:10:18.929: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 06:10:18.931
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:18.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:19.013
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-15e9ce24-9621-4ce3-9282-8f68cc66e631 11/15/23 06:10:19.022
    STEP: Creating a pod to test consume configMaps 11/15/23 06:10:19.04
    Nov 15 06:10:19.077: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-149d1ba1-5dbc-4592-99f5-1439c64c46c6" in namespace "projected-6310" to be "Succeeded or Failed"
    Nov 15 06:10:19.099: INFO: Pod "pod-projected-configmaps-149d1ba1-5dbc-4592-99f5-1439c64c46c6": Phase="Pending", Reason="", readiness=false. Elapsed: 22.180577ms
    Nov 15 06:10:21.114: INFO: Pod "pod-projected-configmaps-149d1ba1-5dbc-4592-99f5-1439c64c46c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0367951s
    Nov 15 06:10:23.115: INFO: Pod "pod-projected-configmaps-149d1ba1-5dbc-4592-99f5-1439c64c46c6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037380005s
    Nov 15 06:10:25.115: INFO: Pod "pod-projected-configmaps-149d1ba1-5dbc-4592-99f5-1439c64c46c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037547135s
    STEP: Saw pod success 11/15/23 06:10:25.115
    Nov 15 06:10:25.115: INFO: Pod "pod-projected-configmaps-149d1ba1-5dbc-4592-99f5-1439c64c46c6" satisfied condition "Succeeded or Failed"
    Nov 15 06:10:25.130: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-configmaps-149d1ba1-5dbc-4592-99f5-1439c64c46c6 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 11/15/23 06:10:25.163
    Nov 15 06:10:25.229: INFO: Waiting for pod pod-projected-configmaps-149d1ba1-5dbc-4592-99f5-1439c64c46c6 to disappear
    Nov 15 06:10:25.243: INFO: Pod pod-projected-configmaps-149d1ba1-5dbc-4592-99f5-1439c64c46c6 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:10:25.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6310" for this suite. 11/15/23 06:10:25.263
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:10:25.285
Nov 15 06:10:25.285: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename runtimeclass 11/15/23 06:10:25.286
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:25.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:25.359
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 11/15/23 06:10:25.37
STEP: getting /apis/node.k8s.io 11/15/23 06:10:25.379
STEP: getting /apis/node.k8s.io/v1 11/15/23 06:10:25.383
STEP: creating 11/15/23 06:10:25.388
STEP: watching 11/15/23 06:10:25.465
Nov 15 06:10:25.465: INFO: starting watch
STEP: getting 11/15/23 06:10:25.486
STEP: listing 11/15/23 06:10:25.498
STEP: patching 11/15/23 06:10:25.509
STEP: updating 11/15/23 06:10:25.524
Nov 15 06:10:25.543: INFO: waiting for watch events with expected annotations
STEP: deleting 11/15/23 06:10:25.543
STEP: deleting a collection 11/15/23 06:10:25.608
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Nov 15 06:10:25.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2757" for this suite. 11/15/23 06:10:25.729
------------------------------
â€¢ [0.471 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:10:25.285
    Nov 15 06:10:25.285: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename runtimeclass 11/15/23 06:10:25.286
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:25.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:25.359
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 11/15/23 06:10:25.37
    STEP: getting /apis/node.k8s.io 11/15/23 06:10:25.379
    STEP: getting /apis/node.k8s.io/v1 11/15/23 06:10:25.383
    STEP: creating 11/15/23 06:10:25.388
    STEP: watching 11/15/23 06:10:25.465
    Nov 15 06:10:25.465: INFO: starting watch
    STEP: getting 11/15/23 06:10:25.486
    STEP: listing 11/15/23 06:10:25.498
    STEP: patching 11/15/23 06:10:25.509
    STEP: updating 11/15/23 06:10:25.524
    Nov 15 06:10:25.543: INFO: waiting for watch events with expected annotations
    STEP: deleting 11/15/23 06:10:25.543
    STEP: deleting a collection 11/15/23 06:10:25.608
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:10:25.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2757" for this suite. 11/15/23 06:10:25.729
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:10:25.762
Nov 15 06:10:25.762: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 06:10:25.763
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:25.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:25.839
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 11/15/23 06:10:25.865
Nov 15 06:10:25.866: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: rename a version 11/15/23 06:10:33.891
STEP: check the new version name is served 11/15/23 06:10:33.947
STEP: check the old version name is removed 11/15/23 06:10:38.508
STEP: check the other version is not changed 11/15/23 06:10:39.699
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:10:46.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4678" for this suite. 11/15/23 06:10:46.708
------------------------------
â€¢ [SLOW TEST] [20.977 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:10:25.762
    Nov 15 06:10:25.762: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 06:10:25.763
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:25.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:25.839
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 11/15/23 06:10:25.865
    Nov 15 06:10:25.866: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: rename a version 11/15/23 06:10:33.891
    STEP: check the new version name is served 11/15/23 06:10:33.947
    STEP: check the old version name is removed 11/15/23 06:10:38.508
    STEP: check the other version is not changed 11/15/23 06:10:39.699
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:10:46.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4678" for this suite. 11/15/23 06:10:46.708
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:10:46.739
Nov 15 06:10:46.739: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 06:10:46.74
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:46.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:46.814
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-df86d826-55e9-414b-a4ef-0dfa00b7e1cb 11/15/23 06:10:46.828
STEP: Creating a pod to test consume configMaps 11/15/23 06:10:46.847
Nov 15 06:10:46.888: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6481fcfd-daaa-46ee-9696-acc7f66c3d7b" in namespace "projected-3711" to be "Succeeded or Failed"
Nov 15 06:10:46.912: INFO: Pod "pod-projected-configmaps-6481fcfd-daaa-46ee-9696-acc7f66c3d7b": Phase="Pending", Reason="", readiness=false. Elapsed: 23.533212ms
Nov 15 06:10:48.934: INFO: Pod "pod-projected-configmaps-6481fcfd-daaa-46ee-9696-acc7f66c3d7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045443087s
Nov 15 06:10:50.931: INFO: Pod "pod-projected-configmaps-6481fcfd-daaa-46ee-9696-acc7f66c3d7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04223221s
STEP: Saw pod success 11/15/23 06:10:50.931
Nov 15 06:10:50.931: INFO: Pod "pod-projected-configmaps-6481fcfd-daaa-46ee-9696-acc7f66c3d7b" satisfied condition "Succeeded or Failed"
Nov 15 06:10:50.949: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-configmaps-6481fcfd-daaa-46ee-9696-acc7f66c3d7b container agnhost-container: <nil>
STEP: delete the pod 11/15/23 06:10:51.021
Nov 15 06:10:51.074: INFO: Waiting for pod pod-projected-configmaps-6481fcfd-daaa-46ee-9696-acc7f66c3d7b to disappear
Nov 15 06:10:51.096: INFO: Pod pod-projected-configmaps-6481fcfd-daaa-46ee-9696-acc7f66c3d7b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:10:51.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3711" for this suite. 11/15/23 06:10:51.125
------------------------------
â€¢ [4.412 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:10:46.739
    Nov 15 06:10:46.739: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 06:10:46.74
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:46.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:46.814
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-df86d826-55e9-414b-a4ef-0dfa00b7e1cb 11/15/23 06:10:46.828
    STEP: Creating a pod to test consume configMaps 11/15/23 06:10:46.847
    Nov 15 06:10:46.888: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6481fcfd-daaa-46ee-9696-acc7f66c3d7b" in namespace "projected-3711" to be "Succeeded or Failed"
    Nov 15 06:10:46.912: INFO: Pod "pod-projected-configmaps-6481fcfd-daaa-46ee-9696-acc7f66c3d7b": Phase="Pending", Reason="", readiness=false. Elapsed: 23.533212ms
    Nov 15 06:10:48.934: INFO: Pod "pod-projected-configmaps-6481fcfd-daaa-46ee-9696-acc7f66c3d7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045443087s
    Nov 15 06:10:50.931: INFO: Pod "pod-projected-configmaps-6481fcfd-daaa-46ee-9696-acc7f66c3d7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04223221s
    STEP: Saw pod success 11/15/23 06:10:50.931
    Nov 15 06:10:50.931: INFO: Pod "pod-projected-configmaps-6481fcfd-daaa-46ee-9696-acc7f66c3d7b" satisfied condition "Succeeded or Failed"
    Nov 15 06:10:50.949: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-configmaps-6481fcfd-daaa-46ee-9696-acc7f66c3d7b container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 06:10:51.021
    Nov 15 06:10:51.074: INFO: Waiting for pod pod-projected-configmaps-6481fcfd-daaa-46ee-9696-acc7f66c3d7b to disappear
    Nov 15 06:10:51.096: INFO: Pod pod-projected-configmaps-6481fcfd-daaa-46ee-9696-acc7f66c3d7b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:10:51.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3711" for this suite. 11/15/23 06:10:51.125
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:10:51.152
Nov 15 06:10:51.152: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename disruption 11/15/23 06:10:51.153
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:51.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:51.221
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 11/15/23 06:10:51.254
STEP: Updating PodDisruptionBudget status 11/15/23 06:10:53.291
STEP: Waiting for all pods to be running 11/15/23 06:10:53.327
Nov 15 06:10:53.346: INFO: running pods: 0 < 1
Nov 15 06:10:55.366: INFO: running pods: 0 < 1
STEP: locating a running pod 11/15/23 06:10:57.377
STEP: Waiting for the pdb to be processed 11/15/23 06:10:57.439
STEP: Patching PodDisruptionBudget status 11/15/23 06:10:57.469
STEP: Waiting for the pdb to be processed 11/15/23 06:10:57.504
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Nov 15 06:10:57.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6107" for this suite. 11/15/23 06:10:57.551
------------------------------
â€¢ [SLOW TEST] [6.424 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:10:51.152
    Nov 15 06:10:51.152: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename disruption 11/15/23 06:10:51.153
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:51.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:51.221
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 11/15/23 06:10:51.254
    STEP: Updating PodDisruptionBudget status 11/15/23 06:10:53.291
    STEP: Waiting for all pods to be running 11/15/23 06:10:53.327
    Nov 15 06:10:53.346: INFO: running pods: 0 < 1
    Nov 15 06:10:55.366: INFO: running pods: 0 < 1
    STEP: locating a running pod 11/15/23 06:10:57.377
    STEP: Waiting for the pdb to be processed 11/15/23 06:10:57.439
    STEP: Patching PodDisruptionBudget status 11/15/23 06:10:57.469
    STEP: Waiting for the pdb to be processed 11/15/23 06:10:57.504
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:10:57.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6107" for this suite. 11/15/23 06:10:57.551
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:10:57.577
Nov 15 06:10:57.577: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename container-probe 11/15/23 06:10:57.578
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:57.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:57.641
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-1d755003-7f43-4736-8ed5-6830d999d610 in namespace container-probe-6578 11/15/23 06:10:57.66
Nov 15 06:10:57.709: INFO: Waiting up to 5m0s for pod "liveness-1d755003-7f43-4736-8ed5-6830d999d610" in namespace "container-probe-6578" to be "not pending"
Nov 15 06:10:57.743: INFO: Pod "liveness-1d755003-7f43-4736-8ed5-6830d999d610": Phase="Pending", Reason="", readiness=false. Elapsed: 34.34557ms
Nov 15 06:10:59.764: INFO: Pod "liveness-1d755003-7f43-4736-8ed5-6830d999d610": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054661457s
Nov 15 06:11:01.763: INFO: Pod "liveness-1d755003-7f43-4736-8ed5-6830d999d610": Phase="Running", Reason="", readiness=true. Elapsed: 4.054300036s
Nov 15 06:11:01.764: INFO: Pod "liveness-1d755003-7f43-4736-8ed5-6830d999d610" satisfied condition "not pending"
Nov 15 06:11:01.764: INFO: Started pod liveness-1d755003-7f43-4736-8ed5-6830d999d610 in namespace container-probe-6578
STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 06:11:01.764
Nov 15 06:11:01.782: INFO: Initial restart count of pod liveness-1d755003-7f43-4736-8ed5-6830d999d610 is 0
Nov 15 06:11:19.993: INFO: Restart count of pod container-probe-6578/liveness-1d755003-7f43-4736-8ed5-6830d999d610 is now 1 (18.211016471s elapsed)
STEP: deleting the pod 11/15/23 06:11:19.993
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 15 06:11:20.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6578" for this suite. 11/15/23 06:11:20.116
------------------------------
â€¢ [SLOW TEST] [22.573 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:10:57.577
    Nov 15 06:10:57.577: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename container-probe 11/15/23 06:10:57.578
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:10:57.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:10:57.641
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-1d755003-7f43-4736-8ed5-6830d999d610 in namespace container-probe-6578 11/15/23 06:10:57.66
    Nov 15 06:10:57.709: INFO: Waiting up to 5m0s for pod "liveness-1d755003-7f43-4736-8ed5-6830d999d610" in namespace "container-probe-6578" to be "not pending"
    Nov 15 06:10:57.743: INFO: Pod "liveness-1d755003-7f43-4736-8ed5-6830d999d610": Phase="Pending", Reason="", readiness=false. Elapsed: 34.34557ms
    Nov 15 06:10:59.764: INFO: Pod "liveness-1d755003-7f43-4736-8ed5-6830d999d610": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054661457s
    Nov 15 06:11:01.763: INFO: Pod "liveness-1d755003-7f43-4736-8ed5-6830d999d610": Phase="Running", Reason="", readiness=true. Elapsed: 4.054300036s
    Nov 15 06:11:01.764: INFO: Pod "liveness-1d755003-7f43-4736-8ed5-6830d999d610" satisfied condition "not pending"
    Nov 15 06:11:01.764: INFO: Started pod liveness-1d755003-7f43-4736-8ed5-6830d999d610 in namespace container-probe-6578
    STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 06:11:01.764
    Nov 15 06:11:01.782: INFO: Initial restart count of pod liveness-1d755003-7f43-4736-8ed5-6830d999d610 is 0
    Nov 15 06:11:19.993: INFO: Restart count of pod container-probe-6578/liveness-1d755003-7f43-4736-8ed5-6830d999d610 is now 1 (18.211016471s elapsed)
    STEP: deleting the pod 11/15/23 06:11:19.993
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:11:20.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6578" for this suite. 11/15/23 06:11:20.116
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:11:20.151
Nov 15 06:11:20.151: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 06:11:20.152
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:20.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:20.219
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 11/15/23 06:11:20.233
Nov 15 06:11:20.266: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c16333d2-1db5-48c3-9ddc-69e09a7a3a01" in namespace "projected-4085" to be "Succeeded or Failed"
Nov 15 06:11:20.285: INFO: Pod "downwardapi-volume-c16333d2-1db5-48c3-9ddc-69e09a7a3a01": Phase="Pending", Reason="", readiness=false. Elapsed: 18.932935ms
Nov 15 06:11:22.304: INFO: Pod "downwardapi-volume-c16333d2-1db5-48c3-9ddc-69e09a7a3a01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038208801s
Nov 15 06:11:24.308: INFO: Pod "downwardapi-volume-c16333d2-1db5-48c3-9ddc-69e09a7a3a01": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042038474s
Nov 15 06:11:26.311: INFO: Pod "downwardapi-volume-c16333d2-1db5-48c3-9ddc-69e09a7a3a01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04505185s
STEP: Saw pod success 11/15/23 06:11:26.311
Nov 15 06:11:26.311: INFO: Pod "downwardapi-volume-c16333d2-1db5-48c3-9ddc-69e09a7a3a01" satisfied condition "Succeeded or Failed"
Nov 15 06:11:26.330: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-c16333d2-1db5-48c3-9ddc-69e09a7a3a01 container client-container: <nil>
STEP: delete the pod 11/15/23 06:11:26.377
Nov 15 06:11:26.436: INFO: Waiting for pod downwardapi-volume-c16333d2-1db5-48c3-9ddc-69e09a7a3a01 to disappear
Nov 15 06:11:26.455: INFO: Pod downwardapi-volume-c16333d2-1db5-48c3-9ddc-69e09a7a3a01 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 06:11:26.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4085" for this suite. 11/15/23 06:11:26.484
------------------------------
â€¢ [SLOW TEST] [6.360 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:11:20.151
    Nov 15 06:11:20.151: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 06:11:20.152
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:20.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:20.219
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 11/15/23 06:11:20.233
    Nov 15 06:11:20.266: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c16333d2-1db5-48c3-9ddc-69e09a7a3a01" in namespace "projected-4085" to be "Succeeded or Failed"
    Nov 15 06:11:20.285: INFO: Pod "downwardapi-volume-c16333d2-1db5-48c3-9ddc-69e09a7a3a01": Phase="Pending", Reason="", readiness=false. Elapsed: 18.932935ms
    Nov 15 06:11:22.304: INFO: Pod "downwardapi-volume-c16333d2-1db5-48c3-9ddc-69e09a7a3a01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038208801s
    Nov 15 06:11:24.308: INFO: Pod "downwardapi-volume-c16333d2-1db5-48c3-9ddc-69e09a7a3a01": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042038474s
    Nov 15 06:11:26.311: INFO: Pod "downwardapi-volume-c16333d2-1db5-48c3-9ddc-69e09a7a3a01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04505185s
    STEP: Saw pod success 11/15/23 06:11:26.311
    Nov 15 06:11:26.311: INFO: Pod "downwardapi-volume-c16333d2-1db5-48c3-9ddc-69e09a7a3a01" satisfied condition "Succeeded or Failed"
    Nov 15 06:11:26.330: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-c16333d2-1db5-48c3-9ddc-69e09a7a3a01 container client-container: <nil>
    STEP: delete the pod 11/15/23 06:11:26.377
    Nov 15 06:11:26.436: INFO: Waiting for pod downwardapi-volume-c16333d2-1db5-48c3-9ddc-69e09a7a3a01 to disappear
    Nov 15 06:11:26.455: INFO: Pod downwardapi-volume-c16333d2-1db5-48c3-9ddc-69e09a7a3a01 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:11:26.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4085" for this suite. 11/15/23 06:11:26.484
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:11:26.511
Nov 15 06:11:26.511: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename endpointslice 11/15/23 06:11:26.511
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:26.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:26.579
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 11/15/23 06:11:26.592
STEP: getting /apis/discovery.k8s.io 11/15/23 06:11:26.607
STEP: getting /apis/discovery.k8s.iov1 11/15/23 06:11:26.612
STEP: creating 11/15/23 06:11:26.617
STEP: getting 11/15/23 06:11:26.665
STEP: listing 11/15/23 06:11:26.676
STEP: watching 11/15/23 06:11:26.691
Nov 15 06:11:26.691: INFO: starting watch
STEP: cluster-wide listing 11/15/23 06:11:26.697
STEP: cluster-wide watching 11/15/23 06:11:26.713
Nov 15 06:11:26.714: INFO: starting watch
STEP: patching 11/15/23 06:11:26.725
STEP: updating 11/15/23 06:11:26.745
Nov 15 06:11:26.774: INFO: waiting for watch events with expected annotations
Nov 15 06:11:26.774: INFO: saw patched and updated annotations
STEP: deleting 11/15/23 06:11:26.774
STEP: deleting a collection 11/15/23 06:11:26.817
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Nov 15 06:11:26.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-7521" for this suite. 11/15/23 06:11:26.888
------------------------------
â€¢ [0.405 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:11:26.511
    Nov 15 06:11:26.511: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename endpointslice 11/15/23 06:11:26.511
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:26.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:26.579
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 11/15/23 06:11:26.592
    STEP: getting /apis/discovery.k8s.io 11/15/23 06:11:26.607
    STEP: getting /apis/discovery.k8s.iov1 11/15/23 06:11:26.612
    STEP: creating 11/15/23 06:11:26.617
    STEP: getting 11/15/23 06:11:26.665
    STEP: listing 11/15/23 06:11:26.676
    STEP: watching 11/15/23 06:11:26.691
    Nov 15 06:11:26.691: INFO: starting watch
    STEP: cluster-wide listing 11/15/23 06:11:26.697
    STEP: cluster-wide watching 11/15/23 06:11:26.713
    Nov 15 06:11:26.714: INFO: starting watch
    STEP: patching 11/15/23 06:11:26.725
    STEP: updating 11/15/23 06:11:26.745
    Nov 15 06:11:26.774: INFO: waiting for watch events with expected annotations
    Nov 15 06:11:26.774: INFO: saw patched and updated annotations
    STEP: deleting 11/15/23 06:11:26.774
    STEP: deleting a collection 11/15/23 06:11:26.817
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:11:26.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-7521" for this suite. 11/15/23 06:11:26.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:11:26.916
Nov 15 06:11:26.916: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename secrets 11/15/23 06:11:26.916
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:26.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:26.988
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 11/15/23 06:11:26.998
STEP: listing secrets in all namespaces to ensure that there are more than zero 11/15/23 06:11:27.014
STEP: patching the secret 11/15/23 06:11:27.172
STEP: deleting the secret using a LabelSelector 11/15/23 06:11:27.203
STEP: listing secrets in all namespaces, searching for label name and value in patch 11/15/23 06:11:27.228
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 06:11:27.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5881" for this suite. 11/15/23 06:11:27.434
------------------------------
â€¢ [0.550 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:11:26.916
    Nov 15 06:11:26.916: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename secrets 11/15/23 06:11:26.916
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:26.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:26.988
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 11/15/23 06:11:26.998
    STEP: listing secrets in all namespaces to ensure that there are more than zero 11/15/23 06:11:27.014
    STEP: patching the secret 11/15/23 06:11:27.172
    STEP: deleting the secret using a LabelSelector 11/15/23 06:11:27.203
    STEP: listing secrets in all namespaces, searching for label name and value in patch 11/15/23 06:11:27.228
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:11:27.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5881" for this suite. 11/15/23 06:11:27.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:11:27.469
Nov 15 06:11:27.469: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename security-context-test 11/15/23 06:11:27.47
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:27.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:27.544
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Nov 15 06:11:27.591: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-fb3bf9b2-693b-48d8-9f28-bf64f7e19dc5" in namespace "security-context-test-8033" to be "Succeeded or Failed"
Nov 15 06:11:27.622: INFO: Pod "busybox-readonly-false-fb3bf9b2-693b-48d8-9f28-bf64f7e19dc5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.568337ms
Nov 15 06:11:29.644: INFO: Pod "busybox-readonly-false-fb3bf9b2-693b-48d8-9f28-bf64f7e19dc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052913289s
Nov 15 06:11:31.649: INFO: Pod "busybox-readonly-false-fb3bf9b2-693b-48d8-9f28-bf64f7e19dc5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0576078s
Nov 15 06:11:33.646: INFO: Pod "busybox-readonly-false-fb3bf9b2-693b-48d8-9f28-bf64f7e19dc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054855285s
Nov 15 06:11:33.646: INFO: Pod "busybox-readonly-false-fb3bf9b2-693b-48d8-9f28-bf64f7e19dc5" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Nov 15 06:11:33.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8033" for this suite. 11/15/23 06:11:33.685
------------------------------
â€¢ [SLOW TEST] [6.251 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:11:27.469
    Nov 15 06:11:27.469: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename security-context-test 11/15/23 06:11:27.47
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:27.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:27.544
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Nov 15 06:11:27.591: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-fb3bf9b2-693b-48d8-9f28-bf64f7e19dc5" in namespace "security-context-test-8033" to be "Succeeded or Failed"
    Nov 15 06:11:27.622: INFO: Pod "busybox-readonly-false-fb3bf9b2-693b-48d8-9f28-bf64f7e19dc5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.568337ms
    Nov 15 06:11:29.644: INFO: Pod "busybox-readonly-false-fb3bf9b2-693b-48d8-9f28-bf64f7e19dc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052913289s
    Nov 15 06:11:31.649: INFO: Pod "busybox-readonly-false-fb3bf9b2-693b-48d8-9f28-bf64f7e19dc5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0576078s
    Nov 15 06:11:33.646: INFO: Pod "busybox-readonly-false-fb3bf9b2-693b-48d8-9f28-bf64f7e19dc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054855285s
    Nov 15 06:11:33.646: INFO: Pod "busybox-readonly-false-fb3bf9b2-693b-48d8-9f28-bf64f7e19dc5" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:11:33.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8033" for this suite. 11/15/23 06:11:33.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:11:33.719
Nov 15 06:11:33.719: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename crd-webhook 11/15/23 06:11:33.721
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:33.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:33.784
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 11/15/23 06:11:33.798
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 11/15/23 06:11:34.495
STEP: Deploying the custom resource conversion webhook pod 11/15/23 06:11:34.546
STEP: Wait for the deployment to be ready 11/15/23 06:11:34.586
Nov 15 06:11:34.612: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Nov 15 06:11:36.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 6, 11, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 11, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 11, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 11, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 06:11:38.723
STEP: Verifying the service has paired with the endpoint 11/15/23 06:11:38.782
Nov 15 06:11:39.783: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Nov 15 06:11:39.810: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Creating a v1 custom resource 11/15/23 06:11:42.559
STEP: v2 custom resource should be converted 11/15/23 06:11:42.579
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:11:43.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-4584" for this suite. 11/15/23 06:11:43.326
------------------------------
â€¢ [SLOW TEST] [9.633 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:11:33.719
    Nov 15 06:11:33.719: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename crd-webhook 11/15/23 06:11:33.721
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:33.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:33.784
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 11/15/23 06:11:33.798
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 11/15/23 06:11:34.495
    STEP: Deploying the custom resource conversion webhook pod 11/15/23 06:11:34.546
    STEP: Wait for the deployment to be ready 11/15/23 06:11:34.586
    Nov 15 06:11:34.612: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    Nov 15 06:11:36.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 6, 11, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 11, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 11, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 11, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 06:11:38.723
    STEP: Verifying the service has paired with the endpoint 11/15/23 06:11:38.782
    Nov 15 06:11:39.783: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Nov 15 06:11:39.810: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Creating a v1 custom resource 11/15/23 06:11:42.559
    STEP: v2 custom resource should be converted 11/15/23 06:11:42.579
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:11:43.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-4584" for this suite. 11/15/23 06:11:43.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:11:43.354
Nov 15 06:11:43.354: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 06:11:43.356
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:43.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:43.434
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 11/15/23 06:11:43.451
W1115 06:11:43.495074      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Nov 15 06:11:43.495: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c6b94211-b449-4a75-ae40-2c9084a71c73" in namespace "projected-3734" to be "Succeeded or Failed"
Nov 15 06:11:43.521: INFO: Pod "downwardapi-volume-c6b94211-b449-4a75-ae40-2c9084a71c73": Phase="Pending", Reason="", readiness=false. Elapsed: 25.908649ms
Nov 15 06:11:45.543: INFO: Pod "downwardapi-volume-c6b94211-b449-4a75-ae40-2c9084a71c73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047828102s
Nov 15 06:11:47.542: INFO: Pod "downwardapi-volume-c6b94211-b449-4a75-ae40-2c9084a71c73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047548359s
STEP: Saw pod success 11/15/23 06:11:47.542
Nov 15 06:11:47.543: INFO: Pod "downwardapi-volume-c6b94211-b449-4a75-ae40-2c9084a71c73" satisfied condition "Succeeded or Failed"
Nov 15 06:11:47.562: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-c6b94211-b449-4a75-ae40-2c9084a71c73 container client-container: <nil>
STEP: delete the pod 11/15/23 06:11:47.609
Nov 15 06:11:47.669: INFO: Waiting for pod downwardapi-volume-c6b94211-b449-4a75-ae40-2c9084a71c73 to disappear
Nov 15 06:11:47.689: INFO: Pod downwardapi-volume-c6b94211-b449-4a75-ae40-2c9084a71c73 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 06:11:47.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3734" for this suite. 11/15/23 06:11:47.715
------------------------------
â€¢ [4.388 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:11:43.354
    Nov 15 06:11:43.354: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 06:11:43.356
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:43.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:43.434
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 11/15/23 06:11:43.451
    W1115 06:11:43.495074      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Nov 15 06:11:43.495: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c6b94211-b449-4a75-ae40-2c9084a71c73" in namespace "projected-3734" to be "Succeeded or Failed"
    Nov 15 06:11:43.521: INFO: Pod "downwardapi-volume-c6b94211-b449-4a75-ae40-2c9084a71c73": Phase="Pending", Reason="", readiness=false. Elapsed: 25.908649ms
    Nov 15 06:11:45.543: INFO: Pod "downwardapi-volume-c6b94211-b449-4a75-ae40-2c9084a71c73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047828102s
    Nov 15 06:11:47.542: INFO: Pod "downwardapi-volume-c6b94211-b449-4a75-ae40-2c9084a71c73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047548359s
    STEP: Saw pod success 11/15/23 06:11:47.542
    Nov 15 06:11:47.543: INFO: Pod "downwardapi-volume-c6b94211-b449-4a75-ae40-2c9084a71c73" satisfied condition "Succeeded or Failed"
    Nov 15 06:11:47.562: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-c6b94211-b449-4a75-ae40-2c9084a71c73 container client-container: <nil>
    STEP: delete the pod 11/15/23 06:11:47.609
    Nov 15 06:11:47.669: INFO: Waiting for pod downwardapi-volume-c6b94211-b449-4a75-ae40-2c9084a71c73 to disappear
    Nov 15 06:11:47.689: INFO: Pod downwardapi-volume-c6b94211-b449-4a75-ae40-2c9084a71c73 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:11:47.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3734" for this suite. 11/15/23 06:11:47.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:11:47.747
Nov 15 06:11:47.747: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename security-context 11/15/23 06:11:47.748
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:47.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:47.816
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 11/15/23 06:11:47.831
Nov 15 06:11:47.865: INFO: Waiting up to 5m0s for pod "security-context-eafaffba-f817-4282-91ca-fa36e2c55b6c" in namespace "security-context-9658" to be "Succeeded or Failed"
Nov 15 06:11:47.888: INFO: Pod "security-context-eafaffba-f817-4282-91ca-fa36e2c55b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.198656ms
Nov 15 06:11:49.908: INFO: Pod "security-context-eafaffba-f817-4282-91ca-fa36e2c55b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04224594s
Nov 15 06:11:51.933: INFO: Pod "security-context-eafaffba-f817-4282-91ca-fa36e2c55b6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067338849s
STEP: Saw pod success 11/15/23 06:11:51.933
Nov 15 06:11:51.933: INFO: Pod "security-context-eafaffba-f817-4282-91ca-fa36e2c55b6c" satisfied condition "Succeeded or Failed"
Nov 15 06:11:51.978: INFO: Trying to get logs from node 10.72.152.81 pod security-context-eafaffba-f817-4282-91ca-fa36e2c55b6c container test-container: <nil>
STEP: delete the pod 11/15/23 06:11:52.073
Nov 15 06:11:52.127: INFO: Waiting for pod security-context-eafaffba-f817-4282-91ca-fa36e2c55b6c to disappear
Nov 15 06:11:52.145: INFO: Pod security-context-eafaffba-f817-4282-91ca-fa36e2c55b6c no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Nov 15 06:11:52.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-9658" for this suite. 11/15/23 06:11:52.171
------------------------------
â€¢ [4.451 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:11:47.747
    Nov 15 06:11:47.747: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename security-context 11/15/23 06:11:47.748
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:47.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:47.816
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 11/15/23 06:11:47.831
    Nov 15 06:11:47.865: INFO: Waiting up to 5m0s for pod "security-context-eafaffba-f817-4282-91ca-fa36e2c55b6c" in namespace "security-context-9658" to be "Succeeded or Failed"
    Nov 15 06:11:47.888: INFO: Pod "security-context-eafaffba-f817-4282-91ca-fa36e2c55b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.198656ms
    Nov 15 06:11:49.908: INFO: Pod "security-context-eafaffba-f817-4282-91ca-fa36e2c55b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04224594s
    Nov 15 06:11:51.933: INFO: Pod "security-context-eafaffba-f817-4282-91ca-fa36e2c55b6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067338849s
    STEP: Saw pod success 11/15/23 06:11:51.933
    Nov 15 06:11:51.933: INFO: Pod "security-context-eafaffba-f817-4282-91ca-fa36e2c55b6c" satisfied condition "Succeeded or Failed"
    Nov 15 06:11:51.978: INFO: Trying to get logs from node 10.72.152.81 pod security-context-eafaffba-f817-4282-91ca-fa36e2c55b6c container test-container: <nil>
    STEP: delete the pod 11/15/23 06:11:52.073
    Nov 15 06:11:52.127: INFO: Waiting for pod security-context-eafaffba-f817-4282-91ca-fa36e2c55b6c to disappear
    Nov 15 06:11:52.145: INFO: Pod security-context-eafaffba-f817-4282-91ca-fa36e2c55b6c no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:11:52.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-9658" for this suite. 11/15/23 06:11:52.171
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:11:52.201
Nov 15 06:11:52.201: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubelet-test 11/15/23 06:11:52.202
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:52.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:52.272
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Nov 15 06:11:52.327: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsd3915671-a9ab-42c4-b9aa-686fc3a34f67" in namespace "kubelet-test-2912" to be "running and ready"
Nov 15 06:11:52.346: INFO: Pod "busybox-readonly-fsd3915671-a9ab-42c4-b9aa-686fc3a34f67": Phase="Pending", Reason="", readiness=false. Elapsed: 18.544599ms
Nov 15 06:11:52.346: INFO: The phase of Pod busybox-readonly-fsd3915671-a9ab-42c4-b9aa-686fc3a34f67 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:11:54.372: INFO: Pod "busybox-readonly-fsd3915671-a9ab-42c4-b9aa-686fc3a34f67": Phase="Running", Reason="", readiness=true. Elapsed: 2.044720591s
Nov 15 06:11:54.372: INFO: The phase of Pod busybox-readonly-fsd3915671-a9ab-42c4-b9aa-686fc3a34f67 is Running (Ready = true)
Nov 15 06:11:54.372: INFO: Pod "busybox-readonly-fsd3915671-a9ab-42c4-b9aa-686fc3a34f67" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Nov 15 06:11:54.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2912" for this suite. 11/15/23 06:11:54.48
------------------------------
â€¢ [2.305 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:11:52.201
    Nov 15 06:11:52.201: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubelet-test 11/15/23 06:11:52.202
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:52.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:52.272
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Nov 15 06:11:52.327: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsd3915671-a9ab-42c4-b9aa-686fc3a34f67" in namespace "kubelet-test-2912" to be "running and ready"
    Nov 15 06:11:52.346: INFO: Pod "busybox-readonly-fsd3915671-a9ab-42c4-b9aa-686fc3a34f67": Phase="Pending", Reason="", readiness=false. Elapsed: 18.544599ms
    Nov 15 06:11:52.346: INFO: The phase of Pod busybox-readonly-fsd3915671-a9ab-42c4-b9aa-686fc3a34f67 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:11:54.372: INFO: Pod "busybox-readonly-fsd3915671-a9ab-42c4-b9aa-686fc3a34f67": Phase="Running", Reason="", readiness=true. Elapsed: 2.044720591s
    Nov 15 06:11:54.372: INFO: The phase of Pod busybox-readonly-fsd3915671-a9ab-42c4-b9aa-686fc3a34f67 is Running (Ready = true)
    Nov 15 06:11:54.372: INFO: Pod "busybox-readonly-fsd3915671-a9ab-42c4-b9aa-686fc3a34f67" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:11:54.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2912" for this suite. 11/15/23 06:11:54.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:11:54.506
Nov 15 06:11:54.506: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename job 11/15/23 06:11:54.507
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:54.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:54.576
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 11/15/23 06:11:54.59
W1115 06:11:54.614602      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensure pods equal to parallelism count is attached to the job 11/15/23 06:11:54.614
STEP: patching /status 11/15/23 06:11:58.636
STEP: updating /status 11/15/23 06:11:58.662
STEP: get /status 11/15/23 06:11:58.751
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Nov 15 06:11:58.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6022" for this suite. 11/15/23 06:11:58.798
------------------------------
â€¢ [4.318 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:11:54.506
    Nov 15 06:11:54.506: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename job 11/15/23 06:11:54.507
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:54.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:54.576
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 11/15/23 06:11:54.59
    W1115 06:11:54.614602      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensure pods equal to parallelism count is attached to the job 11/15/23 06:11:54.614
    STEP: patching /status 11/15/23 06:11:58.636
    STEP: updating /status 11/15/23 06:11:58.662
    STEP: get /status 11/15/23 06:11:58.751
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:11:58.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6022" for this suite. 11/15/23 06:11:58.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:11:58.827
Nov 15 06:11:58.827: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename init-container 11/15/23 06:11:58.828
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:58.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:58.905
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 11/15/23 06:11:58.918
Nov 15 06:11:58.918: INFO: PodSpec: initContainers in spec.initContainers
Nov 15 06:12:47.080: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-5f65ffae-8ac0-4b48-91e1-579e182dc9bc", GenerateName:"", Namespace:"init-container-5813", SelfLink:"", UID:"4a657479-5e73-44b2-a4ab-be2bdf3f3233", ResourceVersion:"78765", Generation:0, CreationTimestamp:time.Date(2023, time.November, 15, 6, 11, 58, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"918202232"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"0458acac6cf5a514263371c5f96a63b1f540997c31c038cd46882193323b1db9", "cni.projectcalico.org/podIP":"172.30.10.142/32", "cni.projectcalico.org/podIPs":"172.30.10.142/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.10.142\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.November, 15, 6, 11, 58, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0011ce648), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.November, 15, 6, 12, 0, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0011ce6f0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.November, 15, 6, 12, 0, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0011ce750), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.November, 15, 6, 12, 47, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0011ce828), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-rh2wl", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc008641500), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-rh2wl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00aa9e8a0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-rh2wl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00aa9e900), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-rh2wl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00aa9e840), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00867aed0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.72.152.88", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc008678700), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00867af80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00867afa0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00867afbc), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00867afc0), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0067cc890), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 15, 6, 11, 58, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 15, 6, 11, 58, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 15, 6, 11, 58, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 15, 6, 11, 58, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.72.152.88", PodIP:"172.30.10.142", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.10.142"}}, StartTime:time.Date(2023, time.November, 15, 6, 11, 58, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0086787e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc008678850)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://08296647d5391ee09aa6b8bb625139ad0cdff2907bd2df9500067e533fec7e60", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc008641580), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc008641560), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00867b03f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:12:47.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5813" for this suite. 11/15/23 06:12:47.133
------------------------------
â€¢ [SLOW TEST] [48.333 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:11:58.827
    Nov 15 06:11:58.827: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename init-container 11/15/23 06:11:58.828
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:11:58.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:11:58.905
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 11/15/23 06:11:58.918
    Nov 15 06:11:58.918: INFO: PodSpec: initContainers in spec.initContainers
    Nov 15 06:12:47.080: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-5f65ffae-8ac0-4b48-91e1-579e182dc9bc", GenerateName:"", Namespace:"init-container-5813", SelfLink:"", UID:"4a657479-5e73-44b2-a4ab-be2bdf3f3233", ResourceVersion:"78765", Generation:0, CreationTimestamp:time.Date(2023, time.November, 15, 6, 11, 58, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"918202232"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"0458acac6cf5a514263371c5f96a63b1f540997c31c038cd46882193323b1db9", "cni.projectcalico.org/podIP":"172.30.10.142/32", "cni.projectcalico.org/podIPs":"172.30.10.142/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.10.142\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.November, 15, 6, 11, 58, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0011ce648), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.November, 15, 6, 12, 0, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0011ce6f0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.November, 15, 6, 12, 0, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0011ce750), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.November, 15, 6, 12, 47, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0011ce828), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-rh2wl", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc008641500), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-rh2wl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00aa9e8a0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-rh2wl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00aa9e900), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-rh2wl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00aa9e840), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00867aed0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.72.152.88", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc008678700), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00867af80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00867afa0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00867afbc), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00867afc0), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0067cc890), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 15, 6, 11, 58, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 15, 6, 11, 58, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 15, 6, 11, 58, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 15, 6, 11, 58, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.72.152.88", PodIP:"172.30.10.142", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.10.142"}}, StartTime:time.Date(2023, time.November, 15, 6, 11, 58, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0086787e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc008678850)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://08296647d5391ee09aa6b8bb625139ad0cdff2907bd2df9500067e533fec7e60", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc008641580), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc008641560), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00867b03f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:12:47.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5813" for this suite. 11/15/23 06:12:47.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:12:47.16
Nov 15 06:12:47.160: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename container-lifecycle-hook 11/15/23 06:12:47.162
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:12:47.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:12:47.228
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 11/15/23 06:12:47.266
Nov 15 06:12:47.303: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-184" to be "running and ready"
Nov 15 06:12:47.325: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 21.577549ms
Nov 15 06:12:47.325: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:12:49.344: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.040654187s
Nov 15 06:12:49.344: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Nov 15 06:12:49.344: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 11/15/23 06:12:49.376
Nov 15 06:12:49.405: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-184" to be "running and ready"
Nov 15 06:12:49.422: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 17.646848ms
Nov 15 06:12:49.422: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:12:51.442: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.037716004s
Nov 15 06:12:51.443: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Nov 15 06:12:51.443: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 11/15/23 06:12:51.461
STEP: delete the pod with lifecycle hook 11/15/23 06:12:51.506
Nov 15 06:12:51.546: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 15 06:12:51.572: INFO: Pod pod-with-poststart-http-hook still exists
Nov 15 06:12:53.573: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 15 06:12:53.596: INFO: Pod pod-with-poststart-http-hook still exists
Nov 15 06:12:55.573: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 15 06:12:55.595: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Nov 15 06:12:55.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-184" for this suite. 11/15/23 06:12:55.631
------------------------------
â€¢ [SLOW TEST] [8.500 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:12:47.16
    Nov 15 06:12:47.160: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename container-lifecycle-hook 11/15/23 06:12:47.162
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:12:47.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:12:47.228
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 11/15/23 06:12:47.266
    Nov 15 06:12:47.303: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-184" to be "running and ready"
    Nov 15 06:12:47.325: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 21.577549ms
    Nov 15 06:12:47.325: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:12:49.344: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.040654187s
    Nov 15 06:12:49.344: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Nov 15 06:12:49.344: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 11/15/23 06:12:49.376
    Nov 15 06:12:49.405: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-184" to be "running and ready"
    Nov 15 06:12:49.422: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 17.646848ms
    Nov 15 06:12:49.422: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:12:51.442: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.037716004s
    Nov 15 06:12:51.443: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Nov 15 06:12:51.443: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 11/15/23 06:12:51.461
    STEP: delete the pod with lifecycle hook 11/15/23 06:12:51.506
    Nov 15 06:12:51.546: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Nov 15 06:12:51.572: INFO: Pod pod-with-poststart-http-hook still exists
    Nov 15 06:12:53.573: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Nov 15 06:12:53.596: INFO: Pod pod-with-poststart-http-hook still exists
    Nov 15 06:12:55.573: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Nov 15 06:12:55.595: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:12:55.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-184" for this suite. 11/15/23 06:12:55.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:12:55.661
Nov 15 06:12:55.661: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename disruption 11/15/23 06:12:55.662
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:12:55.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:12:55.727
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:12:55.742
Nov 15 06:12:55.742: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename disruption-2 11/15/23 06:12:55.743
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:12:55.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:12:55.822
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 11/15/23 06:12:55.855
STEP: Waiting for the pdb to be processed 11/15/23 06:12:55.902
STEP: Waiting for the pdb to be processed 11/15/23 06:12:55.942
STEP: listing a collection of PDBs across all namespaces 11/15/23 06:12:55.965
STEP: listing a collection of PDBs in namespace disruption-7114 11/15/23 06:12:55.989
STEP: deleting a collection of PDBs 11/15/23 06:12:56.009
STEP: Waiting for the PDB collection to be deleted 11/15/23 06:12:56.074
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Nov 15 06:12:56.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Nov 15 06:12:56.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-2818" for this suite. 11/15/23 06:12:56.155
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7114" for this suite. 11/15/23 06:12:56.191
------------------------------
â€¢ [0.569 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:12:55.661
    Nov 15 06:12:55.661: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename disruption 11/15/23 06:12:55.662
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:12:55.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:12:55.727
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:12:55.742
    Nov 15 06:12:55.742: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename disruption-2 11/15/23 06:12:55.743
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:12:55.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:12:55.822
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 11/15/23 06:12:55.855
    STEP: Waiting for the pdb to be processed 11/15/23 06:12:55.902
    STEP: Waiting for the pdb to be processed 11/15/23 06:12:55.942
    STEP: listing a collection of PDBs across all namespaces 11/15/23 06:12:55.965
    STEP: listing a collection of PDBs in namespace disruption-7114 11/15/23 06:12:55.989
    STEP: deleting a collection of PDBs 11/15/23 06:12:56.009
    STEP: Waiting for the PDB collection to be deleted 11/15/23 06:12:56.074
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:12:56.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:12:56.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-2818" for this suite. 11/15/23 06:12:56.155
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7114" for this suite. 11/15/23 06:12:56.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:12:56.233
Nov 15 06:12:56.233: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename subpath 11/15/23 06:12:56.233
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:12:56.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:12:56.298
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/15/23 06:12:56.311
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-lhff 11/15/23 06:12:56.348
STEP: Creating a pod to test atomic-volume-subpath 11/15/23 06:12:56.348
Nov 15 06:12:56.381: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lhff" in namespace "subpath-4318" to be "Succeeded or Failed"
Nov 15 06:12:56.403: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Pending", Reason="", readiness=false. Elapsed: 21.846846ms
Nov 15 06:12:58.424: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042168803s
Nov 15 06:13:00.426: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 4.043972126s
Nov 15 06:13:02.424: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 6.042406054s
Nov 15 06:13:04.424: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 8.042120266s
Nov 15 06:13:06.425: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 10.043871832s
Nov 15 06:13:08.423: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 12.041648877s
Nov 15 06:13:10.422: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 14.040283454s
Nov 15 06:13:12.422: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 16.040891995s
Nov 15 06:13:14.431: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 18.049745341s
Nov 15 06:13:16.422: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 20.040631377s
Nov 15 06:13:18.425: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 22.043795729s
Nov 15 06:13:20.423: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=false. Elapsed: 24.041713697s
Nov 15 06:13:22.423: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.041748726s
STEP: Saw pod success 11/15/23 06:13:22.423
Nov 15 06:13:22.424: INFO: Pod "pod-subpath-test-configmap-lhff" satisfied condition "Succeeded or Failed"
Nov 15 06:13:22.447: INFO: Trying to get logs from node 10.72.152.86 pod pod-subpath-test-configmap-lhff container test-container-subpath-configmap-lhff: <nil>
STEP: delete the pod 11/15/23 06:13:22.525
Nov 15 06:13:22.582: INFO: Waiting for pod pod-subpath-test-configmap-lhff to disappear
Nov 15 06:13:22.606: INFO: Pod pod-subpath-test-configmap-lhff no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lhff 11/15/23 06:13:22.606
Nov 15 06:13:22.606: INFO: Deleting pod "pod-subpath-test-configmap-lhff" in namespace "subpath-4318"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Nov 15 06:13:22.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4318" for this suite. 11/15/23 06:13:22.656
------------------------------
â€¢ [SLOW TEST] [26.462 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:12:56.233
    Nov 15 06:12:56.233: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename subpath 11/15/23 06:12:56.233
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:12:56.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:12:56.298
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/15/23 06:12:56.311
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-lhff 11/15/23 06:12:56.348
    STEP: Creating a pod to test atomic-volume-subpath 11/15/23 06:12:56.348
    Nov 15 06:12:56.381: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lhff" in namespace "subpath-4318" to be "Succeeded or Failed"
    Nov 15 06:12:56.403: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Pending", Reason="", readiness=false. Elapsed: 21.846846ms
    Nov 15 06:12:58.424: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042168803s
    Nov 15 06:13:00.426: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 4.043972126s
    Nov 15 06:13:02.424: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 6.042406054s
    Nov 15 06:13:04.424: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 8.042120266s
    Nov 15 06:13:06.425: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 10.043871832s
    Nov 15 06:13:08.423: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 12.041648877s
    Nov 15 06:13:10.422: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 14.040283454s
    Nov 15 06:13:12.422: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 16.040891995s
    Nov 15 06:13:14.431: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 18.049745341s
    Nov 15 06:13:16.422: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 20.040631377s
    Nov 15 06:13:18.425: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=true. Elapsed: 22.043795729s
    Nov 15 06:13:20.423: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Running", Reason="", readiness=false. Elapsed: 24.041713697s
    Nov 15 06:13:22.423: INFO: Pod "pod-subpath-test-configmap-lhff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.041748726s
    STEP: Saw pod success 11/15/23 06:13:22.423
    Nov 15 06:13:22.424: INFO: Pod "pod-subpath-test-configmap-lhff" satisfied condition "Succeeded or Failed"
    Nov 15 06:13:22.447: INFO: Trying to get logs from node 10.72.152.86 pod pod-subpath-test-configmap-lhff container test-container-subpath-configmap-lhff: <nil>
    STEP: delete the pod 11/15/23 06:13:22.525
    Nov 15 06:13:22.582: INFO: Waiting for pod pod-subpath-test-configmap-lhff to disappear
    Nov 15 06:13:22.606: INFO: Pod pod-subpath-test-configmap-lhff no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-lhff 11/15/23 06:13:22.606
    Nov 15 06:13:22.606: INFO: Deleting pod "pod-subpath-test-configmap-lhff" in namespace "subpath-4318"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:13:22.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4318" for this suite. 11/15/23 06:13:22.656
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:13:22.696
Nov 15 06:13:22.696: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename downward-api 11/15/23 06:13:22.697
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:13:22.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:13:22.764
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 11/15/23 06:13:22.778
Nov 15 06:13:22.822: INFO: Waiting up to 5m0s for pod "downward-api-3f2ad5f4-2eef-4249-8d10-b09bb8e99627" in namespace "downward-api-3951" to be "Succeeded or Failed"
Nov 15 06:13:22.846: INFO: Pod "downward-api-3f2ad5f4-2eef-4249-8d10-b09bb8e99627": Phase="Pending", Reason="", readiness=false. Elapsed: 23.215727ms
Nov 15 06:13:24.876: INFO: Pod "downward-api-3f2ad5f4-2eef-4249-8d10-b09bb8e99627": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053653123s
Nov 15 06:13:26.865: INFO: Pod "downward-api-3f2ad5f4-2eef-4249-8d10-b09bb8e99627": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042859475s
Nov 15 06:13:28.865: INFO: Pod "downward-api-3f2ad5f4-2eef-4249-8d10-b09bb8e99627": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042233722s
STEP: Saw pod success 11/15/23 06:13:28.865
Nov 15 06:13:28.865: INFO: Pod "downward-api-3f2ad5f4-2eef-4249-8d10-b09bb8e99627" satisfied condition "Succeeded or Failed"
Nov 15 06:13:28.883: INFO: Trying to get logs from node 10.72.152.81 pod downward-api-3f2ad5f4-2eef-4249-8d10-b09bb8e99627 container dapi-container: <nil>
STEP: delete the pod 11/15/23 06:13:28.929
Nov 15 06:13:28.983: INFO: Waiting for pod downward-api-3f2ad5f4-2eef-4249-8d10-b09bb8e99627 to disappear
Nov 15 06:13:29.000: INFO: Pod downward-api-3f2ad5f4-2eef-4249-8d10-b09bb8e99627 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Nov 15 06:13:29.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3951" for this suite. 11/15/23 06:13:29.034
------------------------------
â€¢ [SLOW TEST] [6.364 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:13:22.696
    Nov 15 06:13:22.696: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename downward-api 11/15/23 06:13:22.697
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:13:22.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:13:22.764
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 11/15/23 06:13:22.778
    Nov 15 06:13:22.822: INFO: Waiting up to 5m0s for pod "downward-api-3f2ad5f4-2eef-4249-8d10-b09bb8e99627" in namespace "downward-api-3951" to be "Succeeded or Failed"
    Nov 15 06:13:22.846: INFO: Pod "downward-api-3f2ad5f4-2eef-4249-8d10-b09bb8e99627": Phase="Pending", Reason="", readiness=false. Elapsed: 23.215727ms
    Nov 15 06:13:24.876: INFO: Pod "downward-api-3f2ad5f4-2eef-4249-8d10-b09bb8e99627": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053653123s
    Nov 15 06:13:26.865: INFO: Pod "downward-api-3f2ad5f4-2eef-4249-8d10-b09bb8e99627": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042859475s
    Nov 15 06:13:28.865: INFO: Pod "downward-api-3f2ad5f4-2eef-4249-8d10-b09bb8e99627": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042233722s
    STEP: Saw pod success 11/15/23 06:13:28.865
    Nov 15 06:13:28.865: INFO: Pod "downward-api-3f2ad5f4-2eef-4249-8d10-b09bb8e99627" satisfied condition "Succeeded or Failed"
    Nov 15 06:13:28.883: INFO: Trying to get logs from node 10.72.152.81 pod downward-api-3f2ad5f4-2eef-4249-8d10-b09bb8e99627 container dapi-container: <nil>
    STEP: delete the pod 11/15/23 06:13:28.929
    Nov 15 06:13:28.983: INFO: Waiting for pod downward-api-3f2ad5f4-2eef-4249-8d10-b09bb8e99627 to disappear
    Nov 15 06:13:29.000: INFO: Pod downward-api-3f2ad5f4-2eef-4249-8d10-b09bb8e99627 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:13:29.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3951" for this suite. 11/15/23 06:13:29.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:13:29.062
Nov 15 06:13:29.062: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename statefulset 11/15/23 06:13:29.063
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:13:29.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:13:29.134
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4251 11/15/23 06:13:29.147
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-4251 11/15/23 06:13:29.215
Nov 15 06:13:29.283: INFO: Found 0 stateful pods, waiting for 1
Nov 15 06:13:39.303: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 11/15/23 06:13:39.342
STEP: Getting /status 11/15/23 06:13:39.381
Nov 15 06:13:39.399: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 11/15/23 06:13:39.399
Nov 15 06:13:39.445: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 11/15/23 06:13:39.445
Nov 15 06:13:39.453: INFO: Observed &StatefulSet event: ADDED
Nov 15 06:13:39.453: INFO: Found Statefulset ss in namespace statefulset-4251 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 15 06:13:39.453: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 11/15/23 06:13:39.453
Nov 15 06:13:39.453: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 15 06:13:39.479: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 11/15/23 06:13:39.479
Nov 15 06:13:39.486: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 15 06:13:39.486: INFO: Deleting all statefulset in ns statefulset-4251
Nov 15 06:13:39.505: INFO: Scaling statefulset ss to 0
Nov 15 06:13:49.584: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 06:13:49.602: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 15 06:13:49.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4251" for this suite. 11/15/23 06:13:49.7
------------------------------
â€¢ [SLOW TEST] [20.670 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:13:29.062
    Nov 15 06:13:29.062: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename statefulset 11/15/23 06:13:29.063
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:13:29.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:13:29.134
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4251 11/15/23 06:13:29.147
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-4251 11/15/23 06:13:29.215
    Nov 15 06:13:29.283: INFO: Found 0 stateful pods, waiting for 1
    Nov 15 06:13:39.303: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 11/15/23 06:13:39.342
    STEP: Getting /status 11/15/23 06:13:39.381
    Nov 15 06:13:39.399: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 11/15/23 06:13:39.399
    Nov 15 06:13:39.445: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 11/15/23 06:13:39.445
    Nov 15 06:13:39.453: INFO: Observed &StatefulSet event: ADDED
    Nov 15 06:13:39.453: INFO: Found Statefulset ss in namespace statefulset-4251 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Nov 15 06:13:39.453: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 11/15/23 06:13:39.453
    Nov 15 06:13:39.453: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Nov 15 06:13:39.479: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 11/15/23 06:13:39.479
    Nov 15 06:13:39.486: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 15 06:13:39.486: INFO: Deleting all statefulset in ns statefulset-4251
    Nov 15 06:13:39.505: INFO: Scaling statefulset ss to 0
    Nov 15 06:13:49.584: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 06:13:49.602: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:13:49.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4251" for this suite. 11/15/23 06:13:49.7
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:13:49.734
Nov 15 06:13:49.734: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename runtimeclass 11/15/23 06:13:49.735
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:13:49.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:13:49.799
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Nov 15 06:13:49.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8870" for this suite. 11/15/23 06:13:49.869
------------------------------
â€¢ [0.166 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:13:49.734
    Nov 15 06:13:49.734: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename runtimeclass 11/15/23 06:13:49.735
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:13:49.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:13:49.799
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:13:49.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8870" for this suite. 11/15/23 06:13:49.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:13:49.902
Nov 15 06:13:49.902: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename deployment 11/15/23 06:13:49.903
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:13:49.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:13:49.984
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
W1115 06:13:51.023057      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Nov 15 06:13:51.040: INFO: Pod name rollover-pod: Found 0 pods out of 1
Nov 15 06:13:56.065: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/15/23 06:13:56.065
Nov 15 06:13:56.065: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Nov 15 06:13:58.086: INFO: Creating deployment "test-rollover-deployment"
Nov 15 06:13:58.120: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Nov 15 06:14:00.143: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Nov 15 06:14:00.171: INFO: Ensure that both replica sets have 1 created replica
Nov 15 06:14:00.207: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Nov 15 06:14:00.238: INFO: Updating deployment test-rollover-deployment
Nov 15 06:14:00.238: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Nov 15 06:14:02.261: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Nov 15 06:14:02.289: INFO: Make sure deployment "test-rollover-deployment" is complete
Nov 15 06:14:02.322: INFO: all replica sets need to contain the pod-template-hash label
Nov 15 06:14:02.322: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 14, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 06:14:04.351: INFO: all replica sets need to contain the pod-template-hash label
Nov 15 06:14:04.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 14, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 06:14:06.351: INFO: all replica sets need to contain the pod-template-hash label
Nov 15 06:14:06.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 14, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 06:14:08.354: INFO: all replica sets need to contain the pod-template-hash label
Nov 15 06:14:08.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 14, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 06:14:10.351: INFO: all replica sets need to contain the pod-template-hash label
Nov 15 06:14:10.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 14, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 06:14:12.352: INFO: 
Nov 15 06:14:12.352: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 15 06:14:12.395: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2111  def384e2-6516-43f0-a993-8c4fdddac735 79872 2 2023-11-15 06:13:58 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-11-15 06:14:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:14:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d540c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-11-15 06:13:58 +0000 UTC,LastTransitionTime:2023-11-15 06:13:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-11-15 06:14:11 +0000 UTC,LastTransitionTime:2023-11-15 06:13:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 15 06:14:12.410: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-2111  825eb053-d344-4c52-a06f-80dc4c615b47 79862 2 2023-11-15 06:14:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment def384e2-6516-43f0-a993-8c4fdddac735 0xc00519ffa7 0xc00519ffa8}] [] [{kube-controller-manager Update apps/v1 2023-11-15 06:14:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"def384e2-6516-43f0-a993-8c4fdddac735\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:14:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0008a6058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 15 06:14:12.410: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Nov 15 06:14:12.411: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2111  5262f795-ba63-4e90-938f-e046968f7833 79871 2 2023-11-15 06:13:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment def384e2-6516-43f0-a993-8c4fdddac735 0xc00519fe77 0xc00519fe78}] [] [{e2e.test Update apps/v1 2023-11-15 06:13:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:14:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"def384e2-6516-43f0-a993-8c4fdddac735\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:14:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00519ff38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 15 06:14:12.411: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-2111  f185a5c1-2b35-49de-b175-d91f002bb6a3 79789 2 2023-11-15 06:13:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment def384e2-6516-43f0-a993-8c4fdddac735 0xc0008a60c7 0xc0008a60c8}] [] [{kube-controller-manager Update apps/v1 2023-11-15 06:14:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"def384e2-6516-43f0-a993-8c4fdddac735\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:14:00 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0008a6238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 15 06:14:12.429: INFO: Pod "test-rollover-deployment-6c6df9974f-mbw8s" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-mbw8s test-rollover-deployment-6c6df9974f- deployment-2111  3849c6db-3228-44c0-9f65-9560d8ca3f1e 79814 0 2023-11-15 06:14:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:5dbcfc62145c96e479126cb40a351bde9575e775a3046b8a92852ab8f10cdc57 cni.projectcalico.org/podIP:172.30.213.184/32 cni.projectcalico.org/podIPs:172.30.213.184/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.213.184"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 825eb053-d344-4c52-a06f-80dc4c615b47 0xc0008a6927 0xc0008a6928}] [] [{kube-controller-manager Update v1 2023-11-15 06:14:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"825eb053-d344-4c52-a06f-80dc4c615b47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:14:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 06:14:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-11-15 06:14:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lg2mg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lg2mg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c40,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-p94lq,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:14:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:14:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:14:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:14:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:172.30.213.184,StartTime:2023-11-15 06:14:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:14:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://52dc0f5147a02af9b8f9b00f055aba1cd01e658f4e8d46c7ab98f7edcc46e834,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.213.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 15 06:14:12.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2111" for this suite. 11/15/23 06:14:12.462
------------------------------
â€¢ [SLOW TEST] [22.594 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:13:49.902
    Nov 15 06:13:49.902: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename deployment 11/15/23 06:13:49.903
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:13:49.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:13:49.984
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    W1115 06:13:51.023057      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Nov 15 06:13:51.040: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Nov 15 06:13:56.065: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/15/23 06:13:56.065
    Nov 15 06:13:56.065: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Nov 15 06:13:58.086: INFO: Creating deployment "test-rollover-deployment"
    Nov 15 06:13:58.120: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Nov 15 06:14:00.143: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Nov 15 06:14:00.171: INFO: Ensure that both replica sets have 1 created replica
    Nov 15 06:14:00.207: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Nov 15 06:14:00.238: INFO: Updating deployment test-rollover-deployment
    Nov 15 06:14:00.238: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Nov 15 06:14:02.261: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Nov 15 06:14:02.289: INFO: Make sure deployment "test-rollover-deployment" is complete
    Nov 15 06:14:02.322: INFO: all replica sets need to contain the pod-template-hash label
    Nov 15 06:14:02.322: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 14, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 06:14:04.351: INFO: all replica sets need to contain the pod-template-hash label
    Nov 15 06:14:04.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 14, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 06:14:06.351: INFO: all replica sets need to contain the pod-template-hash label
    Nov 15 06:14:06.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 14, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 06:14:08.354: INFO: all replica sets need to contain the pod-template-hash label
    Nov 15 06:14:08.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 14, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 06:14:10.351: INFO: all replica sets need to contain the pod-template-hash label
    Nov 15 06:14:10.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 14, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 13, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 06:14:12.352: INFO: 
    Nov 15 06:14:12.352: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 15 06:14:12.395: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2111  def384e2-6516-43f0-a993-8c4fdddac735 79872 2 2023-11-15 06:13:58 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-11-15 06:14:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:14:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d540c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-11-15 06:13:58 +0000 UTC,LastTransitionTime:2023-11-15 06:13:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-11-15 06:14:11 +0000 UTC,LastTransitionTime:2023-11-15 06:13:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Nov 15 06:14:12.410: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-2111  825eb053-d344-4c52-a06f-80dc4c615b47 79862 2 2023-11-15 06:14:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment def384e2-6516-43f0-a993-8c4fdddac735 0xc00519ffa7 0xc00519ffa8}] [] [{kube-controller-manager Update apps/v1 2023-11-15 06:14:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"def384e2-6516-43f0-a993-8c4fdddac735\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:14:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0008a6058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 06:14:12.410: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Nov 15 06:14:12.411: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2111  5262f795-ba63-4e90-938f-e046968f7833 79871 2 2023-11-15 06:13:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment def384e2-6516-43f0-a993-8c4fdddac735 0xc00519fe77 0xc00519fe78}] [] [{e2e.test Update apps/v1 2023-11-15 06:13:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:14:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"def384e2-6516-43f0-a993-8c4fdddac735\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:14:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00519ff38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 06:14:12.411: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-2111  f185a5c1-2b35-49de-b175-d91f002bb6a3 79789 2 2023-11-15 06:13:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment def384e2-6516-43f0-a993-8c4fdddac735 0xc0008a60c7 0xc0008a60c8}] [] [{kube-controller-manager Update apps/v1 2023-11-15 06:14:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"def384e2-6516-43f0-a993-8c4fdddac735\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:14:00 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0008a6238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 06:14:12.429: INFO: Pod "test-rollover-deployment-6c6df9974f-mbw8s" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-mbw8s test-rollover-deployment-6c6df9974f- deployment-2111  3849c6db-3228-44c0-9f65-9560d8ca3f1e 79814 0 2023-11-15 06:14:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:5dbcfc62145c96e479126cb40a351bde9575e775a3046b8a92852ab8f10cdc57 cni.projectcalico.org/podIP:172.30.213.184/32 cni.projectcalico.org/podIPs:172.30.213.184/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.213.184"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 825eb053-d344-4c52-a06f-80dc4c615b47 0xc0008a6927 0xc0008a6928}] [] [{kube-controller-manager Update v1 2023-11-15 06:14:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"825eb053-d344-4c52-a06f-80dc4c615b47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 06:14:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 06:14:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.213.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-11-15 06:14:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lg2mg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lg2mg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.86,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c40,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-p94lq,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:14:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:14:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:14:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:14:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.86,PodIP:172.30.213.184,StartTime:2023-11-15 06:14:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 06:14:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://52dc0f5147a02af9b8f9b00f055aba1cd01e658f4e8d46c7ab98f7edcc46e834,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.213.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:14:12.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2111" for this suite. 11/15/23 06:14:12.462
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:14:12.496
Nov 15 06:14:12.496: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename container-runtime 11/15/23 06:14:12.497
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:14:12.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:14:12.586
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 11/15/23 06:14:12.633
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 11/15/23 06:14:33.088
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 11/15/23 06:14:33.107
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 11/15/23 06:14:33.143
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 11/15/23 06:14:33.143
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 11/15/23 06:14:33.243
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 11/15/23 06:14:37.354
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 11/15/23 06:14:40.435
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 11/15/23 06:14:40.472
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 11/15/23 06:14:40.472
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 11/15/23 06:14:40.6
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 11/15/23 06:14:41.644
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 11/15/23 06:14:48.811
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 11/15/23 06:14:48.852
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 11/15/23 06:14:48.852
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Nov 15 06:14:49.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6448" for this suite. 11/15/23 06:14:49.05
------------------------------
â€¢ [SLOW TEST] [36.602 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:14:12.496
    Nov 15 06:14:12.496: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename container-runtime 11/15/23 06:14:12.497
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:14:12.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:14:12.586
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 11/15/23 06:14:12.633
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 11/15/23 06:14:33.088
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 11/15/23 06:14:33.107
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 11/15/23 06:14:33.143
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 11/15/23 06:14:33.143
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 11/15/23 06:14:33.243
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 11/15/23 06:14:37.354
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 11/15/23 06:14:40.435
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 11/15/23 06:14:40.472
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 11/15/23 06:14:40.472
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 11/15/23 06:14:40.6
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 11/15/23 06:14:41.644
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 11/15/23 06:14:48.811
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 11/15/23 06:14:48.852
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 11/15/23 06:14:48.852
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:14:49.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6448" for this suite. 11/15/23 06:14:49.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:14:49.099
Nov 15 06:14:49.099: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename emptydir 11/15/23 06:14:49.1
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:14:49.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:14:49.174
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 11/15/23 06:14:49.195
Nov 15 06:14:49.286: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-6351e618-0a7c-406d-8285-22c5921e43c4" in namespace "emptydir-7610" to be "running"
Nov 15 06:14:49.325: INFO: Pod "pod-sharedvolume-6351e618-0a7c-406d-8285-22c5921e43c4": Phase="Pending", Reason="", readiness=false. Elapsed: 38.267151ms
Nov 15 06:14:51.344: INFO: Pod "pod-sharedvolume-6351e618-0a7c-406d-8285-22c5921e43c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057470202s
Nov 15 06:14:53.355: INFO: Pod "pod-sharedvolume-6351e618-0a7c-406d-8285-22c5921e43c4": Phase="Running", Reason="", readiness=false. Elapsed: 4.068078018s
Nov 15 06:14:53.355: INFO: Pod "pod-sharedvolume-6351e618-0a7c-406d-8285-22c5921e43c4" satisfied condition "running"
STEP: Reading file content from the nginx-container 11/15/23 06:14:53.355
Nov 15 06:14:53.355: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7610 PodName:pod-sharedvolume-6351e618-0a7c-406d-8285-22c5921e43c4 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 06:14:53.355: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 06:14:53.356: INFO: ExecWithOptions: Clientset creation
Nov 15 06:14:53.356: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-7610/pods/pod-sharedvolume-6351e618-0a7c-406d-8285-22c5921e43c4/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Nov 15 06:14:53.572: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 06:14:53.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7610" for this suite. 11/15/23 06:14:53.603
------------------------------
â€¢ [4.534 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:14:49.099
    Nov 15 06:14:49.099: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename emptydir 11/15/23 06:14:49.1
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:14:49.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:14:49.174
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 11/15/23 06:14:49.195
    Nov 15 06:14:49.286: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-6351e618-0a7c-406d-8285-22c5921e43c4" in namespace "emptydir-7610" to be "running"
    Nov 15 06:14:49.325: INFO: Pod "pod-sharedvolume-6351e618-0a7c-406d-8285-22c5921e43c4": Phase="Pending", Reason="", readiness=false. Elapsed: 38.267151ms
    Nov 15 06:14:51.344: INFO: Pod "pod-sharedvolume-6351e618-0a7c-406d-8285-22c5921e43c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057470202s
    Nov 15 06:14:53.355: INFO: Pod "pod-sharedvolume-6351e618-0a7c-406d-8285-22c5921e43c4": Phase="Running", Reason="", readiness=false. Elapsed: 4.068078018s
    Nov 15 06:14:53.355: INFO: Pod "pod-sharedvolume-6351e618-0a7c-406d-8285-22c5921e43c4" satisfied condition "running"
    STEP: Reading file content from the nginx-container 11/15/23 06:14:53.355
    Nov 15 06:14:53.355: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7610 PodName:pod-sharedvolume-6351e618-0a7c-406d-8285-22c5921e43c4 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 06:14:53.355: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 06:14:53.356: INFO: ExecWithOptions: Clientset creation
    Nov 15 06:14:53.356: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-7610/pods/pod-sharedvolume-6351e618-0a7c-406d-8285-22c5921e43c4/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Nov 15 06:14:53.572: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:14:53.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7610" for this suite. 11/15/23 06:14:53.603
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:14:53.636
Nov 15 06:14:53.636: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename dns 11/15/23 06:14:53.637
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:14:53.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:14:53.718
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4407.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4407.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 11/15/23 06:14:53.744
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4407.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4407.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 11/15/23 06:14:53.744
STEP: creating a pod to probe /etc/hosts 11/15/23 06:14:53.744
STEP: submitting the pod to kubernetes 11/15/23 06:14:53.744
Nov 15 06:14:53.784: INFO: Waiting up to 15m0s for pod "dns-test-eadc091e-d200-452b-aa6c-687dacddc448" in namespace "dns-4407" to be "running"
Nov 15 06:14:53.809: INFO: Pod "dns-test-eadc091e-d200-452b-aa6c-687dacddc448": Phase="Pending", Reason="", readiness=false. Elapsed: 25.105767ms
Nov 15 06:14:55.828: INFO: Pod "dns-test-eadc091e-d200-452b-aa6c-687dacddc448": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043909808s
Nov 15 06:14:57.847: INFO: Pod "dns-test-eadc091e-d200-452b-aa6c-687dacddc448": Phase="Running", Reason="", readiness=true. Elapsed: 4.063467317s
Nov 15 06:14:57.847: INFO: Pod "dns-test-eadc091e-d200-452b-aa6c-687dacddc448" satisfied condition "running"
STEP: retrieving the pod 11/15/23 06:14:57.847
STEP: looking for the results for each expected name from probers 11/15/23 06:14:57.866
Nov 15 06:14:58.099: INFO: DNS probes using dns-4407/dns-test-eadc091e-d200-452b-aa6c-687dacddc448 succeeded

STEP: deleting the pod 11/15/23 06:14:58.099
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 15 06:14:58.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4407" for this suite. 11/15/23 06:14:58.208
------------------------------
â€¢ [4.603 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:14:53.636
    Nov 15 06:14:53.636: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename dns 11/15/23 06:14:53.637
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:14:53.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:14:53.718
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4407.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4407.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     11/15/23 06:14:53.744
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4407.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4407.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     11/15/23 06:14:53.744
    STEP: creating a pod to probe /etc/hosts 11/15/23 06:14:53.744
    STEP: submitting the pod to kubernetes 11/15/23 06:14:53.744
    Nov 15 06:14:53.784: INFO: Waiting up to 15m0s for pod "dns-test-eadc091e-d200-452b-aa6c-687dacddc448" in namespace "dns-4407" to be "running"
    Nov 15 06:14:53.809: INFO: Pod "dns-test-eadc091e-d200-452b-aa6c-687dacddc448": Phase="Pending", Reason="", readiness=false. Elapsed: 25.105767ms
    Nov 15 06:14:55.828: INFO: Pod "dns-test-eadc091e-d200-452b-aa6c-687dacddc448": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043909808s
    Nov 15 06:14:57.847: INFO: Pod "dns-test-eadc091e-d200-452b-aa6c-687dacddc448": Phase="Running", Reason="", readiness=true. Elapsed: 4.063467317s
    Nov 15 06:14:57.847: INFO: Pod "dns-test-eadc091e-d200-452b-aa6c-687dacddc448" satisfied condition "running"
    STEP: retrieving the pod 11/15/23 06:14:57.847
    STEP: looking for the results for each expected name from probers 11/15/23 06:14:57.866
    Nov 15 06:14:58.099: INFO: DNS probes using dns-4407/dns-test-eadc091e-d200-452b-aa6c-687dacddc448 succeeded

    STEP: deleting the pod 11/15/23 06:14:58.099
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:14:58.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4407" for this suite. 11/15/23 06:14:58.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:14:58.24
Nov 15 06:14:58.240: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename svcaccounts 11/15/23 06:14:58.243
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:14:58.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:14:58.316
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Nov 15 06:14:58.410: INFO: Waiting up to 5m0s for pod "pod-service-account-f554f149-ec28-49ea-ad71-7aa1f3e2a766" in namespace "svcaccounts-6748" to be "running"
Nov 15 06:14:58.432: INFO: Pod "pod-service-account-f554f149-ec28-49ea-ad71-7aa1f3e2a766": Phase="Pending", Reason="", readiness=false. Elapsed: 22.070632ms
Nov 15 06:15:00.452: INFO: Pod "pod-service-account-f554f149-ec28-49ea-ad71-7aa1f3e2a766": Phase="Running", Reason="", readiness=true. Elapsed: 2.041929351s
Nov 15 06:15:00.452: INFO: Pod "pod-service-account-f554f149-ec28-49ea-ad71-7aa1f3e2a766" satisfied condition "running"
STEP: reading a file in the container 11/15/23 06:15:00.452
Nov 15 06:15:00.453: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6748 pod-service-account-f554f149-ec28-49ea-ad71-7aa1f3e2a766 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 11/15/23 06:15:00.727
Nov 15 06:15:00.727: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6748 pod-service-account-f554f149-ec28-49ea-ad71-7aa1f3e2a766 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 11/15/23 06:15:01.01
Nov 15 06:15:01.010: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6748 pod-service-account-f554f149-ec28-49ea-ad71-7aa1f3e2a766 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Nov 15 06:15:01.421: INFO: Got root ca configmap in namespace "svcaccounts-6748"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 15 06:15:01.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6748" for this suite. 11/15/23 06:15:01.464
------------------------------
â€¢ [3.253 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:14:58.24
    Nov 15 06:14:58.240: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename svcaccounts 11/15/23 06:14:58.243
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:14:58.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:14:58.316
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Nov 15 06:14:58.410: INFO: Waiting up to 5m0s for pod "pod-service-account-f554f149-ec28-49ea-ad71-7aa1f3e2a766" in namespace "svcaccounts-6748" to be "running"
    Nov 15 06:14:58.432: INFO: Pod "pod-service-account-f554f149-ec28-49ea-ad71-7aa1f3e2a766": Phase="Pending", Reason="", readiness=false. Elapsed: 22.070632ms
    Nov 15 06:15:00.452: INFO: Pod "pod-service-account-f554f149-ec28-49ea-ad71-7aa1f3e2a766": Phase="Running", Reason="", readiness=true. Elapsed: 2.041929351s
    Nov 15 06:15:00.452: INFO: Pod "pod-service-account-f554f149-ec28-49ea-ad71-7aa1f3e2a766" satisfied condition "running"
    STEP: reading a file in the container 11/15/23 06:15:00.452
    Nov 15 06:15:00.453: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6748 pod-service-account-f554f149-ec28-49ea-ad71-7aa1f3e2a766 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 11/15/23 06:15:00.727
    Nov 15 06:15:00.727: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6748 pod-service-account-f554f149-ec28-49ea-ad71-7aa1f3e2a766 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 11/15/23 06:15:01.01
    Nov 15 06:15:01.010: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6748 pod-service-account-f554f149-ec28-49ea-ad71-7aa1f3e2a766 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Nov 15 06:15:01.421: INFO: Got root ca configmap in namespace "svcaccounts-6748"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:15:01.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6748" for this suite. 11/15/23 06:15:01.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:15:01.494
Nov 15 06:15:01.494: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename configmap 11/15/23 06:15:01.495
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:15:01.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:15:01.577
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-4719/configmap-test-4b603e1c-5224-4e95-9d1d-24ca76c9e7bd 11/15/23 06:15:01.63
STEP: Creating a pod to test consume configMaps 11/15/23 06:15:01.654
Nov 15 06:15:01.696: INFO: Waiting up to 5m0s for pod "pod-configmaps-b8b5936e-412d-4a69-b116-60ef393d840a" in namespace "configmap-4719" to be "Succeeded or Failed"
Nov 15 06:15:01.715: INFO: Pod "pod-configmaps-b8b5936e-412d-4a69-b116-60ef393d840a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.495207ms
Nov 15 06:15:03.736: INFO: Pod "pod-configmaps-b8b5936e-412d-4a69-b116-60ef393d840a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039846443s
Nov 15 06:15:05.766: INFO: Pod "pod-configmaps-b8b5936e-412d-4a69-b116-60ef393d840a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070108688s
Nov 15 06:15:07.739: INFO: Pod "pod-configmaps-b8b5936e-412d-4a69-b116-60ef393d840a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043036354s
STEP: Saw pod success 11/15/23 06:15:07.739
Nov 15 06:15:07.739: INFO: Pod "pod-configmaps-b8b5936e-412d-4a69-b116-60ef393d840a" satisfied condition "Succeeded or Failed"
Nov 15 06:15:07.764: INFO: Trying to get logs from node 10.72.152.81 pod pod-configmaps-b8b5936e-412d-4a69-b116-60ef393d840a container env-test: <nil>
STEP: delete the pod 11/15/23 06:15:07.847
Nov 15 06:15:07.914: INFO: Waiting for pod pod-configmaps-b8b5936e-412d-4a69-b116-60ef393d840a to disappear
Nov 15 06:15:07.935: INFO: Pod pod-configmaps-b8b5936e-412d-4a69-b116-60ef393d840a no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:15:07.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4719" for this suite. 11/15/23 06:15:07.965
------------------------------
â€¢ [SLOW TEST] [6.497 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:15:01.494
    Nov 15 06:15:01.494: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename configmap 11/15/23 06:15:01.495
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:15:01.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:15:01.577
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-4719/configmap-test-4b603e1c-5224-4e95-9d1d-24ca76c9e7bd 11/15/23 06:15:01.63
    STEP: Creating a pod to test consume configMaps 11/15/23 06:15:01.654
    Nov 15 06:15:01.696: INFO: Waiting up to 5m0s for pod "pod-configmaps-b8b5936e-412d-4a69-b116-60ef393d840a" in namespace "configmap-4719" to be "Succeeded or Failed"
    Nov 15 06:15:01.715: INFO: Pod "pod-configmaps-b8b5936e-412d-4a69-b116-60ef393d840a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.495207ms
    Nov 15 06:15:03.736: INFO: Pod "pod-configmaps-b8b5936e-412d-4a69-b116-60ef393d840a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039846443s
    Nov 15 06:15:05.766: INFO: Pod "pod-configmaps-b8b5936e-412d-4a69-b116-60ef393d840a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070108688s
    Nov 15 06:15:07.739: INFO: Pod "pod-configmaps-b8b5936e-412d-4a69-b116-60ef393d840a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043036354s
    STEP: Saw pod success 11/15/23 06:15:07.739
    Nov 15 06:15:07.739: INFO: Pod "pod-configmaps-b8b5936e-412d-4a69-b116-60ef393d840a" satisfied condition "Succeeded or Failed"
    Nov 15 06:15:07.764: INFO: Trying to get logs from node 10.72.152.81 pod pod-configmaps-b8b5936e-412d-4a69-b116-60ef393d840a container env-test: <nil>
    STEP: delete the pod 11/15/23 06:15:07.847
    Nov 15 06:15:07.914: INFO: Waiting for pod pod-configmaps-b8b5936e-412d-4a69-b116-60ef393d840a to disappear
    Nov 15 06:15:07.935: INFO: Pod pod-configmaps-b8b5936e-412d-4a69-b116-60ef393d840a no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:15:07.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4719" for this suite. 11/15/23 06:15:07.965
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:15:07.993
Nov 15 06:15:07.993: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename subpath 11/15/23 06:15:07.994
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:15:08.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:15:08.069
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/15/23 06:15:08.083
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-p8gr 11/15/23 06:15:08.128
STEP: Creating a pod to test atomic-volume-subpath 11/15/23 06:15:08.128
Nov 15 06:15:08.155: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-p8gr" in namespace "subpath-9323" to be "Succeeded or Failed"
Nov 15 06:15:08.172: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Pending", Reason="", readiness=false. Elapsed: 16.937439ms
Nov 15 06:15:10.191: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 2.035813631s
Nov 15 06:15:12.193: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 4.038076038s
Nov 15 06:15:14.196: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 6.040808629s
Nov 15 06:15:16.191: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 8.036320989s
Nov 15 06:15:18.192: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 10.036689844s
Nov 15 06:15:20.188: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 12.032832322s
Nov 15 06:15:22.191: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 14.036180073s
Nov 15 06:15:24.202: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 16.04702471s
Nov 15 06:15:26.194: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 18.039120701s
Nov 15 06:15:28.191: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 20.036223644s
Nov 15 06:15:30.192: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 22.036856782s
Nov 15 06:15:32.195: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=false. Elapsed: 24.040364786s
Nov 15 06:15:34.195: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.040287791s
STEP: Saw pod success 11/15/23 06:15:34.195
Nov 15 06:15:34.195: INFO: Pod "pod-subpath-test-configmap-p8gr" satisfied condition "Succeeded or Failed"
Nov 15 06:15:34.211: INFO: Trying to get logs from node 10.72.152.86 pod pod-subpath-test-configmap-p8gr container test-container-subpath-configmap-p8gr: <nil>
STEP: delete the pod 11/15/23 06:15:34.296
Nov 15 06:15:34.360: INFO: Waiting for pod pod-subpath-test-configmap-p8gr to disappear
Nov 15 06:15:34.378: INFO: Pod pod-subpath-test-configmap-p8gr no longer exists
STEP: Deleting pod pod-subpath-test-configmap-p8gr 11/15/23 06:15:34.378
Nov 15 06:15:34.379: INFO: Deleting pod "pod-subpath-test-configmap-p8gr" in namespace "subpath-9323"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Nov 15 06:15:34.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9323" for this suite. 11/15/23 06:15:34.428
------------------------------
â€¢ [SLOW TEST] [26.466 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:15:07.993
    Nov 15 06:15:07.993: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename subpath 11/15/23 06:15:07.994
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:15:08.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:15:08.069
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/15/23 06:15:08.083
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-p8gr 11/15/23 06:15:08.128
    STEP: Creating a pod to test atomic-volume-subpath 11/15/23 06:15:08.128
    Nov 15 06:15:08.155: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-p8gr" in namespace "subpath-9323" to be "Succeeded or Failed"
    Nov 15 06:15:08.172: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Pending", Reason="", readiness=false. Elapsed: 16.937439ms
    Nov 15 06:15:10.191: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 2.035813631s
    Nov 15 06:15:12.193: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 4.038076038s
    Nov 15 06:15:14.196: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 6.040808629s
    Nov 15 06:15:16.191: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 8.036320989s
    Nov 15 06:15:18.192: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 10.036689844s
    Nov 15 06:15:20.188: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 12.032832322s
    Nov 15 06:15:22.191: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 14.036180073s
    Nov 15 06:15:24.202: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 16.04702471s
    Nov 15 06:15:26.194: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 18.039120701s
    Nov 15 06:15:28.191: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 20.036223644s
    Nov 15 06:15:30.192: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=true. Elapsed: 22.036856782s
    Nov 15 06:15:32.195: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Running", Reason="", readiness=false. Elapsed: 24.040364786s
    Nov 15 06:15:34.195: INFO: Pod "pod-subpath-test-configmap-p8gr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.040287791s
    STEP: Saw pod success 11/15/23 06:15:34.195
    Nov 15 06:15:34.195: INFO: Pod "pod-subpath-test-configmap-p8gr" satisfied condition "Succeeded or Failed"
    Nov 15 06:15:34.211: INFO: Trying to get logs from node 10.72.152.86 pod pod-subpath-test-configmap-p8gr container test-container-subpath-configmap-p8gr: <nil>
    STEP: delete the pod 11/15/23 06:15:34.296
    Nov 15 06:15:34.360: INFO: Waiting for pod pod-subpath-test-configmap-p8gr to disappear
    Nov 15 06:15:34.378: INFO: Pod pod-subpath-test-configmap-p8gr no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-p8gr 11/15/23 06:15:34.378
    Nov 15 06:15:34.379: INFO: Deleting pod "pod-subpath-test-configmap-p8gr" in namespace "subpath-9323"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:15:34.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9323" for this suite. 11/15/23 06:15:34.428
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:15:34.459
Nov 15 06:15:34.459: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename configmap 11/15/23 06:15:34.46
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:15:34.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:15:34.529
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 11/15/23 06:15:34.545
STEP: fetching the ConfigMap 11/15/23 06:15:34.564
STEP: patching the ConfigMap 11/15/23 06:15:34.583
STEP: listing all ConfigMaps in all namespaces with a label selector 11/15/23 06:15:34.604
STEP: deleting the ConfigMap by collection with a label selector 11/15/23 06:15:34.71
STEP: listing all ConfigMaps in test namespace 11/15/23 06:15:34.753
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:15:34.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6876" for this suite. 11/15/23 06:15:34.792
------------------------------
â€¢ [0.364 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:15:34.459
    Nov 15 06:15:34.459: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename configmap 11/15/23 06:15:34.46
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:15:34.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:15:34.529
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 11/15/23 06:15:34.545
    STEP: fetching the ConfigMap 11/15/23 06:15:34.564
    STEP: patching the ConfigMap 11/15/23 06:15:34.583
    STEP: listing all ConfigMaps in all namespaces with a label selector 11/15/23 06:15:34.604
    STEP: deleting the ConfigMap by collection with a label selector 11/15/23 06:15:34.71
    STEP: listing all ConfigMaps in test namespace 11/15/23 06:15:34.753
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:15:34.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6876" for this suite. 11/15/23 06:15:34.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:15:34.823
Nov 15 06:15:34.824: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubelet-test 11/15/23 06:15:34.824
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:15:34.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:15:34.891
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Nov 15 06:15:34.945: INFO: Waiting up to 5m0s for pod "busybox-scheduling-bdda97fe-4375-41aa-b317-632a08520862" in namespace "kubelet-test-7482" to be "running and ready"
Nov 15 06:15:34.972: INFO: Pod "busybox-scheduling-bdda97fe-4375-41aa-b317-632a08520862": Phase="Pending", Reason="", readiness=false. Elapsed: 26.916119ms
Nov 15 06:15:34.972: INFO: The phase of Pod busybox-scheduling-bdda97fe-4375-41aa-b317-632a08520862 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:15:36.990: INFO: Pod "busybox-scheduling-bdda97fe-4375-41aa-b317-632a08520862": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045050157s
Nov 15 06:15:36.990: INFO: The phase of Pod busybox-scheduling-bdda97fe-4375-41aa-b317-632a08520862 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:15:38.991: INFO: Pod "busybox-scheduling-bdda97fe-4375-41aa-b317-632a08520862": Phase="Running", Reason="", readiness=true. Elapsed: 4.046312631s
Nov 15 06:15:38.991: INFO: The phase of Pod busybox-scheduling-bdda97fe-4375-41aa-b317-632a08520862 is Running (Ready = true)
Nov 15 06:15:38.991: INFO: Pod "busybox-scheduling-bdda97fe-4375-41aa-b317-632a08520862" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Nov 15 06:15:39.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7482" for this suite. 11/15/23 06:15:39.084
------------------------------
â€¢ [4.288 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:15:34.823
    Nov 15 06:15:34.824: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubelet-test 11/15/23 06:15:34.824
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:15:34.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:15:34.891
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Nov 15 06:15:34.945: INFO: Waiting up to 5m0s for pod "busybox-scheduling-bdda97fe-4375-41aa-b317-632a08520862" in namespace "kubelet-test-7482" to be "running and ready"
    Nov 15 06:15:34.972: INFO: Pod "busybox-scheduling-bdda97fe-4375-41aa-b317-632a08520862": Phase="Pending", Reason="", readiness=false. Elapsed: 26.916119ms
    Nov 15 06:15:34.972: INFO: The phase of Pod busybox-scheduling-bdda97fe-4375-41aa-b317-632a08520862 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:15:36.990: INFO: Pod "busybox-scheduling-bdda97fe-4375-41aa-b317-632a08520862": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045050157s
    Nov 15 06:15:36.990: INFO: The phase of Pod busybox-scheduling-bdda97fe-4375-41aa-b317-632a08520862 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:15:38.991: INFO: Pod "busybox-scheduling-bdda97fe-4375-41aa-b317-632a08520862": Phase="Running", Reason="", readiness=true. Elapsed: 4.046312631s
    Nov 15 06:15:38.991: INFO: The phase of Pod busybox-scheduling-bdda97fe-4375-41aa-b317-632a08520862 is Running (Ready = true)
    Nov 15 06:15:38.991: INFO: Pod "busybox-scheduling-bdda97fe-4375-41aa-b317-632a08520862" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:15:39.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7482" for this suite. 11/15/23 06:15:39.084
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:15:39.113
Nov 15 06:15:39.113: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename var-expansion 11/15/23 06:15:39.114
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:15:39.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:15:39.184
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 11/15/23 06:15:39.198
W1115 06:15:39.235744      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Nov 15 06:15:39.235: INFO: Waiting up to 5m0s for pod "var-expansion-76c1f600-d620-4675-922b-29f6b4b2d9bc" in namespace "var-expansion-2173" to be "Succeeded or Failed"
Nov 15 06:15:39.259: INFO: Pod "var-expansion-76c1f600-d620-4675-922b-29f6b4b2d9bc": Phase="Pending", Reason="", readiness=false. Elapsed: 23.995959ms
Nov 15 06:15:41.279: INFO: Pod "var-expansion-76c1f600-d620-4675-922b-29f6b4b2d9bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043912507s
Nov 15 06:15:43.279: INFO: Pod "var-expansion-76c1f600-d620-4675-922b-29f6b4b2d9bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043347474s
Nov 15 06:15:45.278: INFO: Pod "var-expansion-76c1f600-d620-4675-922b-29f6b4b2d9bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042282413s
STEP: Saw pod success 11/15/23 06:15:45.278
Nov 15 06:15:45.278: INFO: Pod "var-expansion-76c1f600-d620-4675-922b-29f6b4b2d9bc" satisfied condition "Succeeded or Failed"
Nov 15 06:15:45.296: INFO: Trying to get logs from node 10.72.152.86 pod var-expansion-76c1f600-d620-4675-922b-29f6b4b2d9bc container dapi-container: <nil>
STEP: delete the pod 11/15/23 06:15:45.354
Nov 15 06:15:45.417: INFO: Waiting for pod var-expansion-76c1f600-d620-4675-922b-29f6b4b2d9bc to disappear
Nov 15 06:15:45.434: INFO: Pod var-expansion-76c1f600-d620-4675-922b-29f6b4b2d9bc no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 15 06:15:45.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2173" for this suite. 11/15/23 06:15:45.466
------------------------------
â€¢ [SLOW TEST] [6.379 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:15:39.113
    Nov 15 06:15:39.113: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename var-expansion 11/15/23 06:15:39.114
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:15:39.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:15:39.184
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 11/15/23 06:15:39.198
    W1115 06:15:39.235744      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Nov 15 06:15:39.235: INFO: Waiting up to 5m0s for pod "var-expansion-76c1f600-d620-4675-922b-29f6b4b2d9bc" in namespace "var-expansion-2173" to be "Succeeded or Failed"
    Nov 15 06:15:39.259: INFO: Pod "var-expansion-76c1f600-d620-4675-922b-29f6b4b2d9bc": Phase="Pending", Reason="", readiness=false. Elapsed: 23.995959ms
    Nov 15 06:15:41.279: INFO: Pod "var-expansion-76c1f600-d620-4675-922b-29f6b4b2d9bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043912507s
    Nov 15 06:15:43.279: INFO: Pod "var-expansion-76c1f600-d620-4675-922b-29f6b4b2d9bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043347474s
    Nov 15 06:15:45.278: INFO: Pod "var-expansion-76c1f600-d620-4675-922b-29f6b4b2d9bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042282413s
    STEP: Saw pod success 11/15/23 06:15:45.278
    Nov 15 06:15:45.278: INFO: Pod "var-expansion-76c1f600-d620-4675-922b-29f6b4b2d9bc" satisfied condition "Succeeded or Failed"
    Nov 15 06:15:45.296: INFO: Trying to get logs from node 10.72.152.86 pod var-expansion-76c1f600-d620-4675-922b-29f6b4b2d9bc container dapi-container: <nil>
    STEP: delete the pod 11/15/23 06:15:45.354
    Nov 15 06:15:45.417: INFO: Waiting for pod var-expansion-76c1f600-d620-4675-922b-29f6b4b2d9bc to disappear
    Nov 15 06:15:45.434: INFO: Pod var-expansion-76c1f600-d620-4675-922b-29f6b4b2d9bc no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:15:45.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2173" for this suite. 11/15/23 06:15:45.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:15:45.494
Nov 15 06:15:45.494: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 06:15:45.494
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:15:45.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:15:45.574
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 11/15/23 06:15:45.585
Nov 15 06:15:45.587: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 11/15/23 06:15:59.795
Nov 15 06:15:59.795: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 06:16:04.513: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:16:21.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1334" for this suite. 11/15/23 06:16:21.472
------------------------------
â€¢ [SLOW TEST] [36.006 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:15:45.494
    Nov 15 06:15:45.494: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 06:15:45.494
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:15:45.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:15:45.574
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 11/15/23 06:15:45.585
    Nov 15 06:15:45.587: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 11/15/23 06:15:59.795
    Nov 15 06:15:59.795: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 06:16:04.513: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:16:21.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1334" for this suite. 11/15/23 06:16:21.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:16:21.509
Nov 15 06:16:21.509: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename pods 11/15/23 06:16:21.51
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:16:21.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:16:21.583
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 11/15/23 06:16:21.656
STEP: watching for Pod to be ready 11/15/23 06:16:21.693
Nov 15 06:16:21.701: INFO: observed Pod pod-test in namespace pods-3890 in phase Pending with labels: map[test-pod-static:true] & conditions []
Nov 15 06:16:21.702: INFO: observed Pod pod-test in namespace pods-3890 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC  }]
Nov 15 06:16:21.749: INFO: observed Pod pod-test in namespace pods-3890 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC  }]
Nov 15 06:16:22.561: INFO: observed Pod pod-test in namespace pods-3890 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC  }]
Nov 15 06:16:22.648: INFO: observed Pod pod-test in namespace pods-3890 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC  }]
Nov 15 06:16:23.559: INFO: Found Pod pod-test in namespace pods-3890 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:23 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:23 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 11/15/23 06:16:23.577
STEP: getting the Pod and ensuring that it's patched 11/15/23 06:16:23.621
STEP: replacing the Pod's status Ready condition to False 11/15/23 06:16:23.638
STEP: check the Pod again to ensure its Ready conditions are False 11/15/23 06:16:23.688
STEP: deleting the Pod via a Collection with a LabelSelector 11/15/23 06:16:23.689
STEP: watching for the Pod to be deleted 11/15/23 06:16:23.739
Nov 15 06:16:23.746: INFO: observed event type MODIFIED
Nov 15 06:16:24.331: INFO: observed event type MODIFIED
Nov 15 06:16:25.874: INFO: observed event type MODIFIED
Nov 15 06:16:26.575: INFO: observed event type MODIFIED
Nov 15 06:16:26.891: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 06:16:26.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3890" for this suite. 11/15/23 06:16:26.955
------------------------------
â€¢ [SLOW TEST] [5.471 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:16:21.509
    Nov 15 06:16:21.509: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename pods 11/15/23 06:16:21.51
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:16:21.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:16:21.583
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 11/15/23 06:16:21.656
    STEP: watching for Pod to be ready 11/15/23 06:16:21.693
    Nov 15 06:16:21.701: INFO: observed Pod pod-test in namespace pods-3890 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Nov 15 06:16:21.702: INFO: observed Pod pod-test in namespace pods-3890 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC  }]
    Nov 15 06:16:21.749: INFO: observed Pod pod-test in namespace pods-3890 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC  }]
    Nov 15 06:16:22.561: INFO: observed Pod pod-test in namespace pods-3890 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC  }]
    Nov 15 06:16:22.648: INFO: observed Pod pod-test in namespace pods-3890 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC  }]
    Nov 15 06:16:23.559: INFO: Found Pod pod-test in namespace pods-3890 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:23 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:23 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:16:21 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 11/15/23 06:16:23.577
    STEP: getting the Pod and ensuring that it's patched 11/15/23 06:16:23.621
    STEP: replacing the Pod's status Ready condition to False 11/15/23 06:16:23.638
    STEP: check the Pod again to ensure its Ready conditions are False 11/15/23 06:16:23.688
    STEP: deleting the Pod via a Collection with a LabelSelector 11/15/23 06:16:23.689
    STEP: watching for the Pod to be deleted 11/15/23 06:16:23.739
    Nov 15 06:16:23.746: INFO: observed event type MODIFIED
    Nov 15 06:16:24.331: INFO: observed event type MODIFIED
    Nov 15 06:16:25.874: INFO: observed event type MODIFIED
    Nov 15 06:16:26.575: INFO: observed event type MODIFIED
    Nov 15 06:16:26.891: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:16:26.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3890" for this suite. 11/15/23 06:16:26.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:16:26.983
Nov 15 06:16:26.983: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubectl 11/15/23 06:16:26.984
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:16:27.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:16:27.042
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/15/23 06:16:27.056
Nov 15 06:16:27.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2308 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Nov 15 06:16:27.189: INFO: stderr: ""
Nov 15 06:16:27.189: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 11/15/23 06:16:27.189
STEP: verifying the pod e2e-test-httpd-pod was created 11/15/23 06:16:32.24
Nov 15 06:16:32.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2308 get pod e2e-test-httpd-pod -o json'
Nov 15 06:16:32.341: INFO: stderr: ""
Nov 15 06:16:32.341: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"33165972f37769324591fe8bffdfcec9e8e1ae08e48ce9efa05b55080540f190\",\n            \"cni.projectcalico.org/podIP\": \"172.30.213.169/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.213.169/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.213.169\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-11-15T06:16:27Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2308\",\n        \"resourceVersion\": \"81538\",\n        \"uid\": \"d86399a7-2a92-4f70-899a-b4df8969e172\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-46cqw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-rc9fs\"\n            }\n        ],\n        \"nodeName\": \"10.72.152.86\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c48,c7\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-46cqw\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-15T06:16:27Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-15T06:16:29Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-15T06:16:29Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-15T06:16:27Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://a9a81aacdadd7784cf5203b51f030579c9030cf10e4e1589df9d31866d13405c\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-11-15T06:16:28Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.72.152.86\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.213.169\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.213.169\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-11-15T06:16:27Z\"\n    }\n}\n"
STEP: replace the image in the pod 11/15/23 06:16:32.341
Nov 15 06:16:32.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2308 replace -f -'
Nov 15 06:16:33.852: INFO: stderr: ""
Nov 15 06:16:33.852: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 11/15/23 06:16:33.852
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Nov 15 06:16:33.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2308 delete pods e2e-test-httpd-pod'
Nov 15 06:16:35.751: INFO: stderr: ""
Nov 15 06:16:35.751: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 06:16:35.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2308" for this suite. 11/15/23 06:16:35.787
------------------------------
â€¢ [SLOW TEST] [8.831 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:16:26.983
    Nov 15 06:16:26.983: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubectl 11/15/23 06:16:26.984
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:16:27.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:16:27.042
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/15/23 06:16:27.056
    Nov 15 06:16:27.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2308 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Nov 15 06:16:27.189: INFO: stderr: ""
    Nov 15 06:16:27.189: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 11/15/23 06:16:27.189
    STEP: verifying the pod e2e-test-httpd-pod was created 11/15/23 06:16:32.24
    Nov 15 06:16:32.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2308 get pod e2e-test-httpd-pod -o json'
    Nov 15 06:16:32.341: INFO: stderr: ""
    Nov 15 06:16:32.341: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"33165972f37769324591fe8bffdfcec9e8e1ae08e48ce9efa05b55080540f190\",\n            \"cni.projectcalico.org/podIP\": \"172.30.213.169/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.213.169/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.213.169\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-11-15T06:16:27Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2308\",\n        \"resourceVersion\": \"81538\",\n        \"uid\": \"d86399a7-2a92-4f70-899a-b4df8969e172\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-46cqw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-rc9fs\"\n            }\n        ],\n        \"nodeName\": \"10.72.152.86\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c48,c7\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-46cqw\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-15T06:16:27Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-15T06:16:29Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-15T06:16:29Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-15T06:16:27Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://a9a81aacdadd7784cf5203b51f030579c9030cf10e4e1589df9d31866d13405c\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-11-15T06:16:28Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.72.152.86\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.213.169\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.213.169\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-11-15T06:16:27Z\"\n    }\n}\n"
    STEP: replace the image in the pod 11/15/23 06:16:32.341
    Nov 15 06:16:32.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2308 replace -f -'
    Nov 15 06:16:33.852: INFO: stderr: ""
    Nov 15 06:16:33.852: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 11/15/23 06:16:33.852
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Nov 15 06:16:33.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2308 delete pods e2e-test-httpd-pod'
    Nov 15 06:16:35.751: INFO: stderr: ""
    Nov 15 06:16:35.751: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:16:35.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2308" for this suite. 11/15/23 06:16:35.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:16:35.814
Nov 15 06:16:35.814: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename resourcequota 11/15/23 06:16:35.815
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:16:35.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:16:35.883
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 11/15/23 06:16:35.897
STEP: Creating a ResourceQuota 11/15/23 06:16:40.915
STEP: Ensuring resource quota status is calculated 11/15/23 06:16:40.936
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 06:16:42.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6446" for this suite. 11/15/23 06:16:42.992
------------------------------
â€¢ [SLOW TEST] [7.216 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:16:35.814
    Nov 15 06:16:35.814: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename resourcequota 11/15/23 06:16:35.815
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:16:35.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:16:35.883
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 11/15/23 06:16:35.897
    STEP: Creating a ResourceQuota 11/15/23 06:16:40.915
    STEP: Ensuring resource quota status is calculated 11/15/23 06:16:40.936
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:16:42.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6446" for this suite. 11/15/23 06:16:42.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:16:43.034
Nov 15 06:16:43.034: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename security-context 11/15/23 06:16:43.035
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:16:43.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:16:43.106
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 11/15/23 06:16:43.117
Nov 15 06:16:43.155: INFO: Waiting up to 5m0s for pod "security-context-a92e6502-ee9f-4e98-b070-4e2d97c158f6" in namespace "security-context-4879" to be "Succeeded or Failed"
Nov 15 06:16:43.175: INFO: Pod "security-context-a92e6502-ee9f-4e98-b070-4e2d97c158f6": Phase="Pending", Reason="", readiness=false. Elapsed: 20.18147ms
Nov 15 06:16:45.197: INFO: Pod "security-context-a92e6502-ee9f-4e98-b070-4e2d97c158f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042630751s
Nov 15 06:16:47.193: INFO: Pod "security-context-a92e6502-ee9f-4e98-b070-4e2d97c158f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038186683s
STEP: Saw pod success 11/15/23 06:16:47.193
Nov 15 06:16:47.193: INFO: Pod "security-context-a92e6502-ee9f-4e98-b070-4e2d97c158f6" satisfied condition "Succeeded or Failed"
Nov 15 06:16:47.211: INFO: Trying to get logs from node 10.72.152.81 pod security-context-a92e6502-ee9f-4e98-b070-4e2d97c158f6 container test-container: <nil>
STEP: delete the pod 11/15/23 06:16:47.256
Nov 15 06:16:47.318: INFO: Waiting for pod security-context-a92e6502-ee9f-4e98-b070-4e2d97c158f6 to disappear
Nov 15 06:16:47.336: INFO: Pod security-context-a92e6502-ee9f-4e98-b070-4e2d97c158f6 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Nov 15 06:16:47.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-4879" for this suite. 11/15/23 06:16:47.395
------------------------------
â€¢ [4.394 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:16:43.034
    Nov 15 06:16:43.034: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename security-context 11/15/23 06:16:43.035
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:16:43.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:16:43.106
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 11/15/23 06:16:43.117
    Nov 15 06:16:43.155: INFO: Waiting up to 5m0s for pod "security-context-a92e6502-ee9f-4e98-b070-4e2d97c158f6" in namespace "security-context-4879" to be "Succeeded or Failed"
    Nov 15 06:16:43.175: INFO: Pod "security-context-a92e6502-ee9f-4e98-b070-4e2d97c158f6": Phase="Pending", Reason="", readiness=false. Elapsed: 20.18147ms
    Nov 15 06:16:45.197: INFO: Pod "security-context-a92e6502-ee9f-4e98-b070-4e2d97c158f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042630751s
    Nov 15 06:16:47.193: INFO: Pod "security-context-a92e6502-ee9f-4e98-b070-4e2d97c158f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038186683s
    STEP: Saw pod success 11/15/23 06:16:47.193
    Nov 15 06:16:47.193: INFO: Pod "security-context-a92e6502-ee9f-4e98-b070-4e2d97c158f6" satisfied condition "Succeeded or Failed"
    Nov 15 06:16:47.211: INFO: Trying to get logs from node 10.72.152.81 pod security-context-a92e6502-ee9f-4e98-b070-4e2d97c158f6 container test-container: <nil>
    STEP: delete the pod 11/15/23 06:16:47.256
    Nov 15 06:16:47.318: INFO: Waiting for pod security-context-a92e6502-ee9f-4e98-b070-4e2d97c158f6 to disappear
    Nov 15 06:16:47.336: INFO: Pod security-context-a92e6502-ee9f-4e98-b070-4e2d97c158f6 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:16:47.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-4879" for this suite. 11/15/23 06:16:47.395
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:16:47.43
Nov 15 06:16:47.430: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename limitrange 11/15/23 06:16:47.431
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:16:47.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:16:47.499
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 11/15/23 06:16:47.512
STEP: Setting up watch 11/15/23 06:16:47.512
STEP: Submitting a LimitRange 11/15/23 06:16:47.625
STEP: Verifying LimitRange creation was observed 11/15/23 06:16:47.642
STEP: Fetching the LimitRange to ensure it has proper values 11/15/23 06:16:47.642
Nov 15 06:16:47.652: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 15 06:16:47.652: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 11/15/23 06:16:47.652
STEP: Ensuring Pod has resource requirements applied from LimitRange 11/15/23 06:16:47.677
Nov 15 06:16:47.694: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 15 06:16:47.694: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 11/15/23 06:16:47.694
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 11/15/23 06:16:47.72
Nov 15 06:16:47.739: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Nov 15 06:16:47.739: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 11/15/23 06:16:47.739
STEP: Failing to create a Pod with more than max resources 11/15/23 06:16:47.751
STEP: Updating a LimitRange 11/15/23 06:16:47.761
STEP: Verifying LimitRange updating is effective 11/15/23 06:16:47.779
STEP: Creating a Pod with less than former min resources 11/15/23 06:16:49.79
STEP: Failing to create a Pod with more than max resources 11/15/23 06:16:49.812
STEP: Deleting a LimitRange 11/15/23 06:16:49.821
STEP: Verifying the LimitRange was deleted 11/15/23 06:16:49.838
Nov 15 06:16:54.854: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 11/15/23 06:16:54.855
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Nov 15 06:16:54.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-1123" for this suite. 11/15/23 06:16:54.919
------------------------------
â€¢ [SLOW TEST] [7.516 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:16:47.43
    Nov 15 06:16:47.430: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename limitrange 11/15/23 06:16:47.431
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:16:47.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:16:47.499
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 11/15/23 06:16:47.512
    STEP: Setting up watch 11/15/23 06:16:47.512
    STEP: Submitting a LimitRange 11/15/23 06:16:47.625
    STEP: Verifying LimitRange creation was observed 11/15/23 06:16:47.642
    STEP: Fetching the LimitRange to ensure it has proper values 11/15/23 06:16:47.642
    Nov 15 06:16:47.652: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Nov 15 06:16:47.652: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 11/15/23 06:16:47.652
    STEP: Ensuring Pod has resource requirements applied from LimitRange 11/15/23 06:16:47.677
    Nov 15 06:16:47.694: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Nov 15 06:16:47.694: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 11/15/23 06:16:47.694
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 11/15/23 06:16:47.72
    Nov 15 06:16:47.739: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Nov 15 06:16:47.739: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 11/15/23 06:16:47.739
    STEP: Failing to create a Pod with more than max resources 11/15/23 06:16:47.751
    STEP: Updating a LimitRange 11/15/23 06:16:47.761
    STEP: Verifying LimitRange updating is effective 11/15/23 06:16:47.779
    STEP: Creating a Pod with less than former min resources 11/15/23 06:16:49.79
    STEP: Failing to create a Pod with more than max resources 11/15/23 06:16:49.812
    STEP: Deleting a LimitRange 11/15/23 06:16:49.821
    STEP: Verifying the LimitRange was deleted 11/15/23 06:16:49.838
    Nov 15 06:16:54.854: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 11/15/23 06:16:54.855
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:16:54.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-1123" for this suite. 11/15/23 06:16:54.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:16:54.947
Nov 15 06:16:54.947: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename downward-api 11/15/23 06:16:54.948
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:16:55.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:16:55.015
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 11/15/23 06:16:55.028
Nov 15 06:16:55.068: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ccbcd512-dcea-4858-b9d8-8d508bc8136d" in namespace "downward-api-5887" to be "Succeeded or Failed"
Nov 15 06:16:55.087: INFO: Pod "downwardapi-volume-ccbcd512-dcea-4858-b9d8-8d508bc8136d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.974891ms
Nov 15 06:16:57.119: INFO: Pod "downwardapi-volume-ccbcd512-dcea-4858-b9d8-8d508bc8136d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050855765s
Nov 15 06:16:59.107: INFO: Pod "downwardapi-volume-ccbcd512-dcea-4858-b9d8-8d508bc8136d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038876764s
STEP: Saw pod success 11/15/23 06:16:59.107
Nov 15 06:16:59.107: INFO: Pod "downwardapi-volume-ccbcd512-dcea-4858-b9d8-8d508bc8136d" satisfied condition "Succeeded or Failed"
Nov 15 06:16:59.124: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-ccbcd512-dcea-4858-b9d8-8d508bc8136d container client-container: <nil>
STEP: delete the pod 11/15/23 06:16:59.163
Nov 15 06:16:59.256: INFO: Waiting for pod downwardapi-volume-ccbcd512-dcea-4858-b9d8-8d508bc8136d to disappear
Nov 15 06:16:59.298: INFO: Pod downwardapi-volume-ccbcd512-dcea-4858-b9d8-8d508bc8136d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 06:16:59.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5887" for this suite. 11/15/23 06:16:59.357
------------------------------
â€¢ [4.438 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:16:54.947
    Nov 15 06:16:54.947: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename downward-api 11/15/23 06:16:54.948
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:16:55.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:16:55.015
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 11/15/23 06:16:55.028
    Nov 15 06:16:55.068: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ccbcd512-dcea-4858-b9d8-8d508bc8136d" in namespace "downward-api-5887" to be "Succeeded or Failed"
    Nov 15 06:16:55.087: INFO: Pod "downwardapi-volume-ccbcd512-dcea-4858-b9d8-8d508bc8136d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.974891ms
    Nov 15 06:16:57.119: INFO: Pod "downwardapi-volume-ccbcd512-dcea-4858-b9d8-8d508bc8136d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050855765s
    Nov 15 06:16:59.107: INFO: Pod "downwardapi-volume-ccbcd512-dcea-4858-b9d8-8d508bc8136d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038876764s
    STEP: Saw pod success 11/15/23 06:16:59.107
    Nov 15 06:16:59.107: INFO: Pod "downwardapi-volume-ccbcd512-dcea-4858-b9d8-8d508bc8136d" satisfied condition "Succeeded or Failed"
    Nov 15 06:16:59.124: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-ccbcd512-dcea-4858-b9d8-8d508bc8136d container client-container: <nil>
    STEP: delete the pod 11/15/23 06:16:59.163
    Nov 15 06:16:59.256: INFO: Waiting for pod downwardapi-volume-ccbcd512-dcea-4858-b9d8-8d508bc8136d to disappear
    Nov 15 06:16:59.298: INFO: Pod downwardapi-volume-ccbcd512-dcea-4858-b9d8-8d508bc8136d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:16:59.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5887" for this suite. 11/15/23 06:16:59.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:16:59.386
Nov 15 06:16:59.386: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename configmap 11/15/23 06:16:59.388
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:16:59.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:16:59.52
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-efb5f5b7-1352-4750-baa4-fcc7d1d51f85 11/15/23 06:16:59.587
STEP: Creating a pod to test consume configMaps 11/15/23 06:16:59.622
Nov 15 06:16:59.666: INFO: Waiting up to 5m0s for pod "pod-configmaps-02dde9d2-4563-4e4a-9b18-b6fdd62a3dda" in namespace "configmap-8927" to be "Succeeded or Failed"
Nov 15 06:16:59.693: INFO: Pod "pod-configmaps-02dde9d2-4563-4e4a-9b18-b6fdd62a3dda": Phase="Pending", Reason="", readiness=false. Elapsed: 26.107037ms
Nov 15 06:17:01.714: INFO: Pod "pod-configmaps-02dde9d2-4563-4e4a-9b18-b6fdd62a3dda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046938888s
Nov 15 06:17:03.712: INFO: Pod "pod-configmaps-02dde9d2-4563-4e4a-9b18-b6fdd62a3dda": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045373065s
Nov 15 06:17:05.711: INFO: Pod "pod-configmaps-02dde9d2-4563-4e4a-9b18-b6fdd62a3dda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044043485s
STEP: Saw pod success 11/15/23 06:17:05.711
Nov 15 06:17:05.711: INFO: Pod "pod-configmaps-02dde9d2-4563-4e4a-9b18-b6fdd62a3dda" satisfied condition "Succeeded or Failed"
Nov 15 06:17:05.729: INFO: Trying to get logs from node 10.72.152.86 pod pod-configmaps-02dde9d2-4563-4e4a-9b18-b6fdd62a3dda container agnhost-container: <nil>
STEP: delete the pod 11/15/23 06:17:05.769
Nov 15 06:17:05.819: INFO: Waiting for pod pod-configmaps-02dde9d2-4563-4e4a-9b18-b6fdd62a3dda to disappear
Nov 15 06:17:05.838: INFO: Pod pod-configmaps-02dde9d2-4563-4e4a-9b18-b6fdd62a3dda no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:17:05.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8927" for this suite. 11/15/23 06:17:05.866
------------------------------
â€¢ [SLOW TEST] [6.505 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:16:59.386
    Nov 15 06:16:59.386: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename configmap 11/15/23 06:16:59.388
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:16:59.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:16:59.52
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-efb5f5b7-1352-4750-baa4-fcc7d1d51f85 11/15/23 06:16:59.587
    STEP: Creating a pod to test consume configMaps 11/15/23 06:16:59.622
    Nov 15 06:16:59.666: INFO: Waiting up to 5m0s for pod "pod-configmaps-02dde9d2-4563-4e4a-9b18-b6fdd62a3dda" in namespace "configmap-8927" to be "Succeeded or Failed"
    Nov 15 06:16:59.693: INFO: Pod "pod-configmaps-02dde9d2-4563-4e4a-9b18-b6fdd62a3dda": Phase="Pending", Reason="", readiness=false. Elapsed: 26.107037ms
    Nov 15 06:17:01.714: INFO: Pod "pod-configmaps-02dde9d2-4563-4e4a-9b18-b6fdd62a3dda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046938888s
    Nov 15 06:17:03.712: INFO: Pod "pod-configmaps-02dde9d2-4563-4e4a-9b18-b6fdd62a3dda": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045373065s
    Nov 15 06:17:05.711: INFO: Pod "pod-configmaps-02dde9d2-4563-4e4a-9b18-b6fdd62a3dda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044043485s
    STEP: Saw pod success 11/15/23 06:17:05.711
    Nov 15 06:17:05.711: INFO: Pod "pod-configmaps-02dde9d2-4563-4e4a-9b18-b6fdd62a3dda" satisfied condition "Succeeded or Failed"
    Nov 15 06:17:05.729: INFO: Trying to get logs from node 10.72.152.86 pod pod-configmaps-02dde9d2-4563-4e4a-9b18-b6fdd62a3dda container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 06:17:05.769
    Nov 15 06:17:05.819: INFO: Waiting for pod pod-configmaps-02dde9d2-4563-4e4a-9b18-b6fdd62a3dda to disappear
    Nov 15 06:17:05.838: INFO: Pod pod-configmaps-02dde9d2-4563-4e4a-9b18-b6fdd62a3dda no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:17:05.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8927" for this suite. 11/15/23 06:17:05.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:17:05.893
Nov 15 06:17:05.893: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename container-runtime 11/15/23 06:17:05.894
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:05.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:17:05.964
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 11/15/23 06:17:05.975
W1115 06:17:06.005617      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: wait for the container to reach Succeeded 11/15/23 06:17:06.005
STEP: get the container status 11/15/23 06:17:10.102
STEP: the container should be terminated 11/15/23 06:17:10.122
STEP: the termination message should be set 11/15/23 06:17:10.123
Nov 15 06:17:10.123: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 11/15/23 06:17:10.123
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Nov 15 06:17:10.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9803" for this suite. 11/15/23 06:17:10.242
------------------------------
â€¢ [4.376 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:17:05.893
    Nov 15 06:17:05.893: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename container-runtime 11/15/23 06:17:05.894
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:05.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:17:05.964
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 11/15/23 06:17:05.975
    W1115 06:17:06.005617      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: wait for the container to reach Succeeded 11/15/23 06:17:06.005
    STEP: get the container status 11/15/23 06:17:10.102
    STEP: the container should be terminated 11/15/23 06:17:10.122
    STEP: the termination message should be set 11/15/23 06:17:10.123
    Nov 15 06:17:10.123: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 11/15/23 06:17:10.123
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:17:10.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9803" for this suite. 11/15/23 06:17:10.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:17:10.272
Nov 15 06:17:10.272: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename downward-api 11/15/23 06:17:10.273
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:10.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:17:10.34
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 11/15/23 06:17:10.352
Nov 15 06:17:10.388: INFO: Waiting up to 5m0s for pod "labelsupdatec4370ca5-9b83-42b3-b0d2-0b8ad0541e1d" in namespace "downward-api-1787" to be "running and ready"
Nov 15 06:17:10.407: INFO: Pod "labelsupdatec4370ca5-9b83-42b3-b0d2-0b8ad0541e1d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.857861ms
Nov 15 06:17:10.408: INFO: The phase of Pod labelsupdatec4370ca5-9b83-42b3-b0d2-0b8ad0541e1d is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:17:12.430: INFO: Pod "labelsupdatec4370ca5-9b83-42b3-b0d2-0b8ad0541e1d": Phase="Running", Reason="", readiness=true. Elapsed: 2.041474651s
Nov 15 06:17:12.430: INFO: The phase of Pod labelsupdatec4370ca5-9b83-42b3-b0d2-0b8ad0541e1d is Running (Ready = true)
Nov 15 06:17:12.430: INFO: Pod "labelsupdatec4370ca5-9b83-42b3-b0d2-0b8ad0541e1d" satisfied condition "running and ready"
Nov 15 06:17:13.089: INFO: Successfully updated pod "labelsupdatec4370ca5-9b83-42b3-b0d2-0b8ad0541e1d"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 06:17:17.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1787" for this suite. 11/15/23 06:17:17.257
------------------------------
â€¢ [SLOW TEST] [7.012 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:17:10.272
    Nov 15 06:17:10.272: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename downward-api 11/15/23 06:17:10.273
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:10.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:17:10.34
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 11/15/23 06:17:10.352
    Nov 15 06:17:10.388: INFO: Waiting up to 5m0s for pod "labelsupdatec4370ca5-9b83-42b3-b0d2-0b8ad0541e1d" in namespace "downward-api-1787" to be "running and ready"
    Nov 15 06:17:10.407: INFO: Pod "labelsupdatec4370ca5-9b83-42b3-b0d2-0b8ad0541e1d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.857861ms
    Nov 15 06:17:10.408: INFO: The phase of Pod labelsupdatec4370ca5-9b83-42b3-b0d2-0b8ad0541e1d is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:17:12.430: INFO: Pod "labelsupdatec4370ca5-9b83-42b3-b0d2-0b8ad0541e1d": Phase="Running", Reason="", readiness=true. Elapsed: 2.041474651s
    Nov 15 06:17:12.430: INFO: The phase of Pod labelsupdatec4370ca5-9b83-42b3-b0d2-0b8ad0541e1d is Running (Ready = true)
    Nov 15 06:17:12.430: INFO: Pod "labelsupdatec4370ca5-9b83-42b3-b0d2-0b8ad0541e1d" satisfied condition "running and ready"
    Nov 15 06:17:13.089: INFO: Successfully updated pod "labelsupdatec4370ca5-9b83-42b3-b0d2-0b8ad0541e1d"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:17:17.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1787" for this suite. 11/15/23 06:17:17.257
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:17:17.284
Nov 15 06:17:17.284: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename downward-api 11/15/23 06:17:17.285
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:17.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:17:17.356
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 11/15/23 06:17:17.373
Nov 15 06:17:17.405: INFO: Waiting up to 5m0s for pod "downwardapi-volume-11de66d5-a039-4e3f-9b22-6c9d70edc941" in namespace "downward-api-7242" to be "Succeeded or Failed"
Nov 15 06:17:17.428: INFO: Pod "downwardapi-volume-11de66d5-a039-4e3f-9b22-6c9d70edc941": Phase="Pending", Reason="", readiness=false. Elapsed: 22.433048ms
Nov 15 06:17:19.450: INFO: Pod "downwardapi-volume-11de66d5-a039-4e3f-9b22-6c9d70edc941": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044388815s
Nov 15 06:17:21.447: INFO: Pod "downwardapi-volume-11de66d5-a039-4e3f-9b22-6c9d70edc941": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041036497s
STEP: Saw pod success 11/15/23 06:17:21.447
Nov 15 06:17:21.447: INFO: Pod "downwardapi-volume-11de66d5-a039-4e3f-9b22-6c9d70edc941" satisfied condition "Succeeded or Failed"
Nov 15 06:17:21.464: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-11de66d5-a039-4e3f-9b22-6c9d70edc941 container client-container: <nil>
STEP: delete the pod 11/15/23 06:17:21.509
Nov 15 06:17:21.599: INFO: Waiting for pod downwardapi-volume-11de66d5-a039-4e3f-9b22-6c9d70edc941 to disappear
Nov 15 06:17:21.618: INFO: Pod downwardapi-volume-11de66d5-a039-4e3f-9b22-6c9d70edc941 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 06:17:21.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7242" for this suite. 11/15/23 06:17:21.661
------------------------------
â€¢ [4.404 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:17:17.284
    Nov 15 06:17:17.284: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename downward-api 11/15/23 06:17:17.285
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:17.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:17:17.356
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 11/15/23 06:17:17.373
    Nov 15 06:17:17.405: INFO: Waiting up to 5m0s for pod "downwardapi-volume-11de66d5-a039-4e3f-9b22-6c9d70edc941" in namespace "downward-api-7242" to be "Succeeded or Failed"
    Nov 15 06:17:17.428: INFO: Pod "downwardapi-volume-11de66d5-a039-4e3f-9b22-6c9d70edc941": Phase="Pending", Reason="", readiness=false. Elapsed: 22.433048ms
    Nov 15 06:17:19.450: INFO: Pod "downwardapi-volume-11de66d5-a039-4e3f-9b22-6c9d70edc941": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044388815s
    Nov 15 06:17:21.447: INFO: Pod "downwardapi-volume-11de66d5-a039-4e3f-9b22-6c9d70edc941": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041036497s
    STEP: Saw pod success 11/15/23 06:17:21.447
    Nov 15 06:17:21.447: INFO: Pod "downwardapi-volume-11de66d5-a039-4e3f-9b22-6c9d70edc941" satisfied condition "Succeeded or Failed"
    Nov 15 06:17:21.464: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-11de66d5-a039-4e3f-9b22-6c9d70edc941 container client-container: <nil>
    STEP: delete the pod 11/15/23 06:17:21.509
    Nov 15 06:17:21.599: INFO: Waiting for pod downwardapi-volume-11de66d5-a039-4e3f-9b22-6c9d70edc941 to disappear
    Nov 15 06:17:21.618: INFO: Pod downwardapi-volume-11de66d5-a039-4e3f-9b22-6c9d70edc941 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:17:21.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7242" for this suite. 11/15/23 06:17:21.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:17:21.691
Nov 15 06:17:21.691: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename containers 11/15/23 06:17:21.692
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:21.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:17:21.762
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 11/15/23 06:17:21.774
Nov 15 06:17:21.808: INFO: Waiting up to 5m0s for pod "client-containers-dc645537-2f85-42ad-97ac-da79eb55ba9a" in namespace "containers-3564" to be "Succeeded or Failed"
Nov 15 06:17:21.823: INFO: Pod "client-containers-dc645537-2f85-42ad-97ac-da79eb55ba9a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.824913ms
Nov 15 06:17:23.843: INFO: Pod "client-containers-dc645537-2f85-42ad-97ac-da79eb55ba9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035155255s
Nov 15 06:17:25.845: INFO: Pod "client-containers-dc645537-2f85-42ad-97ac-da79eb55ba9a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036447083s
Nov 15 06:17:27.843: INFO: Pod "client-containers-dc645537-2f85-42ad-97ac-da79eb55ba9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035029386s
STEP: Saw pod success 11/15/23 06:17:27.843
Nov 15 06:17:27.843: INFO: Pod "client-containers-dc645537-2f85-42ad-97ac-da79eb55ba9a" satisfied condition "Succeeded or Failed"
Nov 15 06:17:27.862: INFO: Trying to get logs from node 10.72.152.86 pod client-containers-dc645537-2f85-42ad-97ac-da79eb55ba9a container agnhost-container: <nil>
STEP: delete the pod 11/15/23 06:17:27.91
Nov 15 06:17:27.980: INFO: Waiting for pod client-containers-dc645537-2f85-42ad-97ac-da79eb55ba9a to disappear
Nov 15 06:17:27.999: INFO: Pod client-containers-dc645537-2f85-42ad-97ac-da79eb55ba9a no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Nov 15 06:17:27.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3564" for this suite. 11/15/23 06:17:28.03
------------------------------
â€¢ [SLOW TEST] [6.369 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:17:21.691
    Nov 15 06:17:21.691: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename containers 11/15/23 06:17:21.692
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:21.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:17:21.762
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 11/15/23 06:17:21.774
    Nov 15 06:17:21.808: INFO: Waiting up to 5m0s for pod "client-containers-dc645537-2f85-42ad-97ac-da79eb55ba9a" in namespace "containers-3564" to be "Succeeded or Failed"
    Nov 15 06:17:21.823: INFO: Pod "client-containers-dc645537-2f85-42ad-97ac-da79eb55ba9a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.824913ms
    Nov 15 06:17:23.843: INFO: Pod "client-containers-dc645537-2f85-42ad-97ac-da79eb55ba9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035155255s
    Nov 15 06:17:25.845: INFO: Pod "client-containers-dc645537-2f85-42ad-97ac-da79eb55ba9a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036447083s
    Nov 15 06:17:27.843: INFO: Pod "client-containers-dc645537-2f85-42ad-97ac-da79eb55ba9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035029386s
    STEP: Saw pod success 11/15/23 06:17:27.843
    Nov 15 06:17:27.843: INFO: Pod "client-containers-dc645537-2f85-42ad-97ac-da79eb55ba9a" satisfied condition "Succeeded or Failed"
    Nov 15 06:17:27.862: INFO: Trying to get logs from node 10.72.152.86 pod client-containers-dc645537-2f85-42ad-97ac-da79eb55ba9a container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 06:17:27.91
    Nov 15 06:17:27.980: INFO: Waiting for pod client-containers-dc645537-2f85-42ad-97ac-da79eb55ba9a to disappear
    Nov 15 06:17:27.999: INFO: Pod client-containers-dc645537-2f85-42ad-97ac-da79eb55ba9a no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:17:27.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3564" for this suite. 11/15/23 06:17:28.03
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:17:28.061
Nov 15 06:17:28.061: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename sched-pred 11/15/23 06:17:28.062
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:28.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:17:28.131
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Nov 15 06:17:28.151: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 15 06:17:28.201: INFO: Waiting for terminating namespaces to be deleted...
Nov 15 06:17:28.235: INFO: 
Logging pods the apiserver thinks is on node 10.72.152.81 before test
Nov 15 06:17:28.318: INFO: calico-node-p5wd4 from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 06:17:28.318: INFO: calico-typha-76d9767bd5-985rd from calico-system started at 2023-11-15 03:39:41 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 06:17:28.318: INFO: ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-dl6kq from ibm-system started at 2023-11-15 03:48:09 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container ibm-cloud-provider-ip-158-176-89-155 ready: true, restart count 0
Nov 15 06:17:28.318: INFO: ibm-keepalived-watcher-qb8hn from kube-system started at 2023-11-15 03:38:31 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 06:17:28.318: INFO: ibm-master-proxy-static-10.72.152.81 from kube-system started at 2023-11-15 03:38:18 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 06:17:28.318: INFO: 	Container pause ready: true, restart count 0
Nov 15 06:17:28.318: INFO: ibmcloud-block-storage-driver-z7fmw from kube-system started at 2023-11-15 03:38:36 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 06:17:28.318: INFO: vpn-56cd75f85d-qwzcc from kube-system started at 2023-11-15 03:40:24 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container vpn ready: true, restart count 0
Nov 15 06:17:28.318: INFO: tuned-rb6qn from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container tuned ready: true, restart count 0
Nov 15 06:17:28.318: INFO: cluster-storage-operator-6cf6b595c7-m7mfz from openshift-cluster-storage-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Nov 15 06:17:28.318: INFO: csi-snapshot-controller-857d54544d-mwb9h from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 06:17:28.318: INFO: csi-snapshot-controller-operator-56df7685c7-vnvd2 from openshift-cluster-storage-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Nov 15 06:17:28.318: INFO: csi-snapshot-webhook-586f5c484d-qg5n7 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container webhook ready: true, restart count 0
Nov 15 06:17:28.318: INFO: console-68d6458867-wlg2q from openshift-console started at 2023-11-15 03:47:25 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container console ready: true, restart count 0
Nov 15 06:17:28.318: INFO: downloads-7bb648f846-tvqt6 from openshift-console started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container download-server ready: true, restart count 0
Nov 15 06:17:28.318: INFO: dns-default-njlcg from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container dns ready: true, restart count 0
Nov 15 06:17:28.318: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.318: INFO: node-resolver-6lxcm from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 15 06:17:28.318: INFO: cluster-image-registry-operator-64994bbb4-6twjh from openshift-image-registry started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Nov 15 06:17:28.318: INFO: node-ca-897k2 from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container node-ca ready: true, restart count 0
Nov 15 06:17:28.318: INFO: registry-pvc-permissions-lw2bj from openshift-image-registry started at 2023-11-15 03:46:43 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container pvc-permissions ready: false, restart count 0
Nov 15 06:17:28.318: INFO: ingress-canary-jqtjt from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 15 06:17:28.318: INFO: router-default-56777c97d6-szb4s from openshift-ingress started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container router ready: true, restart count 0
Nov 15 06:17:28.318: INFO: insights-operator-85b688b59d-v47wz from openshift-insights started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container insights-operator ready: true, restart count 1
Nov 15 06:17:28.318: INFO: openshift-kube-proxy-p8p2b from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 15 06:17:28.318: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.318: INFO: migrator-697dd4cbc5-kk7zn from openshift-kube-storage-version-migrator started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container migrator ready: true, restart count 0
Nov 15 06:17:28.318: INFO: certified-operators-lqtq6 from openshift-marketplace started at 2023-11-15 03:42:42 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container registry-server ready: true, restart count 0
Nov 15 06:17:28.318: INFO: community-operators-jpmzt from openshift-marketplace started at 2023-11-15 05:06:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container registry-server ready: true, restart count 0
Nov 15 06:17:28.318: INFO: marketplace-operator-55cc9f5b6b-xpbw5 from openshift-marketplace started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container marketplace-operator ready: true, restart count 0
Nov 15 06:17:28.318: INFO: redhat-marketplace-xp5nc from openshift-marketplace started at 2023-11-15 03:42:42 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container registry-server ready: true, restart count 0
Nov 15 06:17:28.318: INFO: redhat-operators-bzltn from openshift-marketplace started at 2023-11-15 04:07:58 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container registry-server ready: true, restart count 0
Nov 15 06:17:28.318: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-11-15 03:46:14 +0000 UTC (6 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container alertmanager ready: true, restart count 1
Nov 15 06:17:28.318: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 15 06:17:28.318: INFO: 	Container config-reloader ready: true, restart count 0
Nov 15 06:17:28.318: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.318: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Nov 15 06:17:28.318: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 15 06:17:28.318: INFO: node-exporter-67458 from openshift-monitoring started at 2023-11-15 03:44:49 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.318: INFO: 	Container node-exporter ready: true, restart count 0
Nov 15 06:17:28.318: INFO: prometheus-adapter-5f5bb574db-r4lkm from openshift-monitoring started at 2023-11-15 03:44:53 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 15 06:17:28.318: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-11-15 03:46:20 +0000 UTC (6 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container config-reloader ready: true, restart count 0
Nov 15 06:17:28.318: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.318: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Nov 15 06:17:28.318: INFO: 	Container prometheus ready: true, restart count 0
Nov 15 06:17:28.318: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 15 06:17:28.318: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 15 06:17:28.318: INFO: prometheus-operator-admission-webhook-c78bf8f99-gxfh4 from openshift-monitoring started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Nov 15 06:17:28.318: INFO: telemeter-client-77c946bb95-nsnds from openshift-monitoring started at 2023-11-15 03:46:15 +0000 UTC (3 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.318: INFO: 	Container reload ready: true, restart count 0
Nov 15 06:17:28.318: INFO: 	Container telemeter-client ready: true, restart count 0
Nov 15 06:17:28.318: INFO: multus-9tc6x from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container kube-multus ready: true, restart count 0
Nov 15 06:17:28.318: INFO: multus-additional-cni-plugins-j2jvc from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.318: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 15 06:17:28.318: INFO: network-metrics-daemon-g5fgz from openshift-multus started at 2023-11-15 03:38:47 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.319: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.319: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 15 06:17:28.319: INFO: network-check-target-6pv9x from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.319: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 15 06:17:28.319: INFO: catalog-operator-798697959c-24lbd from openshift-operator-lifecycle-manager started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.319: INFO: 	Container catalog-operator ready: true, restart count 0
Nov 15 06:17:28.319: INFO: collect-profiles-28333785-ckm22 from openshift-operator-lifecycle-manager started at 2023-11-15 05:45:00 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.319: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 15 06:17:28.319: INFO: collect-profiles-28333800-bgbt2 from openshift-operator-lifecycle-manager started at 2023-11-15 06:00:00 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.319: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 15 06:17:28.319: INFO: collect-profiles-28333815-xxvbf from openshift-operator-lifecycle-manager started at 2023-11-15 06:15:00 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.319: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 15 06:17:28.319: INFO: olm-operator-846bf6bd78-fzxr5 from openshift-operator-lifecycle-manager started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.319: INFO: 	Container olm-operator ready: true, restart count 0
Nov 15 06:17:28.319: INFO: package-server-manager-5b666bf8fd-5v7rb from openshift-operator-lifecycle-manager started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.319: INFO: 	Container package-server-manager ready: true, restart count 0
Nov 15 06:17:28.319: INFO: packageserver-758b547fc-f9wwb from openshift-operator-lifecycle-manager started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.319: INFO: 	Container packageserver ready: true, restart count 0
Nov 15 06:17:28.319: INFO: service-ca-operator-74cb5c9cf5-fqgj5 from openshift-service-ca-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.319: INFO: 	Container service-ca-operator ready: true, restart count 1
Nov 15 06:17:28.319: INFO: service-ca-78fb97bb77-qcz4b from openshift-service-ca started at 2023-11-15 03:41:45 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.319: INFO: 	Container service-ca-controller ready: true, restart count 0
Nov 15 06:17:28.319: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-qvnnx from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.319: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 06:17:28.319: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 06:17:28.319: INFO: 
Logging pods the apiserver thinks is on node 10.72.152.86 before test
Nov 15 06:17:28.383: INFO: calico-node-cmgfg from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 06:17:28.384: INFO: calico-typha-76d9767bd5-tx2mp from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 06:17:28.384: INFO: ibm-keepalived-watcher-qclxn from kube-system started at 2023-11-15 03:38:36 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 06:17:28.384: INFO: ibm-master-proxy-static-10.72.152.86 from kube-system started at 2023-11-15 03:38:24 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container pause ready: true, restart count 0
Nov 15 06:17:28.384: INFO: ibmcloud-block-storage-driver-m8gq4 from kube-system started at 2023-11-15 03:38:43 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 06:17:28.384: INFO: cluster-node-tuning-operator-5f77b58f7-t8gf6 from openshift-cluster-node-tuning-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Nov 15 06:17:28.384: INFO: tuned-5qzzn from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container tuned ready: true, restart count 0
Nov 15 06:17:28.384: INFO: cluster-samples-operator-65684cb854-h7n8t from openshift-cluster-samples-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Nov 15 06:17:28.384: INFO: csi-snapshot-controller-857d54544d-qbhf6 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 06:17:28.384: INFO: csi-snapshot-webhook-586f5c484d-vkpg9 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container webhook ready: true, restart count 0
Nov 15 06:17:28.384: INFO: console-operator-769f9748fb-7tfcc from openshift-console-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container console-operator ready: true, restart count 1
Nov 15 06:17:28.384: INFO: 	Container conversion-webhook-server ready: true, restart count 3
Nov 15 06:17:28.384: INFO: downloads-7bb648f846-sr7nb from openshift-console started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container download-server ready: true, restart count 0
Nov 15 06:17:28.384: INFO: dns-operator-dd9c9c896-9gwtp from openshift-dns-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container dns-operator ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.384: INFO: dns-default-24qc6 from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container dns ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.384: INFO: node-resolver-d54kt from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 15 06:17:28.384: INFO: node-ca-w7vhg from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container node-ca ready: true, restart count 0
Nov 15 06:17:28.384: INFO: ingress-canary-ttx6m from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 15 06:17:28.384: INFO: ingress-operator-6d4d6975f7-qtm2n from openshift-ingress-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container ingress-operator ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.384: INFO: openshift-kube-proxy-x5hq9 from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.384: INFO: kube-storage-version-migrator-operator-5d88b7484-8m2cc from openshift-kube-storage-version-migrator-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Nov 15 06:17:28.384: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-11-15 03:46:47 +0000 UTC (6 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container alertmanager ready: true, restart count 1
Nov 15 06:17:28.384: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container config-reloader ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 15 06:17:28.384: INFO: cluster-monitoring-operator-868f9b56cf-xrfz7 from openshift-monitoring started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Nov 15 06:17:28.384: INFO: kube-state-metrics-f8d796647-f4rgj from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (3 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 15 06:17:28.384: INFO: node-exporter-n68zb from openshift-monitoring started at 2023-11-15 03:44:49 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container node-exporter ready: true, restart count 0
Nov 15 06:17:28.384: INFO: openshift-state-metrics-69bb697b65-6bcg7 from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (3 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Nov 15 06:17:28.384: INFO: prometheus-adapter-5f5bb574db-569xs from openshift-monitoring started at 2023-11-15 03:44:53 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 15 06:17:28.384: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-11-15 03:46:38 +0000 UTC (6 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container config-reloader ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container prometheus ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 15 06:17:28.384: INFO: prometheus-operator-6fcb4d4c46-t74rt from openshift-monitoring started at 2023-11-15 03:44:41 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 15 06:17:28.384: INFO: thanos-querier-56b7586647-8m7mp from openshift-monitoring started at 2023-11-15 03:44:57 +0000 UTC (6 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container thanos-query ready: true, restart count 0
Nov 15 06:17:28.384: INFO: multus-additional-cni-plugins-z6xx7 from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 15 06:17:28.384: INFO: multus-admission-controller-6b76dd856b-6qmnp from openshift-multus started at 2023-11-15 03:44:37 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 15 06:17:28.384: INFO: multus-cltwv from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container kube-multus ready: true, restart count 0
Nov 15 06:17:28.384: INFO: network-metrics-daemon-zbz9n from openshift-multus started at 2023-11-15 03:38:47 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 15 06:17:28.384: INFO: network-check-target-gnph7 from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 15 06:17:28.384: INFO: packageserver-758b547fc-65qc5 from openshift-operator-lifecycle-manager started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container packageserver ready: true, restart count 0
Nov 15 06:17:28.384: INFO: sonobuoy from sonobuoy started at 2023-11-15 05:52:01 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 15 06:17:28.384: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-m6885 from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 06:17:28.384: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-11-15 03:41:49 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.384: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 15 06:17:28.384: INFO: 
Logging pods the apiserver thinks is on node 10.72.152.88 before test
Nov 15 06:17:28.447: INFO: calico-kube-controllers-5dd9d87465-759pm from calico-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.447: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 15 06:17:28.447: INFO: calico-node-lmf6x from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.447: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 06:17:28.447: INFO: managed-storage-validation-webhooks-7457bf6687-9ds5w from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.447: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Nov 15 06:17:28.447: INFO: managed-storage-validation-webhooks-7457bf6687-h522x from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.447: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Nov 15 06:17:28.447: INFO: managed-storage-validation-webhooks-7457bf6687-phdgp from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.447: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Nov 15 06:17:28.447: INFO: ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-vbxww from ibm-system started at 2023-11-15 03:48:09 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.447: INFO: 	Container ibm-cloud-provider-ip-158-176-89-155 ready: true, restart count 0
Nov 15 06:17:28.447: INFO: ibm-file-plugin-5fcf7fb495-xmdts from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.447: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 15 06:17:28.447: INFO: ibm-keepalived-watcher-5jx26 from kube-system started at 2023-11-15 03:38:12 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 06:17:28.448: INFO: ibm-master-proxy-static-10.72.152.88 from kube-system started at 2023-11-15 03:38:04 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 06:17:28.448: INFO: 	Container pause ready: true, restart count 0
Nov 15 06:17:28.448: INFO: ibm-storage-metrics-agent-84fbdc746-5sv68 from kube-system started at 2023-11-15 03:39:51 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Nov 15 06:17:28.448: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Nov 15 06:17:28.448: INFO: ibm-storage-watcher-7445c988b-8ngdm from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 15 06:17:28.448: INFO: ibmcloud-block-storage-driver-9t8rj from kube-system started at 2023-11-15 03:38:17 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 06:17:28.448: INFO: ibmcloud-block-storage-plugin-5774687565-gj9xn from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Nov 15 06:17:28.448: INFO: tuned-q894n from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container tuned ready: true, restart count 0
Nov 15 06:17:28.448: INFO: console-68d6458867-krfqd from openshift-console started at 2023-11-15 03:47:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container console ready: true, restart count 0
Nov 15 06:17:28.448: INFO: dns-default-lmngx from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container dns ready: true, restart count 0
Nov 15 06:17:28.448: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.448: INFO: node-resolver-hwp5f from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 15 06:17:28.448: INFO: image-registry-f74f764d8-w48k4 from openshift-image-registry started at 2023-11-15 03:46:35 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container registry ready: true, restart count 0
Nov 15 06:17:28.448: INFO: node-ca-wrqvd from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container node-ca ready: true, restart count 0
Nov 15 06:17:28.448: INFO: ingress-canary-wvl4p from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 15 06:17:28.448: INFO: router-default-56777c97d6-4bnqq from openshift-ingress started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container router ready: true, restart count 0
Nov 15 06:17:28.448: INFO: openshift-kube-proxy-tzknx from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 15 06:17:28.448: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.448: INFO: node-exporter-2452k from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.448: INFO: 	Container node-exporter ready: true, restart count 0
Nov 15 06:17:28.448: INFO: prometheus-operator-admission-webhook-c78bf8f99-x8vcz from openshift-monitoring started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Nov 15 06:17:28.448: INFO: thanos-querier-56b7586647-qr75b from openshift-monitoring started at 2023-11-15 03:44:58 +0000 UTC (6 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.448: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 15 06:17:28.448: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Nov 15 06:17:28.448: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 15 06:17:28.448: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 15 06:17:28.448: INFO: 	Container thanos-query ready: true, restart count 0
Nov 15 06:17:28.448: INFO: multus-additional-cni-plugins-lcnxr from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 15 06:17:28.448: INFO: multus-admission-controller-6b76dd856b-6zft2 from openshift-multus started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.448: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 15 06:17:28.448: INFO: multus-vqg9w from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container kube-multus ready: true, restart count 0
Nov 15 06:17:28.448: INFO: network-metrics-daemon-9mrwp from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:17:28.448: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 15 06:17:28.448: INFO: network-check-source-5f9c5566b6-grf5l from openshift-network-diagnostics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container check-endpoints ready: true, restart count 0
Nov 15 06:17:28.448: INFO: network-check-target-6rz7p from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 15 06:17:28.448: INFO: network-operator-847f47449c-j9glm from openshift-network-operator started at 2023-11-15 03:38:21 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container network-operator ready: true, restart count 1
Nov 15 06:17:28.448: INFO: metrics-667b585fc4-d5fdk from openshift-roks-metrics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container metrics ready: true, restart count 5
Nov 15 06:17:28.448: INFO: push-gateway-7f9447c646-5mjcp from openshift-roks-metrics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container push-gateway ready: true, restart count 0
Nov 15 06:17:28.448: INFO: sonobuoy-e2e-job-4fb3cc1d32834aa9 from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container e2e ready: true, restart count 0
Nov 15 06:17:28.448: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 06:17:28.448: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-ddv8z from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 06:17:28.448: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 06:17:28.448: INFO: tigera-operator-7dbcb4fb45-rn78j from tigera-operator started at 2023-11-15 03:38:21 +0000 UTC (1 container statuses recorded)
Nov 15 06:17:28.448: INFO: 	Container tigera-operator ready: true, restart count 2
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 11/15/23 06:17:28.448
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1797b7699d609225], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 11/15/23 06:17:28.611
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:17:29.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4595" for this suite. 11/15/23 06:17:29.657
------------------------------
â€¢ [1.623 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:17:28.061
    Nov 15 06:17:28.061: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename sched-pred 11/15/23 06:17:28.062
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:28.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:17:28.131
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Nov 15 06:17:28.151: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Nov 15 06:17:28.201: INFO: Waiting for terminating namespaces to be deleted...
    Nov 15 06:17:28.235: INFO: 
    Logging pods the apiserver thinks is on node 10.72.152.81 before test
    Nov 15 06:17:28.318: INFO: calico-node-p5wd4 from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: calico-typha-76d9767bd5-985rd from calico-system started at 2023-11-15 03:39:41 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-dl6kq from ibm-system started at 2023-11-15 03:48:09 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container ibm-cloud-provider-ip-158-176-89-155 ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: ibm-keepalived-watcher-qb8hn from kube-system started at 2023-11-15 03:38:31 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: ibm-master-proxy-static-10.72.152.81 from kube-system started at 2023-11-15 03:38:18 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: 	Container pause ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: ibmcloud-block-storage-driver-z7fmw from kube-system started at 2023-11-15 03:38:36 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: vpn-56cd75f85d-qwzcc from kube-system started at 2023-11-15 03:40:24 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container vpn ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: tuned-rb6qn from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container tuned ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: cluster-storage-operator-6cf6b595c7-m7mfz from openshift-cluster-storage-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Nov 15 06:17:28.318: INFO: csi-snapshot-controller-857d54544d-mwb9h from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: csi-snapshot-controller-operator-56df7685c7-vnvd2 from openshift-cluster-storage-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: csi-snapshot-webhook-586f5c484d-qg5n7 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container webhook ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: console-68d6458867-wlg2q from openshift-console started at 2023-11-15 03:47:25 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container console ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: downloads-7bb648f846-tvqt6 from openshift-console started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container download-server ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: dns-default-njlcg from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container dns ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: node-resolver-6lxcm from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: cluster-image-registry-operator-64994bbb4-6twjh from openshift-image-registry started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: node-ca-897k2 from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container node-ca ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: registry-pvc-permissions-lw2bj from openshift-image-registry started at 2023-11-15 03:46:43 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container pvc-permissions ready: false, restart count 0
    Nov 15 06:17:28.318: INFO: ingress-canary-jqtjt from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: router-default-56777c97d6-szb4s from openshift-ingress started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container router ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: insights-operator-85b688b59d-v47wz from openshift-insights started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container insights-operator ready: true, restart count 1
    Nov 15 06:17:28.318: INFO: openshift-kube-proxy-p8p2b from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: migrator-697dd4cbc5-kk7zn from openshift-kube-storage-version-migrator started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container migrator ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: certified-operators-lqtq6 from openshift-marketplace started at 2023-11-15 03:42:42 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container registry-server ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: community-operators-jpmzt from openshift-marketplace started at 2023-11-15 05:06:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container registry-server ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: marketplace-operator-55cc9f5b6b-xpbw5 from openshift-marketplace started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container marketplace-operator ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: redhat-marketplace-xp5nc from openshift-marketplace started at 2023-11-15 03:42:42 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container registry-server ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: redhat-operators-bzltn from openshift-marketplace started at 2023-11-15 04:07:58 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container registry-server ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-11-15 03:46:14 +0000 UTC (6 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container alertmanager ready: true, restart count 1
    Nov 15 06:17:28.318: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: node-exporter-67458 from openshift-monitoring started at 2023-11-15 03:44:49 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: prometheus-adapter-5f5bb574db-r4lkm from openshift-monitoring started at 2023-11-15 03:44:53 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-11-15 03:46:20 +0000 UTC (6 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: 	Container prometheus ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: prometheus-operator-admission-webhook-c78bf8f99-gxfh4 from openshift-monitoring started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: telemeter-client-77c946bb95-nsnds from openshift-monitoring started at 2023-11-15 03:46:15 +0000 UTC (3 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: 	Container reload ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: 	Container telemeter-client ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: multus-9tc6x from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: multus-additional-cni-plugins-j2jvc from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.318: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Nov 15 06:17:28.318: INFO: network-metrics-daemon-g5fgz from openshift-multus started at 2023-11-15 03:38:47 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.319: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.319: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Nov 15 06:17:28.319: INFO: network-check-target-6pv9x from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.319: INFO: 	Container network-check-target-container ready: true, restart count 0
    Nov 15 06:17:28.319: INFO: catalog-operator-798697959c-24lbd from openshift-operator-lifecycle-manager started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.319: INFO: 	Container catalog-operator ready: true, restart count 0
    Nov 15 06:17:28.319: INFO: collect-profiles-28333785-ckm22 from openshift-operator-lifecycle-manager started at 2023-11-15 05:45:00 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.319: INFO: 	Container collect-profiles ready: false, restart count 0
    Nov 15 06:17:28.319: INFO: collect-profiles-28333800-bgbt2 from openshift-operator-lifecycle-manager started at 2023-11-15 06:00:00 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.319: INFO: 	Container collect-profiles ready: false, restart count 0
    Nov 15 06:17:28.319: INFO: collect-profiles-28333815-xxvbf from openshift-operator-lifecycle-manager started at 2023-11-15 06:15:00 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.319: INFO: 	Container collect-profiles ready: false, restart count 0
    Nov 15 06:17:28.319: INFO: olm-operator-846bf6bd78-fzxr5 from openshift-operator-lifecycle-manager started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.319: INFO: 	Container olm-operator ready: true, restart count 0
    Nov 15 06:17:28.319: INFO: package-server-manager-5b666bf8fd-5v7rb from openshift-operator-lifecycle-manager started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.319: INFO: 	Container package-server-manager ready: true, restart count 0
    Nov 15 06:17:28.319: INFO: packageserver-758b547fc-f9wwb from openshift-operator-lifecycle-manager started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.319: INFO: 	Container packageserver ready: true, restart count 0
    Nov 15 06:17:28.319: INFO: service-ca-operator-74cb5c9cf5-fqgj5 from openshift-service-ca-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.319: INFO: 	Container service-ca-operator ready: true, restart count 1
    Nov 15 06:17:28.319: INFO: service-ca-78fb97bb77-qcz4b from openshift-service-ca started at 2023-11-15 03:41:45 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.319: INFO: 	Container service-ca-controller ready: true, restart count 0
    Nov 15 06:17:28.319: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-qvnnx from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.319: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 06:17:28.319: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 06:17:28.319: INFO: 
    Logging pods the apiserver thinks is on node 10.72.152.86 before test
    Nov 15 06:17:28.383: INFO: calico-node-cmgfg from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: calico-typha-76d9767bd5-tx2mp from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: ibm-keepalived-watcher-qclxn from kube-system started at 2023-11-15 03:38:36 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: ibm-master-proxy-static-10.72.152.86 from kube-system started at 2023-11-15 03:38:24 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container pause ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: ibmcloud-block-storage-driver-m8gq4 from kube-system started at 2023-11-15 03:38:43 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: cluster-node-tuning-operator-5f77b58f7-t8gf6 from openshift-cluster-node-tuning-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: tuned-5qzzn from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container tuned ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: cluster-samples-operator-65684cb854-h7n8t from openshift-cluster-samples-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: csi-snapshot-controller-857d54544d-qbhf6 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: csi-snapshot-webhook-586f5c484d-vkpg9 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container webhook ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: console-operator-769f9748fb-7tfcc from openshift-console-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container console-operator ready: true, restart count 1
    Nov 15 06:17:28.384: INFO: 	Container conversion-webhook-server ready: true, restart count 3
    Nov 15 06:17:28.384: INFO: downloads-7bb648f846-sr7nb from openshift-console started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container download-server ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: dns-operator-dd9c9c896-9gwtp from openshift-dns-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container dns-operator ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: dns-default-24qc6 from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container dns ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: node-resolver-d54kt from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: node-ca-w7vhg from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container node-ca ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: ingress-canary-ttx6m from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: ingress-operator-6d4d6975f7-qtm2n from openshift-ingress-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container ingress-operator ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: openshift-kube-proxy-x5hq9 from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: kube-storage-version-migrator-operator-5d88b7484-8m2cc from openshift-kube-storage-version-migrator-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Nov 15 06:17:28.384: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-11-15 03:46:47 +0000 UTC (6 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container alertmanager ready: true, restart count 1
    Nov 15 06:17:28.384: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: cluster-monitoring-operator-868f9b56cf-xrfz7 from openshift-monitoring started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: kube-state-metrics-f8d796647-f4rgj from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (3 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: node-exporter-n68zb from openshift-monitoring started at 2023-11-15 03:44:49 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: openshift-state-metrics-69bb697b65-6bcg7 from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (3 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: prometheus-adapter-5f5bb574db-569xs from openshift-monitoring started at 2023-11-15 03:44:53 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-11-15 03:46:38 +0000 UTC (6 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container prometheus ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: prometheus-operator-6fcb4d4c46-t74rt from openshift-monitoring started at 2023-11-15 03:44:41 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container prometheus-operator ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: thanos-querier-56b7586647-8m7mp from openshift-monitoring started at 2023-11-15 03:44:57 +0000 UTC (6 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container oauth-proxy ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container thanos-query ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: multus-additional-cni-plugins-z6xx7 from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: multus-admission-controller-6b76dd856b-6qmnp from openshift-multus started at 2023-11-15 03:44:37 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: multus-cltwv from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: network-metrics-daemon-zbz9n from openshift-multus started at 2023-11-15 03:38:47 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: network-check-target-gnph7 from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container network-check-target-container ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: packageserver-758b547fc-65qc5 from openshift-operator-lifecycle-manager started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container packageserver ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: sonobuoy from sonobuoy started at 2023-11-15 05:52:01 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-m6885 from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-11-15 03:41:49 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.384: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    Nov 15 06:17:28.384: INFO: 
    Logging pods the apiserver thinks is on node 10.72.152.88 before test
    Nov 15 06:17:28.447: INFO: calico-kube-controllers-5dd9d87465-759pm from calico-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.447: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Nov 15 06:17:28.447: INFO: calico-node-lmf6x from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.447: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 06:17:28.447: INFO: managed-storage-validation-webhooks-7457bf6687-9ds5w from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.447: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Nov 15 06:17:28.447: INFO: managed-storage-validation-webhooks-7457bf6687-h522x from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.447: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Nov 15 06:17:28.447: INFO: managed-storage-validation-webhooks-7457bf6687-phdgp from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.447: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Nov 15 06:17:28.447: INFO: ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-vbxww from ibm-system started at 2023-11-15 03:48:09 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.447: INFO: 	Container ibm-cloud-provider-ip-158-176-89-155 ready: true, restart count 0
    Nov 15 06:17:28.447: INFO: ibm-file-plugin-5fcf7fb495-xmdts from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.447: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Nov 15 06:17:28.447: INFO: ibm-keepalived-watcher-5jx26 from kube-system started at 2023-11-15 03:38:12 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: ibm-master-proxy-static-10.72.152.88 from kube-system started at 2023-11-15 03:38:04 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: 	Container pause ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: ibm-storage-metrics-agent-84fbdc746-5sv68 from kube-system started at 2023-11-15 03:39:51 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: ibm-storage-watcher-7445c988b-8ngdm from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: ibmcloud-block-storage-driver-9t8rj from kube-system started at 2023-11-15 03:38:17 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: ibmcloud-block-storage-plugin-5774687565-gj9xn from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: tuned-q894n from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container tuned ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: console-68d6458867-krfqd from openshift-console started at 2023-11-15 03:47:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container console ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: dns-default-lmngx from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container dns ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: node-resolver-hwp5f from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: image-registry-f74f764d8-w48k4 from openshift-image-registry started at 2023-11-15 03:46:35 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container registry ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: node-ca-wrqvd from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container node-ca ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: ingress-canary-wvl4p from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: router-default-56777c97d6-4bnqq from openshift-ingress started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container router ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: openshift-kube-proxy-tzknx from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: node-exporter-2452k from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: prometheus-operator-admission-webhook-c78bf8f99-x8vcz from openshift-monitoring started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: thanos-querier-56b7586647-qr75b from openshift-monitoring started at 2023-11-15 03:44:58 +0000 UTC (6 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: 	Container oauth-proxy ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: 	Container thanos-query ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: multus-additional-cni-plugins-lcnxr from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: multus-admission-controller-6b76dd856b-6zft2 from openshift-multus started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: multus-vqg9w from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: network-metrics-daemon-9mrwp from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: network-check-source-5f9c5566b6-grf5l from openshift-network-diagnostics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container check-endpoints ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: network-check-target-6rz7p from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container network-check-target-container ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: network-operator-847f47449c-j9glm from openshift-network-operator started at 2023-11-15 03:38:21 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container network-operator ready: true, restart count 1
    Nov 15 06:17:28.448: INFO: metrics-667b585fc4-d5fdk from openshift-roks-metrics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container metrics ready: true, restart count 5
    Nov 15 06:17:28.448: INFO: push-gateway-7f9447c646-5mjcp from openshift-roks-metrics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container push-gateway ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: sonobuoy-e2e-job-4fb3cc1d32834aa9 from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container e2e ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-ddv8z from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 06:17:28.448: INFO: tigera-operator-7dbcb4fb45-rn78j from tigera-operator started at 2023-11-15 03:38:21 +0000 UTC (1 container statuses recorded)
    Nov 15 06:17:28.448: INFO: 	Container tigera-operator ready: true, restart count 2
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 11/15/23 06:17:28.448
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1797b7699d609225], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 11/15/23 06:17:28.611
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:17:29.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4595" for this suite. 11/15/23 06:17:29.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:17:29.691
Nov 15 06:17:29.691: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename configmap 11/15/23 06:17:29.692
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:29.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:17:29.765
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-b729e25b-9779-47a1-8735-c6439b7cca40 11/15/23 06:17:29.779
STEP: Creating a pod to test consume configMaps 11/15/23 06:17:29.796
Nov 15 06:17:29.836: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bb4941d-fc03-4ad5-bf9a-dd14dbb67be9" in namespace "configmap-328" to be "Succeeded or Failed"
Nov 15 06:17:29.867: INFO: Pod "pod-configmaps-0bb4941d-fc03-4ad5-bf9a-dd14dbb67be9": Phase="Pending", Reason="", readiness=false. Elapsed: 31.333079ms
Nov 15 06:17:31.888: INFO: Pod "pod-configmaps-0bb4941d-fc03-4ad5-bf9a-dd14dbb67be9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052892413s
Nov 15 06:17:33.891: INFO: Pod "pod-configmaps-0bb4941d-fc03-4ad5-bf9a-dd14dbb67be9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054953775s
Nov 15 06:17:35.888: INFO: Pod "pod-configmaps-0bb4941d-fc03-4ad5-bf9a-dd14dbb67be9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052747794s
STEP: Saw pod success 11/15/23 06:17:35.888
Nov 15 06:17:35.889: INFO: Pod "pod-configmaps-0bb4941d-fc03-4ad5-bf9a-dd14dbb67be9" satisfied condition "Succeeded or Failed"
Nov 15 06:17:35.907: INFO: Trying to get logs from node 10.72.152.86 pod pod-configmaps-0bb4941d-fc03-4ad5-bf9a-dd14dbb67be9 container agnhost-container: <nil>
STEP: delete the pod 11/15/23 06:17:35.956
Nov 15 06:17:36.024: INFO: Waiting for pod pod-configmaps-0bb4941d-fc03-4ad5-bf9a-dd14dbb67be9 to disappear
Nov 15 06:17:36.046: INFO: Pod pod-configmaps-0bb4941d-fc03-4ad5-bf9a-dd14dbb67be9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:17:36.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-328" for this suite. 11/15/23 06:17:36.081
------------------------------
â€¢ [SLOW TEST] [6.420 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:17:29.691
    Nov 15 06:17:29.691: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename configmap 11/15/23 06:17:29.692
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:29.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:17:29.765
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-b729e25b-9779-47a1-8735-c6439b7cca40 11/15/23 06:17:29.779
    STEP: Creating a pod to test consume configMaps 11/15/23 06:17:29.796
    Nov 15 06:17:29.836: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bb4941d-fc03-4ad5-bf9a-dd14dbb67be9" in namespace "configmap-328" to be "Succeeded or Failed"
    Nov 15 06:17:29.867: INFO: Pod "pod-configmaps-0bb4941d-fc03-4ad5-bf9a-dd14dbb67be9": Phase="Pending", Reason="", readiness=false. Elapsed: 31.333079ms
    Nov 15 06:17:31.888: INFO: Pod "pod-configmaps-0bb4941d-fc03-4ad5-bf9a-dd14dbb67be9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052892413s
    Nov 15 06:17:33.891: INFO: Pod "pod-configmaps-0bb4941d-fc03-4ad5-bf9a-dd14dbb67be9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054953775s
    Nov 15 06:17:35.888: INFO: Pod "pod-configmaps-0bb4941d-fc03-4ad5-bf9a-dd14dbb67be9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052747794s
    STEP: Saw pod success 11/15/23 06:17:35.888
    Nov 15 06:17:35.889: INFO: Pod "pod-configmaps-0bb4941d-fc03-4ad5-bf9a-dd14dbb67be9" satisfied condition "Succeeded or Failed"
    Nov 15 06:17:35.907: INFO: Trying to get logs from node 10.72.152.86 pod pod-configmaps-0bb4941d-fc03-4ad5-bf9a-dd14dbb67be9 container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 06:17:35.956
    Nov 15 06:17:36.024: INFO: Waiting for pod pod-configmaps-0bb4941d-fc03-4ad5-bf9a-dd14dbb67be9 to disappear
    Nov 15 06:17:36.046: INFO: Pod pod-configmaps-0bb4941d-fc03-4ad5-bf9a-dd14dbb67be9 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:17:36.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-328" for this suite. 11/15/23 06:17:36.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:17:36.114
Nov 15 06:17:36.114: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename services 11/15/23 06:17:36.115
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:36.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:17:36.178
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 11/15/23 06:17:36.219
STEP: waiting for available Endpoint 11/15/23 06:17:36.25
STEP: listing all Endpoints 11/15/23 06:17:36.257
STEP: updating the Endpoint 11/15/23 06:17:36.283
STEP: fetching the Endpoint 11/15/23 06:17:36.314
STEP: patching the Endpoint 11/15/23 06:17:36.336
STEP: fetching the Endpoint 11/15/23 06:17:36.374
STEP: deleting the Endpoint by Collection 11/15/23 06:17:36.391
STEP: waiting for Endpoint deletion 11/15/23 06:17:36.438
STEP: fetching the Endpoint 11/15/23 06:17:36.444
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 06:17:36.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9400" for this suite. 11/15/23 06:17:36.487
------------------------------
â€¢ [0.406 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:17:36.114
    Nov 15 06:17:36.114: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename services 11/15/23 06:17:36.115
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:36.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:17:36.178
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 11/15/23 06:17:36.219
    STEP: waiting for available Endpoint 11/15/23 06:17:36.25
    STEP: listing all Endpoints 11/15/23 06:17:36.257
    STEP: updating the Endpoint 11/15/23 06:17:36.283
    STEP: fetching the Endpoint 11/15/23 06:17:36.314
    STEP: patching the Endpoint 11/15/23 06:17:36.336
    STEP: fetching the Endpoint 11/15/23 06:17:36.374
    STEP: deleting the Endpoint by Collection 11/15/23 06:17:36.391
    STEP: waiting for Endpoint deletion 11/15/23 06:17:36.438
    STEP: fetching the Endpoint 11/15/23 06:17:36.444
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:17:36.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9400" for this suite. 11/15/23 06:17:36.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:17:36.523
Nov 15 06:17:36.523: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename namespaces 11/15/23 06:17:36.524
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:36.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:17:36.61
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 11/15/23 06:17:36.624
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:36.682
STEP: Creating a service in the namespace 11/15/23 06:17:36.693
STEP: Deleting the namespace 11/15/23 06:17:36.728
STEP: Waiting for the namespace to be removed. 11/15/23 06:17:36.764
STEP: Recreating the namespace 11/15/23 06:17:44.784
STEP: Verifying there is no service in the namespace 11/15/23 06:17:44.839
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:17:44.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7466" for this suite. 11/15/23 06:17:44.896
STEP: Destroying namespace "nsdeletetest-1591" for this suite. 11/15/23 06:17:44.93
Nov 15 06:17:44.952: INFO: Namespace nsdeletetest-1591 was already deleted
STEP: Destroying namespace "nsdeletetest-6422" for this suite. 11/15/23 06:17:44.952
------------------------------
â€¢ [SLOW TEST] [8.458 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:17:36.523
    Nov 15 06:17:36.523: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename namespaces 11/15/23 06:17:36.524
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:36.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:17:36.61
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 11/15/23 06:17:36.624
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:36.682
    STEP: Creating a service in the namespace 11/15/23 06:17:36.693
    STEP: Deleting the namespace 11/15/23 06:17:36.728
    STEP: Waiting for the namespace to be removed. 11/15/23 06:17:36.764
    STEP: Recreating the namespace 11/15/23 06:17:44.784
    STEP: Verifying there is no service in the namespace 11/15/23 06:17:44.839
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:17:44.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7466" for this suite. 11/15/23 06:17:44.896
    STEP: Destroying namespace "nsdeletetest-1591" for this suite. 11/15/23 06:17:44.93
    Nov 15 06:17:44.952: INFO: Namespace nsdeletetest-1591 was already deleted
    STEP: Destroying namespace "nsdeletetest-6422" for this suite. 11/15/23 06:17:44.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:17:44.982
Nov 15 06:17:44.982: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename gc 11/15/23 06:17:44.983
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:45.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:17:45.051
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 11/15/23 06:17:45.095
STEP: create the rc2 11/15/23 06:17:45.117
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 11/15/23 06:17:50.166
STEP: delete the rc simpletest-rc-to-be-deleted 11/15/23 06:17:52.288
STEP: wait for the rc to be deleted 11/15/23 06:17:52.347
STEP: Gathering metrics 11/15/23 06:17:57.451
W1115 06:17:57.500895      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 15 06:17:57.500: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Nov 15 06:17:57.500: INFO: Deleting pod "simpletest-rc-to-be-deleted-22448" in namespace "gc-79"
Nov 15 06:17:57.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-292lx" in namespace "gc-79"
Nov 15 06:17:57.618: INFO: Deleting pod "simpletest-rc-to-be-deleted-2n5r2" in namespace "gc-79"
Nov 15 06:17:57.668: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wgk4" in namespace "gc-79"
Nov 15 06:17:57.715: INFO: Deleting pod "simpletest-rc-to-be-deleted-424pj" in namespace "gc-79"
Nov 15 06:17:57.789: INFO: Deleting pod "simpletest-rc-to-be-deleted-4296x" in namespace "gc-79"
Nov 15 06:17:57.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-42fnc" in namespace "gc-79"
Nov 15 06:17:57.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-4bh7t" in namespace "gc-79"
Nov 15 06:17:57.969: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lmmz" in namespace "gc-79"
Nov 15 06:17:58.015: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hqd8" in namespace "gc-79"
Nov 15 06:17:58.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nkxc" in namespace "gc-79"
Nov 15 06:17:58.140: INFO: Deleting pod "simpletest-rc-to-be-deleted-5pzqg" in namespace "gc-79"
Nov 15 06:17:58.198: INFO: Deleting pod "simpletest-rc-to-be-deleted-5sc29" in namespace "gc-79"
Nov 15 06:17:58.254: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xrh5" in namespace "gc-79"
Nov 15 06:17:58.333: INFO: Deleting pod "simpletest-rc-to-be-deleted-66fv6" in namespace "gc-79"
Nov 15 06:17:58.397: INFO: Deleting pod "simpletest-rc-to-be-deleted-68w4s" in namespace "gc-79"
Nov 15 06:17:58.457: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gmwt" in namespace "gc-79"
Nov 15 06:17:58.526: INFO: Deleting pod "simpletest-rc-to-be-deleted-6p9sb" in namespace "gc-79"
Nov 15 06:17:58.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-6xrf5" in namespace "gc-79"
Nov 15 06:17:58.638: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ztl2" in namespace "gc-79"
Nov 15 06:17:58.697: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kj2h" in namespace "gc-79"
Nov 15 06:17:58.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-7n46d" in namespace "gc-79"
Nov 15 06:17:58.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pt96" in namespace "gc-79"
Nov 15 06:17:58.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-7zzvm" in namespace "gc-79"
Nov 15 06:17:58.928: INFO: Deleting pod "simpletest-rc-to-be-deleted-849qv" in namespace "gc-79"
Nov 15 06:17:58.973: INFO: Deleting pod "simpletest-rc-to-be-deleted-8lq74" in namespace "gc-79"
Nov 15 06:17:59.036: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zfb9" in namespace "gc-79"
Nov 15 06:17:59.109: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fzxc" in namespace "gc-79"
Nov 15 06:17:59.169: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qk7v" in namespace "gc-79"
Nov 15 06:17:59.221: INFO: Deleting pod "simpletest-rc-to-be-deleted-9s65v" in namespace "gc-79"
Nov 15 06:17:59.286: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wxsn" in namespace "gc-79"
Nov 15 06:17:59.343: INFO: Deleting pod "simpletest-rc-to-be-deleted-9x8rw" in namespace "gc-79"
Nov 15 06:17:59.413: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xnqh" in namespace "gc-79"
Nov 15 06:17:59.492: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcmmf" in namespace "gc-79"
Nov 15 06:17:59.593: INFO: Deleting pod "simpletest-rc-to-be-deleted-brdwt" in namespace "gc-79"
Nov 15 06:17:59.684: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5f26" in namespace "gc-79"
Nov 15 06:17:59.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-cphxd" in namespace "gc-79"
Nov 15 06:17:59.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6xhp" in namespace "gc-79"
Nov 15 06:17:59.921: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfb85" in namespace "gc-79"
Nov 15 06:17:59.982: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkpbc" in namespace "gc-79"
Nov 15 06:18:00.051: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnsgq" in namespace "gc-79"
Nov 15 06:18:00.131: INFO: Deleting pod "simpletest-rc-to-be-deleted-dp9bb" in namespace "gc-79"
Nov 15 06:18:00.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjpwx" in namespace "gc-79"
Nov 15 06:18:00.283: INFO: Deleting pod "simpletest-rc-to-be-deleted-frkzd" in namespace "gc-79"
Nov 15 06:18:00.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxzht" in namespace "gc-79"
Nov 15 06:18:00.416: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7b9z" in namespace "gc-79"
Nov 15 06:18:00.539: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7xrn" in namespace "gc-79"
Nov 15 06:18:00.619: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9g7x" in namespace "gc-79"
Nov 15 06:18:00.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-hptc4" in namespace "gc-79"
Nov 15 06:18:00.746: INFO: Deleting pod "simpletest-rc-to-be-deleted-ht5h5" in namespace "gc-79"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 15 06:18:00.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-79" for this suite. 11/15/23 06:18:00.886
------------------------------
â€¢ [SLOW TEST] [15.930 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:17:44.982
    Nov 15 06:17:44.982: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename gc 11/15/23 06:17:44.983
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:17:45.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:17:45.051
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 11/15/23 06:17:45.095
    STEP: create the rc2 11/15/23 06:17:45.117
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 11/15/23 06:17:50.166
    STEP: delete the rc simpletest-rc-to-be-deleted 11/15/23 06:17:52.288
    STEP: wait for the rc to be deleted 11/15/23 06:17:52.347
    STEP: Gathering metrics 11/15/23 06:17:57.451
    W1115 06:17:57.500895      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Nov 15 06:17:57.500: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Nov 15 06:17:57.500: INFO: Deleting pod "simpletest-rc-to-be-deleted-22448" in namespace "gc-79"
    Nov 15 06:17:57.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-292lx" in namespace "gc-79"
    Nov 15 06:17:57.618: INFO: Deleting pod "simpletest-rc-to-be-deleted-2n5r2" in namespace "gc-79"
    Nov 15 06:17:57.668: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wgk4" in namespace "gc-79"
    Nov 15 06:17:57.715: INFO: Deleting pod "simpletest-rc-to-be-deleted-424pj" in namespace "gc-79"
    Nov 15 06:17:57.789: INFO: Deleting pod "simpletest-rc-to-be-deleted-4296x" in namespace "gc-79"
    Nov 15 06:17:57.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-42fnc" in namespace "gc-79"
    Nov 15 06:17:57.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-4bh7t" in namespace "gc-79"
    Nov 15 06:17:57.969: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lmmz" in namespace "gc-79"
    Nov 15 06:17:58.015: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hqd8" in namespace "gc-79"
    Nov 15 06:17:58.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nkxc" in namespace "gc-79"
    Nov 15 06:17:58.140: INFO: Deleting pod "simpletest-rc-to-be-deleted-5pzqg" in namespace "gc-79"
    Nov 15 06:17:58.198: INFO: Deleting pod "simpletest-rc-to-be-deleted-5sc29" in namespace "gc-79"
    Nov 15 06:17:58.254: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xrh5" in namespace "gc-79"
    Nov 15 06:17:58.333: INFO: Deleting pod "simpletest-rc-to-be-deleted-66fv6" in namespace "gc-79"
    Nov 15 06:17:58.397: INFO: Deleting pod "simpletest-rc-to-be-deleted-68w4s" in namespace "gc-79"
    Nov 15 06:17:58.457: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gmwt" in namespace "gc-79"
    Nov 15 06:17:58.526: INFO: Deleting pod "simpletest-rc-to-be-deleted-6p9sb" in namespace "gc-79"
    Nov 15 06:17:58.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-6xrf5" in namespace "gc-79"
    Nov 15 06:17:58.638: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ztl2" in namespace "gc-79"
    Nov 15 06:17:58.697: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kj2h" in namespace "gc-79"
    Nov 15 06:17:58.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-7n46d" in namespace "gc-79"
    Nov 15 06:17:58.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pt96" in namespace "gc-79"
    Nov 15 06:17:58.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-7zzvm" in namespace "gc-79"
    Nov 15 06:17:58.928: INFO: Deleting pod "simpletest-rc-to-be-deleted-849qv" in namespace "gc-79"
    Nov 15 06:17:58.973: INFO: Deleting pod "simpletest-rc-to-be-deleted-8lq74" in namespace "gc-79"
    Nov 15 06:17:59.036: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zfb9" in namespace "gc-79"
    Nov 15 06:17:59.109: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fzxc" in namespace "gc-79"
    Nov 15 06:17:59.169: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qk7v" in namespace "gc-79"
    Nov 15 06:17:59.221: INFO: Deleting pod "simpletest-rc-to-be-deleted-9s65v" in namespace "gc-79"
    Nov 15 06:17:59.286: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wxsn" in namespace "gc-79"
    Nov 15 06:17:59.343: INFO: Deleting pod "simpletest-rc-to-be-deleted-9x8rw" in namespace "gc-79"
    Nov 15 06:17:59.413: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xnqh" in namespace "gc-79"
    Nov 15 06:17:59.492: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcmmf" in namespace "gc-79"
    Nov 15 06:17:59.593: INFO: Deleting pod "simpletest-rc-to-be-deleted-brdwt" in namespace "gc-79"
    Nov 15 06:17:59.684: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5f26" in namespace "gc-79"
    Nov 15 06:17:59.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-cphxd" in namespace "gc-79"
    Nov 15 06:17:59.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6xhp" in namespace "gc-79"
    Nov 15 06:17:59.921: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfb85" in namespace "gc-79"
    Nov 15 06:17:59.982: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkpbc" in namespace "gc-79"
    Nov 15 06:18:00.051: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnsgq" in namespace "gc-79"
    Nov 15 06:18:00.131: INFO: Deleting pod "simpletest-rc-to-be-deleted-dp9bb" in namespace "gc-79"
    Nov 15 06:18:00.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjpwx" in namespace "gc-79"
    Nov 15 06:18:00.283: INFO: Deleting pod "simpletest-rc-to-be-deleted-frkzd" in namespace "gc-79"
    Nov 15 06:18:00.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxzht" in namespace "gc-79"
    Nov 15 06:18:00.416: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7b9z" in namespace "gc-79"
    Nov 15 06:18:00.539: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7xrn" in namespace "gc-79"
    Nov 15 06:18:00.619: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9g7x" in namespace "gc-79"
    Nov 15 06:18:00.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-hptc4" in namespace "gc-79"
    Nov 15 06:18:00.746: INFO: Deleting pod "simpletest-rc-to-be-deleted-ht5h5" in namespace "gc-79"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:18:00.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-79" for this suite. 11/15/23 06:18:00.886
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:18:00.92
Nov 15 06:18:00.920: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename resourcequota 11/15/23 06:18:00.921
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:18:00.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:18:01.012
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 11/15/23 06:18:01.03
STEP: Creating a ResourceQuota 11/15/23 06:18:06.047
STEP: Ensuring resource quota status is calculated 11/15/23 06:18:06.069
STEP: Creating a ReplicationController 11/15/23 06:18:08.086
STEP: Ensuring resource quota status captures replication controller creation 11/15/23 06:18:08.203
STEP: Deleting a ReplicationController 11/15/23 06:18:10.221
STEP: Ensuring resource quota status released usage 11/15/23 06:18:10.245
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 06:18:12.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7926" for this suite. 11/15/23 06:18:12.298
------------------------------
â€¢ [SLOW TEST] [11.424 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:18:00.92
    Nov 15 06:18:00.920: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename resourcequota 11/15/23 06:18:00.921
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:18:00.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:18:01.012
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 11/15/23 06:18:01.03
    STEP: Creating a ResourceQuota 11/15/23 06:18:06.047
    STEP: Ensuring resource quota status is calculated 11/15/23 06:18:06.069
    STEP: Creating a ReplicationController 11/15/23 06:18:08.086
    STEP: Ensuring resource quota status captures replication controller creation 11/15/23 06:18:08.203
    STEP: Deleting a ReplicationController 11/15/23 06:18:10.221
    STEP: Ensuring resource quota status released usage 11/15/23 06:18:10.245
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:18:12.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7926" for this suite. 11/15/23 06:18:12.298
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:18:12.345
Nov 15 06:18:12.346: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename job 11/15/23 06:18:12.346
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:18:12.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:18:12.418
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 11/15/23 06:18:12.451
STEP: Patching the Job 11/15/23 06:18:12.471
STEP: Watching for Job to be patched 11/15/23 06:18:12.532
Nov 15 06:18:12.540: INFO: Event ADDED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking:]
Nov 15 06:18:12.540: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking:]
Nov 15 06:18:12.540: INFO: Event MODIFIED found for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 11/15/23 06:18:12.54
STEP: Watching for Job to be updated 11/15/23 06:18:12.581
Nov 15 06:18:12.589: INFO: Event MODIFIED found for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 06:18:12.589: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 11/15/23 06:18:12.589
Nov 15 06:18:12.603: INFO: Job: e2e-52ffv as labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv]
STEP: Waiting for job to complete 11/15/23 06:18:12.604
STEP: Delete a job collection with a labelselector 11/15/23 06:18:22.62
STEP: Watching for Job to be deleted 11/15/23 06:18:22.656
Nov 15 06:18:22.664: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 06:18:22.664: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 06:18:22.664: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 06:18:22.664: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 06:18:22.664: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 06:18:22.664: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 06:18:22.668: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 06:18:22.668: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 06:18:22.668: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 06:18:22.668: INFO: Event DELETED found for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 11/15/23 06:18:22.668
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Nov 15 06:18:22.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3638" for this suite. 11/15/23 06:18:22.728
------------------------------
â€¢ [SLOW TEST] [10.411 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:18:12.345
    Nov 15 06:18:12.346: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename job 11/15/23 06:18:12.346
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:18:12.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:18:12.418
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 11/15/23 06:18:12.451
    STEP: Patching the Job 11/15/23 06:18:12.471
    STEP: Watching for Job to be patched 11/15/23 06:18:12.532
    Nov 15 06:18:12.540: INFO: Event ADDED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking:]
    Nov 15 06:18:12.540: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking:]
    Nov 15 06:18:12.540: INFO: Event MODIFIED found for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 11/15/23 06:18:12.54
    STEP: Watching for Job to be updated 11/15/23 06:18:12.581
    Nov 15 06:18:12.589: INFO: Event MODIFIED found for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 06:18:12.589: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 11/15/23 06:18:12.589
    Nov 15 06:18:12.603: INFO: Job: e2e-52ffv as labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv]
    STEP: Waiting for job to complete 11/15/23 06:18:12.604
    STEP: Delete a job collection with a labelselector 11/15/23 06:18:22.62
    STEP: Watching for Job to be deleted 11/15/23 06:18:22.656
    Nov 15 06:18:22.664: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 06:18:22.664: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 06:18:22.664: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 06:18:22.664: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 06:18:22.664: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 06:18:22.664: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 06:18:22.668: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 06:18:22.668: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 06:18:22.668: INFO: Event MODIFIED observed for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 06:18:22.668: INFO: Event DELETED found for Job e2e-52ffv in namespace job-3638 with labels: map[e2e-52ffv:patched e2e-job-label:e2e-52ffv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 11/15/23 06:18:22.668
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:18:22.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3638" for this suite. 11/15/23 06:18:22.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:18:22.757
Nov 15 06:18:22.757: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 06:18:22.759
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:18:22.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:18:22.833
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-6843d3dd-1682-4598-975c-8f84190e7fec 11/15/23 06:18:22.846
STEP: Creating a pod to test consume secrets 11/15/23 06:18:22.861
Nov 15 06:18:22.895: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c0855225-49cf-488b-bdf3-027c5dc16f7c" in namespace "projected-4428" to be "Succeeded or Failed"
Nov 15 06:18:22.912: INFO: Pod "pod-projected-secrets-c0855225-49cf-488b-bdf3-027c5dc16f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.165506ms
Nov 15 06:18:24.931: INFO: Pod "pod-projected-secrets-c0855225-49cf-488b-bdf3-027c5dc16f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036155038s
Nov 15 06:18:26.942: INFO: Pod "pod-projected-secrets-c0855225-49cf-488b-bdf3-027c5dc16f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047413832s
Nov 15 06:18:28.935: INFO: Pod "pod-projected-secrets-c0855225-49cf-488b-bdf3-027c5dc16f7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040004309s
STEP: Saw pod success 11/15/23 06:18:28.935
Nov 15 06:18:28.935: INFO: Pod "pod-projected-secrets-c0855225-49cf-488b-bdf3-027c5dc16f7c" satisfied condition "Succeeded or Failed"
Nov 15 06:18:28.956: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-secrets-c0855225-49cf-488b-bdf3-027c5dc16f7c container projected-secret-volume-test: <nil>
STEP: delete the pod 11/15/23 06:18:28.996
Nov 15 06:18:29.054: INFO: Waiting for pod pod-projected-secrets-c0855225-49cf-488b-bdf3-027c5dc16f7c to disappear
Nov 15 06:18:29.072: INFO: Pod pod-projected-secrets-c0855225-49cf-488b-bdf3-027c5dc16f7c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 15 06:18:29.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4428" for this suite. 11/15/23 06:18:29.101
------------------------------
â€¢ [SLOW TEST] [6.371 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:18:22.757
    Nov 15 06:18:22.757: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 06:18:22.759
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:18:22.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:18:22.833
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-6843d3dd-1682-4598-975c-8f84190e7fec 11/15/23 06:18:22.846
    STEP: Creating a pod to test consume secrets 11/15/23 06:18:22.861
    Nov 15 06:18:22.895: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c0855225-49cf-488b-bdf3-027c5dc16f7c" in namespace "projected-4428" to be "Succeeded or Failed"
    Nov 15 06:18:22.912: INFO: Pod "pod-projected-secrets-c0855225-49cf-488b-bdf3-027c5dc16f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.165506ms
    Nov 15 06:18:24.931: INFO: Pod "pod-projected-secrets-c0855225-49cf-488b-bdf3-027c5dc16f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036155038s
    Nov 15 06:18:26.942: INFO: Pod "pod-projected-secrets-c0855225-49cf-488b-bdf3-027c5dc16f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047413832s
    Nov 15 06:18:28.935: INFO: Pod "pod-projected-secrets-c0855225-49cf-488b-bdf3-027c5dc16f7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040004309s
    STEP: Saw pod success 11/15/23 06:18:28.935
    Nov 15 06:18:28.935: INFO: Pod "pod-projected-secrets-c0855225-49cf-488b-bdf3-027c5dc16f7c" satisfied condition "Succeeded or Failed"
    Nov 15 06:18:28.956: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-secrets-c0855225-49cf-488b-bdf3-027c5dc16f7c container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 06:18:28.996
    Nov 15 06:18:29.054: INFO: Waiting for pod pod-projected-secrets-c0855225-49cf-488b-bdf3-027c5dc16f7c to disappear
    Nov 15 06:18:29.072: INFO: Pod pod-projected-secrets-c0855225-49cf-488b-bdf3-027c5dc16f7c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:18:29.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4428" for this suite. 11/15/23 06:18:29.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:18:29.131
Nov 15 06:18:29.131: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename configmap 11/15/23 06:18:29.132
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:18:29.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:18:29.207
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-6b10d35a-7cd9-4931-ba73-7d37541ad2e7 11/15/23 06:18:29.22
STEP: Creating a pod to test consume configMaps 11/15/23 06:18:29.243
Nov 15 06:18:29.281: INFO: Waiting up to 5m0s for pod "pod-configmaps-532a624b-fe48-4e70-8390-5545d494c119" in namespace "configmap-7846" to be "Succeeded or Failed"
Nov 15 06:18:29.306: INFO: Pod "pod-configmaps-532a624b-fe48-4e70-8390-5545d494c119": Phase="Pending", Reason="", readiness=false. Elapsed: 25.340212ms
Nov 15 06:18:31.326: INFO: Pod "pod-configmaps-532a624b-fe48-4e70-8390-5545d494c119": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044880852s
Nov 15 06:18:33.325: INFO: Pod "pod-configmaps-532a624b-fe48-4e70-8390-5545d494c119": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043498137s
Nov 15 06:18:35.324: INFO: Pod "pod-configmaps-532a624b-fe48-4e70-8390-5545d494c119": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043279329s
STEP: Saw pod success 11/15/23 06:18:35.324
Nov 15 06:18:35.325: INFO: Pod "pod-configmaps-532a624b-fe48-4e70-8390-5545d494c119" satisfied condition "Succeeded or Failed"
Nov 15 06:18:35.347: INFO: Trying to get logs from node 10.72.152.86 pod pod-configmaps-532a624b-fe48-4e70-8390-5545d494c119 container agnhost-container: <nil>
STEP: delete the pod 11/15/23 06:18:35.391
Nov 15 06:18:35.453: INFO: Waiting for pod pod-configmaps-532a624b-fe48-4e70-8390-5545d494c119 to disappear
Nov 15 06:18:35.471: INFO: Pod pod-configmaps-532a624b-fe48-4e70-8390-5545d494c119 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:18:35.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7846" for this suite. 11/15/23 06:18:35.5
------------------------------
â€¢ [SLOW TEST] [6.398 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:18:29.131
    Nov 15 06:18:29.131: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename configmap 11/15/23 06:18:29.132
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:18:29.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:18:29.207
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-6b10d35a-7cd9-4931-ba73-7d37541ad2e7 11/15/23 06:18:29.22
    STEP: Creating a pod to test consume configMaps 11/15/23 06:18:29.243
    Nov 15 06:18:29.281: INFO: Waiting up to 5m0s for pod "pod-configmaps-532a624b-fe48-4e70-8390-5545d494c119" in namespace "configmap-7846" to be "Succeeded or Failed"
    Nov 15 06:18:29.306: INFO: Pod "pod-configmaps-532a624b-fe48-4e70-8390-5545d494c119": Phase="Pending", Reason="", readiness=false. Elapsed: 25.340212ms
    Nov 15 06:18:31.326: INFO: Pod "pod-configmaps-532a624b-fe48-4e70-8390-5545d494c119": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044880852s
    Nov 15 06:18:33.325: INFO: Pod "pod-configmaps-532a624b-fe48-4e70-8390-5545d494c119": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043498137s
    Nov 15 06:18:35.324: INFO: Pod "pod-configmaps-532a624b-fe48-4e70-8390-5545d494c119": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043279329s
    STEP: Saw pod success 11/15/23 06:18:35.324
    Nov 15 06:18:35.325: INFO: Pod "pod-configmaps-532a624b-fe48-4e70-8390-5545d494c119" satisfied condition "Succeeded or Failed"
    Nov 15 06:18:35.347: INFO: Trying to get logs from node 10.72.152.86 pod pod-configmaps-532a624b-fe48-4e70-8390-5545d494c119 container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 06:18:35.391
    Nov 15 06:18:35.453: INFO: Waiting for pod pod-configmaps-532a624b-fe48-4e70-8390-5545d494c119 to disappear
    Nov 15 06:18:35.471: INFO: Pod pod-configmaps-532a624b-fe48-4e70-8390-5545d494c119 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:18:35.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7846" for this suite. 11/15/23 06:18:35.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:18:35.531
Nov 15 06:18:35.531: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename sched-pred 11/15/23 06:18:35.532
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:18:35.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:18:35.59
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Nov 15 06:18:35.600: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 15 06:18:35.644: INFO: Waiting for terminating namespaces to be deleted...
Nov 15 06:18:35.669: INFO: 
Logging pods the apiserver thinks is on node 10.72.152.81 before test
Nov 15 06:18:35.729: INFO: calico-node-p5wd4 from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 06:18:35.730: INFO: calico-typha-76d9767bd5-985rd from calico-system started at 2023-11-15 03:39:41 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 06:18:35.730: INFO: ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-dl6kq from ibm-system started at 2023-11-15 03:48:09 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container ibm-cloud-provider-ip-158-176-89-155 ready: true, restart count 0
Nov 15 06:18:35.730: INFO: ibm-keepalived-watcher-qb8hn from kube-system started at 2023-11-15 03:38:31 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 06:18:35.730: INFO: ibm-master-proxy-static-10.72.152.81 from kube-system started at 2023-11-15 03:38:18 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 06:18:35.730: INFO: 	Container pause ready: true, restart count 0
Nov 15 06:18:35.730: INFO: ibmcloud-block-storage-driver-z7fmw from kube-system started at 2023-11-15 03:38:36 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 06:18:35.730: INFO: vpn-56cd75f85d-qwzcc from kube-system started at 2023-11-15 03:40:24 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container vpn ready: true, restart count 0
Nov 15 06:18:35.730: INFO: tuned-rb6qn from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container tuned ready: true, restart count 0
Nov 15 06:18:35.730: INFO: cluster-storage-operator-6cf6b595c7-m7mfz from openshift-cluster-storage-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Nov 15 06:18:35.730: INFO: csi-snapshot-controller-857d54544d-mwb9h from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 06:18:35.730: INFO: csi-snapshot-controller-operator-56df7685c7-vnvd2 from openshift-cluster-storage-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Nov 15 06:18:35.730: INFO: csi-snapshot-webhook-586f5c484d-qg5n7 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container webhook ready: true, restart count 0
Nov 15 06:18:35.730: INFO: console-68d6458867-wlg2q from openshift-console started at 2023-11-15 03:47:25 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container console ready: true, restart count 0
Nov 15 06:18:35.730: INFO: downloads-7bb648f846-tvqt6 from openshift-console started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container download-server ready: true, restart count 0
Nov 15 06:18:35.730: INFO: dns-default-njlcg from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container dns ready: true, restart count 0
Nov 15 06:18:35.730: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.730: INFO: node-resolver-6lxcm from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 15 06:18:35.730: INFO: cluster-image-registry-operator-64994bbb4-6twjh from openshift-image-registry started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Nov 15 06:18:35.730: INFO: node-ca-897k2 from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container node-ca ready: true, restart count 0
Nov 15 06:18:35.730: INFO: registry-pvc-permissions-lw2bj from openshift-image-registry started at 2023-11-15 03:46:43 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container pvc-permissions ready: false, restart count 0
Nov 15 06:18:35.730: INFO: ingress-canary-jqtjt from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 15 06:18:35.730: INFO: router-default-56777c97d6-szb4s from openshift-ingress started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container router ready: true, restart count 0
Nov 15 06:18:35.730: INFO: insights-operator-85b688b59d-v47wz from openshift-insights started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container insights-operator ready: true, restart count 1
Nov 15 06:18:35.730: INFO: openshift-kube-proxy-p8p2b from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 15 06:18:35.730: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.730: INFO: migrator-697dd4cbc5-kk7zn from openshift-kube-storage-version-migrator started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container migrator ready: true, restart count 0
Nov 15 06:18:35.730: INFO: certified-operators-lqtq6 from openshift-marketplace started at 2023-11-15 03:42:42 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.730: INFO: 	Container registry-server ready: true, restart count 0
Nov 15 06:18:35.731: INFO: community-operators-jpmzt from openshift-marketplace started at 2023-11-15 05:06:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container registry-server ready: true, restart count 0
Nov 15 06:18:35.731: INFO: marketplace-operator-55cc9f5b6b-xpbw5 from openshift-marketplace started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container marketplace-operator ready: true, restart count 0
Nov 15 06:18:35.731: INFO: redhat-marketplace-xp5nc from openshift-marketplace started at 2023-11-15 03:42:42 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container registry-server ready: true, restart count 0
Nov 15 06:18:35.731: INFO: redhat-operators-bzltn from openshift-marketplace started at 2023-11-15 04:07:58 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container registry-server ready: true, restart count 0
Nov 15 06:18:35.731: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-11-15 03:46:14 +0000 UTC (6 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container alertmanager ready: true, restart count 1
Nov 15 06:18:35.731: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 15 06:18:35.731: INFO: 	Container config-reloader ready: true, restart count 0
Nov 15 06:18:35.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.731: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Nov 15 06:18:35.731: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 15 06:18:35.731: INFO: node-exporter-67458 from openshift-monitoring started at 2023-11-15 03:44:49 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.731: INFO: 	Container node-exporter ready: true, restart count 0
Nov 15 06:18:35.731: INFO: prometheus-adapter-5f5bb574db-r4lkm from openshift-monitoring started at 2023-11-15 03:44:53 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 15 06:18:35.731: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-11-15 03:46:20 +0000 UTC (6 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container config-reloader ready: true, restart count 0
Nov 15 06:18:35.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.731: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Nov 15 06:18:35.731: INFO: 	Container prometheus ready: true, restart count 0
Nov 15 06:18:35.731: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 15 06:18:35.731: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 15 06:18:35.731: INFO: prometheus-operator-admission-webhook-c78bf8f99-gxfh4 from openshift-monitoring started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Nov 15 06:18:35.731: INFO: telemeter-client-77c946bb95-nsnds from openshift-monitoring started at 2023-11-15 03:46:15 +0000 UTC (3 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.731: INFO: 	Container reload ready: true, restart count 0
Nov 15 06:18:35.731: INFO: 	Container telemeter-client ready: true, restart count 0
Nov 15 06:18:35.731: INFO: multus-9tc6x from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container kube-multus ready: true, restart count 0
Nov 15 06:18:35.731: INFO: multus-additional-cni-plugins-j2jvc from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 15 06:18:35.731: INFO: network-metrics-daemon-g5fgz from openshift-multus started at 2023-11-15 03:38:47 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.731: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 15 06:18:35.731: INFO: network-check-target-6pv9x from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 15 06:18:35.731: INFO: catalog-operator-798697959c-24lbd from openshift-operator-lifecycle-manager started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container catalog-operator ready: true, restart count 0
Nov 15 06:18:35.731: INFO: collect-profiles-28333785-ckm22 from openshift-operator-lifecycle-manager started at 2023-11-15 05:45:00 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 15 06:18:35.731: INFO: collect-profiles-28333800-bgbt2 from openshift-operator-lifecycle-manager started at 2023-11-15 06:00:00 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 15 06:18:35.731: INFO: collect-profiles-28333815-xxvbf from openshift-operator-lifecycle-manager started at 2023-11-15 06:15:00 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 15 06:18:35.731: INFO: olm-operator-846bf6bd78-fzxr5 from openshift-operator-lifecycle-manager started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.731: INFO: 	Container olm-operator ready: true, restart count 0
Nov 15 06:18:35.731: INFO: package-server-manager-5b666bf8fd-5v7rb from openshift-operator-lifecycle-manager started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.732: INFO: 	Container package-server-manager ready: true, restart count 0
Nov 15 06:18:35.732: INFO: packageserver-758b547fc-f9wwb from openshift-operator-lifecycle-manager started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.732: INFO: 	Container packageserver ready: true, restart count 0
Nov 15 06:18:35.732: INFO: service-ca-operator-74cb5c9cf5-fqgj5 from openshift-service-ca-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.732: INFO: 	Container service-ca-operator ready: true, restart count 1
Nov 15 06:18:35.732: INFO: service-ca-78fb97bb77-qcz4b from openshift-service-ca started at 2023-11-15 03:41:45 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.732: INFO: 	Container service-ca-controller ready: true, restart count 0
Nov 15 06:18:35.732: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-qvnnx from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.732: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 06:18:35.732: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 06:18:35.732: INFO: 
Logging pods the apiserver thinks is on node 10.72.152.86 before test
Nov 15 06:18:35.793: INFO: calico-node-cmgfg from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 06:18:35.793: INFO: calico-typha-76d9767bd5-tx2mp from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 06:18:35.793: INFO: ibm-keepalived-watcher-qclxn from kube-system started at 2023-11-15 03:38:36 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 06:18:35.793: INFO: ibm-master-proxy-static-10.72.152.86 from kube-system started at 2023-11-15 03:38:24 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container pause ready: true, restart count 0
Nov 15 06:18:35.793: INFO: ibmcloud-block-storage-driver-m8gq4 from kube-system started at 2023-11-15 03:38:43 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 06:18:35.793: INFO: cluster-node-tuning-operator-5f77b58f7-t8gf6 from openshift-cluster-node-tuning-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Nov 15 06:18:35.793: INFO: tuned-5qzzn from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container tuned ready: true, restart count 0
Nov 15 06:18:35.793: INFO: cluster-samples-operator-65684cb854-h7n8t from openshift-cluster-samples-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Nov 15 06:18:35.793: INFO: csi-snapshot-controller-857d54544d-qbhf6 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 06:18:35.793: INFO: csi-snapshot-webhook-586f5c484d-vkpg9 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container webhook ready: true, restart count 0
Nov 15 06:18:35.793: INFO: console-operator-769f9748fb-7tfcc from openshift-console-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container console-operator ready: true, restart count 1
Nov 15 06:18:35.793: INFO: 	Container conversion-webhook-server ready: true, restart count 3
Nov 15 06:18:35.793: INFO: downloads-7bb648f846-sr7nb from openshift-console started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container download-server ready: true, restart count 0
Nov 15 06:18:35.793: INFO: dns-operator-dd9c9c896-9gwtp from openshift-dns-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container dns-operator ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.793: INFO: dns-default-24qc6 from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container dns ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.793: INFO: node-resolver-d54kt from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 15 06:18:35.793: INFO: node-ca-w7vhg from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container node-ca ready: true, restart count 0
Nov 15 06:18:35.793: INFO: ingress-canary-ttx6m from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 15 06:18:35.793: INFO: ingress-operator-6d4d6975f7-qtm2n from openshift-ingress-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container ingress-operator ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.793: INFO: openshift-kube-proxy-x5hq9 from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.793: INFO: kube-storage-version-migrator-operator-5d88b7484-8m2cc from openshift-kube-storage-version-migrator-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Nov 15 06:18:35.793: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-11-15 03:46:47 +0000 UTC (6 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container alertmanager ready: true, restart count 1
Nov 15 06:18:35.793: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container config-reloader ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 15 06:18:35.793: INFO: cluster-monitoring-operator-868f9b56cf-xrfz7 from openshift-monitoring started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Nov 15 06:18:35.793: INFO: kube-state-metrics-f8d796647-f4rgj from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (3 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 15 06:18:35.793: INFO: node-exporter-n68zb from openshift-monitoring started at 2023-11-15 03:44:49 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container node-exporter ready: true, restart count 0
Nov 15 06:18:35.793: INFO: openshift-state-metrics-69bb697b65-6bcg7 from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (3 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Nov 15 06:18:35.793: INFO: prometheus-adapter-5f5bb574db-569xs from openshift-monitoring started at 2023-11-15 03:44:53 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 15 06:18:35.793: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-11-15 03:46:38 +0000 UTC (6 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container config-reloader ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container prometheus ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 15 06:18:35.793: INFO: prometheus-operator-6fcb4d4c46-t74rt from openshift-monitoring started at 2023-11-15 03:44:41 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 15 06:18:35.793: INFO: thanos-querier-56b7586647-8m7mp from openshift-monitoring started at 2023-11-15 03:44:57 +0000 UTC (6 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container thanos-query ready: true, restart count 0
Nov 15 06:18:35.793: INFO: multus-additional-cni-plugins-z6xx7 from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 15 06:18:35.793: INFO: multus-admission-controller-6b76dd856b-6qmnp from openshift-multus started at 2023-11-15 03:44:37 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 15 06:18:35.793: INFO: multus-cltwv from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container kube-multus ready: true, restart count 0
Nov 15 06:18:35.793: INFO: network-metrics-daemon-zbz9n from openshift-multus started at 2023-11-15 03:38:47 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 15 06:18:35.793: INFO: network-check-target-gnph7 from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 15 06:18:35.793: INFO: packageserver-758b547fc-65qc5 from openshift-operator-lifecycle-manager started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container packageserver ready: true, restart count 0
Nov 15 06:18:35.793: INFO: sonobuoy from sonobuoy started at 2023-11-15 05:52:01 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 15 06:18:35.793: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-m6885 from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 06:18:35.793: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-11-15 03:41:49 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.793: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 15 06:18:35.793: INFO: 
Logging pods the apiserver thinks is on node 10.72.152.88 before test
Nov 15 06:18:35.857: INFO: calico-kube-controllers-5dd9d87465-759pm from calico-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 15 06:18:35.857: INFO: calico-node-lmf6x from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 06:18:35.857: INFO: managed-storage-validation-webhooks-7457bf6687-9ds5w from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Nov 15 06:18:35.857: INFO: managed-storage-validation-webhooks-7457bf6687-h522x from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Nov 15 06:18:35.857: INFO: managed-storage-validation-webhooks-7457bf6687-phdgp from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Nov 15 06:18:35.857: INFO: ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-vbxww from ibm-system started at 2023-11-15 03:48:09 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container ibm-cloud-provider-ip-158-176-89-155 ready: true, restart count 0
Nov 15 06:18:35.857: INFO: ibm-file-plugin-5fcf7fb495-xmdts from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 15 06:18:35.857: INFO: ibm-keepalived-watcher-5jx26 from kube-system started at 2023-11-15 03:38:12 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 06:18:35.857: INFO: ibm-master-proxy-static-10.72.152.88 from kube-system started at 2023-11-15 03:38:04 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 06:18:35.857: INFO: 	Container pause ready: true, restart count 0
Nov 15 06:18:35.857: INFO: ibm-storage-metrics-agent-84fbdc746-5sv68 from kube-system started at 2023-11-15 03:39:51 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Nov 15 06:18:35.857: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Nov 15 06:18:35.857: INFO: ibm-storage-watcher-7445c988b-8ngdm from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 15 06:18:35.857: INFO: ibmcloud-block-storage-driver-9t8rj from kube-system started at 2023-11-15 03:38:17 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 06:18:35.857: INFO: ibmcloud-block-storage-plugin-5774687565-gj9xn from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Nov 15 06:18:35.857: INFO: tuned-q894n from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container tuned ready: true, restart count 0
Nov 15 06:18:35.857: INFO: console-68d6458867-krfqd from openshift-console started at 2023-11-15 03:47:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container console ready: true, restart count 0
Nov 15 06:18:35.857: INFO: dns-default-lmngx from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container dns ready: true, restart count 0
Nov 15 06:18:35.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.857: INFO: node-resolver-hwp5f from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 15 06:18:35.857: INFO: image-registry-f74f764d8-w48k4 from openshift-image-registry started at 2023-11-15 03:46:35 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container registry ready: true, restart count 0
Nov 15 06:18:35.857: INFO: node-ca-wrqvd from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container node-ca ready: true, restart count 0
Nov 15 06:18:35.857: INFO: ingress-canary-wvl4p from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 15 06:18:35.857: INFO: router-default-56777c97d6-4bnqq from openshift-ingress started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container router ready: true, restart count 0
Nov 15 06:18:35.857: INFO: openshift-kube-proxy-tzknx from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 15 06:18:35.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.857: INFO: node-exporter-2452k from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.857: INFO: 	Container node-exporter ready: true, restart count 0
Nov 15 06:18:35.857: INFO: prometheus-operator-admission-webhook-c78bf8f99-x8vcz from openshift-monitoring started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Nov 15 06:18:35.857: INFO: thanos-querier-56b7586647-qr75b from openshift-monitoring started at 2023-11-15 03:44:58 +0000 UTC (6 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.857: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 15 06:18:35.857: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Nov 15 06:18:35.857: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 15 06:18:35.857: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 15 06:18:35.857: INFO: 	Container thanos-query ready: true, restart count 0
Nov 15 06:18:35.857: INFO: multus-additional-cni-plugins-lcnxr from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 15 06:18:35.857: INFO: multus-admission-controller-6b76dd856b-6zft2 from openshift-multus started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.857: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 15 06:18:35.857: INFO: multus-vqg9w from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container kube-multus ready: true, restart count 0
Nov 15 06:18:35.857: INFO: network-metrics-daemon-9mrwp from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:18:35.857: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 15 06:18:35.857: INFO: network-check-source-5f9c5566b6-grf5l from openshift-network-diagnostics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container check-endpoints ready: true, restart count 0
Nov 15 06:18:35.857: INFO: network-check-target-6rz7p from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 15 06:18:35.857: INFO: network-operator-847f47449c-j9glm from openshift-network-operator started at 2023-11-15 03:38:21 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container network-operator ready: true, restart count 1
Nov 15 06:18:35.857: INFO: metrics-667b585fc4-d5fdk from openshift-roks-metrics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container metrics ready: true, restart count 5
Nov 15 06:18:35.857: INFO: push-gateway-7f9447c646-5mjcp from openshift-roks-metrics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container push-gateway ready: true, restart count 0
Nov 15 06:18:35.857: INFO: sonobuoy-e2e-job-4fb3cc1d32834aa9 from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container e2e ready: true, restart count 0
Nov 15 06:18:35.857: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 06:18:35.857: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-ddv8z from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 06:18:35.857: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 06:18:35.857: INFO: tigera-operator-7dbcb4fb45-rn78j from tigera-operator started at 2023-11-15 03:38:21 +0000 UTC (1 container statuses recorded)
Nov 15 06:18:35.857: INFO: 	Container tigera-operator ready: true, restart count 2
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 11/15/23 06:18:35.858
Nov 15 06:18:35.888: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5270" to be "running"
Nov 15 06:18:35.920: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 32.116647ms
Nov 15 06:18:37.939: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050424514s
Nov 15 06:18:39.940: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.051905383s
Nov 15 06:18:39.940: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 11/15/23 06:18:39.959
STEP: Trying to apply a random label on the found node. 11/15/23 06:18:40.03
STEP: verifying the node has the label kubernetes.io/e2e-80123b52-6a84-404f-999e-f227b0f6026d 95 11/15/23 06:18:40.069
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 11/15/23 06:18:40.09
Nov 15 06:18:40.119: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5270" to be "not pending"
Nov 15 06:18:40.137: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 17.275046ms
Nov 15 06:18:42.155: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.035962076s
Nov 15 06:18:42.155: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.72.152.81 on the node which pod4 resides and expect not scheduled 11/15/23 06:18:42.155
Nov 15 06:18:42.182: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5270" to be "not pending"
Nov 15 06:18:42.218: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 35.594776ms
Nov 15 06:18:44.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055189397s
Nov 15 06:18:46.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058252463s
Nov 15 06:18:48.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054849352s
Nov 15 06:18:50.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.056195848s
Nov 15 06:18:52.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.055354374s
Nov 15 06:18:54.236: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.054012056s
Nov 15 06:18:56.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.055785162s
Nov 15 06:18:58.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.05527288s
Nov 15 06:19:00.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.054980799s
Nov 15 06:19:02.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.057086315s
Nov 15 06:19:04.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.056175789s
Nov 15 06:19:06.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.054943607s
Nov 15 06:19:08.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.055950309s
Nov 15 06:19:10.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.070919718s
Nov 15 06:19:12.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.069356598s
Nov 15 06:19:14.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.056449113s
Nov 15 06:19:16.241: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.058739443s
Nov 15 06:19:18.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.056009076s
Nov 15 06:19:20.234: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.051539772s
Nov 15 06:19:22.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.055917676s
Nov 15 06:19:24.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.05510381s
Nov 15 06:19:26.243: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.061208891s
Nov 15 06:19:28.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.064553255s
Nov 15 06:19:30.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.056550672s
Nov 15 06:19:32.242: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.059598999s
Nov 15 06:19:34.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.054587893s
Nov 15 06:19:36.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.055292363s
Nov 15 06:19:38.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.066032001s
Nov 15 06:19:40.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.052507903s
Nov 15 06:19:42.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.05460463s
Nov 15 06:19:44.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.056140841s
Nov 15 06:19:46.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.056032654s
Nov 15 06:19:48.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.055155921s
Nov 15 06:19:50.236: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.053787264s
Nov 15 06:19:52.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.054494644s
Nov 15 06:19:54.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.056028185s
Nov 15 06:19:56.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.05556156s
Nov 15 06:19:58.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.055866851s
Nov 15 06:20:00.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.057717684s
Nov 15 06:20:02.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.054951332s
Nov 15 06:20:04.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.055303048s
Nov 15 06:20:06.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.056535632s
Nov 15 06:20:08.244: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.061885972s
Nov 15 06:20:10.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.055771836s
Nov 15 06:20:12.265: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.082797612s
Nov 15 06:20:14.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.055316933s
Nov 15 06:20:16.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.069939403s
Nov 15 06:20:18.236: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.053821413s
Nov 15 06:20:20.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.054645002s
Nov 15 06:20:22.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.070473563s
Nov 15 06:20:24.243: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.060924692s
Nov 15 06:20:26.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.055585988s
Nov 15 06:20:28.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.056562895s
Nov 15 06:20:30.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.055915237s
Nov 15 06:20:32.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.054498564s
Nov 15 06:20:34.264: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.081816963s
Nov 15 06:20:36.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.055460393s
Nov 15 06:20:38.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.054898547s
Nov 15 06:20:40.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.056340452s
Nov 15 06:20:42.285: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.103052188s
Nov 15 06:20:44.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.057863059s
Nov 15 06:20:46.244: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.061491287s
Nov 15 06:20:48.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.057337535s
Nov 15 06:20:50.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.053093691s
Nov 15 06:20:52.241: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.058506024s
Nov 15 06:20:54.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.055160115s
Nov 15 06:20:56.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.055386266s
Nov 15 06:20:58.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.056421933s
Nov 15 06:21:00.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.056036206s
Nov 15 06:21:02.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.065000508s
Nov 15 06:21:04.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.055516571s
Nov 15 06:21:06.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.056379622s
Nov 15 06:21:08.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.05516021s
Nov 15 06:21:10.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.056163596s
Nov 15 06:21:12.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.057641921s
Nov 15 06:21:14.241: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.058898638s
Nov 15 06:21:16.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.055818169s
Nov 15 06:21:18.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.057072012s
Nov 15 06:21:20.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.055162997s
Nov 15 06:21:22.244: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.061493214s
Nov 15 06:21:24.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.057040539s
Nov 15 06:21:26.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.057986663s
Nov 15 06:21:28.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.055448309s
Nov 15 06:21:30.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.055109665s
Nov 15 06:21:32.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.054358115s
Nov 15 06:21:34.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.055948015s
Nov 15 06:21:36.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.057769963s
Nov 15 06:21:38.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.057907045s
Nov 15 06:21:40.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.055987129s
Nov 15 06:21:42.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.069991486s
Nov 15 06:21:44.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.056963886s
Nov 15 06:21:46.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.056882652s
Nov 15 06:21:48.244: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.06184054s
Nov 15 06:21:50.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.055105466s
Nov 15 06:21:52.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.056254626s
Nov 15 06:21:54.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.057691792s
Nov 15 06:21:56.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.055834046s
Nov 15 06:21:58.242: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.059925488s
Nov 15 06:22:00.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.058223907s
Nov 15 06:22:02.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.055396581s
Nov 15 06:22:04.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.057637978s
Nov 15 06:22:06.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.057699162s
Nov 15 06:22:08.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.054354692s
Nov 15 06:22:10.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.055643316s
Nov 15 06:22:12.241: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.058670434s
Nov 15 06:22:14.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.055768715s
Nov 15 06:22:16.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.054603458s
Nov 15 06:22:18.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.05528703s
Nov 15 06:22:20.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.057883387s
Nov 15 06:22:22.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.056086834s
Nov 15 06:22:24.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.055462354s
Nov 15 06:22:26.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.057825116s
Nov 15 06:22:28.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.056834887s
Nov 15 06:22:30.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.055334333s
Nov 15 06:22:32.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.055637052s
Nov 15 06:22:34.241: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.058366775s
Nov 15 06:22:36.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.06291157s
Nov 15 06:22:38.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.056396963s
Nov 15 06:22:40.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.057253976s
Nov 15 06:22:42.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.056282235s
Nov 15 06:22:44.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.05719596s
Nov 15 06:22:46.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.056422528s
Nov 15 06:22:48.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.055192794s
Nov 15 06:22:50.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.057203344s
Nov 15 06:22:52.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.054970911s
Nov 15 06:22:54.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.055756653s
Nov 15 06:22:56.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.054859835s
Nov 15 06:22:58.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.054364686s
Nov 15 06:23:00.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.057213369s
Nov 15 06:23:02.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.053264622s
Nov 15 06:23:04.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.055461978s
Nov 15 06:23:06.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.055430881s
Nov 15 06:23:08.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.054674895s
Nov 15 06:23:10.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.056804796s
Nov 15 06:23:12.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.072379827s
Nov 15 06:23:14.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.055204036s
Nov 15 06:23:16.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.056072766s
Nov 15 06:23:18.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.055832931s
Nov 15 06:23:20.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.056243028s
Nov 15 06:23:22.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.054269307s
Nov 15 06:23:24.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.089232485s
Nov 15 06:23:26.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.056843058s
Nov 15 06:23:28.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.05429456s
Nov 15 06:23:30.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.055094297s
Nov 15 06:23:32.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.055840219s
Nov 15 06:23:34.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.055832625s
Nov 15 06:23:36.236: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.054218366s
Nov 15 06:23:38.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.054681343s
Nov 15 06:23:40.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.054892195s
Nov 15 06:23:42.277: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.094755564s
Nov 15 06:23:42.311: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.129152622s
STEP: removing the label kubernetes.io/e2e-80123b52-6a84-404f-999e-f227b0f6026d off the node 10.72.152.81 11/15/23 06:23:42.311
STEP: verifying the node doesn't have the label kubernetes.io/e2e-80123b52-6a84-404f-999e-f227b0f6026d 11/15/23 06:23:42.438
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:23:42.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5270" for this suite. 11/15/23 06:23:42.525
------------------------------
â€¢ [SLOW TEST] [307.053 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:18:35.531
    Nov 15 06:18:35.531: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename sched-pred 11/15/23 06:18:35.532
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:18:35.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:18:35.59
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Nov 15 06:18:35.600: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Nov 15 06:18:35.644: INFO: Waiting for terminating namespaces to be deleted...
    Nov 15 06:18:35.669: INFO: 
    Logging pods the apiserver thinks is on node 10.72.152.81 before test
    Nov 15 06:18:35.729: INFO: calico-node-p5wd4 from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: calico-typha-76d9767bd5-985rd from calico-system started at 2023-11-15 03:39:41 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-dl6kq from ibm-system started at 2023-11-15 03:48:09 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container ibm-cloud-provider-ip-158-176-89-155 ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: ibm-keepalived-watcher-qb8hn from kube-system started at 2023-11-15 03:38:31 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: ibm-master-proxy-static-10.72.152.81 from kube-system started at 2023-11-15 03:38:18 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: 	Container pause ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: ibmcloud-block-storage-driver-z7fmw from kube-system started at 2023-11-15 03:38:36 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: vpn-56cd75f85d-qwzcc from kube-system started at 2023-11-15 03:40:24 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container vpn ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: tuned-rb6qn from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container tuned ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: cluster-storage-operator-6cf6b595c7-m7mfz from openshift-cluster-storage-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Nov 15 06:18:35.730: INFO: csi-snapshot-controller-857d54544d-mwb9h from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: csi-snapshot-controller-operator-56df7685c7-vnvd2 from openshift-cluster-storage-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: csi-snapshot-webhook-586f5c484d-qg5n7 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container webhook ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: console-68d6458867-wlg2q from openshift-console started at 2023-11-15 03:47:25 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container console ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: downloads-7bb648f846-tvqt6 from openshift-console started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container download-server ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: dns-default-njlcg from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container dns ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: node-resolver-6lxcm from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: cluster-image-registry-operator-64994bbb4-6twjh from openshift-image-registry started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: node-ca-897k2 from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container node-ca ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: registry-pvc-permissions-lw2bj from openshift-image-registry started at 2023-11-15 03:46:43 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container pvc-permissions ready: false, restart count 0
    Nov 15 06:18:35.730: INFO: ingress-canary-jqtjt from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: router-default-56777c97d6-szb4s from openshift-ingress started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container router ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: insights-operator-85b688b59d-v47wz from openshift-insights started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container insights-operator ready: true, restart count 1
    Nov 15 06:18:35.730: INFO: openshift-kube-proxy-p8p2b from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: migrator-697dd4cbc5-kk7zn from openshift-kube-storage-version-migrator started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container migrator ready: true, restart count 0
    Nov 15 06:18:35.730: INFO: certified-operators-lqtq6 from openshift-marketplace started at 2023-11-15 03:42:42 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.730: INFO: 	Container registry-server ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: community-operators-jpmzt from openshift-marketplace started at 2023-11-15 05:06:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container registry-server ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: marketplace-operator-55cc9f5b6b-xpbw5 from openshift-marketplace started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container marketplace-operator ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: redhat-marketplace-xp5nc from openshift-marketplace started at 2023-11-15 03:42:42 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container registry-server ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: redhat-operators-bzltn from openshift-marketplace started at 2023-11-15 04:07:58 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container registry-server ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-11-15 03:46:14 +0000 UTC (6 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container alertmanager ready: true, restart count 1
    Nov 15 06:18:35.731: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: node-exporter-67458 from openshift-monitoring started at 2023-11-15 03:44:49 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: prometheus-adapter-5f5bb574db-r4lkm from openshift-monitoring started at 2023-11-15 03:44:53 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-11-15 03:46:20 +0000 UTC (6 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: 	Container prometheus ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: prometheus-operator-admission-webhook-c78bf8f99-gxfh4 from openshift-monitoring started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: telemeter-client-77c946bb95-nsnds from openshift-monitoring started at 2023-11-15 03:46:15 +0000 UTC (3 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: 	Container reload ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: 	Container telemeter-client ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: multus-9tc6x from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: multus-additional-cni-plugins-j2jvc from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: network-metrics-daemon-g5fgz from openshift-multus started at 2023-11-15 03:38:47 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: network-check-target-6pv9x from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container network-check-target-container ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: catalog-operator-798697959c-24lbd from openshift-operator-lifecycle-manager started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container catalog-operator ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: collect-profiles-28333785-ckm22 from openshift-operator-lifecycle-manager started at 2023-11-15 05:45:00 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container collect-profiles ready: false, restart count 0
    Nov 15 06:18:35.731: INFO: collect-profiles-28333800-bgbt2 from openshift-operator-lifecycle-manager started at 2023-11-15 06:00:00 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container collect-profiles ready: false, restart count 0
    Nov 15 06:18:35.731: INFO: collect-profiles-28333815-xxvbf from openshift-operator-lifecycle-manager started at 2023-11-15 06:15:00 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container collect-profiles ready: false, restart count 0
    Nov 15 06:18:35.731: INFO: olm-operator-846bf6bd78-fzxr5 from openshift-operator-lifecycle-manager started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.731: INFO: 	Container olm-operator ready: true, restart count 0
    Nov 15 06:18:35.731: INFO: package-server-manager-5b666bf8fd-5v7rb from openshift-operator-lifecycle-manager started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.732: INFO: 	Container package-server-manager ready: true, restart count 0
    Nov 15 06:18:35.732: INFO: packageserver-758b547fc-f9wwb from openshift-operator-lifecycle-manager started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.732: INFO: 	Container packageserver ready: true, restart count 0
    Nov 15 06:18:35.732: INFO: service-ca-operator-74cb5c9cf5-fqgj5 from openshift-service-ca-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.732: INFO: 	Container service-ca-operator ready: true, restart count 1
    Nov 15 06:18:35.732: INFO: service-ca-78fb97bb77-qcz4b from openshift-service-ca started at 2023-11-15 03:41:45 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.732: INFO: 	Container service-ca-controller ready: true, restart count 0
    Nov 15 06:18:35.732: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-qvnnx from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.732: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 06:18:35.732: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 06:18:35.732: INFO: 
    Logging pods the apiserver thinks is on node 10.72.152.86 before test
    Nov 15 06:18:35.793: INFO: calico-node-cmgfg from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: calico-typha-76d9767bd5-tx2mp from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: ibm-keepalived-watcher-qclxn from kube-system started at 2023-11-15 03:38:36 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: ibm-master-proxy-static-10.72.152.86 from kube-system started at 2023-11-15 03:38:24 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container pause ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: ibmcloud-block-storage-driver-m8gq4 from kube-system started at 2023-11-15 03:38:43 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: cluster-node-tuning-operator-5f77b58f7-t8gf6 from openshift-cluster-node-tuning-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: tuned-5qzzn from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container tuned ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: cluster-samples-operator-65684cb854-h7n8t from openshift-cluster-samples-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: csi-snapshot-controller-857d54544d-qbhf6 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: csi-snapshot-webhook-586f5c484d-vkpg9 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container webhook ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: console-operator-769f9748fb-7tfcc from openshift-console-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container console-operator ready: true, restart count 1
    Nov 15 06:18:35.793: INFO: 	Container conversion-webhook-server ready: true, restart count 3
    Nov 15 06:18:35.793: INFO: downloads-7bb648f846-sr7nb from openshift-console started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container download-server ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: dns-operator-dd9c9c896-9gwtp from openshift-dns-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container dns-operator ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: dns-default-24qc6 from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container dns ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: node-resolver-d54kt from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: node-ca-w7vhg from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container node-ca ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: ingress-canary-ttx6m from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: ingress-operator-6d4d6975f7-qtm2n from openshift-ingress-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container ingress-operator ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: openshift-kube-proxy-x5hq9 from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: kube-storage-version-migrator-operator-5d88b7484-8m2cc from openshift-kube-storage-version-migrator-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Nov 15 06:18:35.793: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-11-15 03:46:47 +0000 UTC (6 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container alertmanager ready: true, restart count 1
    Nov 15 06:18:35.793: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: cluster-monitoring-operator-868f9b56cf-xrfz7 from openshift-monitoring started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: kube-state-metrics-f8d796647-f4rgj from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (3 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: node-exporter-n68zb from openshift-monitoring started at 2023-11-15 03:44:49 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: openshift-state-metrics-69bb697b65-6bcg7 from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (3 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: prometheus-adapter-5f5bb574db-569xs from openshift-monitoring started at 2023-11-15 03:44:53 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-11-15 03:46:38 +0000 UTC (6 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container prometheus ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: prometheus-operator-6fcb4d4c46-t74rt from openshift-monitoring started at 2023-11-15 03:44:41 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container prometheus-operator ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: thanos-querier-56b7586647-8m7mp from openshift-monitoring started at 2023-11-15 03:44:57 +0000 UTC (6 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container oauth-proxy ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container thanos-query ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: multus-additional-cni-plugins-z6xx7 from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: multus-admission-controller-6b76dd856b-6qmnp from openshift-multus started at 2023-11-15 03:44:37 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: multus-cltwv from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: network-metrics-daemon-zbz9n from openshift-multus started at 2023-11-15 03:38:47 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: network-check-target-gnph7 from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container network-check-target-container ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: packageserver-758b547fc-65qc5 from openshift-operator-lifecycle-manager started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container packageserver ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: sonobuoy from sonobuoy started at 2023-11-15 05:52:01 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-m6885 from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-11-15 03:41:49 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.793: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    Nov 15 06:18:35.793: INFO: 
    Logging pods the apiserver thinks is on node 10.72.152.88 before test
    Nov 15 06:18:35.857: INFO: calico-kube-controllers-5dd9d87465-759pm from calico-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: calico-node-lmf6x from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: managed-storage-validation-webhooks-7457bf6687-9ds5w from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: managed-storage-validation-webhooks-7457bf6687-h522x from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: managed-storage-validation-webhooks-7457bf6687-phdgp from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-vbxww from ibm-system started at 2023-11-15 03:48:09 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container ibm-cloud-provider-ip-158-176-89-155 ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: ibm-file-plugin-5fcf7fb495-xmdts from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: ibm-keepalived-watcher-5jx26 from kube-system started at 2023-11-15 03:38:12 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: ibm-master-proxy-static-10.72.152.88 from kube-system started at 2023-11-15 03:38:04 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: 	Container pause ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: ibm-storage-metrics-agent-84fbdc746-5sv68 from kube-system started at 2023-11-15 03:39:51 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: ibm-storage-watcher-7445c988b-8ngdm from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: ibmcloud-block-storage-driver-9t8rj from kube-system started at 2023-11-15 03:38:17 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: ibmcloud-block-storage-plugin-5774687565-gj9xn from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: tuned-q894n from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container tuned ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: console-68d6458867-krfqd from openshift-console started at 2023-11-15 03:47:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container console ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: dns-default-lmngx from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container dns ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: node-resolver-hwp5f from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: image-registry-f74f764d8-w48k4 from openshift-image-registry started at 2023-11-15 03:46:35 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container registry ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: node-ca-wrqvd from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container node-ca ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: ingress-canary-wvl4p from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: router-default-56777c97d6-4bnqq from openshift-ingress started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container router ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: openshift-kube-proxy-tzknx from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: node-exporter-2452k from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: prometheus-operator-admission-webhook-c78bf8f99-x8vcz from openshift-monitoring started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: thanos-querier-56b7586647-qr75b from openshift-monitoring started at 2023-11-15 03:44:58 +0000 UTC (6 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: 	Container oauth-proxy ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: 	Container thanos-query ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: multus-additional-cni-plugins-lcnxr from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: multus-admission-controller-6b76dd856b-6zft2 from openshift-multus started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: multus-vqg9w from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: network-metrics-daemon-9mrwp from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: network-check-source-5f9c5566b6-grf5l from openshift-network-diagnostics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container check-endpoints ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: network-check-target-6rz7p from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container network-check-target-container ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: network-operator-847f47449c-j9glm from openshift-network-operator started at 2023-11-15 03:38:21 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container network-operator ready: true, restart count 1
    Nov 15 06:18:35.857: INFO: metrics-667b585fc4-d5fdk from openshift-roks-metrics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container metrics ready: true, restart count 5
    Nov 15 06:18:35.857: INFO: push-gateway-7f9447c646-5mjcp from openshift-roks-metrics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container push-gateway ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: sonobuoy-e2e-job-4fb3cc1d32834aa9 from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container e2e ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-ddv8z from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 06:18:35.857: INFO: tigera-operator-7dbcb4fb45-rn78j from tigera-operator started at 2023-11-15 03:38:21 +0000 UTC (1 container statuses recorded)
    Nov 15 06:18:35.857: INFO: 	Container tigera-operator ready: true, restart count 2
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 11/15/23 06:18:35.858
    Nov 15 06:18:35.888: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5270" to be "running"
    Nov 15 06:18:35.920: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 32.116647ms
    Nov 15 06:18:37.939: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050424514s
    Nov 15 06:18:39.940: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.051905383s
    Nov 15 06:18:39.940: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 11/15/23 06:18:39.959
    STEP: Trying to apply a random label on the found node. 11/15/23 06:18:40.03
    STEP: verifying the node has the label kubernetes.io/e2e-80123b52-6a84-404f-999e-f227b0f6026d 95 11/15/23 06:18:40.069
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 11/15/23 06:18:40.09
    Nov 15 06:18:40.119: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5270" to be "not pending"
    Nov 15 06:18:40.137: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 17.275046ms
    Nov 15 06:18:42.155: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.035962076s
    Nov 15 06:18:42.155: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.72.152.81 on the node which pod4 resides and expect not scheduled 11/15/23 06:18:42.155
    Nov 15 06:18:42.182: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5270" to be "not pending"
    Nov 15 06:18:42.218: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 35.594776ms
    Nov 15 06:18:44.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055189397s
    Nov 15 06:18:46.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058252463s
    Nov 15 06:18:48.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054849352s
    Nov 15 06:18:50.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.056195848s
    Nov 15 06:18:52.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.055354374s
    Nov 15 06:18:54.236: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.054012056s
    Nov 15 06:18:56.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.055785162s
    Nov 15 06:18:58.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.05527288s
    Nov 15 06:19:00.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.054980799s
    Nov 15 06:19:02.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.057086315s
    Nov 15 06:19:04.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.056175789s
    Nov 15 06:19:06.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.054943607s
    Nov 15 06:19:08.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.055950309s
    Nov 15 06:19:10.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.070919718s
    Nov 15 06:19:12.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.069356598s
    Nov 15 06:19:14.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.056449113s
    Nov 15 06:19:16.241: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.058739443s
    Nov 15 06:19:18.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.056009076s
    Nov 15 06:19:20.234: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.051539772s
    Nov 15 06:19:22.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.055917676s
    Nov 15 06:19:24.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.05510381s
    Nov 15 06:19:26.243: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.061208891s
    Nov 15 06:19:28.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.064553255s
    Nov 15 06:19:30.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.056550672s
    Nov 15 06:19:32.242: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.059598999s
    Nov 15 06:19:34.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.054587893s
    Nov 15 06:19:36.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.055292363s
    Nov 15 06:19:38.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.066032001s
    Nov 15 06:19:40.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.052507903s
    Nov 15 06:19:42.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.05460463s
    Nov 15 06:19:44.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.056140841s
    Nov 15 06:19:46.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.056032654s
    Nov 15 06:19:48.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.055155921s
    Nov 15 06:19:50.236: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.053787264s
    Nov 15 06:19:52.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.054494644s
    Nov 15 06:19:54.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.056028185s
    Nov 15 06:19:56.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.05556156s
    Nov 15 06:19:58.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.055866851s
    Nov 15 06:20:00.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.057717684s
    Nov 15 06:20:02.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.054951332s
    Nov 15 06:20:04.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.055303048s
    Nov 15 06:20:06.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.056535632s
    Nov 15 06:20:08.244: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.061885972s
    Nov 15 06:20:10.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.055771836s
    Nov 15 06:20:12.265: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.082797612s
    Nov 15 06:20:14.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.055316933s
    Nov 15 06:20:16.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.069939403s
    Nov 15 06:20:18.236: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.053821413s
    Nov 15 06:20:20.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.054645002s
    Nov 15 06:20:22.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.070473563s
    Nov 15 06:20:24.243: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.060924692s
    Nov 15 06:20:26.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.055585988s
    Nov 15 06:20:28.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.056562895s
    Nov 15 06:20:30.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.055915237s
    Nov 15 06:20:32.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.054498564s
    Nov 15 06:20:34.264: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.081816963s
    Nov 15 06:20:36.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.055460393s
    Nov 15 06:20:38.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.054898547s
    Nov 15 06:20:40.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.056340452s
    Nov 15 06:20:42.285: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.103052188s
    Nov 15 06:20:44.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.057863059s
    Nov 15 06:20:46.244: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.061491287s
    Nov 15 06:20:48.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.057337535s
    Nov 15 06:20:50.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.053093691s
    Nov 15 06:20:52.241: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.058506024s
    Nov 15 06:20:54.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.055160115s
    Nov 15 06:20:56.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.055386266s
    Nov 15 06:20:58.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.056421933s
    Nov 15 06:21:00.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.056036206s
    Nov 15 06:21:02.247: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.065000508s
    Nov 15 06:21:04.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.055516571s
    Nov 15 06:21:06.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.056379622s
    Nov 15 06:21:08.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.05516021s
    Nov 15 06:21:10.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.056163596s
    Nov 15 06:21:12.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.057641921s
    Nov 15 06:21:14.241: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.058898638s
    Nov 15 06:21:16.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.055818169s
    Nov 15 06:21:18.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.057072012s
    Nov 15 06:21:20.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.055162997s
    Nov 15 06:21:22.244: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.061493214s
    Nov 15 06:21:24.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.057040539s
    Nov 15 06:21:26.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.057986663s
    Nov 15 06:21:28.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.055448309s
    Nov 15 06:21:30.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.055109665s
    Nov 15 06:21:32.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.054358115s
    Nov 15 06:21:34.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.055948015s
    Nov 15 06:21:36.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.057769963s
    Nov 15 06:21:38.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.057907045s
    Nov 15 06:21:40.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.055987129s
    Nov 15 06:21:42.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.069991486s
    Nov 15 06:21:44.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.056963886s
    Nov 15 06:21:46.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.056882652s
    Nov 15 06:21:48.244: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.06184054s
    Nov 15 06:21:50.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.055105466s
    Nov 15 06:21:52.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.056254626s
    Nov 15 06:21:54.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.057691792s
    Nov 15 06:21:56.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.055834046s
    Nov 15 06:21:58.242: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.059925488s
    Nov 15 06:22:00.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.058223907s
    Nov 15 06:22:02.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.055396581s
    Nov 15 06:22:04.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.057637978s
    Nov 15 06:22:06.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.057699162s
    Nov 15 06:22:08.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.054354692s
    Nov 15 06:22:10.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.055643316s
    Nov 15 06:22:12.241: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.058670434s
    Nov 15 06:22:14.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.055768715s
    Nov 15 06:22:16.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.054603458s
    Nov 15 06:22:18.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.05528703s
    Nov 15 06:22:20.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.057883387s
    Nov 15 06:22:22.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.056086834s
    Nov 15 06:22:24.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.055462354s
    Nov 15 06:22:26.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.057825116s
    Nov 15 06:22:28.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.056834887s
    Nov 15 06:22:30.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.055334333s
    Nov 15 06:22:32.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.055637052s
    Nov 15 06:22:34.241: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.058366775s
    Nov 15 06:22:36.245: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.06291157s
    Nov 15 06:22:38.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.056396963s
    Nov 15 06:22:40.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.057253976s
    Nov 15 06:22:42.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.056282235s
    Nov 15 06:22:44.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.05719596s
    Nov 15 06:22:46.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.056422528s
    Nov 15 06:22:48.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.055192794s
    Nov 15 06:22:50.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.057203344s
    Nov 15 06:22:52.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.054970911s
    Nov 15 06:22:54.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.055756653s
    Nov 15 06:22:56.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.054859835s
    Nov 15 06:22:58.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.054364686s
    Nov 15 06:23:00.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.057213369s
    Nov 15 06:23:02.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.053264622s
    Nov 15 06:23:04.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.055461978s
    Nov 15 06:23:06.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.055430881s
    Nov 15 06:23:08.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.054674895s
    Nov 15 06:23:10.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.056804796s
    Nov 15 06:23:12.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.072379827s
    Nov 15 06:23:14.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.055204036s
    Nov 15 06:23:16.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.056072766s
    Nov 15 06:23:18.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.055832931s
    Nov 15 06:23:20.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.056243028s
    Nov 15 06:23:22.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.054269307s
    Nov 15 06:23:24.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.089232485s
    Nov 15 06:23:26.239: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.056843058s
    Nov 15 06:23:28.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.05429456s
    Nov 15 06:23:30.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.055094297s
    Nov 15 06:23:32.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.055840219s
    Nov 15 06:23:34.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.055832625s
    Nov 15 06:23:36.236: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.054218366s
    Nov 15 06:23:38.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.054681343s
    Nov 15 06:23:40.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.054892195s
    Nov 15 06:23:42.277: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.094755564s
    Nov 15 06:23:42.311: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.129152622s
    STEP: removing the label kubernetes.io/e2e-80123b52-6a84-404f-999e-f227b0f6026d off the node 10.72.152.81 11/15/23 06:23:42.311
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-80123b52-6a84-404f-999e-f227b0f6026d 11/15/23 06:23:42.438
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:23:42.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5270" for this suite. 11/15/23 06:23:42.525
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:23:42.586
Nov 15 06:23:42.586: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename configmap 11/15/23 06:23:42.587
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:23:42.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:23:42.752
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-38bf6634-e1df-4ec5-bfe4-de239a63ad35 11/15/23 06:23:42.821
STEP: Creating a pod to test consume configMaps 11/15/23 06:23:42.842
Nov 15 06:23:42.897: INFO: Waiting up to 5m0s for pod "pod-configmaps-989bec7e-8eba-4f91-b7ea-c6af747d6829" in namespace "configmap-8919" to be "Succeeded or Failed"
Nov 15 06:23:42.915: INFO: Pod "pod-configmaps-989bec7e-8eba-4f91-b7ea-c6af747d6829": Phase="Pending", Reason="", readiness=false. Elapsed: 18.482497ms
Nov 15 06:23:44.938: INFO: Pod "pod-configmaps-989bec7e-8eba-4f91-b7ea-c6af747d6829": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041060778s
Nov 15 06:23:46.942: INFO: Pod "pod-configmaps-989bec7e-8eba-4f91-b7ea-c6af747d6829": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044707503s
Nov 15 06:23:48.934: INFO: Pod "pod-configmaps-989bec7e-8eba-4f91-b7ea-c6af747d6829": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037276843s
STEP: Saw pod success 11/15/23 06:23:48.934
Nov 15 06:23:48.934: INFO: Pod "pod-configmaps-989bec7e-8eba-4f91-b7ea-c6af747d6829" satisfied condition "Succeeded or Failed"
Nov 15 06:23:48.952: INFO: Trying to get logs from node 10.72.152.86 pod pod-configmaps-989bec7e-8eba-4f91-b7ea-c6af747d6829 container agnhost-container: <nil>
STEP: delete the pod 11/15/23 06:23:49.044
Nov 15 06:23:49.101: INFO: Waiting for pod pod-configmaps-989bec7e-8eba-4f91-b7ea-c6af747d6829 to disappear
Nov 15 06:23:49.119: INFO: Pod pod-configmaps-989bec7e-8eba-4f91-b7ea-c6af747d6829 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:23:49.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8919" for this suite. 11/15/23 06:23:49.15
------------------------------
â€¢ [SLOW TEST] [6.591 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:23:42.586
    Nov 15 06:23:42.586: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename configmap 11/15/23 06:23:42.587
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:23:42.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:23:42.752
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-38bf6634-e1df-4ec5-bfe4-de239a63ad35 11/15/23 06:23:42.821
    STEP: Creating a pod to test consume configMaps 11/15/23 06:23:42.842
    Nov 15 06:23:42.897: INFO: Waiting up to 5m0s for pod "pod-configmaps-989bec7e-8eba-4f91-b7ea-c6af747d6829" in namespace "configmap-8919" to be "Succeeded or Failed"
    Nov 15 06:23:42.915: INFO: Pod "pod-configmaps-989bec7e-8eba-4f91-b7ea-c6af747d6829": Phase="Pending", Reason="", readiness=false. Elapsed: 18.482497ms
    Nov 15 06:23:44.938: INFO: Pod "pod-configmaps-989bec7e-8eba-4f91-b7ea-c6af747d6829": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041060778s
    Nov 15 06:23:46.942: INFO: Pod "pod-configmaps-989bec7e-8eba-4f91-b7ea-c6af747d6829": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044707503s
    Nov 15 06:23:48.934: INFO: Pod "pod-configmaps-989bec7e-8eba-4f91-b7ea-c6af747d6829": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037276843s
    STEP: Saw pod success 11/15/23 06:23:48.934
    Nov 15 06:23:48.934: INFO: Pod "pod-configmaps-989bec7e-8eba-4f91-b7ea-c6af747d6829" satisfied condition "Succeeded or Failed"
    Nov 15 06:23:48.952: INFO: Trying to get logs from node 10.72.152.86 pod pod-configmaps-989bec7e-8eba-4f91-b7ea-c6af747d6829 container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 06:23:49.044
    Nov 15 06:23:49.101: INFO: Waiting for pod pod-configmaps-989bec7e-8eba-4f91-b7ea-c6af747d6829 to disappear
    Nov 15 06:23:49.119: INFO: Pod pod-configmaps-989bec7e-8eba-4f91-b7ea-c6af747d6829 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:23:49.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8919" for this suite. 11/15/23 06:23:49.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:23:49.178
Nov 15 06:23:49.178: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename webhook 11/15/23 06:23:49.179
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:23:49.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:23:49.256
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 06:23:49.324
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:23:49.624
STEP: Deploying the webhook pod 11/15/23 06:23:49.675
STEP: Wait for the deployment to be ready 11/15/23 06:23:49.707
Nov 15 06:23:49.741: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 06:23:51.784: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 6, 23, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 23, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 23, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 23, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 06:23:53.797
STEP: Verifying the service has paired with the endpoint 11/15/23 06:23:53.842
Nov 15 06:23:54.843: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 11/15/23 06:23:54.864
STEP: create a pod that should be denied by the webhook 11/15/23 06:23:54.93
STEP: create a pod that causes the webhook to hang 11/15/23 06:23:54.977
STEP: create a configmap that should be denied by the webhook 11/15/23 06:24:05.011
STEP: create a configmap that should be admitted by the webhook 11/15/23 06:24:05.078
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 11/15/23 06:24:05.121
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 11/15/23 06:24:05.164
STEP: create a namespace that bypass the webhook 11/15/23 06:24:05.193
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 11/15/23 06:24:05.222
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:24:05.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5258" for this suite. 11/15/23 06:24:05.502
STEP: Destroying namespace "webhook-5258-markers" for this suite. 11/15/23 06:24:05.53
------------------------------
â€¢ [SLOW TEST] [16.389 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:23:49.178
    Nov 15 06:23:49.178: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename webhook 11/15/23 06:23:49.179
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:23:49.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:23:49.256
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 06:23:49.324
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:23:49.624
    STEP: Deploying the webhook pod 11/15/23 06:23:49.675
    STEP: Wait for the deployment to be ready 11/15/23 06:23:49.707
    Nov 15 06:23:49.741: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 06:23:51.784: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 6, 23, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 23, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 23, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 23, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 06:23:53.797
    STEP: Verifying the service has paired with the endpoint 11/15/23 06:23:53.842
    Nov 15 06:23:54.843: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 11/15/23 06:23:54.864
    STEP: create a pod that should be denied by the webhook 11/15/23 06:23:54.93
    STEP: create a pod that causes the webhook to hang 11/15/23 06:23:54.977
    STEP: create a configmap that should be denied by the webhook 11/15/23 06:24:05.011
    STEP: create a configmap that should be admitted by the webhook 11/15/23 06:24:05.078
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 11/15/23 06:24:05.121
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 11/15/23 06:24:05.164
    STEP: create a namespace that bypass the webhook 11/15/23 06:24:05.193
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 11/15/23 06:24:05.222
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:24:05.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5258" for this suite. 11/15/23 06:24:05.502
    STEP: Destroying namespace "webhook-5258-markers" for this suite. 11/15/23 06:24:05.53
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:24:05.568
Nov 15 06:24:05.568: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename limitrange 11/15/23 06:24:05.569
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:24:05.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:24:05.641
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-sjdk8" in namespace "limitrange-643" 11/15/23 06:24:05.659
STEP: Creating another limitRange in another namespace 11/15/23 06:24:05.675
Nov 15 06:24:05.726: INFO: Namespace "e2e-limitrange-sjdk8-9374" created
Nov 15 06:24:05.726: INFO: Creating LimitRange "e2e-limitrange-sjdk8" in namespace "e2e-limitrange-sjdk8-9374"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-sjdk8" 11/15/23 06:24:05.758
Nov 15 06:24:05.775: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-sjdk8" in "limitrange-643" namespace 11/15/23 06:24:05.775
Nov 15 06:24:05.795: INFO: LimitRange "e2e-limitrange-sjdk8" has been patched
STEP: Delete LimitRange "e2e-limitrange-sjdk8" by Collection with labelSelector: "e2e-limitrange-sjdk8=patched" 11/15/23 06:24:05.795
STEP: Confirm that the limitRange "e2e-limitrange-sjdk8" has been deleted 11/15/23 06:24:05.819
Nov 15 06:24:05.819: INFO: Requesting list of LimitRange to confirm quantity
Nov 15 06:24:05.830: INFO: Found 0 LimitRange with label "e2e-limitrange-sjdk8=patched"
Nov 15 06:24:05.830: INFO: LimitRange "e2e-limitrange-sjdk8" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-sjdk8" 11/15/23 06:24:05.83
Nov 15 06:24:05.841: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Nov 15 06:24:05.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-643" for this suite. 11/15/23 06:24:05.868
STEP: Destroying namespace "e2e-limitrange-sjdk8-9374" for this suite. 11/15/23 06:24:05.896
------------------------------
â€¢ [0.358 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:24:05.568
    Nov 15 06:24:05.568: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename limitrange 11/15/23 06:24:05.569
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:24:05.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:24:05.641
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-sjdk8" in namespace "limitrange-643" 11/15/23 06:24:05.659
    STEP: Creating another limitRange in another namespace 11/15/23 06:24:05.675
    Nov 15 06:24:05.726: INFO: Namespace "e2e-limitrange-sjdk8-9374" created
    Nov 15 06:24:05.726: INFO: Creating LimitRange "e2e-limitrange-sjdk8" in namespace "e2e-limitrange-sjdk8-9374"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-sjdk8" 11/15/23 06:24:05.758
    Nov 15 06:24:05.775: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-sjdk8" in "limitrange-643" namespace 11/15/23 06:24:05.775
    Nov 15 06:24:05.795: INFO: LimitRange "e2e-limitrange-sjdk8" has been patched
    STEP: Delete LimitRange "e2e-limitrange-sjdk8" by Collection with labelSelector: "e2e-limitrange-sjdk8=patched" 11/15/23 06:24:05.795
    STEP: Confirm that the limitRange "e2e-limitrange-sjdk8" has been deleted 11/15/23 06:24:05.819
    Nov 15 06:24:05.819: INFO: Requesting list of LimitRange to confirm quantity
    Nov 15 06:24:05.830: INFO: Found 0 LimitRange with label "e2e-limitrange-sjdk8=patched"
    Nov 15 06:24:05.830: INFO: LimitRange "e2e-limitrange-sjdk8" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-sjdk8" 11/15/23 06:24:05.83
    Nov 15 06:24:05.841: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:24:05.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-643" for this suite. 11/15/23 06:24:05.868
    STEP: Destroying namespace "e2e-limitrange-sjdk8-9374" for this suite. 11/15/23 06:24:05.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:24:05.926
Nov 15 06:24:05.926: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename svcaccounts 11/15/23 06:24:05.927
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:24:05.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:24:05.998
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  11/15/23 06:24:06.015
Nov 15 06:24:06.052: INFO: Waiting up to 5m0s for pod "test-pod-8d54674e-9083-43b3-842f-42feb5059709" in namespace "svcaccounts-5896" to be "Succeeded or Failed"
Nov 15 06:24:06.068: INFO: Pod "test-pod-8d54674e-9083-43b3-842f-42feb5059709": Phase="Pending", Reason="", readiness=false. Elapsed: 15.04026ms
Nov 15 06:24:08.088: INFO: Pod "test-pod-8d54674e-9083-43b3-842f-42feb5059709": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035768165s
Nov 15 06:24:10.088: INFO: Pod "test-pod-8d54674e-9083-43b3-842f-42feb5059709": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035792124s
STEP: Saw pod success 11/15/23 06:24:10.088
Nov 15 06:24:10.089: INFO: Pod "test-pod-8d54674e-9083-43b3-842f-42feb5059709" satisfied condition "Succeeded or Failed"
Nov 15 06:24:10.107: INFO: Trying to get logs from node 10.72.152.86 pod test-pod-8d54674e-9083-43b3-842f-42feb5059709 container agnhost-container: <nil>
STEP: delete the pod 11/15/23 06:24:10.148
Nov 15 06:24:10.208: INFO: Waiting for pod test-pod-8d54674e-9083-43b3-842f-42feb5059709 to disappear
Nov 15 06:24:10.235: INFO: Pod test-pod-8d54674e-9083-43b3-842f-42feb5059709 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 15 06:24:10.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5896" for this suite. 11/15/23 06:24:10.264
------------------------------
â€¢ [4.363 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:24:05.926
    Nov 15 06:24:05.926: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename svcaccounts 11/15/23 06:24:05.927
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:24:05.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:24:05.998
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  11/15/23 06:24:06.015
    Nov 15 06:24:06.052: INFO: Waiting up to 5m0s for pod "test-pod-8d54674e-9083-43b3-842f-42feb5059709" in namespace "svcaccounts-5896" to be "Succeeded or Failed"
    Nov 15 06:24:06.068: INFO: Pod "test-pod-8d54674e-9083-43b3-842f-42feb5059709": Phase="Pending", Reason="", readiness=false. Elapsed: 15.04026ms
    Nov 15 06:24:08.088: INFO: Pod "test-pod-8d54674e-9083-43b3-842f-42feb5059709": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035768165s
    Nov 15 06:24:10.088: INFO: Pod "test-pod-8d54674e-9083-43b3-842f-42feb5059709": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035792124s
    STEP: Saw pod success 11/15/23 06:24:10.088
    Nov 15 06:24:10.089: INFO: Pod "test-pod-8d54674e-9083-43b3-842f-42feb5059709" satisfied condition "Succeeded or Failed"
    Nov 15 06:24:10.107: INFO: Trying to get logs from node 10.72.152.86 pod test-pod-8d54674e-9083-43b3-842f-42feb5059709 container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 06:24:10.148
    Nov 15 06:24:10.208: INFO: Waiting for pod test-pod-8d54674e-9083-43b3-842f-42feb5059709 to disappear
    Nov 15 06:24:10.235: INFO: Pod test-pod-8d54674e-9083-43b3-842f-42feb5059709 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:24:10.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5896" for this suite. 11/15/23 06:24:10.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:24:10.291
Nov 15 06:24:10.291: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename pods 11/15/23 06:24:10.292
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:24:10.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:24:10.366
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Nov 15 06:24:10.378: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: creating the pod 11/15/23 06:24:10.378
STEP: submitting the pod to kubernetes 11/15/23 06:24:10.378
Nov 15 06:24:10.417: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-431d1f58-4417-4412-8a8a-13e57f140b59" in namespace "pods-9119" to be "running and ready"
Nov 15 06:24:10.438: INFO: Pod "pod-exec-websocket-431d1f58-4417-4412-8a8a-13e57f140b59": Phase="Pending", Reason="", readiness=false. Elapsed: 20.89024ms
Nov 15 06:24:10.438: INFO: The phase of Pod pod-exec-websocket-431d1f58-4417-4412-8a8a-13e57f140b59 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:24:12.459: INFO: Pod "pod-exec-websocket-431d1f58-4417-4412-8a8a-13e57f140b59": Phase="Running", Reason="", readiness=true. Elapsed: 2.041471409s
Nov 15 06:24:12.459: INFO: The phase of Pod pod-exec-websocket-431d1f58-4417-4412-8a8a-13e57f140b59 is Running (Ready = true)
Nov 15 06:24:12.459: INFO: Pod "pod-exec-websocket-431d1f58-4417-4412-8a8a-13e57f140b59" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 06:24:12.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9119" for this suite. 11/15/23 06:24:12.702
------------------------------
â€¢ [2.461 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:24:10.291
    Nov 15 06:24:10.291: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename pods 11/15/23 06:24:10.292
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:24:10.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:24:10.366
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Nov 15 06:24:10.378: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: creating the pod 11/15/23 06:24:10.378
    STEP: submitting the pod to kubernetes 11/15/23 06:24:10.378
    Nov 15 06:24:10.417: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-431d1f58-4417-4412-8a8a-13e57f140b59" in namespace "pods-9119" to be "running and ready"
    Nov 15 06:24:10.438: INFO: Pod "pod-exec-websocket-431d1f58-4417-4412-8a8a-13e57f140b59": Phase="Pending", Reason="", readiness=false. Elapsed: 20.89024ms
    Nov 15 06:24:10.438: INFO: The phase of Pod pod-exec-websocket-431d1f58-4417-4412-8a8a-13e57f140b59 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:24:12.459: INFO: Pod "pod-exec-websocket-431d1f58-4417-4412-8a8a-13e57f140b59": Phase="Running", Reason="", readiness=true. Elapsed: 2.041471409s
    Nov 15 06:24:12.459: INFO: The phase of Pod pod-exec-websocket-431d1f58-4417-4412-8a8a-13e57f140b59 is Running (Ready = true)
    Nov 15 06:24:12.459: INFO: Pod "pod-exec-websocket-431d1f58-4417-4412-8a8a-13e57f140b59" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:24:12.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9119" for this suite. 11/15/23 06:24:12.702
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:24:12.752
Nov 15 06:24:12.753: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename webhook 11/15/23 06:24:12.754
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:24:12.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:24:12.823
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 06:24:12.934
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:24:13.485
STEP: Deploying the webhook pod 11/15/23 06:24:13.52
STEP: Wait for the deployment to be ready 11/15/23 06:24:13.555
Nov 15 06:24:13.590: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/15/23 06:24:15.633
STEP: Verifying the service has paired with the endpoint 11/15/23 06:24:15.667
Nov 15 06:24:16.668: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Nov 15 06:24:16.686: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2950-crds.webhook.example.com via the AdmissionRegistration API 11/15/23 06:24:17.23
STEP: Creating a custom resource that should be mutated by the webhook 11/15/23 06:24:17.293
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:24:20.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8868" for this suite. 11/15/23 06:24:20.356
STEP: Destroying namespace "webhook-8868-markers" for this suite. 11/15/23 06:24:20.383
------------------------------
â€¢ [SLOW TEST] [7.658 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:24:12.752
    Nov 15 06:24:12.753: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename webhook 11/15/23 06:24:12.754
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:24:12.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:24:12.823
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 06:24:12.934
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:24:13.485
    STEP: Deploying the webhook pod 11/15/23 06:24:13.52
    STEP: Wait for the deployment to be ready 11/15/23 06:24:13.555
    Nov 15 06:24:13.590: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/15/23 06:24:15.633
    STEP: Verifying the service has paired with the endpoint 11/15/23 06:24:15.667
    Nov 15 06:24:16.668: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Nov 15 06:24:16.686: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2950-crds.webhook.example.com via the AdmissionRegistration API 11/15/23 06:24:17.23
    STEP: Creating a custom resource that should be mutated by the webhook 11/15/23 06:24:17.293
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:24:20.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8868" for this suite. 11/15/23 06:24:20.356
    STEP: Destroying namespace "webhook-8868-markers" for this suite. 11/15/23 06:24:20.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:24:20.414
Nov 15 06:24:20.414: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename var-expansion 11/15/23 06:24:20.415
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:24:20.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:24:20.489
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Nov 15 06:24:20.540: INFO: Waiting up to 2m0s for pod "var-expansion-9fe34303-78cc-4733-a0a3-d2be27659e24" in namespace "var-expansion-2697" to be "container 0 failed with reason CreateContainerConfigError"
Nov 15 06:24:20.557: INFO: Pod "var-expansion-9fe34303-78cc-4733-a0a3-d2be27659e24": Phase="Pending", Reason="", readiness=false. Elapsed: 17.609912ms
Nov 15 06:24:22.576: INFO: Pod "var-expansion-9fe34303-78cc-4733-a0a3-d2be27659e24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036809803s
Nov 15 06:24:22.576: INFO: Pod "var-expansion-9fe34303-78cc-4733-a0a3-d2be27659e24" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Nov 15 06:24:22.577: INFO: Deleting pod "var-expansion-9fe34303-78cc-4733-a0a3-d2be27659e24" in namespace "var-expansion-2697"
Nov 15 06:24:22.614: INFO: Wait up to 5m0s for pod "var-expansion-9fe34303-78cc-4733-a0a3-d2be27659e24" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 15 06:24:26.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2697" for this suite. 11/15/23 06:24:26.693
------------------------------
â€¢ [SLOW TEST] [6.303 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:24:20.414
    Nov 15 06:24:20.414: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename var-expansion 11/15/23 06:24:20.415
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:24:20.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:24:20.489
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Nov 15 06:24:20.540: INFO: Waiting up to 2m0s for pod "var-expansion-9fe34303-78cc-4733-a0a3-d2be27659e24" in namespace "var-expansion-2697" to be "container 0 failed with reason CreateContainerConfigError"
    Nov 15 06:24:20.557: INFO: Pod "var-expansion-9fe34303-78cc-4733-a0a3-d2be27659e24": Phase="Pending", Reason="", readiness=false. Elapsed: 17.609912ms
    Nov 15 06:24:22.576: INFO: Pod "var-expansion-9fe34303-78cc-4733-a0a3-d2be27659e24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036809803s
    Nov 15 06:24:22.576: INFO: Pod "var-expansion-9fe34303-78cc-4733-a0a3-d2be27659e24" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Nov 15 06:24:22.577: INFO: Deleting pod "var-expansion-9fe34303-78cc-4733-a0a3-d2be27659e24" in namespace "var-expansion-2697"
    Nov 15 06:24:22.614: INFO: Wait up to 5m0s for pod "var-expansion-9fe34303-78cc-4733-a0a3-d2be27659e24" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:24:26.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2697" for this suite. 11/15/23 06:24:26.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:24:26.718
Nov 15 06:24:26.718: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename services 11/15/23 06:24:26.719
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:24:26.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:24:26.783
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-5682 11/15/23 06:24:26.813
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5682 to expose endpoints map[] 11/15/23 06:24:26.85
Nov 15 06:24:26.897: INFO: successfully validated that service endpoint-test2 in namespace services-5682 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5682 11/15/23 06:24:26.897
Nov 15 06:24:26.924: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5682" to be "running and ready"
Nov 15 06:24:26.945: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.841174ms
Nov 15 06:24:26.945: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:24:28.965: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040233917s
Nov 15 06:24:28.965: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:24:30.965: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.040282251s
Nov 15 06:24:30.965: INFO: The phase of Pod pod1 is Running (Ready = true)
Nov 15 06:24:30.965: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5682 to expose endpoints map[pod1:[80]] 11/15/23 06:24:30.982
Nov 15 06:24:31.037: INFO: successfully validated that service endpoint-test2 in namespace services-5682 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 11/15/23 06:24:31.037
Nov 15 06:24:31.038: INFO: Creating new exec pod
Nov 15 06:24:31.066: INFO: Waiting up to 5m0s for pod "execpodj6tlw" in namespace "services-5682" to be "running"
Nov 15 06:24:31.084: INFO: Pod "execpodj6tlw": Phase="Pending", Reason="", readiness=false. Elapsed: 17.458215ms
Nov 15 06:24:33.103: INFO: Pod "execpodj6tlw": Phase="Running", Reason="", readiness=true. Elapsed: 2.037289022s
Nov 15 06:24:33.103: INFO: Pod "execpodj6tlw" satisfied condition "running"
Nov 15 06:24:34.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-5682 exec execpodj6tlw -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Nov 15 06:24:34.423: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 15 06:24:34.423: INFO: stdout: ""
Nov 15 06:24:34.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-5682 exec execpodj6tlw -- /bin/sh -x -c nc -v -z -w 2 172.21.107.168 80'
Nov 15 06:24:34.734: INFO: stderr: "+ nc -v -z -w 2 172.21.107.168 80\nConnection to 172.21.107.168 80 port [tcp/http] succeeded!\n"
Nov 15 06:24:34.734: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-5682 11/15/23 06:24:34.734
Nov 15 06:24:34.761: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5682" to be "running and ready"
Nov 15 06:24:34.780: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.499824ms
Nov 15 06:24:34.780: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:24:36.805: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.044361806s
Nov 15 06:24:36.805: INFO: The phase of Pod pod2 is Running (Ready = true)
Nov 15 06:24:36.805: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5682 to expose endpoints map[pod1:[80] pod2:[80]] 11/15/23 06:24:36.825
Nov 15 06:24:36.905: INFO: successfully validated that service endpoint-test2 in namespace services-5682 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 11/15/23 06:24:36.905
Nov 15 06:24:37.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-5682 exec execpodj6tlw -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Nov 15 06:24:38.222: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 15 06:24:38.222: INFO: stdout: ""
Nov 15 06:24:38.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-5682 exec execpodj6tlw -- /bin/sh -x -c nc -v -z -w 2 172.21.107.168 80'
Nov 15 06:24:38.599: INFO: stderr: "+ nc -v -z -w 2 172.21.107.168 80\nConnection to 172.21.107.168 80 port [tcp/http] succeeded!\n"
Nov 15 06:24:38.599: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-5682 11/15/23 06:24:38.599
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5682 to expose endpoints map[pod2:[80]] 11/15/23 06:24:38.669
Nov 15 06:24:38.724: INFO: successfully validated that service endpoint-test2 in namespace services-5682 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 11/15/23 06:24:38.724
Nov 15 06:24:39.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-5682 exec execpodj6tlw -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Nov 15 06:24:40.026: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 15 06:24:40.026: INFO: stdout: ""
Nov 15 06:24:40.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-5682 exec execpodj6tlw -- /bin/sh -x -c nc -v -z -w 2 172.21.107.168 80'
Nov 15 06:24:40.309: INFO: stderr: "+ nc -v -z -w 2 172.21.107.168 80\nConnection to 172.21.107.168 80 port [tcp/http] succeeded!\n"
Nov 15 06:24:40.309: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-5682 11/15/23 06:24:40.309
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5682 to expose endpoints map[] 11/15/23 06:24:40.369
Nov 15 06:24:40.408: INFO: successfully validated that service endpoint-test2 in namespace services-5682 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 06:24:40.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5682" for this suite. 11/15/23 06:24:40.515
------------------------------
â€¢ [SLOW TEST] [13.824 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:24:26.718
    Nov 15 06:24:26.718: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename services 11/15/23 06:24:26.719
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:24:26.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:24:26.783
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-5682 11/15/23 06:24:26.813
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5682 to expose endpoints map[] 11/15/23 06:24:26.85
    Nov 15 06:24:26.897: INFO: successfully validated that service endpoint-test2 in namespace services-5682 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5682 11/15/23 06:24:26.897
    Nov 15 06:24:26.924: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5682" to be "running and ready"
    Nov 15 06:24:26.945: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.841174ms
    Nov 15 06:24:26.945: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:24:28.965: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040233917s
    Nov 15 06:24:28.965: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:24:30.965: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.040282251s
    Nov 15 06:24:30.965: INFO: The phase of Pod pod1 is Running (Ready = true)
    Nov 15 06:24:30.965: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5682 to expose endpoints map[pod1:[80]] 11/15/23 06:24:30.982
    Nov 15 06:24:31.037: INFO: successfully validated that service endpoint-test2 in namespace services-5682 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 11/15/23 06:24:31.037
    Nov 15 06:24:31.038: INFO: Creating new exec pod
    Nov 15 06:24:31.066: INFO: Waiting up to 5m0s for pod "execpodj6tlw" in namespace "services-5682" to be "running"
    Nov 15 06:24:31.084: INFO: Pod "execpodj6tlw": Phase="Pending", Reason="", readiness=false. Elapsed: 17.458215ms
    Nov 15 06:24:33.103: INFO: Pod "execpodj6tlw": Phase="Running", Reason="", readiness=true. Elapsed: 2.037289022s
    Nov 15 06:24:33.103: INFO: Pod "execpodj6tlw" satisfied condition "running"
    Nov 15 06:24:34.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-5682 exec execpodj6tlw -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Nov 15 06:24:34.423: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Nov 15 06:24:34.423: INFO: stdout: ""
    Nov 15 06:24:34.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-5682 exec execpodj6tlw -- /bin/sh -x -c nc -v -z -w 2 172.21.107.168 80'
    Nov 15 06:24:34.734: INFO: stderr: "+ nc -v -z -w 2 172.21.107.168 80\nConnection to 172.21.107.168 80 port [tcp/http] succeeded!\n"
    Nov 15 06:24:34.734: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-5682 11/15/23 06:24:34.734
    Nov 15 06:24:34.761: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5682" to be "running and ready"
    Nov 15 06:24:34.780: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.499824ms
    Nov 15 06:24:34.780: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:24:36.805: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.044361806s
    Nov 15 06:24:36.805: INFO: The phase of Pod pod2 is Running (Ready = true)
    Nov 15 06:24:36.805: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5682 to expose endpoints map[pod1:[80] pod2:[80]] 11/15/23 06:24:36.825
    Nov 15 06:24:36.905: INFO: successfully validated that service endpoint-test2 in namespace services-5682 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 11/15/23 06:24:36.905
    Nov 15 06:24:37.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-5682 exec execpodj6tlw -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Nov 15 06:24:38.222: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Nov 15 06:24:38.222: INFO: stdout: ""
    Nov 15 06:24:38.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-5682 exec execpodj6tlw -- /bin/sh -x -c nc -v -z -w 2 172.21.107.168 80'
    Nov 15 06:24:38.599: INFO: stderr: "+ nc -v -z -w 2 172.21.107.168 80\nConnection to 172.21.107.168 80 port [tcp/http] succeeded!\n"
    Nov 15 06:24:38.599: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-5682 11/15/23 06:24:38.599
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5682 to expose endpoints map[pod2:[80]] 11/15/23 06:24:38.669
    Nov 15 06:24:38.724: INFO: successfully validated that service endpoint-test2 in namespace services-5682 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 11/15/23 06:24:38.724
    Nov 15 06:24:39.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-5682 exec execpodj6tlw -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Nov 15 06:24:40.026: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Nov 15 06:24:40.026: INFO: stdout: ""
    Nov 15 06:24:40.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-5682 exec execpodj6tlw -- /bin/sh -x -c nc -v -z -w 2 172.21.107.168 80'
    Nov 15 06:24:40.309: INFO: stderr: "+ nc -v -z -w 2 172.21.107.168 80\nConnection to 172.21.107.168 80 port [tcp/http] succeeded!\n"
    Nov 15 06:24:40.309: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-5682 11/15/23 06:24:40.309
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5682 to expose endpoints map[] 11/15/23 06:24:40.369
    Nov 15 06:24:40.408: INFO: successfully validated that service endpoint-test2 in namespace services-5682 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:24:40.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5682" for this suite. 11/15/23 06:24:40.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:24:40.544
Nov 15 06:24:40.544: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename cronjob 11/15/23 06:24:40.545
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:24:40.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:24:40.627
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 11/15/23 06:24:40.64
W1115 06:24:40.670989      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a job is scheduled 11/15/23 06:24:40.671
STEP: Ensuring exactly one is scheduled 11/15/23 06:25:00.687
STEP: Ensuring exactly one running job exists by listing jobs explicitly 11/15/23 06:25:00.707
STEP: Ensuring the job is replaced with a new one 11/15/23 06:25:00.723
STEP: Removing cronjob 11/15/23 06:26:00.741
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Nov 15 06:26:00.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1120" for this suite. 11/15/23 06:26:00.81
------------------------------
â€¢ [SLOW TEST] [80.291 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:24:40.544
    Nov 15 06:24:40.544: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename cronjob 11/15/23 06:24:40.545
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:24:40.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:24:40.627
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 11/15/23 06:24:40.64
    W1115 06:24:40.670989      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring a job is scheduled 11/15/23 06:24:40.671
    STEP: Ensuring exactly one is scheduled 11/15/23 06:25:00.687
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 11/15/23 06:25:00.707
    STEP: Ensuring the job is replaced with a new one 11/15/23 06:25:00.723
    STEP: Removing cronjob 11/15/23 06:26:00.741
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:26:00.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1120" for this suite. 11/15/23 06:26:00.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:26:00.838
Nov 15 06:26:00.838: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename replication-controller 11/15/23 06:26:00.839
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:00.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:00.911
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 11/15/23 06:26:00.957
STEP: waiting for RC to be added 11/15/23 06:26:00.977
STEP: waiting for available Replicas 11/15/23 06:26:00.977
STEP: patching ReplicationController 11/15/23 06:26:03.647
STEP: waiting for RC to be modified 11/15/23 06:26:03.698
STEP: patching ReplicationController status 11/15/23 06:26:03.698
STEP: waiting for RC to be modified 11/15/23 06:26:03.753
STEP: waiting for available Replicas 11/15/23 06:26:03.753
STEP: fetching ReplicationController status 11/15/23 06:26:03.755
STEP: patching ReplicationController scale 11/15/23 06:26:03.772
STEP: waiting for RC to be modified 11/15/23 06:26:03.794
STEP: waiting for ReplicationController's scale to be the max amount 11/15/23 06:26:03.794
STEP: fetching ReplicationController; ensuring that it's patched 11/15/23 06:26:05.904
STEP: updating ReplicationController status 11/15/23 06:26:05.921
STEP: waiting for RC to be modified 11/15/23 06:26:05.943
STEP: listing all ReplicationControllers 11/15/23 06:26:05.943
STEP: checking that ReplicationController has expected values 11/15/23 06:26:05.959
STEP: deleting ReplicationControllers by collection 11/15/23 06:26:05.959
STEP: waiting for ReplicationController to have a DELETED watchEvent 11/15/23 06:26:05.992
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Nov 15 06:26:06.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9055" for this suite. 11/15/23 06:26:06.167
------------------------------
â€¢ [SLOW TEST] [5.356 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:26:00.838
    Nov 15 06:26:00.838: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename replication-controller 11/15/23 06:26:00.839
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:00.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:00.911
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 11/15/23 06:26:00.957
    STEP: waiting for RC to be added 11/15/23 06:26:00.977
    STEP: waiting for available Replicas 11/15/23 06:26:00.977
    STEP: patching ReplicationController 11/15/23 06:26:03.647
    STEP: waiting for RC to be modified 11/15/23 06:26:03.698
    STEP: patching ReplicationController status 11/15/23 06:26:03.698
    STEP: waiting for RC to be modified 11/15/23 06:26:03.753
    STEP: waiting for available Replicas 11/15/23 06:26:03.753
    STEP: fetching ReplicationController status 11/15/23 06:26:03.755
    STEP: patching ReplicationController scale 11/15/23 06:26:03.772
    STEP: waiting for RC to be modified 11/15/23 06:26:03.794
    STEP: waiting for ReplicationController's scale to be the max amount 11/15/23 06:26:03.794
    STEP: fetching ReplicationController; ensuring that it's patched 11/15/23 06:26:05.904
    STEP: updating ReplicationController status 11/15/23 06:26:05.921
    STEP: waiting for RC to be modified 11/15/23 06:26:05.943
    STEP: listing all ReplicationControllers 11/15/23 06:26:05.943
    STEP: checking that ReplicationController has expected values 11/15/23 06:26:05.959
    STEP: deleting ReplicationControllers by collection 11/15/23 06:26:05.959
    STEP: waiting for ReplicationController to have a DELETED watchEvent 11/15/23 06:26:05.992
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:26:06.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9055" for this suite. 11/15/23 06:26:06.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:26:06.195
Nov 15 06:26:06.195: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename subpath 11/15/23 06:26:06.196
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:06.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:06.262
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/15/23 06:26:06.275
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-84ch 11/15/23 06:26:06.312
STEP: Creating a pod to test atomic-volume-subpath 11/15/23 06:26:06.312
Nov 15 06:26:06.351: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-84ch" in namespace "subpath-5616" to be "Succeeded or Failed"
Nov 15 06:26:06.372: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Pending", Reason="", readiness=false. Elapsed: 20.826803ms
Nov 15 06:26:08.396: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04496216s
Nov 15 06:26:10.392: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 4.04100405s
Nov 15 06:26:12.391: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 6.039919991s
Nov 15 06:26:14.391: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 8.039639517s
Nov 15 06:26:16.392: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 10.041139933s
Nov 15 06:26:18.414: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 12.062804831s
Nov 15 06:26:20.391: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 14.039252953s
Nov 15 06:26:22.393: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 16.041401321s
Nov 15 06:26:24.391: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 18.040057341s
Nov 15 06:26:26.403: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 20.052042218s
Nov 15 06:26:28.391: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 22.039871918s
Nov 15 06:26:30.392: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=false. Elapsed: 24.040979401s
Nov 15 06:26:32.391: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.039495817s
STEP: Saw pod success 11/15/23 06:26:32.391
Nov 15 06:26:32.391: INFO: Pod "pod-subpath-test-projected-84ch" satisfied condition "Succeeded or Failed"
Nov 15 06:26:32.411: INFO: Trying to get logs from node 10.72.152.86 pod pod-subpath-test-projected-84ch container test-container-subpath-projected-84ch: <nil>
STEP: delete the pod 11/15/23 06:26:32.493
Nov 15 06:26:32.554: INFO: Waiting for pod pod-subpath-test-projected-84ch to disappear
Nov 15 06:26:32.576: INFO: Pod pod-subpath-test-projected-84ch no longer exists
STEP: Deleting pod pod-subpath-test-projected-84ch 11/15/23 06:26:32.576
Nov 15 06:26:32.576: INFO: Deleting pod "pod-subpath-test-projected-84ch" in namespace "subpath-5616"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Nov 15 06:26:32.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5616" for this suite. 11/15/23 06:26:32.624
------------------------------
â€¢ [SLOW TEST] [26.458 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:26:06.195
    Nov 15 06:26:06.195: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename subpath 11/15/23 06:26:06.196
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:06.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:06.262
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/15/23 06:26:06.275
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-84ch 11/15/23 06:26:06.312
    STEP: Creating a pod to test atomic-volume-subpath 11/15/23 06:26:06.312
    Nov 15 06:26:06.351: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-84ch" in namespace "subpath-5616" to be "Succeeded or Failed"
    Nov 15 06:26:06.372: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Pending", Reason="", readiness=false. Elapsed: 20.826803ms
    Nov 15 06:26:08.396: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04496216s
    Nov 15 06:26:10.392: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 4.04100405s
    Nov 15 06:26:12.391: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 6.039919991s
    Nov 15 06:26:14.391: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 8.039639517s
    Nov 15 06:26:16.392: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 10.041139933s
    Nov 15 06:26:18.414: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 12.062804831s
    Nov 15 06:26:20.391: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 14.039252953s
    Nov 15 06:26:22.393: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 16.041401321s
    Nov 15 06:26:24.391: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 18.040057341s
    Nov 15 06:26:26.403: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 20.052042218s
    Nov 15 06:26:28.391: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=true. Elapsed: 22.039871918s
    Nov 15 06:26:30.392: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Running", Reason="", readiness=false. Elapsed: 24.040979401s
    Nov 15 06:26:32.391: INFO: Pod "pod-subpath-test-projected-84ch": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.039495817s
    STEP: Saw pod success 11/15/23 06:26:32.391
    Nov 15 06:26:32.391: INFO: Pod "pod-subpath-test-projected-84ch" satisfied condition "Succeeded or Failed"
    Nov 15 06:26:32.411: INFO: Trying to get logs from node 10.72.152.86 pod pod-subpath-test-projected-84ch container test-container-subpath-projected-84ch: <nil>
    STEP: delete the pod 11/15/23 06:26:32.493
    Nov 15 06:26:32.554: INFO: Waiting for pod pod-subpath-test-projected-84ch to disappear
    Nov 15 06:26:32.576: INFO: Pod pod-subpath-test-projected-84ch no longer exists
    STEP: Deleting pod pod-subpath-test-projected-84ch 11/15/23 06:26:32.576
    Nov 15 06:26:32.576: INFO: Deleting pod "pod-subpath-test-projected-84ch" in namespace "subpath-5616"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:26:32.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5616" for this suite. 11/15/23 06:26:32.624
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:26:32.654
Nov 15 06:26:32.654: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename emptydir 11/15/23 06:26:32.655
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:32.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:32.745
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 11/15/23 06:26:32.76
Nov 15 06:26:32.810: INFO: Waiting up to 5m0s for pod "pod-f584419b-5eff-4ccc-90ae-cf324a91ce89" in namespace "emptydir-3243" to be "Succeeded or Failed"
Nov 15 06:26:32.829: INFO: Pod "pod-f584419b-5eff-4ccc-90ae-cf324a91ce89": Phase="Pending", Reason="", readiness=false. Elapsed: 18.893242ms
Nov 15 06:26:34.852: INFO: Pod "pod-f584419b-5eff-4ccc-90ae-cf324a91ce89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042810295s
Nov 15 06:26:36.850: INFO: Pod "pod-f584419b-5eff-4ccc-90ae-cf324a91ce89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040729738s
STEP: Saw pod success 11/15/23 06:26:36.85
Nov 15 06:26:36.850: INFO: Pod "pod-f584419b-5eff-4ccc-90ae-cf324a91ce89" satisfied condition "Succeeded or Failed"
Nov 15 06:26:36.868: INFO: Trying to get logs from node 10.72.152.86 pod pod-f584419b-5eff-4ccc-90ae-cf324a91ce89 container test-container: <nil>
STEP: delete the pod 11/15/23 06:26:36.909
Nov 15 06:26:36.968: INFO: Waiting for pod pod-f584419b-5eff-4ccc-90ae-cf324a91ce89 to disappear
Nov 15 06:26:36.986: INFO: Pod pod-f584419b-5eff-4ccc-90ae-cf324a91ce89 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 06:26:36.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3243" for this suite. 11/15/23 06:26:37.016
------------------------------
â€¢ [4.388 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:26:32.654
    Nov 15 06:26:32.654: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename emptydir 11/15/23 06:26:32.655
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:32.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:32.745
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 11/15/23 06:26:32.76
    Nov 15 06:26:32.810: INFO: Waiting up to 5m0s for pod "pod-f584419b-5eff-4ccc-90ae-cf324a91ce89" in namespace "emptydir-3243" to be "Succeeded or Failed"
    Nov 15 06:26:32.829: INFO: Pod "pod-f584419b-5eff-4ccc-90ae-cf324a91ce89": Phase="Pending", Reason="", readiness=false. Elapsed: 18.893242ms
    Nov 15 06:26:34.852: INFO: Pod "pod-f584419b-5eff-4ccc-90ae-cf324a91ce89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042810295s
    Nov 15 06:26:36.850: INFO: Pod "pod-f584419b-5eff-4ccc-90ae-cf324a91ce89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040729738s
    STEP: Saw pod success 11/15/23 06:26:36.85
    Nov 15 06:26:36.850: INFO: Pod "pod-f584419b-5eff-4ccc-90ae-cf324a91ce89" satisfied condition "Succeeded or Failed"
    Nov 15 06:26:36.868: INFO: Trying to get logs from node 10.72.152.86 pod pod-f584419b-5eff-4ccc-90ae-cf324a91ce89 container test-container: <nil>
    STEP: delete the pod 11/15/23 06:26:36.909
    Nov 15 06:26:36.968: INFO: Waiting for pod pod-f584419b-5eff-4ccc-90ae-cf324a91ce89 to disappear
    Nov 15 06:26:36.986: INFO: Pod pod-f584419b-5eff-4ccc-90ae-cf324a91ce89 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:26:36.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3243" for this suite. 11/15/23 06:26:37.016
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:26:37.044
Nov 15 06:26:37.044: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubectl 11/15/23 06:26:37.045
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:37.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:37.111
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Nov 15 06:26:37.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1378 create -f -'
Nov 15 06:26:37.718: INFO: stderr: ""
Nov 15 06:26:37.718: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Nov 15 06:26:37.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1378 create -f -'
Nov 15 06:26:38.198: INFO: stderr: ""
Nov 15 06:26:38.198: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 11/15/23 06:26:38.198
Nov 15 06:26:39.218: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 06:26:39.218: INFO: Found 0 / 1
Nov 15 06:26:40.219: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 06:26:40.219: INFO: Found 1 / 1
Nov 15 06:26:40.219: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 15 06:26:40.237: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 06:26:40.237: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 15 06:26:40.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1378 describe pod agnhost-primary-hwzk9'
Nov 15 06:26:40.398: INFO: stderr: ""
Nov 15 06:26:40.398: INFO: stdout: "Name:             agnhost-primary-hwzk9\nNamespace:        kubectl-1378\nPriority:         0\nService Account:  default\nNode:             10.72.152.86/10.72.152.86\nStart Time:       Wed, 15 Nov 2023 06:26:37 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: ad4f61651d77463224f0e51a020093bedbb7d3c572995ecff8273b081256d468\n                  cni.projectcalico.org/podIP: 172.30.213.185/32\n                  cni.projectcalico.org/podIPs: 172.30.213.185/32\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.213.185\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               172.30.213.185\nIPs:\n  IP:           172.30.213.185\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://a2d57df00f761fb9159db1e65a02b08d7ebcbe48d4a8272772405711a944406d\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 15 Nov 2023 06:26:39 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qlcdf (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-qlcdf:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       3s    default-scheduler  Successfully assigned kubectl-1378/agnhost-primary-hwzk9 to 10.72.152.86\n  Normal  AddedInterface  2s    multus             Add eth0 [172.30.213.185/32] from k8s-pod-network\n  Normal  Pulled          2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
Nov 15 06:26:40.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1378 describe rc agnhost-primary'
Nov 15 06:26:40.565: INFO: stderr: ""
Nov 15 06:26:40.565: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1378\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-hwzk9\n"
Nov 15 06:26:40.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1378 describe service agnhost-primary'
Nov 15 06:26:40.716: INFO: stderr: ""
Nov 15 06:26:40.716: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1378\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.153.119\nIPs:               172.21.153.119\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.213.185:6379\nSession Affinity:  None\nEvents:            <none>\n"
Nov 15 06:26:40.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1378 describe node 10.72.152.81'
Nov 15 06:26:41.219: INFO: stderr: ""
Nov 15 06:26:41.219: INFO: stdout: "Name:               10.72.152.81\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-gb\n                    failure-domain.beta.kubernetes.io/zone=lon06\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=158.176.115.186\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.72.152.81\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_8_64\n                    ibm-cloud.kubernetes.io/region=eu-gb\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cla3eugl0cqpvagdodmg-kubee2epvg6-default-00000143\n                    ibm-cloud.kubernetes.io/worker-pool-id=cla3eugl0cqpvagdodmg-aa4832b\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.13.19_1545_openshift\n                    ibm-cloud.kubernetes.io/zone=lon06\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.72.152.81\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2722974\n                    publicVLAN=2722972\n                    topology.kubernetes.io/region=eu-gb\n                    topology.kubernetes.io/zone=lon06\nAnnotations:        projectcalico.org/IPv4Address: 10.72.152.81/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.214.128\nCreationTimestamp:  Wed, 15 Nov 2023 03:38:30 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.72.152.81\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 15 Nov 2023 06:26:40 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 15 Nov 2023 03:40:03 +0000   Wed, 15 Nov 2023 03:40:03 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 15 Nov 2023 06:23:07 +0000   Wed, 15 Nov 2023 03:38:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 15 Nov 2023 06:23:07 +0000   Wed, 15 Nov 2023 03:38:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 15 Nov 2023 06:23:07 +0000   Wed, 15 Nov 2023 03:38:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 15 Nov 2023 06:23:07 +0000   Wed, 15 Nov 2023 03:39:57 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.72.152.81\n  ExternalIP:  158.176.115.186\n  Hostname:    10.72.152.81\nCapacity:\n  cpu:                  4\n  ephemeral-storage:    102609848Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               16382396Ki\n  pods:                 110\nAllocatable:\n  cpu:                  3910m\n  ephemeral-storage:    93913280025\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               13594044Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                              94749e42da644c92a4b28e26febb61e4\n  System UUID:                             0e93d020-cc44-6a8a-708b-067909615714\n  Boot ID:                                 44730339-928e-4018-8c4a-3af421b32027\n  Kernel Version:                          4.18.0-477.27.1.el8_8.x86_64\n  OS Image:                                Red Hat Enterprise Linux 8.8 (Ootpa)\n  Operating System:                        linux\n  Architecture:                            amd64\n  Container Runtime Version:               cri-o://1.26.4-4.1.rhaos4.13.git92b763a.el8\n  Kubelet Version:                         v1.26.9+636f2be\n  Kube-Proxy Version:                      v1.26.9+636f2be\nPodCIDR:                                   172.30.1.0/24\nPodCIDRs:                                  172.30.1.0/24\nProviderID:                                ibm://fee034388aa6435883a1f720010ab3a2///cla3eugl0cqpvagdodmg/kube-cla3eugl0cqpvagdodmg-kubee2epvg6-default-00000143\nNon-terminated Pods:                       (45 in total)\n  Namespace                                Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                                ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                            calico-node-p5wd4                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         167m\n  calico-system                            calico-typha-76d9767bd5-985rd                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         167m\n  ibm-system                               ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-dl6kq      5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         158m\n  kube-system                              ibm-keepalived-watcher-qb8hn                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         168m\n  kube-system                              ibm-master-proxy-static-10.72.152.81                       26m (0%)      300m (7%)   32001024 (0%)    512M (3%)      166m\n  kube-system                              ibmcloud-block-storage-driver-z7fmw                        50m (1%)      500m (12%)  100Mi (0%)       300Mi (2%)     168m\n  kube-system                              vpn-56cd75f85d-qwzcc                                       5m (0%)       0 (0%)      5Mi (0%)         0 (0%)         166m\n  openshift-cluster-node-tuning-operator   tuned-rb6qn                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         162m\n  openshift-cluster-storage-operator       cluster-storage-operator-6cf6b595c7-m7mfz                  10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         165m\n  openshift-cluster-storage-operator       csi-snapshot-controller-857d54544d-mwb9h                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         165m\n  openshift-cluster-storage-operator       csi-snapshot-controller-operator-56df7685c7-vnvd2          10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         165m\n  openshift-cluster-storage-operator       csi-snapshot-webhook-586f5c484d-qg5n7                      10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         165m\n  openshift-console                        console-68d6458867-wlg2q                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         159m\n  openshift-console                        downloads-7bb648f846-tvqt6                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         165m\n  openshift-dns                            dns-default-njlcg                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         162m\n  openshift-dns                            node-resolver-6lxcm                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         162m\n  openshift-image-registry                 cluster-image-registry-operator-64994bbb4-6twjh            10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         165m\n  openshift-image-registry                 node-ca-897k2                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         162m\n  openshift-ingress-canary                 ingress-canary-jqtjt                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         162m\n  openshift-ingress                        router-default-56777c97d6-szb4s                            100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         162m\n  openshift-insights                       insights-operator-85b688b59d-v47wz                         10m (0%)      0 (0%)      30Mi (0%)        0 (0%)         165m\n  openshift-kube-proxy                     openshift-kube-proxy-p8p2b                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         167m\n  openshift-kube-storage-version-migrator  migrator-697dd4cbc5-kk7zn                                  10m (0%)      0 (0%)      200Mi (1%)       0 (0%)         165m\n  openshift-marketplace                    certified-operators-lqtq6                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         163m\n  openshift-marketplace                    community-operators-jpmzt                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         80m\n  openshift-marketplace                    marketplace-operator-55cc9f5b6b-xpbw5                      10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         165m\n  openshift-marketplace                    redhat-marketplace-xp5nc                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         163m\n  openshift-marketplace                    redhat-operators-bzltn                                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         138m\n  openshift-monitoring                     alertmanager-main-1                                        9m (0%)       0 (0%)      120Mi (0%)       0 (0%)         160m\n  openshift-monitoring                     node-exporter-67458                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         161m\n  openshift-monitoring                     prometheus-adapter-5f5bb574db-r4lkm                        1m (0%)       0 (0%)      40Mi (0%)        0 (0%)         161m\n  openshift-monitoring                     prometheus-k8s-1                                           75m (1%)      0 (0%)      1104Mi (8%)      0 (0%)         160m\n  openshift-monitoring                     prometheus-operator-admission-webhook-c78bf8f99-gxfh4      5m (0%)       0 (0%)      30Mi (0%)        0 (0%)         162m\n  openshift-monitoring                     telemeter-client-77c946bb95-nsnds                          3m (0%)       0 (0%)      70Mi (0%)        0 (0%)         160m\n  openshift-multus                         multus-9tc6x                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         167m\n  openshift-multus                         multus-additional-cni-plugins-j2jvc                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         167m\n  openshift-multus                         network-metrics-daemon-g5fgz                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         167m\n  openshift-network-diagnostics            network-check-target-6pv9x                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         167m\n  openshift-operator-lifecycle-manager     catalog-operator-798697959c-24lbd                          10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         165m\n  openshift-operator-lifecycle-manager     olm-operator-846bf6bd78-fzxr5                              10m (0%)      0 (0%)      160Mi (1%)       0 (0%)         165m\n  openshift-operator-lifecycle-manager     package-server-manager-5b666bf8fd-5v7rb                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         165m\n  openshift-operator-lifecycle-manager     packageserver-758b547fc-f9wwb                              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         162m\n  openshift-service-ca-operator            service-ca-operator-74cb5c9cf5-fqgj5                       10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         165m\n  openshift-service-ca                     service-ca-78fb97bb77-qcz4b                                10m (0%)      0 (0%)      120Mi (0%)       0 (0%)         164m\n  sonobuoy                                 sonobuoy-systemd-logs-daemon-set-997738c51257402b-qvnnx    0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests         Limits\n  --------             --------         ------\n  cpu                  1248m (31%)      800m (20%)\n  memory               4094483Ki (30%)  826572800 (5%)\n  ephemeral-storage    0 (0%)           0 (0%)\n  hugepages-1Gi        0 (0%)           0 (0%)\n  hugepages-2Mi        0 (0%)           0 (0%)\n  example.com/fakecpu  0                0\nEvents:\n  Type    Reason                   Age                  From                   Message\n  ----    ------                   ----                 ----                   -------\n  Normal  Starting                 167m                 kube-proxy             \n  Normal  Starting                 168m                 kubelet                Starting kubelet.\n  Normal  NodeAllocatableEnforced  168m                 kubelet                Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  168m (x8 over 168m)  kubelet                Node 10.72.152.81 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    168m (x8 over 168m)  kubelet                Node 10.72.152.81 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     168m (x7 over 168m)  kubelet                Node 10.72.152.81 status is now: NodeHasSufficientPID\n  Normal  RegisteredNode           168m                 node-controller        Node 10.72.152.81 event: Registered Node 10.72.152.81 in Controller\n  Normal  Synced                   168m                 cloud-node-controller  Node synced successfully\n  Normal  RegisteredNode           162m                 node-controller        Node 10.72.152.81 event: Registered Node 10.72.152.81 in Controller\n"
Nov 15 06:26:41.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1378 describe namespace kubectl-1378'
Nov 15 06:26:41.359: INFO: stderr: ""
Nov 15 06:26:41.359: INFO: stdout: "Name:         kubectl-1378\nLabels:       e2e-framework=kubectl\n              e2e-run=ecce4ee9-09c9-4c3e-878b-b9821315fe37\n              kubernetes.io/metadata.name=kubectl-1378\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c51,c50\n              openshift.io/sa.scc.supplemental-groups: 1002650000/10000\n              openshift.io/sa.scc.uid-range: 1002650000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 06:26:41.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1378" for this suite. 11/15/23 06:26:41.391
------------------------------
â€¢ [4.376 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:26:37.044
    Nov 15 06:26:37.044: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubectl 11/15/23 06:26:37.045
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:37.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:37.111
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Nov 15 06:26:37.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1378 create -f -'
    Nov 15 06:26:37.718: INFO: stderr: ""
    Nov 15 06:26:37.718: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Nov 15 06:26:37.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1378 create -f -'
    Nov 15 06:26:38.198: INFO: stderr: ""
    Nov 15 06:26:38.198: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 11/15/23 06:26:38.198
    Nov 15 06:26:39.218: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 06:26:39.218: INFO: Found 0 / 1
    Nov 15 06:26:40.219: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 06:26:40.219: INFO: Found 1 / 1
    Nov 15 06:26:40.219: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Nov 15 06:26:40.237: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 06:26:40.237: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Nov 15 06:26:40.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1378 describe pod agnhost-primary-hwzk9'
    Nov 15 06:26:40.398: INFO: stderr: ""
    Nov 15 06:26:40.398: INFO: stdout: "Name:             agnhost-primary-hwzk9\nNamespace:        kubectl-1378\nPriority:         0\nService Account:  default\nNode:             10.72.152.86/10.72.152.86\nStart Time:       Wed, 15 Nov 2023 06:26:37 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: ad4f61651d77463224f0e51a020093bedbb7d3c572995ecff8273b081256d468\n                  cni.projectcalico.org/podIP: 172.30.213.185/32\n                  cni.projectcalico.org/podIPs: 172.30.213.185/32\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.213.185\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               172.30.213.185\nIPs:\n  IP:           172.30.213.185\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://a2d57df00f761fb9159db1e65a02b08d7ebcbe48d4a8272772405711a944406d\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 15 Nov 2023 06:26:39 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qlcdf (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-qlcdf:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       3s    default-scheduler  Successfully assigned kubectl-1378/agnhost-primary-hwzk9 to 10.72.152.86\n  Normal  AddedInterface  2s    multus             Add eth0 [172.30.213.185/32] from k8s-pod-network\n  Normal  Pulled          2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
    Nov 15 06:26:40.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1378 describe rc agnhost-primary'
    Nov 15 06:26:40.565: INFO: stderr: ""
    Nov 15 06:26:40.565: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1378\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-hwzk9\n"
    Nov 15 06:26:40.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1378 describe service agnhost-primary'
    Nov 15 06:26:40.716: INFO: stderr: ""
    Nov 15 06:26:40.716: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1378\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.153.119\nIPs:               172.21.153.119\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.213.185:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Nov 15 06:26:40.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1378 describe node 10.72.152.81'
    Nov 15 06:26:41.219: INFO: stderr: ""
    Nov 15 06:26:41.219: INFO: stdout: "Name:               10.72.152.81\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-gb\n                    failure-domain.beta.kubernetes.io/zone=lon06\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=158.176.115.186\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.72.152.81\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_8_64\n                    ibm-cloud.kubernetes.io/region=eu-gb\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cla3eugl0cqpvagdodmg-kubee2epvg6-default-00000143\n                    ibm-cloud.kubernetes.io/worker-pool-id=cla3eugl0cqpvagdodmg-aa4832b\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.13.19_1545_openshift\n                    ibm-cloud.kubernetes.io/zone=lon06\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.72.152.81\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2722974\n                    publicVLAN=2722972\n                    topology.kubernetes.io/region=eu-gb\n                    topology.kubernetes.io/zone=lon06\nAnnotations:        projectcalico.org/IPv4Address: 10.72.152.81/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.214.128\nCreationTimestamp:  Wed, 15 Nov 2023 03:38:30 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.72.152.81\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 15 Nov 2023 06:26:40 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 15 Nov 2023 03:40:03 +0000   Wed, 15 Nov 2023 03:40:03 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 15 Nov 2023 06:23:07 +0000   Wed, 15 Nov 2023 03:38:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 15 Nov 2023 06:23:07 +0000   Wed, 15 Nov 2023 03:38:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 15 Nov 2023 06:23:07 +0000   Wed, 15 Nov 2023 03:38:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 15 Nov 2023 06:23:07 +0000   Wed, 15 Nov 2023 03:39:57 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.72.152.81\n  ExternalIP:  158.176.115.186\n  Hostname:    10.72.152.81\nCapacity:\n  cpu:                  4\n  ephemeral-storage:    102609848Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               16382396Ki\n  pods:                 110\nAllocatable:\n  cpu:                  3910m\n  ephemeral-storage:    93913280025\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               13594044Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                              94749e42da644c92a4b28e26febb61e4\n  System UUID:                             0e93d020-cc44-6a8a-708b-067909615714\n  Boot ID:                                 44730339-928e-4018-8c4a-3af421b32027\n  Kernel Version:                          4.18.0-477.27.1.el8_8.x86_64\n  OS Image:                                Red Hat Enterprise Linux 8.8 (Ootpa)\n  Operating System:                        linux\n  Architecture:                            amd64\n  Container Runtime Version:               cri-o://1.26.4-4.1.rhaos4.13.git92b763a.el8\n  Kubelet Version:                         v1.26.9+636f2be\n  Kube-Proxy Version:                      v1.26.9+636f2be\nPodCIDR:                                   172.30.1.0/24\nPodCIDRs:                                  172.30.1.0/24\nProviderID:                                ibm://fee034388aa6435883a1f720010ab3a2///cla3eugl0cqpvagdodmg/kube-cla3eugl0cqpvagdodmg-kubee2epvg6-default-00000143\nNon-terminated Pods:                       (45 in total)\n  Namespace                                Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                                ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                            calico-node-p5wd4                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         167m\n  calico-system                            calico-typha-76d9767bd5-985rd                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         167m\n  ibm-system                               ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-dl6kq      5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         158m\n  kube-system                              ibm-keepalived-watcher-qb8hn                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         168m\n  kube-system                              ibm-master-proxy-static-10.72.152.81                       26m (0%)      300m (7%)   32001024 (0%)    512M (3%)      166m\n  kube-system                              ibmcloud-block-storage-driver-z7fmw                        50m (1%)      500m (12%)  100Mi (0%)       300Mi (2%)     168m\n  kube-system                              vpn-56cd75f85d-qwzcc                                       5m (0%)       0 (0%)      5Mi (0%)         0 (0%)         166m\n  openshift-cluster-node-tuning-operator   tuned-rb6qn                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         162m\n  openshift-cluster-storage-operator       cluster-storage-operator-6cf6b595c7-m7mfz                  10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         165m\n  openshift-cluster-storage-operator       csi-snapshot-controller-857d54544d-mwb9h                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         165m\n  openshift-cluster-storage-operator       csi-snapshot-controller-operator-56df7685c7-vnvd2          10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         165m\n  openshift-cluster-storage-operator       csi-snapshot-webhook-586f5c484d-qg5n7                      10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         165m\n  openshift-console                        console-68d6458867-wlg2q                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         159m\n  openshift-console                        downloads-7bb648f846-tvqt6                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         165m\n  openshift-dns                            dns-default-njlcg                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         162m\n  openshift-dns                            node-resolver-6lxcm                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         162m\n  openshift-image-registry                 cluster-image-registry-operator-64994bbb4-6twjh            10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         165m\n  openshift-image-registry                 node-ca-897k2                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         162m\n  openshift-ingress-canary                 ingress-canary-jqtjt                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         162m\n  openshift-ingress                        router-default-56777c97d6-szb4s                            100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         162m\n  openshift-insights                       insights-operator-85b688b59d-v47wz                         10m (0%)      0 (0%)      30Mi (0%)        0 (0%)         165m\n  openshift-kube-proxy                     openshift-kube-proxy-p8p2b                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         167m\n  openshift-kube-storage-version-migrator  migrator-697dd4cbc5-kk7zn                                  10m (0%)      0 (0%)      200Mi (1%)       0 (0%)         165m\n  openshift-marketplace                    certified-operators-lqtq6                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         163m\n  openshift-marketplace                    community-operators-jpmzt                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         80m\n  openshift-marketplace                    marketplace-operator-55cc9f5b6b-xpbw5                      10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         165m\n  openshift-marketplace                    redhat-marketplace-xp5nc                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         163m\n  openshift-marketplace                    redhat-operators-bzltn                                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         138m\n  openshift-monitoring                     alertmanager-main-1                                        9m (0%)       0 (0%)      120Mi (0%)       0 (0%)         160m\n  openshift-monitoring                     node-exporter-67458                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         161m\n  openshift-monitoring                     prometheus-adapter-5f5bb574db-r4lkm                        1m (0%)       0 (0%)      40Mi (0%)        0 (0%)         161m\n  openshift-monitoring                     prometheus-k8s-1                                           75m (1%)      0 (0%)      1104Mi (8%)      0 (0%)         160m\n  openshift-monitoring                     prometheus-operator-admission-webhook-c78bf8f99-gxfh4      5m (0%)       0 (0%)      30Mi (0%)        0 (0%)         162m\n  openshift-monitoring                     telemeter-client-77c946bb95-nsnds                          3m (0%)       0 (0%)      70Mi (0%)        0 (0%)         160m\n  openshift-multus                         multus-9tc6x                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         167m\n  openshift-multus                         multus-additional-cni-plugins-j2jvc                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         167m\n  openshift-multus                         network-metrics-daemon-g5fgz                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         167m\n  openshift-network-diagnostics            network-check-target-6pv9x                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         167m\n  openshift-operator-lifecycle-manager     catalog-operator-798697959c-24lbd                          10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         165m\n  openshift-operator-lifecycle-manager     olm-operator-846bf6bd78-fzxr5                              10m (0%)      0 (0%)      160Mi (1%)       0 (0%)         165m\n  openshift-operator-lifecycle-manager     package-server-manager-5b666bf8fd-5v7rb                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         165m\n  openshift-operator-lifecycle-manager     packageserver-758b547fc-f9wwb                              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         162m\n  openshift-service-ca-operator            service-ca-operator-74cb5c9cf5-fqgj5                       10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         165m\n  openshift-service-ca                     service-ca-78fb97bb77-qcz4b                                10m (0%)      0 (0%)      120Mi (0%)       0 (0%)         164m\n  sonobuoy                                 sonobuoy-systemd-logs-daemon-set-997738c51257402b-qvnnx    0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests         Limits\n  --------             --------         ------\n  cpu                  1248m (31%)      800m (20%)\n  memory               4094483Ki (30%)  826572800 (5%)\n  ephemeral-storage    0 (0%)           0 (0%)\n  hugepages-1Gi        0 (0%)           0 (0%)\n  hugepages-2Mi        0 (0%)           0 (0%)\n  example.com/fakecpu  0                0\nEvents:\n  Type    Reason                   Age                  From                   Message\n  ----    ------                   ----                 ----                   -------\n  Normal  Starting                 167m                 kube-proxy             \n  Normal  Starting                 168m                 kubelet                Starting kubelet.\n  Normal  NodeAllocatableEnforced  168m                 kubelet                Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  168m (x8 over 168m)  kubelet                Node 10.72.152.81 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    168m (x8 over 168m)  kubelet                Node 10.72.152.81 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     168m (x7 over 168m)  kubelet                Node 10.72.152.81 status is now: NodeHasSufficientPID\n  Normal  RegisteredNode           168m                 node-controller        Node 10.72.152.81 event: Registered Node 10.72.152.81 in Controller\n  Normal  Synced                   168m                 cloud-node-controller  Node synced successfully\n  Normal  RegisteredNode           162m                 node-controller        Node 10.72.152.81 event: Registered Node 10.72.152.81 in Controller\n"
    Nov 15 06:26:41.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1378 describe namespace kubectl-1378'
    Nov 15 06:26:41.359: INFO: stderr: ""
    Nov 15 06:26:41.359: INFO: stdout: "Name:         kubectl-1378\nLabels:       e2e-framework=kubectl\n              e2e-run=ecce4ee9-09c9-4c3e-878b-b9821315fe37\n              kubernetes.io/metadata.name=kubectl-1378\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c51,c50\n              openshift.io/sa.scc.supplemental-groups: 1002650000/10000\n              openshift.io/sa.scc.uid-range: 1002650000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:26:41.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1378" for this suite. 11/15/23 06:26:41.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:26:41.422
Nov 15 06:26:41.422: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename svcaccounts 11/15/23 06:26:41.423
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:41.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:41.484
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Nov 15 06:26:41.591: INFO: created pod pod-service-account-defaultsa
Nov 15 06:26:41.591: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov 15 06:26:41.632: INFO: created pod pod-service-account-mountsa
Nov 15 06:26:41.636: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov 15 06:26:41.668: INFO: created pod pod-service-account-nomountsa
Nov 15 06:26:41.668: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov 15 06:26:41.692: INFO: created pod pod-service-account-defaultsa-mountspec
Nov 15 06:26:41.692: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov 15 06:26:41.737: INFO: created pod pod-service-account-mountsa-mountspec
Nov 15 06:26:41.737: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov 15 06:26:41.765: INFO: created pod pod-service-account-nomountsa-mountspec
Nov 15 06:26:41.765: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov 15 06:26:41.792: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov 15 06:26:41.792: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov 15 06:26:41.812: INFO: created pod pod-service-account-mountsa-nomountspec
Nov 15 06:26:41.812: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov 15 06:26:41.835: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov 15 06:26:41.835: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 15 06:26:41.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1643" for this suite. 11/15/23 06:26:41.861
------------------------------
â€¢ [0.469 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:26:41.422
    Nov 15 06:26:41.422: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename svcaccounts 11/15/23 06:26:41.423
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:41.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:41.484
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Nov 15 06:26:41.591: INFO: created pod pod-service-account-defaultsa
    Nov 15 06:26:41.591: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Nov 15 06:26:41.632: INFO: created pod pod-service-account-mountsa
    Nov 15 06:26:41.636: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Nov 15 06:26:41.668: INFO: created pod pod-service-account-nomountsa
    Nov 15 06:26:41.668: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Nov 15 06:26:41.692: INFO: created pod pod-service-account-defaultsa-mountspec
    Nov 15 06:26:41.692: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Nov 15 06:26:41.737: INFO: created pod pod-service-account-mountsa-mountspec
    Nov 15 06:26:41.737: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Nov 15 06:26:41.765: INFO: created pod pod-service-account-nomountsa-mountspec
    Nov 15 06:26:41.765: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Nov 15 06:26:41.792: INFO: created pod pod-service-account-defaultsa-nomountspec
    Nov 15 06:26:41.792: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Nov 15 06:26:41.812: INFO: created pod pod-service-account-mountsa-nomountspec
    Nov 15 06:26:41.812: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Nov 15 06:26:41.835: INFO: created pod pod-service-account-nomountsa-nomountspec
    Nov 15 06:26:41.835: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:26:41.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1643" for this suite. 11/15/23 06:26:41.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:26:41.893
Nov 15 06:26:41.893: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename var-expansion 11/15/23 06:26:41.893
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:41.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:41.972
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 11/15/23 06:26:41.985
Nov 15 06:26:42.057: INFO: Waiting up to 5m0s for pod "var-expansion-bc513f25-585c-4bca-97f3-cb1d5a019361" in namespace "var-expansion-5359" to be "Succeeded or Failed"
Nov 15 06:26:42.137: INFO: Pod "var-expansion-bc513f25-585c-4bca-97f3-cb1d5a019361": Phase="Pending", Reason="", readiness=false. Elapsed: 80.126641ms
Nov 15 06:26:44.157: INFO: Pod "var-expansion-bc513f25-585c-4bca-97f3-cb1d5a019361": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09999154s
Nov 15 06:26:46.156: INFO: Pod "var-expansion-bc513f25-585c-4bca-97f3-cb1d5a019361": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.098714579s
STEP: Saw pod success 11/15/23 06:26:46.156
Nov 15 06:26:46.156: INFO: Pod "var-expansion-bc513f25-585c-4bca-97f3-cb1d5a019361" satisfied condition "Succeeded or Failed"
Nov 15 06:26:46.176: INFO: Trying to get logs from node 10.72.152.81 pod var-expansion-bc513f25-585c-4bca-97f3-cb1d5a019361 container dapi-container: <nil>
STEP: delete the pod 11/15/23 06:26:46.248
Nov 15 06:26:46.309: INFO: Waiting for pod var-expansion-bc513f25-585c-4bca-97f3-cb1d5a019361 to disappear
Nov 15 06:26:46.324: INFO: Pod var-expansion-bc513f25-585c-4bca-97f3-cb1d5a019361 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 15 06:26:46.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5359" for this suite. 11/15/23 06:26:46.364
------------------------------
â€¢ [4.497 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:26:41.893
    Nov 15 06:26:41.893: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename var-expansion 11/15/23 06:26:41.893
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:41.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:41.972
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 11/15/23 06:26:41.985
    Nov 15 06:26:42.057: INFO: Waiting up to 5m0s for pod "var-expansion-bc513f25-585c-4bca-97f3-cb1d5a019361" in namespace "var-expansion-5359" to be "Succeeded or Failed"
    Nov 15 06:26:42.137: INFO: Pod "var-expansion-bc513f25-585c-4bca-97f3-cb1d5a019361": Phase="Pending", Reason="", readiness=false. Elapsed: 80.126641ms
    Nov 15 06:26:44.157: INFO: Pod "var-expansion-bc513f25-585c-4bca-97f3-cb1d5a019361": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09999154s
    Nov 15 06:26:46.156: INFO: Pod "var-expansion-bc513f25-585c-4bca-97f3-cb1d5a019361": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.098714579s
    STEP: Saw pod success 11/15/23 06:26:46.156
    Nov 15 06:26:46.156: INFO: Pod "var-expansion-bc513f25-585c-4bca-97f3-cb1d5a019361" satisfied condition "Succeeded or Failed"
    Nov 15 06:26:46.176: INFO: Trying to get logs from node 10.72.152.81 pod var-expansion-bc513f25-585c-4bca-97f3-cb1d5a019361 container dapi-container: <nil>
    STEP: delete the pod 11/15/23 06:26:46.248
    Nov 15 06:26:46.309: INFO: Waiting for pod var-expansion-bc513f25-585c-4bca-97f3-cb1d5a019361 to disappear
    Nov 15 06:26:46.324: INFO: Pod var-expansion-bc513f25-585c-4bca-97f3-cb1d5a019361 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:26:46.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5359" for this suite. 11/15/23 06:26:46.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:26:46.392
Nov 15 06:26:46.393: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename resourcequota 11/15/23 06:26:46.394
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:46.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:46.477
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 11/15/23 06:26:46.487
STEP: Getting a ResourceQuota 11/15/23 06:26:46.511
STEP: Listing all ResourceQuotas with LabelSelector 11/15/23 06:26:46.527
STEP: Patching the ResourceQuota 11/15/23 06:26:46.557
STEP: Deleting a Collection of ResourceQuotas 11/15/23 06:26:46.587
STEP: Verifying the deleted ResourceQuota 11/15/23 06:26:46.638
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 06:26:46.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-442" for this suite. 11/15/23 06:26:46.677
------------------------------
â€¢ [0.311 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:26:46.392
    Nov 15 06:26:46.393: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename resourcequota 11/15/23 06:26:46.394
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:46.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:46.477
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 11/15/23 06:26:46.487
    STEP: Getting a ResourceQuota 11/15/23 06:26:46.511
    STEP: Listing all ResourceQuotas with LabelSelector 11/15/23 06:26:46.527
    STEP: Patching the ResourceQuota 11/15/23 06:26:46.557
    STEP: Deleting a Collection of ResourceQuotas 11/15/23 06:26:46.587
    STEP: Verifying the deleted ResourceQuota 11/15/23 06:26:46.638
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:26:46.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-442" for this suite. 11/15/23 06:26:46.677
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:26:46.704
Nov 15 06:26:46.704: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename emptydir 11/15/23 06:26:46.704
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:46.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:46.772
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 11/15/23 06:26:46.8
Nov 15 06:26:46.859: INFO: Waiting up to 5m0s for pod "pod-117c9e3f-1754-41cd-91a0-2b4b0ac7a21b" in namespace "emptydir-2472" to be "Succeeded or Failed"
Nov 15 06:26:46.889: INFO: Pod "pod-117c9e3f-1754-41cd-91a0-2b4b0ac7a21b": Phase="Pending", Reason="", readiness=false. Elapsed: 30.704241ms
Nov 15 06:26:48.916: INFO: Pod "pod-117c9e3f-1754-41cd-91a0-2b4b0ac7a21b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057559813s
Nov 15 06:26:50.910: INFO: Pod "pod-117c9e3f-1754-41cd-91a0-2b4b0ac7a21b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050896814s
Nov 15 06:26:52.923: INFO: Pod "pod-117c9e3f-1754-41cd-91a0-2b4b0ac7a21b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06401058s
STEP: Saw pod success 11/15/23 06:26:52.923
Nov 15 06:26:52.923: INFO: Pod "pod-117c9e3f-1754-41cd-91a0-2b4b0ac7a21b" satisfied condition "Succeeded or Failed"
Nov 15 06:26:52.963: INFO: Trying to get logs from node 10.72.152.86 pod pod-117c9e3f-1754-41cd-91a0-2b4b0ac7a21b container test-container: <nil>
STEP: delete the pod 11/15/23 06:26:53.037
Nov 15 06:26:53.092: INFO: Waiting for pod pod-117c9e3f-1754-41cd-91a0-2b4b0ac7a21b to disappear
Nov 15 06:26:53.111: INFO: Pod pod-117c9e3f-1754-41cd-91a0-2b4b0ac7a21b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 06:26:53.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2472" for this suite. 11/15/23 06:26:53.139
------------------------------
â€¢ [SLOW TEST] [6.461 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:26:46.704
    Nov 15 06:26:46.704: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename emptydir 11/15/23 06:26:46.704
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:46.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:46.772
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 11/15/23 06:26:46.8
    Nov 15 06:26:46.859: INFO: Waiting up to 5m0s for pod "pod-117c9e3f-1754-41cd-91a0-2b4b0ac7a21b" in namespace "emptydir-2472" to be "Succeeded or Failed"
    Nov 15 06:26:46.889: INFO: Pod "pod-117c9e3f-1754-41cd-91a0-2b4b0ac7a21b": Phase="Pending", Reason="", readiness=false. Elapsed: 30.704241ms
    Nov 15 06:26:48.916: INFO: Pod "pod-117c9e3f-1754-41cd-91a0-2b4b0ac7a21b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057559813s
    Nov 15 06:26:50.910: INFO: Pod "pod-117c9e3f-1754-41cd-91a0-2b4b0ac7a21b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050896814s
    Nov 15 06:26:52.923: INFO: Pod "pod-117c9e3f-1754-41cd-91a0-2b4b0ac7a21b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06401058s
    STEP: Saw pod success 11/15/23 06:26:52.923
    Nov 15 06:26:52.923: INFO: Pod "pod-117c9e3f-1754-41cd-91a0-2b4b0ac7a21b" satisfied condition "Succeeded or Failed"
    Nov 15 06:26:52.963: INFO: Trying to get logs from node 10.72.152.86 pod pod-117c9e3f-1754-41cd-91a0-2b4b0ac7a21b container test-container: <nil>
    STEP: delete the pod 11/15/23 06:26:53.037
    Nov 15 06:26:53.092: INFO: Waiting for pod pod-117c9e3f-1754-41cd-91a0-2b4b0ac7a21b to disappear
    Nov 15 06:26:53.111: INFO: Pod pod-117c9e3f-1754-41cd-91a0-2b4b0ac7a21b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:26:53.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2472" for this suite. 11/15/23 06:26:53.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:26:53.166
Nov 15 06:26:53.166: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename svcaccounts 11/15/23 06:26:53.167
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:53.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:53.241
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Nov 15 06:26:53.287: INFO: Got root ca configmap in namespace "svcaccounts-8181"
Nov 15 06:26:53.318: INFO: Deleted root ca configmap in namespace "svcaccounts-8181"
STEP: waiting for a new root ca configmap created 11/15/23 06:26:53.82
Nov 15 06:26:53.838: INFO: Recreated root ca configmap in namespace "svcaccounts-8181"
Nov 15 06:26:53.856: INFO: Updated root ca configmap in namespace "svcaccounts-8181"
STEP: waiting for the root ca configmap reconciled 11/15/23 06:26:54.357
Nov 15 06:26:54.373: INFO: Reconciled root ca configmap in namespace "svcaccounts-8181"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 15 06:26:54.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8181" for this suite. 11/15/23 06:26:54.4
------------------------------
â€¢ [1.260 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:26:53.166
    Nov 15 06:26:53.166: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename svcaccounts 11/15/23 06:26:53.167
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:53.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:53.241
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Nov 15 06:26:53.287: INFO: Got root ca configmap in namespace "svcaccounts-8181"
    Nov 15 06:26:53.318: INFO: Deleted root ca configmap in namespace "svcaccounts-8181"
    STEP: waiting for a new root ca configmap created 11/15/23 06:26:53.82
    Nov 15 06:26:53.838: INFO: Recreated root ca configmap in namespace "svcaccounts-8181"
    Nov 15 06:26:53.856: INFO: Updated root ca configmap in namespace "svcaccounts-8181"
    STEP: waiting for the root ca configmap reconciled 11/15/23 06:26:54.357
    Nov 15 06:26:54.373: INFO: Reconciled root ca configmap in namespace "svcaccounts-8181"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:26:54.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8181" for this suite. 11/15/23 06:26:54.4
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:26:54.426
Nov 15 06:26:54.426: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename csistoragecapacity 11/15/23 06:26:54.427
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:54.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:54.509
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 11/15/23 06:26:54.524
STEP: getting /apis/storage.k8s.io 11/15/23 06:26:54.536
STEP: getting /apis/storage.k8s.io/v1 11/15/23 06:26:54.542
STEP: creating 11/15/23 06:26:54.549
STEP: watching 11/15/23 06:26:54.613
Nov 15 06:26:54.613: INFO: starting watch
STEP: getting 11/15/23 06:26:54.641
STEP: listing in namespace 11/15/23 06:26:54.653
STEP: listing across namespaces 11/15/23 06:26:54.664
STEP: patching 11/15/23 06:26:54.675
STEP: updating 11/15/23 06:26:54.691
Nov 15 06:26:54.707: INFO: waiting for watch events with expected annotations in namespace
Nov 15 06:26:54.707: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 11/15/23 06:26:54.707
STEP: deleting a collection 11/15/23 06:26:54.751
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Nov 15 06:26:54.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-2067" for this suite. 11/15/23 06:26:54.833
------------------------------
â€¢ [0.435 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:26:54.426
    Nov 15 06:26:54.426: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename csistoragecapacity 11/15/23 06:26:54.427
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:54.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:54.509
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 11/15/23 06:26:54.524
    STEP: getting /apis/storage.k8s.io 11/15/23 06:26:54.536
    STEP: getting /apis/storage.k8s.io/v1 11/15/23 06:26:54.542
    STEP: creating 11/15/23 06:26:54.549
    STEP: watching 11/15/23 06:26:54.613
    Nov 15 06:26:54.613: INFO: starting watch
    STEP: getting 11/15/23 06:26:54.641
    STEP: listing in namespace 11/15/23 06:26:54.653
    STEP: listing across namespaces 11/15/23 06:26:54.664
    STEP: patching 11/15/23 06:26:54.675
    STEP: updating 11/15/23 06:26:54.691
    Nov 15 06:26:54.707: INFO: waiting for watch events with expected annotations in namespace
    Nov 15 06:26:54.707: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 11/15/23 06:26:54.707
    STEP: deleting a collection 11/15/23 06:26:54.751
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:26:54.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-2067" for this suite. 11/15/23 06:26:54.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:26:54.864
Nov 15 06:26:54.864: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename emptydir 11/15/23 06:26:54.865
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:54.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:54.924
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 11/15/23 06:26:54.937
Nov 15 06:26:54.974: INFO: Waiting up to 5m0s for pod "pod-1069e016-391a-4296-a167-7fba56429cad" in namespace "emptydir-866" to be "Succeeded or Failed"
Nov 15 06:26:54.994: INFO: Pod "pod-1069e016-391a-4296-a167-7fba56429cad": Phase="Pending", Reason="", readiness=false. Elapsed: 19.890517ms
Nov 15 06:26:57.013: INFO: Pod "pod-1069e016-391a-4296-a167-7fba56429cad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039000038s
Nov 15 06:26:59.011: INFO: Pod "pod-1069e016-391a-4296-a167-7fba56429cad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036400634s
STEP: Saw pod success 11/15/23 06:26:59.011
Nov 15 06:26:59.011: INFO: Pod "pod-1069e016-391a-4296-a167-7fba56429cad" satisfied condition "Succeeded or Failed"
Nov 15 06:26:59.028: INFO: Trying to get logs from node 10.72.152.86 pod pod-1069e016-391a-4296-a167-7fba56429cad container test-container: <nil>
STEP: delete the pod 11/15/23 06:26:59.099
Nov 15 06:26:59.170: INFO: Waiting for pod pod-1069e016-391a-4296-a167-7fba56429cad to disappear
Nov 15 06:26:59.187: INFO: Pod pod-1069e016-391a-4296-a167-7fba56429cad no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 06:26:59.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-866" for this suite. 11/15/23 06:26:59.217
------------------------------
â€¢ [4.407 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:26:54.864
    Nov 15 06:26:54.864: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename emptydir 11/15/23 06:26:54.865
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:54.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:54.924
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 11/15/23 06:26:54.937
    Nov 15 06:26:54.974: INFO: Waiting up to 5m0s for pod "pod-1069e016-391a-4296-a167-7fba56429cad" in namespace "emptydir-866" to be "Succeeded or Failed"
    Nov 15 06:26:54.994: INFO: Pod "pod-1069e016-391a-4296-a167-7fba56429cad": Phase="Pending", Reason="", readiness=false. Elapsed: 19.890517ms
    Nov 15 06:26:57.013: INFO: Pod "pod-1069e016-391a-4296-a167-7fba56429cad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039000038s
    Nov 15 06:26:59.011: INFO: Pod "pod-1069e016-391a-4296-a167-7fba56429cad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036400634s
    STEP: Saw pod success 11/15/23 06:26:59.011
    Nov 15 06:26:59.011: INFO: Pod "pod-1069e016-391a-4296-a167-7fba56429cad" satisfied condition "Succeeded or Failed"
    Nov 15 06:26:59.028: INFO: Trying to get logs from node 10.72.152.86 pod pod-1069e016-391a-4296-a167-7fba56429cad container test-container: <nil>
    STEP: delete the pod 11/15/23 06:26:59.099
    Nov 15 06:26:59.170: INFO: Waiting for pod pod-1069e016-391a-4296-a167-7fba56429cad to disappear
    Nov 15 06:26:59.187: INFO: Pod pod-1069e016-391a-4296-a167-7fba56429cad no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:26:59.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-866" for this suite. 11/15/23 06:26:59.217
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:26:59.273
Nov 15 06:26:59.273: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename emptydir-wrapper 11/15/23 06:26:59.274
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:59.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:59.38
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 11/15/23 06:26:59.423
STEP: Creating RC which spawns configmap-volume pods 11/15/23 06:27:00.426
Nov 15 06:27:00.479: INFO: Pod name wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6: Found 0 pods out of 5
Nov 15 06:27:05.517: INFO: Pod name wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6: Found 5 pods out of 5
STEP: Ensuring each pod is running 11/15/23 06:27:05.517
Nov 15 06:27:05.518: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-7gvjv" in namespace "emptydir-wrapper-1282" to be "running"
Nov 15 06:27:05.543: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-7gvjv": Phase="Running", Reason="", readiness=true. Elapsed: 25.861514ms
Nov 15 06:27:05.544: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-7gvjv" satisfied condition "running"
Nov 15 06:27:05.544: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-gc2hz" in namespace "emptydir-wrapper-1282" to be "running"
Nov 15 06:27:05.563: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-gc2hz": Phase="Running", Reason="", readiness=true. Elapsed: 19.422969ms
Nov 15 06:27:05.563: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-gc2hz" satisfied condition "running"
Nov 15 06:27:05.563: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-qdjck" in namespace "emptydir-wrapper-1282" to be "running"
Nov 15 06:27:05.586: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-qdjck": Phase="Running", Reason="", readiness=true. Elapsed: 23.053629ms
Nov 15 06:27:05.586: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-qdjck" satisfied condition "running"
Nov 15 06:27:05.586: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-vszcp" in namespace "emptydir-wrapper-1282" to be "running"
Nov 15 06:27:05.607: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-vszcp": Phase="Running", Reason="", readiness=true. Elapsed: 20.949629ms
Nov 15 06:27:05.608: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-vszcp" satisfied condition "running"
Nov 15 06:27:05.608: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-w4tbp" in namespace "emptydir-wrapper-1282" to be "running"
Nov 15 06:27:05.626: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-w4tbp": Phase="Running", Reason="", readiness=true. Elapsed: 18.314233ms
Nov 15 06:27:05.626: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-w4tbp" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6 in namespace emptydir-wrapper-1282, will wait for the garbage collector to delete the pods 11/15/23 06:27:05.626
Nov 15 06:27:05.738: INFO: Deleting ReplicationController wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6 took: 25.907057ms
Nov 15 06:27:05.839: INFO: Terminating ReplicationController wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6 pods took: 100.844185ms
STEP: Creating RC which spawns configmap-volume pods 11/15/23 06:27:09.292
Nov 15 06:27:09.516: INFO: Pod name wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d: Found 0 pods out of 5
Nov 15 06:27:14.560: INFO: Pod name wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d: Found 5 pods out of 5
STEP: Ensuring each pod is running 11/15/23 06:27:14.56
Nov 15 06:27:14.560: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-47gjq" in namespace "emptydir-wrapper-1282" to be "running"
Nov 15 06:27:14.582: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-47gjq": Phase="Running", Reason="", readiness=true. Elapsed: 22.316067ms
Nov 15 06:27:14.582: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-47gjq" satisfied condition "running"
Nov 15 06:27:14.582: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-57zhq" in namespace "emptydir-wrapper-1282" to be "running"
Nov 15 06:27:14.603: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-57zhq": Phase="Running", Reason="", readiness=true. Elapsed: 20.345714ms
Nov 15 06:27:14.603: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-57zhq" satisfied condition "running"
Nov 15 06:27:14.603: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-5cbg5" in namespace "emptydir-wrapper-1282" to be "running"
Nov 15 06:27:14.623: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-5cbg5": Phase="Running", Reason="", readiness=true. Elapsed: 20.200312ms
Nov 15 06:27:14.623: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-5cbg5" satisfied condition "running"
Nov 15 06:27:14.623: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-hckj2" in namespace "emptydir-wrapper-1282" to be "running"
Nov 15 06:27:14.641: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-hckj2": Phase="Running", Reason="", readiness=true. Elapsed: 18.609509ms
Nov 15 06:27:14.641: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-hckj2" satisfied condition "running"
Nov 15 06:27:14.641: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-xqf6s" in namespace "emptydir-wrapper-1282" to be "running"
Nov 15 06:27:14.662: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-xqf6s": Phase="Running", Reason="", readiness=true. Elapsed: 20.486284ms
Nov 15 06:27:14.662: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-xqf6s" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d in namespace emptydir-wrapper-1282, will wait for the garbage collector to delete the pods 11/15/23 06:27:14.662
Nov 15 06:27:14.760: INFO: Deleting ReplicationController wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d took: 27.194211ms
Nov 15 06:27:14.861: INFO: Terminating ReplicationController wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d pods took: 100.874908ms
STEP: Creating RC which spawns configmap-volume pods 11/15/23 06:27:17.996
Nov 15 06:27:18.048: INFO: Pod name wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862: Found 0 pods out of 5
Nov 15 06:27:23.095: INFO: Pod name wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862: Found 5 pods out of 5
STEP: Ensuring each pod is running 11/15/23 06:27:23.095
Nov 15 06:27:23.095: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-kqnwd" in namespace "emptydir-wrapper-1282" to be "running"
Nov 15 06:27:23.113: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-kqnwd": Phase="Running", Reason="", readiness=true. Elapsed: 17.697738ms
Nov 15 06:27:23.113: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-kqnwd" satisfied condition "running"
Nov 15 06:27:23.113: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-kwl94" in namespace "emptydir-wrapper-1282" to be "running"
Nov 15 06:27:23.142: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-kwl94": Phase="Running", Reason="", readiness=true. Elapsed: 29.209555ms
Nov 15 06:27:23.142: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-kwl94" satisfied condition "running"
Nov 15 06:27:23.143: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-mv2pb" in namespace "emptydir-wrapper-1282" to be "running"
Nov 15 06:27:23.164: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-mv2pb": Phase="Running", Reason="", readiness=true. Elapsed: 21.314865ms
Nov 15 06:27:23.164: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-mv2pb" satisfied condition "running"
Nov 15 06:27:23.164: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-w7rpk" in namespace "emptydir-wrapper-1282" to be "running"
Nov 15 06:27:23.187: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-w7rpk": Phase="Running", Reason="", readiness=true. Elapsed: 23.259442ms
Nov 15 06:27:23.187: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-w7rpk" satisfied condition "running"
Nov 15 06:27:23.187: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-xmdzh" in namespace "emptydir-wrapper-1282" to be "running"
Nov 15 06:27:23.207: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-xmdzh": Phase="Running", Reason="", readiness=true. Elapsed: 19.907835ms
Nov 15 06:27:23.207: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-xmdzh" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862 in namespace emptydir-wrapper-1282, will wait for the garbage collector to delete the pods 11/15/23 06:27:23.207
Nov 15 06:27:23.302: INFO: Deleting ReplicationController wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862 took: 26.631405ms
Nov 15 06:27:23.503: INFO: Terminating ReplicationController wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862 pods took: 200.958475ms
STEP: Cleaning up the configMaps 11/15/23 06:27:26.803
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 06:27:28.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1282" for this suite. 11/15/23 06:27:28.223
------------------------------
â€¢ [SLOW TEST] [28.974 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:26:59.273
    Nov 15 06:26:59.273: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename emptydir-wrapper 11/15/23 06:26:59.274
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:26:59.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:26:59.38
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 11/15/23 06:26:59.423
    STEP: Creating RC which spawns configmap-volume pods 11/15/23 06:27:00.426
    Nov 15 06:27:00.479: INFO: Pod name wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6: Found 0 pods out of 5
    Nov 15 06:27:05.517: INFO: Pod name wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6: Found 5 pods out of 5
    STEP: Ensuring each pod is running 11/15/23 06:27:05.517
    Nov 15 06:27:05.518: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-7gvjv" in namespace "emptydir-wrapper-1282" to be "running"
    Nov 15 06:27:05.543: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-7gvjv": Phase="Running", Reason="", readiness=true. Elapsed: 25.861514ms
    Nov 15 06:27:05.544: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-7gvjv" satisfied condition "running"
    Nov 15 06:27:05.544: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-gc2hz" in namespace "emptydir-wrapper-1282" to be "running"
    Nov 15 06:27:05.563: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-gc2hz": Phase="Running", Reason="", readiness=true. Elapsed: 19.422969ms
    Nov 15 06:27:05.563: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-gc2hz" satisfied condition "running"
    Nov 15 06:27:05.563: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-qdjck" in namespace "emptydir-wrapper-1282" to be "running"
    Nov 15 06:27:05.586: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-qdjck": Phase="Running", Reason="", readiness=true. Elapsed: 23.053629ms
    Nov 15 06:27:05.586: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-qdjck" satisfied condition "running"
    Nov 15 06:27:05.586: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-vszcp" in namespace "emptydir-wrapper-1282" to be "running"
    Nov 15 06:27:05.607: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-vszcp": Phase="Running", Reason="", readiness=true. Elapsed: 20.949629ms
    Nov 15 06:27:05.608: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-vszcp" satisfied condition "running"
    Nov 15 06:27:05.608: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-w4tbp" in namespace "emptydir-wrapper-1282" to be "running"
    Nov 15 06:27:05.626: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-w4tbp": Phase="Running", Reason="", readiness=true. Elapsed: 18.314233ms
    Nov 15 06:27:05.626: INFO: Pod "wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6-w4tbp" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6 in namespace emptydir-wrapper-1282, will wait for the garbage collector to delete the pods 11/15/23 06:27:05.626
    Nov 15 06:27:05.738: INFO: Deleting ReplicationController wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6 took: 25.907057ms
    Nov 15 06:27:05.839: INFO: Terminating ReplicationController wrapped-volume-race-c4327da0-02a1-4bdd-b1eb-1548b39f9cf6 pods took: 100.844185ms
    STEP: Creating RC which spawns configmap-volume pods 11/15/23 06:27:09.292
    Nov 15 06:27:09.516: INFO: Pod name wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d: Found 0 pods out of 5
    Nov 15 06:27:14.560: INFO: Pod name wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d: Found 5 pods out of 5
    STEP: Ensuring each pod is running 11/15/23 06:27:14.56
    Nov 15 06:27:14.560: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-47gjq" in namespace "emptydir-wrapper-1282" to be "running"
    Nov 15 06:27:14.582: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-47gjq": Phase="Running", Reason="", readiness=true. Elapsed: 22.316067ms
    Nov 15 06:27:14.582: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-47gjq" satisfied condition "running"
    Nov 15 06:27:14.582: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-57zhq" in namespace "emptydir-wrapper-1282" to be "running"
    Nov 15 06:27:14.603: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-57zhq": Phase="Running", Reason="", readiness=true. Elapsed: 20.345714ms
    Nov 15 06:27:14.603: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-57zhq" satisfied condition "running"
    Nov 15 06:27:14.603: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-5cbg5" in namespace "emptydir-wrapper-1282" to be "running"
    Nov 15 06:27:14.623: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-5cbg5": Phase="Running", Reason="", readiness=true. Elapsed: 20.200312ms
    Nov 15 06:27:14.623: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-5cbg5" satisfied condition "running"
    Nov 15 06:27:14.623: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-hckj2" in namespace "emptydir-wrapper-1282" to be "running"
    Nov 15 06:27:14.641: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-hckj2": Phase="Running", Reason="", readiness=true. Elapsed: 18.609509ms
    Nov 15 06:27:14.641: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-hckj2" satisfied condition "running"
    Nov 15 06:27:14.641: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-xqf6s" in namespace "emptydir-wrapper-1282" to be "running"
    Nov 15 06:27:14.662: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-xqf6s": Phase="Running", Reason="", readiness=true. Elapsed: 20.486284ms
    Nov 15 06:27:14.662: INFO: Pod "wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d-xqf6s" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d in namespace emptydir-wrapper-1282, will wait for the garbage collector to delete the pods 11/15/23 06:27:14.662
    Nov 15 06:27:14.760: INFO: Deleting ReplicationController wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d took: 27.194211ms
    Nov 15 06:27:14.861: INFO: Terminating ReplicationController wrapped-volume-race-f97548e8-1ce3-4c50-b386-cbdd9c6cf57d pods took: 100.874908ms
    STEP: Creating RC which spawns configmap-volume pods 11/15/23 06:27:17.996
    Nov 15 06:27:18.048: INFO: Pod name wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862: Found 0 pods out of 5
    Nov 15 06:27:23.095: INFO: Pod name wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862: Found 5 pods out of 5
    STEP: Ensuring each pod is running 11/15/23 06:27:23.095
    Nov 15 06:27:23.095: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-kqnwd" in namespace "emptydir-wrapper-1282" to be "running"
    Nov 15 06:27:23.113: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-kqnwd": Phase="Running", Reason="", readiness=true. Elapsed: 17.697738ms
    Nov 15 06:27:23.113: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-kqnwd" satisfied condition "running"
    Nov 15 06:27:23.113: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-kwl94" in namespace "emptydir-wrapper-1282" to be "running"
    Nov 15 06:27:23.142: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-kwl94": Phase="Running", Reason="", readiness=true. Elapsed: 29.209555ms
    Nov 15 06:27:23.142: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-kwl94" satisfied condition "running"
    Nov 15 06:27:23.143: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-mv2pb" in namespace "emptydir-wrapper-1282" to be "running"
    Nov 15 06:27:23.164: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-mv2pb": Phase="Running", Reason="", readiness=true. Elapsed: 21.314865ms
    Nov 15 06:27:23.164: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-mv2pb" satisfied condition "running"
    Nov 15 06:27:23.164: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-w7rpk" in namespace "emptydir-wrapper-1282" to be "running"
    Nov 15 06:27:23.187: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-w7rpk": Phase="Running", Reason="", readiness=true. Elapsed: 23.259442ms
    Nov 15 06:27:23.187: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-w7rpk" satisfied condition "running"
    Nov 15 06:27:23.187: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-xmdzh" in namespace "emptydir-wrapper-1282" to be "running"
    Nov 15 06:27:23.207: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-xmdzh": Phase="Running", Reason="", readiness=true. Elapsed: 19.907835ms
    Nov 15 06:27:23.207: INFO: Pod "wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862-xmdzh" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862 in namespace emptydir-wrapper-1282, will wait for the garbage collector to delete the pods 11/15/23 06:27:23.207
    Nov 15 06:27:23.302: INFO: Deleting ReplicationController wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862 took: 26.631405ms
    Nov 15 06:27:23.503: INFO: Terminating ReplicationController wrapped-volume-race-9b5bed93-41f1-4234-bb0a-dba49aab4862 pods took: 200.958475ms
    STEP: Cleaning up the configMaps 11/15/23 06:27:26.803
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:27:28.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1282" for this suite. 11/15/23 06:27:28.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:27:28.269
Nov 15 06:27:28.269: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename job 11/15/23 06:27:28.27
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:27:28.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:27:28.337
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 11/15/23 06:27:28.353
W1115 06:27:28.372951      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism 11/15/23 06:27:28.373
STEP: Orphaning one of the Job's Pods 11/15/23 06:27:32.39
Nov 15 06:27:32.964: INFO: Successfully updated pod "adopt-release-dcq9b"
STEP: Checking that the Job readopts the Pod 11/15/23 06:27:32.965
Nov 15 06:27:32.965: INFO: Waiting up to 15m0s for pod "adopt-release-dcq9b" in namespace "job-3939" to be "adopted"
Nov 15 06:27:32.982: INFO: Pod "adopt-release-dcq9b": Phase="Running", Reason="", readiness=true. Elapsed: 17.29266ms
Nov 15 06:27:35.002: INFO: Pod "adopt-release-dcq9b": Phase="Running", Reason="", readiness=true. Elapsed: 2.036918605s
Nov 15 06:27:35.002: INFO: Pod "adopt-release-dcq9b" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 11/15/23 06:27:35.002
Nov 15 06:27:35.554: INFO: Successfully updated pod "adopt-release-dcq9b"
STEP: Checking that the Job releases the Pod 11/15/23 06:27:35.554
Nov 15 06:27:35.554: INFO: Waiting up to 15m0s for pod "adopt-release-dcq9b" in namespace "job-3939" to be "released"
Nov 15 06:27:35.572: INFO: Pod "adopt-release-dcq9b": Phase="Running", Reason="", readiness=true. Elapsed: 18.046958ms
Nov 15 06:27:37.622: INFO: Pod "adopt-release-dcq9b": Phase="Running", Reason="", readiness=true. Elapsed: 2.068222601s
Nov 15 06:27:37.622: INFO: Pod "adopt-release-dcq9b" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Nov 15 06:27:37.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3939" for this suite. 11/15/23 06:27:37.693
------------------------------
â€¢ [SLOW TEST] [9.450 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:27:28.269
    Nov 15 06:27:28.269: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename job 11/15/23 06:27:28.27
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:27:28.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:27:28.337
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 11/15/23 06:27:28.353
    W1115 06:27:28.372951      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring active pods == parallelism 11/15/23 06:27:28.373
    STEP: Orphaning one of the Job's Pods 11/15/23 06:27:32.39
    Nov 15 06:27:32.964: INFO: Successfully updated pod "adopt-release-dcq9b"
    STEP: Checking that the Job readopts the Pod 11/15/23 06:27:32.965
    Nov 15 06:27:32.965: INFO: Waiting up to 15m0s for pod "adopt-release-dcq9b" in namespace "job-3939" to be "adopted"
    Nov 15 06:27:32.982: INFO: Pod "adopt-release-dcq9b": Phase="Running", Reason="", readiness=true. Elapsed: 17.29266ms
    Nov 15 06:27:35.002: INFO: Pod "adopt-release-dcq9b": Phase="Running", Reason="", readiness=true. Elapsed: 2.036918605s
    Nov 15 06:27:35.002: INFO: Pod "adopt-release-dcq9b" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 11/15/23 06:27:35.002
    Nov 15 06:27:35.554: INFO: Successfully updated pod "adopt-release-dcq9b"
    STEP: Checking that the Job releases the Pod 11/15/23 06:27:35.554
    Nov 15 06:27:35.554: INFO: Waiting up to 15m0s for pod "adopt-release-dcq9b" in namespace "job-3939" to be "released"
    Nov 15 06:27:35.572: INFO: Pod "adopt-release-dcq9b": Phase="Running", Reason="", readiness=true. Elapsed: 18.046958ms
    Nov 15 06:27:37.622: INFO: Pod "adopt-release-dcq9b": Phase="Running", Reason="", readiness=true. Elapsed: 2.068222601s
    Nov 15 06:27:37.622: INFO: Pod "adopt-release-dcq9b" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:27:37.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3939" for this suite. 11/15/23 06:27:37.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:27:37.722
Nov 15 06:27:37.722: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename svcaccounts 11/15/23 06:27:37.723
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:27:37.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:27:37.816
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Nov 15 06:27:37.879: INFO: created pod
Nov 15 06:27:37.879: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8608" to be "Succeeded or Failed"
Nov 15 06:27:37.898: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 19.901896ms
Nov 15 06:27:39.918: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039372489s
Nov 15 06:27:41.921: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042066282s
Nov 15 06:27:43.919: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040019803s
STEP: Saw pod success 11/15/23 06:27:43.919
Nov 15 06:27:43.919: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Nov 15 06:28:13.919: INFO: polling logs
Nov 15 06:28:13.964: INFO: Pod logs: 
I1115 06:27:39.280441       1 log.go:198] OK: Got token
I1115 06:27:39.280552       1 log.go:198] validating with in-cluster discovery
I1115 06:27:39.281090       1 log.go:198] OK: got issuer https://kubernetes.default.svc
I1115 06:27:39.281124       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-8608:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1700030258, NotBefore:1700029658, IssuedAt:1700029658, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8608", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"0aa7bd24-7b11-4b89-bfbb-db0ca59ba3c6"}}}
I1115 06:27:39.297734       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I1115 06:27:39.314480       1 log.go:198] OK: Validated signature on JWT
I1115 06:27:39.314563       1 log.go:198] OK: Got valid claims from token!
I1115 06:27:39.314581       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-8608:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1700030258, NotBefore:1700029658, IssuedAt:1700029658, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8608", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"0aa7bd24-7b11-4b89-bfbb-db0ca59ba3c6"}}}

Nov 15 06:28:13.964: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 15 06:28:13.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8608" for this suite. 11/15/23 06:28:14.012
------------------------------
â€¢ [SLOW TEST] [36.316 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:27:37.722
    Nov 15 06:27:37.722: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename svcaccounts 11/15/23 06:27:37.723
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:27:37.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:27:37.816
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Nov 15 06:27:37.879: INFO: created pod
    Nov 15 06:27:37.879: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8608" to be "Succeeded or Failed"
    Nov 15 06:27:37.898: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 19.901896ms
    Nov 15 06:27:39.918: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039372489s
    Nov 15 06:27:41.921: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042066282s
    Nov 15 06:27:43.919: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040019803s
    STEP: Saw pod success 11/15/23 06:27:43.919
    Nov 15 06:27:43.919: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Nov 15 06:28:13.919: INFO: polling logs
    Nov 15 06:28:13.964: INFO: Pod logs: 
    I1115 06:27:39.280441       1 log.go:198] OK: Got token
    I1115 06:27:39.280552       1 log.go:198] validating with in-cluster discovery
    I1115 06:27:39.281090       1 log.go:198] OK: got issuer https://kubernetes.default.svc
    I1115 06:27:39.281124       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-8608:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1700030258, NotBefore:1700029658, IssuedAt:1700029658, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8608", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"0aa7bd24-7b11-4b89-bfbb-db0ca59ba3c6"}}}
    I1115 06:27:39.297734       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I1115 06:27:39.314480       1 log.go:198] OK: Validated signature on JWT
    I1115 06:27:39.314563       1 log.go:198] OK: Got valid claims from token!
    I1115 06:27:39.314581       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-8608:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1700030258, NotBefore:1700029658, IssuedAt:1700029658, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8608", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"0aa7bd24-7b11-4b89-bfbb-db0ca59ba3c6"}}}

    Nov 15 06:28:13.964: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:28:13.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8608" for this suite. 11/15/23 06:28:14.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:28:14.039
Nov 15 06:28:14.039: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 06:28:14.04
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:28:14.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:28:14.121
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 11/15/23 06:28:14.132
W1115 06:28:14.178022      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Nov 15 06:28:14.178: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b0fa363a-783a-4e91-b5d5-50cd2a55052a" in namespace "projected-1395" to be "Succeeded or Failed"
Nov 15 06:28:14.208: INFO: Pod "downwardapi-volume-b0fa363a-783a-4e91-b5d5-50cd2a55052a": Phase="Pending", Reason="", readiness=false. Elapsed: 30.318767ms
Nov 15 06:28:16.227: INFO: Pod "downwardapi-volume-b0fa363a-783a-4e91-b5d5-50cd2a55052a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049263612s
Nov 15 06:28:18.232: INFO: Pod "downwardapi-volume-b0fa363a-783a-4e91-b5d5-50cd2a55052a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053930063s
Nov 15 06:28:20.230: INFO: Pod "downwardapi-volume-b0fa363a-783a-4e91-b5d5-50cd2a55052a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052464379s
STEP: Saw pod success 11/15/23 06:28:20.23
Nov 15 06:28:20.230: INFO: Pod "downwardapi-volume-b0fa363a-783a-4e91-b5d5-50cd2a55052a" satisfied condition "Succeeded or Failed"
Nov 15 06:28:20.248: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-b0fa363a-783a-4e91-b5d5-50cd2a55052a container client-container: <nil>
STEP: delete the pod 11/15/23 06:28:20.294
Nov 15 06:28:20.339: INFO: Waiting for pod downwardapi-volume-b0fa363a-783a-4e91-b5d5-50cd2a55052a to disappear
Nov 15 06:28:20.357: INFO: Pod downwardapi-volume-b0fa363a-783a-4e91-b5d5-50cd2a55052a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 06:28:20.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1395" for this suite. 11/15/23 06:28:20.387
------------------------------
â€¢ [SLOW TEST] [6.375 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:28:14.039
    Nov 15 06:28:14.039: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 06:28:14.04
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:28:14.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:28:14.121
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 11/15/23 06:28:14.132
    W1115 06:28:14.178022      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Nov 15 06:28:14.178: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b0fa363a-783a-4e91-b5d5-50cd2a55052a" in namespace "projected-1395" to be "Succeeded or Failed"
    Nov 15 06:28:14.208: INFO: Pod "downwardapi-volume-b0fa363a-783a-4e91-b5d5-50cd2a55052a": Phase="Pending", Reason="", readiness=false. Elapsed: 30.318767ms
    Nov 15 06:28:16.227: INFO: Pod "downwardapi-volume-b0fa363a-783a-4e91-b5d5-50cd2a55052a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049263612s
    Nov 15 06:28:18.232: INFO: Pod "downwardapi-volume-b0fa363a-783a-4e91-b5d5-50cd2a55052a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053930063s
    Nov 15 06:28:20.230: INFO: Pod "downwardapi-volume-b0fa363a-783a-4e91-b5d5-50cd2a55052a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052464379s
    STEP: Saw pod success 11/15/23 06:28:20.23
    Nov 15 06:28:20.230: INFO: Pod "downwardapi-volume-b0fa363a-783a-4e91-b5d5-50cd2a55052a" satisfied condition "Succeeded or Failed"
    Nov 15 06:28:20.248: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-b0fa363a-783a-4e91-b5d5-50cd2a55052a container client-container: <nil>
    STEP: delete the pod 11/15/23 06:28:20.294
    Nov 15 06:28:20.339: INFO: Waiting for pod downwardapi-volume-b0fa363a-783a-4e91-b5d5-50cd2a55052a to disappear
    Nov 15 06:28:20.357: INFO: Pod downwardapi-volume-b0fa363a-783a-4e91-b5d5-50cd2a55052a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:28:20.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1395" for this suite. 11/15/23 06:28:20.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:28:20.416
Nov 15 06:28:20.416: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename configmap 11/15/23 06:28:20.417
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:28:20.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:28:20.491
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-c1a1651e-58f0-4566-9eaa-aab0f0806bcb 11/15/23 06:28:20.504
STEP: Creating a pod to test consume configMaps 11/15/23 06:28:20.531
Nov 15 06:28:20.583: INFO: Waiting up to 5m0s for pod "pod-configmaps-83cb0f70-a6b5-4c34-99e1-696afb1f0b12" in namespace "configmap-7740" to be "Succeeded or Failed"
Nov 15 06:28:20.607: INFO: Pod "pod-configmaps-83cb0f70-a6b5-4c34-99e1-696afb1f0b12": Phase="Pending", Reason="", readiness=false. Elapsed: 24.151397ms
Nov 15 06:28:22.629: INFO: Pod "pod-configmaps-83cb0f70-a6b5-4c34-99e1-696afb1f0b12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046396744s
Nov 15 06:28:24.625: INFO: Pod "pod-configmaps-83cb0f70-a6b5-4c34-99e1-696afb1f0b12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042800691s
STEP: Saw pod success 11/15/23 06:28:24.625
Nov 15 06:28:24.626: INFO: Pod "pod-configmaps-83cb0f70-a6b5-4c34-99e1-696afb1f0b12" satisfied condition "Succeeded or Failed"
Nov 15 06:28:24.644: INFO: Trying to get logs from node 10.72.152.86 pod pod-configmaps-83cb0f70-a6b5-4c34-99e1-696afb1f0b12 container agnhost-container: <nil>
STEP: delete the pod 11/15/23 06:28:24.687
Nov 15 06:28:24.748: INFO: Waiting for pod pod-configmaps-83cb0f70-a6b5-4c34-99e1-696afb1f0b12 to disappear
Nov 15 06:28:24.765: INFO: Pod pod-configmaps-83cb0f70-a6b5-4c34-99e1-696afb1f0b12 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:28:24.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7740" for this suite. 11/15/23 06:28:24.795
------------------------------
â€¢ [4.405 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:28:20.416
    Nov 15 06:28:20.416: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename configmap 11/15/23 06:28:20.417
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:28:20.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:28:20.491
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-c1a1651e-58f0-4566-9eaa-aab0f0806bcb 11/15/23 06:28:20.504
    STEP: Creating a pod to test consume configMaps 11/15/23 06:28:20.531
    Nov 15 06:28:20.583: INFO: Waiting up to 5m0s for pod "pod-configmaps-83cb0f70-a6b5-4c34-99e1-696afb1f0b12" in namespace "configmap-7740" to be "Succeeded or Failed"
    Nov 15 06:28:20.607: INFO: Pod "pod-configmaps-83cb0f70-a6b5-4c34-99e1-696afb1f0b12": Phase="Pending", Reason="", readiness=false. Elapsed: 24.151397ms
    Nov 15 06:28:22.629: INFO: Pod "pod-configmaps-83cb0f70-a6b5-4c34-99e1-696afb1f0b12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046396744s
    Nov 15 06:28:24.625: INFO: Pod "pod-configmaps-83cb0f70-a6b5-4c34-99e1-696afb1f0b12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042800691s
    STEP: Saw pod success 11/15/23 06:28:24.625
    Nov 15 06:28:24.626: INFO: Pod "pod-configmaps-83cb0f70-a6b5-4c34-99e1-696afb1f0b12" satisfied condition "Succeeded or Failed"
    Nov 15 06:28:24.644: INFO: Trying to get logs from node 10.72.152.86 pod pod-configmaps-83cb0f70-a6b5-4c34-99e1-696afb1f0b12 container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 06:28:24.687
    Nov 15 06:28:24.748: INFO: Waiting for pod pod-configmaps-83cb0f70-a6b5-4c34-99e1-696afb1f0b12 to disappear
    Nov 15 06:28:24.765: INFO: Pod pod-configmaps-83cb0f70-a6b5-4c34-99e1-696afb1f0b12 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:28:24.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7740" for this suite. 11/15/23 06:28:24.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:28:24.821
Nov 15 06:28:24.821: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename container-probe 11/15/23 06:28:24.822
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:28:24.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:28:24.9
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Nov 15 06:28:24.945: INFO: Waiting up to 5m0s for pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b" in namespace "container-probe-752" to be "running and ready"
Nov 15 06:28:24.969: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.32411ms
Nov 15 06:28:24.969: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:28:26.989: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044063095s
Nov 15 06:28:26.989: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:28:28.992: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=false. Elapsed: 4.047767164s
Nov 15 06:28:28.992: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = false)
Nov 15 06:28:30.988: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=false. Elapsed: 6.043302379s
Nov 15 06:28:30.988: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = false)
Nov 15 06:28:32.990: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=false. Elapsed: 8.045406658s
Nov 15 06:28:32.990: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = false)
Nov 15 06:28:34.988: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=false. Elapsed: 10.04294917s
Nov 15 06:28:34.988: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = false)
Nov 15 06:28:36.989: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=false. Elapsed: 12.044104597s
Nov 15 06:28:36.989: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = false)
Nov 15 06:28:38.994: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=false. Elapsed: 14.048864549s
Nov 15 06:28:38.994: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = false)
Nov 15 06:28:40.988: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=false. Elapsed: 16.042909655s
Nov 15 06:28:40.988: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = false)
Nov 15 06:28:42.999: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=false. Elapsed: 18.054556913s
Nov 15 06:28:42.999: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = false)
Nov 15 06:28:44.991: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=false. Elapsed: 20.046677093s
Nov 15 06:28:44.991: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = false)
Nov 15 06:28:46.989: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=true. Elapsed: 22.044365402s
Nov 15 06:28:46.989: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = true)
Nov 15 06:28:46.989: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b" satisfied condition "running and ready"
Nov 15 06:28:47.008: INFO: Container started at 2023-11-15 06:28:26 +0000 UTC, pod became ready at 2023-11-15 06:28:45 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 15 06:28:47.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-752" for this suite. 11/15/23 06:28:47.04
------------------------------
â€¢ [SLOW TEST] [22.244 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:28:24.821
    Nov 15 06:28:24.821: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename container-probe 11/15/23 06:28:24.822
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:28:24.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:28:24.9
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Nov 15 06:28:24.945: INFO: Waiting up to 5m0s for pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b" in namespace "container-probe-752" to be "running and ready"
    Nov 15 06:28:24.969: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.32411ms
    Nov 15 06:28:24.969: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:28:26.989: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044063095s
    Nov 15 06:28:26.989: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:28:28.992: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=false. Elapsed: 4.047767164s
    Nov 15 06:28:28.992: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = false)
    Nov 15 06:28:30.988: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=false. Elapsed: 6.043302379s
    Nov 15 06:28:30.988: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = false)
    Nov 15 06:28:32.990: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=false. Elapsed: 8.045406658s
    Nov 15 06:28:32.990: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = false)
    Nov 15 06:28:34.988: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=false. Elapsed: 10.04294917s
    Nov 15 06:28:34.988: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = false)
    Nov 15 06:28:36.989: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=false. Elapsed: 12.044104597s
    Nov 15 06:28:36.989: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = false)
    Nov 15 06:28:38.994: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=false. Elapsed: 14.048864549s
    Nov 15 06:28:38.994: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = false)
    Nov 15 06:28:40.988: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=false. Elapsed: 16.042909655s
    Nov 15 06:28:40.988: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = false)
    Nov 15 06:28:42.999: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=false. Elapsed: 18.054556913s
    Nov 15 06:28:42.999: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = false)
    Nov 15 06:28:44.991: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=false. Elapsed: 20.046677093s
    Nov 15 06:28:44.991: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = false)
    Nov 15 06:28:46.989: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b": Phase="Running", Reason="", readiness=true. Elapsed: 22.044365402s
    Nov 15 06:28:46.989: INFO: The phase of Pod test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b is Running (Ready = true)
    Nov 15 06:28:46.989: INFO: Pod "test-webserver-f9919904-9edf-475d-a9de-ab76e4b1361b" satisfied condition "running and ready"
    Nov 15 06:28:47.008: INFO: Container started at 2023-11-15 06:28:26 +0000 UTC, pod became ready at 2023-11-15 06:28:45 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:28:47.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-752" for this suite. 11/15/23 06:28:47.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:28:47.066
Nov 15 06:28:47.066: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename configmap 11/15/23 06:28:47.067
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:28:47.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:28:47.132
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
Nov 15 06:28:47.164: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-53d4d935-4993-4b13-98c8-f2977120cf18 11/15/23 06:28:47.164
STEP: Creating the pod 11/15/23 06:28:47.186
Nov 15 06:28:47.227: INFO: Waiting up to 5m0s for pod "pod-configmaps-ab92a052-eb97-4b07-a53c-ed683bad8599" in namespace "configmap-5212" to be "running"
Nov 15 06:28:47.246: INFO: Pod "pod-configmaps-ab92a052-eb97-4b07-a53c-ed683bad8599": Phase="Pending", Reason="", readiness=false. Elapsed: 19.534991ms
Nov 15 06:28:49.269: INFO: Pod "pod-configmaps-ab92a052-eb97-4b07-a53c-ed683bad8599": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041829791s
Nov 15 06:28:51.267: INFO: Pod "pod-configmaps-ab92a052-eb97-4b07-a53c-ed683bad8599": Phase="Running", Reason="", readiness=false. Elapsed: 4.040507471s
Nov 15 06:28:51.267: INFO: Pod "pod-configmaps-ab92a052-eb97-4b07-a53c-ed683bad8599" satisfied condition "running"
STEP: Waiting for pod with text data 11/15/23 06:28:51.267
STEP: Waiting for pod with binary data 11/15/23 06:28:51.336
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:28:51.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5212" for this suite. 11/15/23 06:28:51.413
------------------------------
â€¢ [4.373 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:28:47.066
    Nov 15 06:28:47.066: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename configmap 11/15/23 06:28:47.067
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:28:47.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:28:47.132
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    Nov 15 06:28:47.164: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name configmap-test-upd-53d4d935-4993-4b13-98c8-f2977120cf18 11/15/23 06:28:47.164
    STEP: Creating the pod 11/15/23 06:28:47.186
    Nov 15 06:28:47.227: INFO: Waiting up to 5m0s for pod "pod-configmaps-ab92a052-eb97-4b07-a53c-ed683bad8599" in namespace "configmap-5212" to be "running"
    Nov 15 06:28:47.246: INFO: Pod "pod-configmaps-ab92a052-eb97-4b07-a53c-ed683bad8599": Phase="Pending", Reason="", readiness=false. Elapsed: 19.534991ms
    Nov 15 06:28:49.269: INFO: Pod "pod-configmaps-ab92a052-eb97-4b07-a53c-ed683bad8599": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041829791s
    Nov 15 06:28:51.267: INFO: Pod "pod-configmaps-ab92a052-eb97-4b07-a53c-ed683bad8599": Phase="Running", Reason="", readiness=false. Elapsed: 4.040507471s
    Nov 15 06:28:51.267: INFO: Pod "pod-configmaps-ab92a052-eb97-4b07-a53c-ed683bad8599" satisfied condition "running"
    STEP: Waiting for pod with text data 11/15/23 06:28:51.267
    STEP: Waiting for pod with binary data 11/15/23 06:28:51.336
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:28:51.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5212" for this suite. 11/15/23 06:28:51.413
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:28:51.441
Nov 15 06:28:51.441: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename events 11/15/23 06:28:51.441
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:28:51.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:28:51.506
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 11/15/23 06:28:51.519
STEP: get a list of Events with a label in the current namespace 11/15/23 06:28:51.589
STEP: delete a list of events 11/15/23 06:28:51.622
Nov 15 06:28:51.623: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 11/15/23 06:28:51.734
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Nov 15 06:28:51.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-5770" for this suite. 11/15/23 06:28:51.772
------------------------------
â€¢ [0.362 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:28:51.441
    Nov 15 06:28:51.441: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename events 11/15/23 06:28:51.441
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:28:51.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:28:51.506
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 11/15/23 06:28:51.519
    STEP: get a list of Events with a label in the current namespace 11/15/23 06:28:51.589
    STEP: delete a list of events 11/15/23 06:28:51.622
    Nov 15 06:28:51.623: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 11/15/23 06:28:51.734
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:28:51.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-5770" for this suite. 11/15/23 06:28:51.772
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:28:51.803
Nov 15 06:28:51.803: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename downward-api 11/15/23 06:28:51.804
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:28:51.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:28:51.867
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 11/15/23 06:28:51.884
W1115 06:28:51.930803      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Nov 15 06:28:51.930: INFO: Waiting up to 5m0s for pod "annotationupdate8a2e79d1-f241-4d6a-ac12-e0a918667d0d" in namespace "downward-api-541" to be "running and ready"
Nov 15 06:28:51.949: INFO: Pod "annotationupdate8a2e79d1-f241-4d6a-ac12-e0a918667d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.730943ms
Nov 15 06:28:51.949: INFO: The phase of Pod annotationupdate8a2e79d1-f241-4d6a-ac12-e0a918667d0d is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:28:53.982: INFO: Pod "annotationupdate8a2e79d1-f241-4d6a-ac12-e0a918667d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051670835s
Nov 15 06:28:53.982: INFO: The phase of Pod annotationupdate8a2e79d1-f241-4d6a-ac12-e0a918667d0d is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:28:55.974: INFO: Pod "annotationupdate8a2e79d1-f241-4d6a-ac12-e0a918667d0d": Phase="Running", Reason="", readiness=true. Elapsed: 4.043185408s
Nov 15 06:28:55.974: INFO: The phase of Pod annotationupdate8a2e79d1-f241-4d6a-ac12-e0a918667d0d is Running (Ready = true)
Nov 15 06:28:55.974: INFO: Pod "annotationupdate8a2e79d1-f241-4d6a-ac12-e0a918667d0d" satisfied condition "running and ready"
Nov 15 06:28:56.616: INFO: Successfully updated pod "annotationupdate8a2e79d1-f241-4d6a-ac12-e0a918667d0d"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 06:28:58.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-541" for this suite. 11/15/23 06:28:58.788
------------------------------
â€¢ [SLOW TEST] [7.010 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:28:51.803
    Nov 15 06:28:51.803: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename downward-api 11/15/23 06:28:51.804
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:28:51.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:28:51.867
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 11/15/23 06:28:51.884
    W1115 06:28:51.930803      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Nov 15 06:28:51.930: INFO: Waiting up to 5m0s for pod "annotationupdate8a2e79d1-f241-4d6a-ac12-e0a918667d0d" in namespace "downward-api-541" to be "running and ready"
    Nov 15 06:28:51.949: INFO: Pod "annotationupdate8a2e79d1-f241-4d6a-ac12-e0a918667d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.730943ms
    Nov 15 06:28:51.949: INFO: The phase of Pod annotationupdate8a2e79d1-f241-4d6a-ac12-e0a918667d0d is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:28:53.982: INFO: Pod "annotationupdate8a2e79d1-f241-4d6a-ac12-e0a918667d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051670835s
    Nov 15 06:28:53.982: INFO: The phase of Pod annotationupdate8a2e79d1-f241-4d6a-ac12-e0a918667d0d is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:28:55.974: INFO: Pod "annotationupdate8a2e79d1-f241-4d6a-ac12-e0a918667d0d": Phase="Running", Reason="", readiness=true. Elapsed: 4.043185408s
    Nov 15 06:28:55.974: INFO: The phase of Pod annotationupdate8a2e79d1-f241-4d6a-ac12-e0a918667d0d is Running (Ready = true)
    Nov 15 06:28:55.974: INFO: Pod "annotationupdate8a2e79d1-f241-4d6a-ac12-e0a918667d0d" satisfied condition "running and ready"
    Nov 15 06:28:56.616: INFO: Successfully updated pod "annotationupdate8a2e79d1-f241-4d6a-ac12-e0a918667d0d"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:28:58.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-541" for this suite. 11/15/23 06:28:58.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:28:58.815
Nov 15 06:28:58.815: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename dns 11/15/23 06:28:58.816
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:28:58.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:28:58.885
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 11/15/23 06:28:58.895
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4380 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4380;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4380 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4380;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4380.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4380.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4380.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4380.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4380.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4380.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4380.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4380.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4380.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4380.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4380.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4380.svc;check="$$(dig +notcp +noall +answer +search 132.94.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.94.132_udp@PTR;check="$$(dig +tcp +noall +answer +search 132.94.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.94.132_tcp@PTR;sleep 1; done
 11/15/23 06:28:58.961
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4380 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4380;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4380 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4380;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4380.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4380.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4380.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4380.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4380.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4380.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4380.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4380.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4380.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4380.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4380.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4380.svc;check="$$(dig +notcp +noall +answer +search 132.94.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.94.132_udp@PTR;check="$$(dig +tcp +noall +answer +search 132.94.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.94.132_tcp@PTR;sleep 1; done
 11/15/23 06:28:58.961
STEP: creating a pod to probe DNS 11/15/23 06:28:58.961
STEP: submitting the pod to kubernetes 11/15/23 06:28:58.961
Nov 15 06:28:58.995: INFO: Waiting up to 15m0s for pod "dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe" in namespace "dns-4380" to be "running"
Nov 15 06:28:59.013: INFO: Pod "dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 18.447688ms
Nov 15 06:29:01.039: INFO: Pod "dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe": Phase="Running", Reason="", readiness=true. Elapsed: 2.043964023s
Nov 15 06:29:01.039: INFO: Pod "dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe" satisfied condition "running"
STEP: retrieving the pod 11/15/23 06:29:01.039
STEP: looking for the results for each expected name from probers 11/15/23 06:29:01.062
Nov 15 06:29:01.132: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
Nov 15 06:29:01.164: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
Nov 15 06:29:01.207: INFO: Unable to read wheezy_udp@dns-test-service.dns-4380 from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
Nov 15 06:29:01.242: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4380 from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
Nov 15 06:29:01.268: INFO: Unable to read wheezy_udp@dns-test-service.dns-4380.svc from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
Nov 15 06:29:01.298: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4380.svc from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
Nov 15 06:29:01.367: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4380.svc from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
Nov 15 06:29:01.395: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4380.svc from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
Nov 15 06:29:01.569: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
Nov 15 06:29:01.600: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
Nov 15 06:29:01.638: INFO: Unable to read jessie_udp@dns-test-service.dns-4380 from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
Nov 15 06:29:01.678: INFO: Unable to read jessie_tcp@dns-test-service.dns-4380 from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
Nov 15 06:29:01.709: INFO: Unable to read jessie_udp@dns-test-service.dns-4380.svc from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
Nov 15 06:29:01.732: INFO: Unable to read jessie_tcp@dns-test-service.dns-4380.svc from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
Nov 15 06:29:01.761: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4380.svc from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
Nov 15 06:29:01.788: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4380.svc from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
Nov 15 06:29:01.938: INFO: Lookups using dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4380 wheezy_tcp@dns-test-service.dns-4380 wheezy_udp@dns-test-service.dns-4380.svc wheezy_tcp@dns-test-service.dns-4380.svc wheezy_udp@_http._tcp.dns-test-service.dns-4380.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4380.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4380 jessie_tcp@dns-test-service.dns-4380 jessie_udp@dns-test-service.dns-4380.svc jessie_tcp@dns-test-service.dns-4380.svc jessie_udp@_http._tcp.dns-test-service.dns-4380.svc jessie_tcp@_http._tcp.dns-test-service.dns-4380.svc]

Nov 15 06:29:07.780: INFO: DNS probes using dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe succeeded

STEP: deleting the pod 11/15/23 06:29:07.78
STEP: deleting the test service 11/15/23 06:29:07.837
STEP: deleting the test headless service 11/15/23 06:29:07.906
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 15 06:29:07.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4380" for this suite. 11/15/23 06:29:08.022
------------------------------
â€¢ [SLOW TEST] [9.232 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:28:58.815
    Nov 15 06:28:58.815: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename dns 11/15/23 06:28:58.816
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:28:58.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:28:58.885
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 11/15/23 06:28:58.895
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4380 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4380;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4380 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4380;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4380.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4380.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4380.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4380.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4380.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4380.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4380.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4380.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4380.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4380.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4380.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4380.svc;check="$$(dig +notcp +noall +answer +search 132.94.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.94.132_udp@PTR;check="$$(dig +tcp +noall +answer +search 132.94.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.94.132_tcp@PTR;sleep 1; done
     11/15/23 06:28:58.961
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4380 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4380;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4380 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4380;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4380.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4380.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4380.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4380.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4380.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4380.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4380.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4380.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4380.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4380.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4380.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4380.svc;check="$$(dig +notcp +noall +answer +search 132.94.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.94.132_udp@PTR;check="$$(dig +tcp +noall +answer +search 132.94.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.94.132_tcp@PTR;sleep 1; done
     11/15/23 06:28:58.961
    STEP: creating a pod to probe DNS 11/15/23 06:28:58.961
    STEP: submitting the pod to kubernetes 11/15/23 06:28:58.961
    Nov 15 06:28:58.995: INFO: Waiting up to 15m0s for pod "dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe" in namespace "dns-4380" to be "running"
    Nov 15 06:28:59.013: INFO: Pod "dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 18.447688ms
    Nov 15 06:29:01.039: INFO: Pod "dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe": Phase="Running", Reason="", readiness=true. Elapsed: 2.043964023s
    Nov 15 06:29:01.039: INFO: Pod "dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe" satisfied condition "running"
    STEP: retrieving the pod 11/15/23 06:29:01.039
    STEP: looking for the results for each expected name from probers 11/15/23 06:29:01.062
    Nov 15 06:29:01.132: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
    Nov 15 06:29:01.164: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
    Nov 15 06:29:01.207: INFO: Unable to read wheezy_udp@dns-test-service.dns-4380 from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
    Nov 15 06:29:01.242: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4380 from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
    Nov 15 06:29:01.268: INFO: Unable to read wheezy_udp@dns-test-service.dns-4380.svc from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
    Nov 15 06:29:01.298: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4380.svc from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
    Nov 15 06:29:01.367: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4380.svc from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
    Nov 15 06:29:01.395: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4380.svc from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
    Nov 15 06:29:01.569: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
    Nov 15 06:29:01.600: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
    Nov 15 06:29:01.638: INFO: Unable to read jessie_udp@dns-test-service.dns-4380 from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
    Nov 15 06:29:01.678: INFO: Unable to read jessie_tcp@dns-test-service.dns-4380 from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
    Nov 15 06:29:01.709: INFO: Unable to read jessie_udp@dns-test-service.dns-4380.svc from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
    Nov 15 06:29:01.732: INFO: Unable to read jessie_tcp@dns-test-service.dns-4380.svc from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
    Nov 15 06:29:01.761: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4380.svc from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
    Nov 15 06:29:01.788: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4380.svc from pod dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe: the server could not find the requested resource (get pods dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe)
    Nov 15 06:29:01.938: INFO: Lookups using dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4380 wheezy_tcp@dns-test-service.dns-4380 wheezy_udp@dns-test-service.dns-4380.svc wheezy_tcp@dns-test-service.dns-4380.svc wheezy_udp@_http._tcp.dns-test-service.dns-4380.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4380.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4380 jessie_tcp@dns-test-service.dns-4380 jessie_udp@dns-test-service.dns-4380.svc jessie_tcp@dns-test-service.dns-4380.svc jessie_udp@_http._tcp.dns-test-service.dns-4380.svc jessie_tcp@_http._tcp.dns-test-service.dns-4380.svc]

    Nov 15 06:29:07.780: INFO: DNS probes using dns-4380/dns-test-2f4b257f-4c9e-419e-8001-3f9514455bfe succeeded

    STEP: deleting the pod 11/15/23 06:29:07.78
    STEP: deleting the test service 11/15/23 06:29:07.837
    STEP: deleting the test headless service 11/15/23 06:29:07.906
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:29:07.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4380" for this suite. 11/15/23 06:29:08.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:29:08.05
Nov 15 06:29:08.050: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 06:29:08.05
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:29:08.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:29:08.117
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-81db4e36-e6fc-4969-a674-73a005f10064 11/15/23 06:29:08.13
STEP: Creating a pod to test consume secrets 11/15/23 06:29:08.146
Nov 15 06:29:08.190: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-46133014-0d0a-45f1-be16-58141b4ca06d" in namespace "projected-6109" to be "Succeeded or Failed"
Nov 15 06:29:08.208: INFO: Pod "pod-projected-secrets-46133014-0d0a-45f1-be16-58141b4ca06d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.573897ms
Nov 15 06:29:10.229: INFO: Pod "pod-projected-secrets-46133014-0d0a-45f1-be16-58141b4ca06d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038960101s
Nov 15 06:29:12.228: INFO: Pod "pod-projected-secrets-46133014-0d0a-45f1-be16-58141b4ca06d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037927573s
Nov 15 06:29:14.228: INFO: Pod "pod-projected-secrets-46133014-0d0a-45f1-be16-58141b4ca06d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038155875s
STEP: Saw pod success 11/15/23 06:29:14.228
Nov 15 06:29:14.228: INFO: Pod "pod-projected-secrets-46133014-0d0a-45f1-be16-58141b4ca06d" satisfied condition "Succeeded or Failed"
Nov 15 06:29:14.244: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-secrets-46133014-0d0a-45f1-be16-58141b4ca06d container projected-secret-volume-test: <nil>
STEP: delete the pod 11/15/23 06:29:14.289
Nov 15 06:29:14.346: INFO: Waiting for pod pod-projected-secrets-46133014-0d0a-45f1-be16-58141b4ca06d to disappear
Nov 15 06:29:14.365: INFO: Pod pod-projected-secrets-46133014-0d0a-45f1-be16-58141b4ca06d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 15 06:29:14.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6109" for this suite. 11/15/23 06:29:14.441
------------------------------
â€¢ [SLOW TEST] [6.415 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:29:08.05
    Nov 15 06:29:08.050: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 06:29:08.05
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:29:08.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:29:08.117
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-81db4e36-e6fc-4969-a674-73a005f10064 11/15/23 06:29:08.13
    STEP: Creating a pod to test consume secrets 11/15/23 06:29:08.146
    Nov 15 06:29:08.190: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-46133014-0d0a-45f1-be16-58141b4ca06d" in namespace "projected-6109" to be "Succeeded or Failed"
    Nov 15 06:29:08.208: INFO: Pod "pod-projected-secrets-46133014-0d0a-45f1-be16-58141b4ca06d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.573897ms
    Nov 15 06:29:10.229: INFO: Pod "pod-projected-secrets-46133014-0d0a-45f1-be16-58141b4ca06d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038960101s
    Nov 15 06:29:12.228: INFO: Pod "pod-projected-secrets-46133014-0d0a-45f1-be16-58141b4ca06d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037927573s
    Nov 15 06:29:14.228: INFO: Pod "pod-projected-secrets-46133014-0d0a-45f1-be16-58141b4ca06d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038155875s
    STEP: Saw pod success 11/15/23 06:29:14.228
    Nov 15 06:29:14.228: INFO: Pod "pod-projected-secrets-46133014-0d0a-45f1-be16-58141b4ca06d" satisfied condition "Succeeded or Failed"
    Nov 15 06:29:14.244: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-secrets-46133014-0d0a-45f1-be16-58141b4ca06d container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 06:29:14.289
    Nov 15 06:29:14.346: INFO: Waiting for pod pod-projected-secrets-46133014-0d0a-45f1-be16-58141b4ca06d to disappear
    Nov 15 06:29:14.365: INFO: Pod pod-projected-secrets-46133014-0d0a-45f1-be16-58141b4ca06d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:29:14.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6109" for this suite. 11/15/23 06:29:14.441
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:29:14.469
Nov 15 06:29:14.469: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename webhook 11/15/23 06:29:14.47
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:29:14.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:29:14.549
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 06:29:14.617
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:29:15.05
STEP: Deploying the webhook pod 11/15/23 06:29:15.112
STEP: Wait for the deployment to be ready 11/15/23 06:29:15.15
Nov 15 06:29:15.193: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/15/23 06:29:17.237
STEP: Verifying the service has paired with the endpoint 11/15/23 06:29:17.276
Nov 15 06:29:18.277: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 11/15/23 06:29:18.296
STEP: Registering slow webhook via the AdmissionRegistration API 11/15/23 06:29:18.296
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 11/15/23 06:29:18.363
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 11/15/23 06:29:19.411
STEP: Registering slow webhook via the AdmissionRegistration API 11/15/23 06:29:19.411
STEP: Having no error when timeout is longer than webhook latency 11/15/23 06:29:20.538
STEP: Registering slow webhook via the AdmissionRegistration API 11/15/23 06:29:20.539
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 11/15/23 06:29:25.719
STEP: Registering slow webhook via the AdmissionRegistration API 11/15/23 06:29:25.719
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:29:30.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1889" for this suite. 11/15/23 06:29:31.021
STEP: Destroying namespace "webhook-1889-markers" for this suite. 11/15/23 06:29:31.049
------------------------------
â€¢ [SLOW TEST] [16.612 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:29:14.469
    Nov 15 06:29:14.469: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename webhook 11/15/23 06:29:14.47
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:29:14.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:29:14.549
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 06:29:14.617
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:29:15.05
    STEP: Deploying the webhook pod 11/15/23 06:29:15.112
    STEP: Wait for the deployment to be ready 11/15/23 06:29:15.15
    Nov 15 06:29:15.193: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/15/23 06:29:17.237
    STEP: Verifying the service has paired with the endpoint 11/15/23 06:29:17.276
    Nov 15 06:29:18.277: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 11/15/23 06:29:18.296
    STEP: Registering slow webhook via the AdmissionRegistration API 11/15/23 06:29:18.296
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 11/15/23 06:29:18.363
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 11/15/23 06:29:19.411
    STEP: Registering slow webhook via the AdmissionRegistration API 11/15/23 06:29:19.411
    STEP: Having no error when timeout is longer than webhook latency 11/15/23 06:29:20.538
    STEP: Registering slow webhook via the AdmissionRegistration API 11/15/23 06:29:20.539
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 11/15/23 06:29:25.719
    STEP: Registering slow webhook via the AdmissionRegistration API 11/15/23 06:29:25.719
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:29:30.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1889" for this suite. 11/15/23 06:29:31.021
    STEP: Destroying namespace "webhook-1889-markers" for this suite. 11/15/23 06:29:31.049
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:29:31.081
Nov 15 06:29:31.081: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename sched-preemption 11/15/23 06:29:31.082
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:29:31.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:29:31.163
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Nov 15 06:29:31.248: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 15 06:30:31.538: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 11/15/23 06:30:31.569
Nov 15 06:30:31.669: INFO: Created pod: pod0-0-sched-preemption-low-priority
Nov 15 06:30:31.697: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Nov 15 06:30:31.771: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Nov 15 06:30:31.798: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Nov 15 06:30:31.858: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Nov 15 06:30:31.883: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 11/15/23 06:30:31.883
Nov 15 06:30:31.884: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-954" to be "running"
Nov 15 06:30:31.901: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 17.807134ms
Nov 15 06:30:33.922: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.038580218s
Nov 15 06:30:33.922: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Nov 15 06:30:33.922: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-954" to be "running"
Nov 15 06:30:33.941: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.648938ms
Nov 15 06:30:33.941: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Nov 15 06:30:33.941: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-954" to be "running"
Nov 15 06:30:33.957: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 16.543097ms
Nov 15 06:30:33.957: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Nov 15 06:30:33.957: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-954" to be "running"
Nov 15 06:30:33.975: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 17.602959ms
Nov 15 06:30:35.994: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.036657749s
Nov 15 06:30:35.994: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Nov 15 06:30:35.994: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-954" to be "running"
Nov 15 06:30:36.012: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.145321ms
Nov 15 06:30:36.012: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Nov 15 06:30:36.012: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-954" to be "running"
Nov 15 06:30:36.030: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.492568ms
Nov 15 06:30:36.030: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 11/15/23 06:30:36.03
Nov 15 06:30:36.060: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-954" to be "running"
Nov 15 06:30:36.077: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.921018ms
Nov 15 06:30:38.096: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035933104s
Nov 15 06:30:40.097: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037764225s
Nov 15 06:30:42.113: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.052784297s
Nov 15 06:30:42.113: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:30:42.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-954" for this suite. 11/15/23 06:30:42.501
------------------------------
â€¢ [SLOW TEST] [71.450 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:29:31.081
    Nov 15 06:29:31.081: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename sched-preemption 11/15/23 06:29:31.082
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:29:31.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:29:31.163
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Nov 15 06:29:31.248: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 15 06:30:31.538: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 11/15/23 06:30:31.569
    Nov 15 06:30:31.669: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Nov 15 06:30:31.697: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Nov 15 06:30:31.771: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Nov 15 06:30:31.798: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Nov 15 06:30:31.858: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Nov 15 06:30:31.883: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 11/15/23 06:30:31.883
    Nov 15 06:30:31.884: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-954" to be "running"
    Nov 15 06:30:31.901: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 17.807134ms
    Nov 15 06:30:33.922: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.038580218s
    Nov 15 06:30:33.922: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Nov 15 06:30:33.922: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-954" to be "running"
    Nov 15 06:30:33.941: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.648938ms
    Nov 15 06:30:33.941: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Nov 15 06:30:33.941: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-954" to be "running"
    Nov 15 06:30:33.957: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 16.543097ms
    Nov 15 06:30:33.957: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Nov 15 06:30:33.957: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-954" to be "running"
    Nov 15 06:30:33.975: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 17.602959ms
    Nov 15 06:30:35.994: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.036657749s
    Nov 15 06:30:35.994: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Nov 15 06:30:35.994: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-954" to be "running"
    Nov 15 06:30:36.012: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.145321ms
    Nov 15 06:30:36.012: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Nov 15 06:30:36.012: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-954" to be "running"
    Nov 15 06:30:36.030: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.492568ms
    Nov 15 06:30:36.030: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 11/15/23 06:30:36.03
    Nov 15 06:30:36.060: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-954" to be "running"
    Nov 15 06:30:36.077: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.921018ms
    Nov 15 06:30:38.096: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035933104s
    Nov 15 06:30:40.097: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037764225s
    Nov 15 06:30:42.113: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.052784297s
    Nov 15 06:30:42.113: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:30:42.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-954" for this suite. 11/15/23 06:30:42.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:30:42.532
Nov 15 06:30:42.532: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename secrets 11/15/23 06:30:42.533
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:30:42.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:30:42.605
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-ba01021e-98ae-4382-a7fd-9f620656be6a 11/15/23 06:30:42.619
STEP: Creating a pod to test consume secrets 11/15/23 06:30:42.643
Nov 15 06:30:42.683: INFO: Waiting up to 5m0s for pod "pod-secrets-3e3e5c80-d6fb-4856-81cb-18d3a117a6f3" in namespace "secrets-9114" to be "Succeeded or Failed"
Nov 15 06:30:42.702: INFO: Pod "pod-secrets-3e3e5c80-d6fb-4856-81cb-18d3a117a6f3": Phase="Pending", Reason="", readiness=false. Elapsed: 19.22615ms
Nov 15 06:30:44.721: INFO: Pod "pod-secrets-3e3e5c80-d6fb-4856-81cb-18d3a117a6f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038454938s
Nov 15 06:30:46.722: INFO: Pod "pod-secrets-3e3e5c80-d6fb-4856-81cb-18d3a117a6f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039697069s
STEP: Saw pod success 11/15/23 06:30:46.722
Nov 15 06:30:46.723: INFO: Pod "pod-secrets-3e3e5c80-d6fb-4856-81cb-18d3a117a6f3" satisfied condition "Succeeded or Failed"
Nov 15 06:30:46.742: INFO: Trying to get logs from node 10.72.152.86 pod pod-secrets-3e3e5c80-d6fb-4856-81cb-18d3a117a6f3 container secret-volume-test: <nil>
STEP: delete the pod 11/15/23 06:30:46.812
Nov 15 06:30:46.879: INFO: Waiting for pod pod-secrets-3e3e5c80-d6fb-4856-81cb-18d3a117a6f3 to disappear
Nov 15 06:30:46.897: INFO: Pod pod-secrets-3e3e5c80-d6fb-4856-81cb-18d3a117a6f3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 06:30:46.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9114" for this suite. 11/15/23 06:30:46.926
------------------------------
â€¢ [4.419 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:30:42.532
    Nov 15 06:30:42.532: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename secrets 11/15/23 06:30:42.533
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:30:42.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:30:42.605
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-ba01021e-98ae-4382-a7fd-9f620656be6a 11/15/23 06:30:42.619
    STEP: Creating a pod to test consume secrets 11/15/23 06:30:42.643
    Nov 15 06:30:42.683: INFO: Waiting up to 5m0s for pod "pod-secrets-3e3e5c80-d6fb-4856-81cb-18d3a117a6f3" in namespace "secrets-9114" to be "Succeeded or Failed"
    Nov 15 06:30:42.702: INFO: Pod "pod-secrets-3e3e5c80-d6fb-4856-81cb-18d3a117a6f3": Phase="Pending", Reason="", readiness=false. Elapsed: 19.22615ms
    Nov 15 06:30:44.721: INFO: Pod "pod-secrets-3e3e5c80-d6fb-4856-81cb-18d3a117a6f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038454938s
    Nov 15 06:30:46.722: INFO: Pod "pod-secrets-3e3e5c80-d6fb-4856-81cb-18d3a117a6f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039697069s
    STEP: Saw pod success 11/15/23 06:30:46.722
    Nov 15 06:30:46.723: INFO: Pod "pod-secrets-3e3e5c80-d6fb-4856-81cb-18d3a117a6f3" satisfied condition "Succeeded or Failed"
    Nov 15 06:30:46.742: INFO: Trying to get logs from node 10.72.152.86 pod pod-secrets-3e3e5c80-d6fb-4856-81cb-18d3a117a6f3 container secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 06:30:46.812
    Nov 15 06:30:46.879: INFO: Waiting for pod pod-secrets-3e3e5c80-d6fb-4856-81cb-18d3a117a6f3 to disappear
    Nov 15 06:30:46.897: INFO: Pod pod-secrets-3e3e5c80-d6fb-4856-81cb-18d3a117a6f3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:30:46.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9114" for this suite. 11/15/23 06:30:46.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:30:46.953
Nov 15 06:30:46.953: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename resourcequota 11/15/23 06:30:46.954
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:30:47.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:30:47.022
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 11/15/23 06:30:47.033
STEP: Ensuring ResourceQuota status is calculated 11/15/23 06:30:47.061
STEP: Creating a ResourceQuota with not terminating scope 11/15/23 06:30:49.079
STEP: Ensuring ResourceQuota status is calculated 11/15/23 06:30:49.1
STEP: Creating a long running pod 11/15/23 06:30:51.15
STEP: Ensuring resource quota with not terminating scope captures the pod usage 11/15/23 06:30:51.201
STEP: Ensuring resource quota with terminating scope ignored the pod usage 11/15/23 06:30:53.217
STEP: Deleting the pod 11/15/23 06:30:55.236
STEP: Ensuring resource quota status released the pod usage 11/15/23 06:30:55.281
STEP: Creating a terminating pod 11/15/23 06:30:57.298
STEP: Ensuring resource quota with terminating scope captures the pod usage 11/15/23 06:30:57.34
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 11/15/23 06:30:59.379
STEP: Deleting the pod 11/15/23 06:31:01.401
STEP: Ensuring resource quota status released the pod usage 11/15/23 06:31:01.456
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 06:31:03.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4622" for this suite. 11/15/23 06:31:03.506
------------------------------
â€¢ [SLOW TEST] [16.581 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:30:46.953
    Nov 15 06:30:46.953: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename resourcequota 11/15/23 06:30:46.954
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:30:47.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:30:47.022
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 11/15/23 06:30:47.033
    STEP: Ensuring ResourceQuota status is calculated 11/15/23 06:30:47.061
    STEP: Creating a ResourceQuota with not terminating scope 11/15/23 06:30:49.079
    STEP: Ensuring ResourceQuota status is calculated 11/15/23 06:30:49.1
    STEP: Creating a long running pod 11/15/23 06:30:51.15
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 11/15/23 06:30:51.201
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 11/15/23 06:30:53.217
    STEP: Deleting the pod 11/15/23 06:30:55.236
    STEP: Ensuring resource quota status released the pod usage 11/15/23 06:30:55.281
    STEP: Creating a terminating pod 11/15/23 06:30:57.298
    STEP: Ensuring resource quota with terminating scope captures the pod usage 11/15/23 06:30:57.34
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 11/15/23 06:30:59.379
    STEP: Deleting the pod 11/15/23 06:31:01.401
    STEP: Ensuring resource quota status released the pod usage 11/15/23 06:31:01.456
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:31:03.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4622" for this suite. 11/15/23 06:31:03.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:31:03.536
Nov 15 06:31:03.536: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename services 11/15/23 06:31:03.537
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:31:03.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:31:03.603
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-2611 11/15/23 06:31:03.615
STEP: creating replication controller nodeport-test in namespace services-2611 11/15/23 06:31:03.704
I1115 06:31:03.726326      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-2611, replica count: 2
I1115 06:31:06.777753      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 06:31:06.777: INFO: Creating new exec pod
Nov 15 06:31:06.827: INFO: Waiting up to 5m0s for pod "execpodkkhz4" in namespace "services-2611" to be "running"
Nov 15 06:31:06.890: INFO: Pod "execpodkkhz4": Phase="Pending", Reason="", readiness=false. Elapsed: 62.843003ms
Nov 15 06:31:08.911: INFO: Pod "execpodkkhz4": Phase="Running", Reason="", readiness=true. Elapsed: 2.083423253s
Nov 15 06:31:08.911: INFO: Pod "execpodkkhz4" satisfied condition "running"
Nov 15 06:31:09.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2611 exec execpodkkhz4 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Nov 15 06:31:10.275: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Nov 15 06:31:10.275: INFO: stdout: ""
Nov 15 06:31:10.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2611 exec execpodkkhz4 -- /bin/sh -x -c nc -v -z -w 2 172.21.116.18 80'
Nov 15 06:31:10.618: INFO: stderr: "+ nc -v -z -w 2 172.21.116.18 80\nConnection to 172.21.116.18 80 port [tcp/http] succeeded!\n"
Nov 15 06:31:10.619: INFO: stdout: ""
Nov 15 06:31:10.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2611 exec execpodkkhz4 -- /bin/sh -x -c nc -v -z -w 2 10.72.152.81 31048'
Nov 15 06:31:10.923: INFO: stderr: "+ nc -v -z -w 2 10.72.152.81 31048\nConnection to 10.72.152.81 31048 port [tcp/*] succeeded!\n"
Nov 15 06:31:10.923: INFO: stdout: ""
Nov 15 06:31:10.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2611 exec execpodkkhz4 -- /bin/sh -x -c nc -v -z -w 2 10.72.152.86 31048'
Nov 15 06:31:11.283: INFO: stderr: "+ nc -v -z -w 2 10.72.152.86 31048\nConnection to 10.72.152.86 31048 port [tcp/*] succeeded!\n"
Nov 15 06:31:11.283: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 06:31:11.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2611" for this suite. 11/15/23 06:31:11.312
------------------------------
â€¢ [SLOW TEST] [7.804 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:31:03.536
    Nov 15 06:31:03.536: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename services 11/15/23 06:31:03.537
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:31:03.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:31:03.603
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-2611 11/15/23 06:31:03.615
    STEP: creating replication controller nodeport-test in namespace services-2611 11/15/23 06:31:03.704
    I1115 06:31:03.726326      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-2611, replica count: 2
    I1115 06:31:06.777753      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 15 06:31:06.777: INFO: Creating new exec pod
    Nov 15 06:31:06.827: INFO: Waiting up to 5m0s for pod "execpodkkhz4" in namespace "services-2611" to be "running"
    Nov 15 06:31:06.890: INFO: Pod "execpodkkhz4": Phase="Pending", Reason="", readiness=false. Elapsed: 62.843003ms
    Nov 15 06:31:08.911: INFO: Pod "execpodkkhz4": Phase="Running", Reason="", readiness=true. Elapsed: 2.083423253s
    Nov 15 06:31:08.911: INFO: Pod "execpodkkhz4" satisfied condition "running"
    Nov 15 06:31:09.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2611 exec execpodkkhz4 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Nov 15 06:31:10.275: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Nov 15 06:31:10.275: INFO: stdout: ""
    Nov 15 06:31:10.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2611 exec execpodkkhz4 -- /bin/sh -x -c nc -v -z -w 2 172.21.116.18 80'
    Nov 15 06:31:10.618: INFO: stderr: "+ nc -v -z -w 2 172.21.116.18 80\nConnection to 172.21.116.18 80 port [tcp/http] succeeded!\n"
    Nov 15 06:31:10.619: INFO: stdout: ""
    Nov 15 06:31:10.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2611 exec execpodkkhz4 -- /bin/sh -x -c nc -v -z -w 2 10.72.152.81 31048'
    Nov 15 06:31:10.923: INFO: stderr: "+ nc -v -z -w 2 10.72.152.81 31048\nConnection to 10.72.152.81 31048 port [tcp/*] succeeded!\n"
    Nov 15 06:31:10.923: INFO: stdout: ""
    Nov 15 06:31:10.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2611 exec execpodkkhz4 -- /bin/sh -x -c nc -v -z -w 2 10.72.152.86 31048'
    Nov 15 06:31:11.283: INFO: stderr: "+ nc -v -z -w 2 10.72.152.86 31048\nConnection to 10.72.152.86 31048 port [tcp/*] succeeded!\n"
    Nov 15 06:31:11.283: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:31:11.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2611" for this suite. 11/15/23 06:31:11.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:31:11.34
Nov 15 06:31:11.341: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 06:31:11.341
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:31:11.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:31:11.406
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 11/15/23 06:31:11.418
Nov 15 06:31:11.419: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 06:31:16.060: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:31:33.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6303" for this suite. 11/15/23 06:31:33.76
------------------------------
â€¢ [SLOW TEST] [22.443 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:31:11.34
    Nov 15 06:31:11.341: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 06:31:11.341
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:31:11.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:31:11.406
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 11/15/23 06:31:11.418
    Nov 15 06:31:11.419: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 06:31:16.060: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:31:33.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6303" for this suite. 11/15/23 06:31:33.76
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:31:33.785
Nov 15 06:31:33.786: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 06:31:33.787
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:31:33.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:31:33.903
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 11/15/23 06:31:33.918
Nov 15 06:31:33.975: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d15b5a6-f9e9-48d8-b52d-1ec7a714a3c3" in namespace "projected-8789" to be "Succeeded or Failed"
Nov 15 06:31:33.987: INFO: Pod "downwardapi-volume-3d15b5a6-f9e9-48d8-b52d-1ec7a714a3c3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.512159ms
Nov 15 06:31:36.000: INFO: Pod "downwardapi-volume-3d15b5a6-f9e9-48d8-b52d-1ec7a714a3c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024641304s
Nov 15 06:31:38.000: INFO: Pod "downwardapi-volume-3d15b5a6-f9e9-48d8-b52d-1ec7a714a3c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02484817s
STEP: Saw pod success 11/15/23 06:31:38
Nov 15 06:31:38.001: INFO: Pod "downwardapi-volume-3d15b5a6-f9e9-48d8-b52d-1ec7a714a3c3" satisfied condition "Succeeded or Failed"
Nov 15 06:31:38.023: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-3d15b5a6-f9e9-48d8-b52d-1ec7a714a3c3 container client-container: <nil>
STEP: delete the pod 11/15/23 06:31:38.085
Nov 15 06:31:38.113: INFO: Waiting for pod downwardapi-volume-3d15b5a6-f9e9-48d8-b52d-1ec7a714a3c3 to disappear
Nov 15 06:31:38.124: INFO: Pod downwardapi-volume-3d15b5a6-f9e9-48d8-b52d-1ec7a714a3c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 06:31:38.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8789" for this suite. 11/15/23 06:31:38.147
------------------------------
â€¢ [4.379 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:31:33.785
    Nov 15 06:31:33.786: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 06:31:33.787
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:31:33.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:31:33.903
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 11/15/23 06:31:33.918
    Nov 15 06:31:33.975: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d15b5a6-f9e9-48d8-b52d-1ec7a714a3c3" in namespace "projected-8789" to be "Succeeded or Failed"
    Nov 15 06:31:33.987: INFO: Pod "downwardapi-volume-3d15b5a6-f9e9-48d8-b52d-1ec7a714a3c3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.512159ms
    Nov 15 06:31:36.000: INFO: Pod "downwardapi-volume-3d15b5a6-f9e9-48d8-b52d-1ec7a714a3c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024641304s
    Nov 15 06:31:38.000: INFO: Pod "downwardapi-volume-3d15b5a6-f9e9-48d8-b52d-1ec7a714a3c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02484817s
    STEP: Saw pod success 11/15/23 06:31:38
    Nov 15 06:31:38.001: INFO: Pod "downwardapi-volume-3d15b5a6-f9e9-48d8-b52d-1ec7a714a3c3" satisfied condition "Succeeded or Failed"
    Nov 15 06:31:38.023: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-3d15b5a6-f9e9-48d8-b52d-1ec7a714a3c3 container client-container: <nil>
    STEP: delete the pod 11/15/23 06:31:38.085
    Nov 15 06:31:38.113: INFO: Waiting for pod downwardapi-volume-3d15b5a6-f9e9-48d8-b52d-1ec7a714a3c3 to disappear
    Nov 15 06:31:38.124: INFO: Pod downwardapi-volume-3d15b5a6-f9e9-48d8-b52d-1ec7a714a3c3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:31:38.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8789" for this suite. 11/15/23 06:31:38.147
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:31:38.165
Nov 15 06:31:38.165: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename ingressclass 11/15/23 06:31:38.166
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:31:38.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:31:38.232
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 11/15/23 06:31:38.243
STEP: getting /apis/networking.k8s.io 11/15/23 06:31:38.257
STEP: getting /apis/networking.k8s.iov1 11/15/23 06:31:38.262
STEP: creating 11/15/23 06:31:38.265
STEP: getting 11/15/23 06:31:38.318
STEP: listing 11/15/23 06:31:38.34
STEP: watching 11/15/23 06:31:38.355
Nov 15 06:31:38.355: INFO: starting watch
STEP: patching 11/15/23 06:31:38.36
STEP: updating 11/15/23 06:31:38.377
Nov 15 06:31:38.394: INFO: waiting for watch events with expected annotations
Nov 15 06:31:38.395: INFO: saw patched and updated annotations
STEP: deleting 11/15/23 06:31:38.395
STEP: deleting a collection 11/15/23 06:31:38.45
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Nov 15 06:31:38.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-2964" for this suite. 11/15/23 06:31:38.543
------------------------------
â€¢ [0.403 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:31:38.165
    Nov 15 06:31:38.165: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename ingressclass 11/15/23 06:31:38.166
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:31:38.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:31:38.232
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 11/15/23 06:31:38.243
    STEP: getting /apis/networking.k8s.io 11/15/23 06:31:38.257
    STEP: getting /apis/networking.k8s.iov1 11/15/23 06:31:38.262
    STEP: creating 11/15/23 06:31:38.265
    STEP: getting 11/15/23 06:31:38.318
    STEP: listing 11/15/23 06:31:38.34
    STEP: watching 11/15/23 06:31:38.355
    Nov 15 06:31:38.355: INFO: starting watch
    STEP: patching 11/15/23 06:31:38.36
    STEP: updating 11/15/23 06:31:38.377
    Nov 15 06:31:38.394: INFO: waiting for watch events with expected annotations
    Nov 15 06:31:38.395: INFO: saw patched and updated annotations
    STEP: deleting 11/15/23 06:31:38.395
    STEP: deleting a collection 11/15/23 06:31:38.45
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:31:38.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-2964" for this suite. 11/15/23 06:31:38.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:31:38.569
Nov 15 06:31:38.570: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 06:31:38.571
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:31:38.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:31:38.648
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-6f19b143-b355-4323-82b1-9fde331ba11d 11/15/23 06:31:38.659
STEP: Creating a pod to test consume configMaps 11/15/23 06:31:38.676
Nov 15 06:31:38.709: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4f08295b-7028-44ab-a22a-d527d5a45af4" in namespace "projected-530" to be "Succeeded or Failed"
Nov 15 06:31:38.722: INFO: Pod "pod-projected-configmaps-4f08295b-7028-44ab-a22a-d527d5a45af4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.600321ms
Nov 15 06:31:40.735: INFO: Pod "pod-projected-configmaps-4f08295b-7028-44ab-a22a-d527d5a45af4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025778815s
Nov 15 06:31:42.736: INFO: Pod "pod-projected-configmaps-4f08295b-7028-44ab-a22a-d527d5a45af4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026957408s
Nov 15 06:31:44.748: INFO: Pod "pod-projected-configmaps-4f08295b-7028-44ab-a22a-d527d5a45af4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038955749s
STEP: Saw pod success 11/15/23 06:31:44.748
Nov 15 06:31:44.748: INFO: Pod "pod-projected-configmaps-4f08295b-7028-44ab-a22a-d527d5a45af4" satisfied condition "Succeeded or Failed"
Nov 15 06:31:44.761: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-configmaps-4f08295b-7028-44ab-a22a-d527d5a45af4 container agnhost-container: <nil>
STEP: delete the pod 11/15/23 06:31:44.79
Nov 15 06:31:44.821: INFO: Waiting for pod pod-projected-configmaps-4f08295b-7028-44ab-a22a-d527d5a45af4 to disappear
Nov 15 06:31:44.834: INFO: Pod pod-projected-configmaps-4f08295b-7028-44ab-a22a-d527d5a45af4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:31:44.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-530" for this suite. 11/15/23 06:31:44.861
------------------------------
â€¢ [SLOW TEST] [6.314 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:31:38.569
    Nov 15 06:31:38.570: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 06:31:38.571
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:31:38.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:31:38.648
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-6f19b143-b355-4323-82b1-9fde331ba11d 11/15/23 06:31:38.659
    STEP: Creating a pod to test consume configMaps 11/15/23 06:31:38.676
    Nov 15 06:31:38.709: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4f08295b-7028-44ab-a22a-d527d5a45af4" in namespace "projected-530" to be "Succeeded or Failed"
    Nov 15 06:31:38.722: INFO: Pod "pod-projected-configmaps-4f08295b-7028-44ab-a22a-d527d5a45af4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.600321ms
    Nov 15 06:31:40.735: INFO: Pod "pod-projected-configmaps-4f08295b-7028-44ab-a22a-d527d5a45af4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025778815s
    Nov 15 06:31:42.736: INFO: Pod "pod-projected-configmaps-4f08295b-7028-44ab-a22a-d527d5a45af4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026957408s
    Nov 15 06:31:44.748: INFO: Pod "pod-projected-configmaps-4f08295b-7028-44ab-a22a-d527d5a45af4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038955749s
    STEP: Saw pod success 11/15/23 06:31:44.748
    Nov 15 06:31:44.748: INFO: Pod "pod-projected-configmaps-4f08295b-7028-44ab-a22a-d527d5a45af4" satisfied condition "Succeeded or Failed"
    Nov 15 06:31:44.761: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-configmaps-4f08295b-7028-44ab-a22a-d527d5a45af4 container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 06:31:44.79
    Nov 15 06:31:44.821: INFO: Waiting for pod pod-projected-configmaps-4f08295b-7028-44ab-a22a-d527d5a45af4 to disappear
    Nov 15 06:31:44.834: INFO: Pod pod-projected-configmaps-4f08295b-7028-44ab-a22a-d527d5a45af4 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:31:44.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-530" for this suite. 11/15/23 06:31:44.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:31:44.884
Nov 15 06:31:44.884: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename configmap 11/15/23 06:31:44.885
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:31:44.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:31:44.954
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-2a923f30-1bd6-4410-ad08-029f0c617109 11/15/23 06:31:44.964
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:31:44.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3328" for this suite. 11/15/23 06:31:44.998
------------------------------
â€¢ [0.134 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:31:44.884
    Nov 15 06:31:44.884: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename configmap 11/15/23 06:31:44.885
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:31:44.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:31:44.954
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-2a923f30-1bd6-4410-ad08-029f0c617109 11/15/23 06:31:44.964
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:31:44.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3328" for this suite. 11/15/23 06:31:44.998
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:31:45.019
Nov 15 06:31:45.019: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 06:31:45.021
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:31:45.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:31:45.089
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-8efc044f-bea4-4163-8db2-120d98d920a2 11/15/23 06:31:45.102
STEP: Creating a pod to test consume configMaps 11/15/23 06:31:45.117
Nov 15 06:31:45.163: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8ea0e6ee-9bfd-4d67-b399-34708c5edb66" in namespace "projected-1828" to be "Succeeded or Failed"
Nov 15 06:31:45.177: INFO: Pod "pod-projected-configmaps-8ea0e6ee-9bfd-4d67-b399-34708c5edb66": Phase="Pending", Reason="", readiness=false. Elapsed: 14.510926ms
Nov 15 06:31:47.205: INFO: Pod "pod-projected-configmaps-8ea0e6ee-9bfd-4d67-b399-34708c5edb66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042159953s
Nov 15 06:31:49.221: INFO: Pod "pod-projected-configmaps-8ea0e6ee-9bfd-4d67-b399-34708c5edb66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058050028s
Nov 15 06:31:51.192: INFO: Pod "pod-projected-configmaps-8ea0e6ee-9bfd-4d67-b399-34708c5edb66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029161425s
STEP: Saw pod success 11/15/23 06:31:51.192
Nov 15 06:31:51.192: INFO: Pod "pod-projected-configmaps-8ea0e6ee-9bfd-4d67-b399-34708c5edb66" satisfied condition "Succeeded or Failed"
Nov 15 06:31:51.207: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-configmaps-8ea0e6ee-9bfd-4d67-b399-34708c5edb66 container agnhost-container: <nil>
STEP: delete the pod 11/15/23 06:31:51.232
Nov 15 06:31:51.273: INFO: Waiting for pod pod-projected-configmaps-8ea0e6ee-9bfd-4d67-b399-34708c5edb66 to disappear
Nov 15 06:31:51.285: INFO: Pod pod-projected-configmaps-8ea0e6ee-9bfd-4d67-b399-34708c5edb66 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:31:51.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1828" for this suite. 11/15/23 06:31:51.308
------------------------------
â€¢ [SLOW TEST] [6.312 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:31:45.019
    Nov 15 06:31:45.019: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 06:31:45.021
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:31:45.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:31:45.089
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-8efc044f-bea4-4163-8db2-120d98d920a2 11/15/23 06:31:45.102
    STEP: Creating a pod to test consume configMaps 11/15/23 06:31:45.117
    Nov 15 06:31:45.163: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8ea0e6ee-9bfd-4d67-b399-34708c5edb66" in namespace "projected-1828" to be "Succeeded or Failed"
    Nov 15 06:31:45.177: INFO: Pod "pod-projected-configmaps-8ea0e6ee-9bfd-4d67-b399-34708c5edb66": Phase="Pending", Reason="", readiness=false. Elapsed: 14.510926ms
    Nov 15 06:31:47.205: INFO: Pod "pod-projected-configmaps-8ea0e6ee-9bfd-4d67-b399-34708c5edb66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042159953s
    Nov 15 06:31:49.221: INFO: Pod "pod-projected-configmaps-8ea0e6ee-9bfd-4d67-b399-34708c5edb66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058050028s
    Nov 15 06:31:51.192: INFO: Pod "pod-projected-configmaps-8ea0e6ee-9bfd-4d67-b399-34708c5edb66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029161425s
    STEP: Saw pod success 11/15/23 06:31:51.192
    Nov 15 06:31:51.192: INFO: Pod "pod-projected-configmaps-8ea0e6ee-9bfd-4d67-b399-34708c5edb66" satisfied condition "Succeeded or Failed"
    Nov 15 06:31:51.207: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-configmaps-8ea0e6ee-9bfd-4d67-b399-34708c5edb66 container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 06:31:51.232
    Nov 15 06:31:51.273: INFO: Waiting for pod pod-projected-configmaps-8ea0e6ee-9bfd-4d67-b399-34708c5edb66 to disappear
    Nov 15 06:31:51.285: INFO: Pod pod-projected-configmaps-8ea0e6ee-9bfd-4d67-b399-34708c5edb66 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:31:51.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1828" for this suite. 11/15/23 06:31:51.308
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:31:51.332
Nov 15 06:31:51.332: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename var-expansion 11/15/23 06:31:51.334
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:31:51.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:31:51.41
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 11/15/23 06:31:51.421
STEP: waiting for pod running 11/15/23 06:31:51.454
Nov 15 06:31:51.454: INFO: Waiting up to 2m0s for pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576" in namespace "var-expansion-1578" to be "running"
Nov 15 06:31:51.466: INFO: Pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576": Phase="Pending", Reason="", readiness=false. Elapsed: 11.467169ms
Nov 15 06:31:53.492: INFO: Pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576": Phase="Running", Reason="", readiness=true. Elapsed: 2.038073922s
Nov 15 06:31:53.493: INFO: Pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576" satisfied condition "running"
STEP: creating a file in subpath 11/15/23 06:31:53.493
Nov 15 06:31:53.505: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1578 PodName:var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 06:31:53.505: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 06:31:53.506: INFO: ExecWithOptions: Clientset creation
Nov 15 06:31:53.506: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-1578/pods/var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 11/15/23 06:31:53.696
Nov 15 06:31:53.709: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1578 PodName:var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 06:31:53.709: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 06:31:53.709: INFO: ExecWithOptions: Clientset creation
Nov 15 06:31:53.710: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-1578/pods/var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 11/15/23 06:31:53.896
Nov 15 06:31:54.429: INFO: Successfully updated pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576"
STEP: waiting for annotated pod running 11/15/23 06:31:54.43
Nov 15 06:31:54.430: INFO: Waiting up to 2m0s for pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576" in namespace "var-expansion-1578" to be "running"
Nov 15 06:31:54.441: INFO: Pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576": Phase="Running", Reason="", readiness=true. Elapsed: 11.587068ms
Nov 15 06:31:54.441: INFO: Pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576" satisfied condition "running"
STEP: deleting the pod gracefully 11/15/23 06:31:54.441
Nov 15 06:31:54.441: INFO: Deleting pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576" in namespace "var-expansion-1578"
Nov 15 06:31:54.461: INFO: Wait up to 5m0s for pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 15 06:32:28.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1578" for this suite. 11/15/23 06:32:28.508
------------------------------
â€¢ [SLOW TEST] [37.202 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:31:51.332
    Nov 15 06:31:51.332: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename var-expansion 11/15/23 06:31:51.334
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:31:51.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:31:51.41
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 11/15/23 06:31:51.421
    STEP: waiting for pod running 11/15/23 06:31:51.454
    Nov 15 06:31:51.454: INFO: Waiting up to 2m0s for pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576" in namespace "var-expansion-1578" to be "running"
    Nov 15 06:31:51.466: INFO: Pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576": Phase="Pending", Reason="", readiness=false. Elapsed: 11.467169ms
    Nov 15 06:31:53.492: INFO: Pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576": Phase="Running", Reason="", readiness=true. Elapsed: 2.038073922s
    Nov 15 06:31:53.493: INFO: Pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576" satisfied condition "running"
    STEP: creating a file in subpath 11/15/23 06:31:53.493
    Nov 15 06:31:53.505: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1578 PodName:var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 06:31:53.505: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 06:31:53.506: INFO: ExecWithOptions: Clientset creation
    Nov 15 06:31:53.506: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-1578/pods/var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 11/15/23 06:31:53.696
    Nov 15 06:31:53.709: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1578 PodName:var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 06:31:53.709: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 06:31:53.709: INFO: ExecWithOptions: Clientset creation
    Nov 15 06:31:53.710: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-1578/pods/var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 11/15/23 06:31:53.896
    Nov 15 06:31:54.429: INFO: Successfully updated pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576"
    STEP: waiting for annotated pod running 11/15/23 06:31:54.43
    Nov 15 06:31:54.430: INFO: Waiting up to 2m0s for pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576" in namespace "var-expansion-1578" to be "running"
    Nov 15 06:31:54.441: INFO: Pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576": Phase="Running", Reason="", readiness=true. Elapsed: 11.587068ms
    Nov 15 06:31:54.441: INFO: Pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576" satisfied condition "running"
    STEP: deleting the pod gracefully 11/15/23 06:31:54.441
    Nov 15 06:31:54.441: INFO: Deleting pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576" in namespace "var-expansion-1578"
    Nov 15 06:31:54.461: INFO: Wait up to 5m0s for pod "var-expansion-ba4ff82f-e6d8-424e-a8cf-1b852df0d576" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:32:28.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1578" for this suite. 11/15/23 06:32:28.508
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:32:28.534
Nov 15 06:32:28.534: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename sysctl 11/15/23 06:32:28.535
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:32:28.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:32:28.599
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 11/15/23 06:32:28.61
STEP: Watching for error events or started pod 11/15/23 06:32:28.645
STEP: Waiting for pod completion 11/15/23 06:32:30.658
Nov 15 06:32:30.658: INFO: Waiting up to 3m0s for pod "sysctl-841164a5-66b2-448c-bb88-fa72f4a869ea" in namespace "sysctl-7360" to be "completed"
Nov 15 06:32:30.670: INFO: Pod "sysctl-841164a5-66b2-448c-bb88-fa72f4a869ea": Phase="Pending", Reason="", readiness=false. Elapsed: 11.878746ms
Nov 15 06:32:32.685: INFO: Pod "sysctl-841164a5-66b2-448c-bb88-fa72f4a869ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026372209s
Nov 15 06:32:32.685: INFO: Pod "sysctl-841164a5-66b2-448c-bb88-fa72f4a869ea" satisfied condition "completed"
STEP: Checking that the pod succeeded 11/15/23 06:32:32.696
STEP: Getting logs from the pod 11/15/23 06:32:32.696
STEP: Checking that the sysctl is actually updated 11/15/23 06:32:32.753
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:32:32.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-7360" for this suite. 11/15/23 06:32:32.785
------------------------------
â€¢ [4.272 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:32:28.534
    Nov 15 06:32:28.534: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename sysctl 11/15/23 06:32:28.535
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:32:28.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:32:28.599
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 11/15/23 06:32:28.61
    STEP: Watching for error events or started pod 11/15/23 06:32:28.645
    STEP: Waiting for pod completion 11/15/23 06:32:30.658
    Nov 15 06:32:30.658: INFO: Waiting up to 3m0s for pod "sysctl-841164a5-66b2-448c-bb88-fa72f4a869ea" in namespace "sysctl-7360" to be "completed"
    Nov 15 06:32:30.670: INFO: Pod "sysctl-841164a5-66b2-448c-bb88-fa72f4a869ea": Phase="Pending", Reason="", readiness=false. Elapsed: 11.878746ms
    Nov 15 06:32:32.685: INFO: Pod "sysctl-841164a5-66b2-448c-bb88-fa72f4a869ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026372209s
    Nov 15 06:32:32.685: INFO: Pod "sysctl-841164a5-66b2-448c-bb88-fa72f4a869ea" satisfied condition "completed"
    STEP: Checking that the pod succeeded 11/15/23 06:32:32.696
    STEP: Getting logs from the pod 11/15/23 06:32:32.696
    STEP: Checking that the sysctl is actually updated 11/15/23 06:32:32.753
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:32:32.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-7360" for this suite. 11/15/23 06:32:32.785
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:32:32.811
Nov 15 06:32:32.811: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename disruption 11/15/23 06:32:32.812
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:32:32.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:32:32.884
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 11/15/23 06:32:32.895
STEP: Waiting for the pdb to be processed 11/15/23 06:32:32.93
STEP: First trying to evict a pod which shouldn't be evictable 11/15/23 06:32:32.974
STEP: Waiting for all pods to be running 11/15/23 06:32:32.974
Nov 15 06:32:32.987: INFO: pods: 0 < 3
Nov 15 06:32:35.000: INFO: running pods: 2 < 3
STEP: locating a running pod 11/15/23 06:32:37.003
STEP: Updating the pdb to allow a pod to be evicted 11/15/23 06:32:37.039
STEP: Waiting for the pdb to be processed 11/15/23 06:32:37.072
STEP: Trying to evict the same pod we tried earlier which should now be evictable 11/15/23 06:32:39.106
STEP: Waiting for all pods to be running 11/15/23 06:32:39.107
STEP: Waiting for the pdb to observed all healthy pods 11/15/23 06:32:39.119
STEP: Patching the pdb to disallow a pod to be evicted 11/15/23 06:32:39.206
STEP: Waiting for the pdb to be processed 11/15/23 06:32:39.249
STEP: Waiting for all pods to be running 11/15/23 06:32:41.279
STEP: locating a running pod 11/15/23 06:32:41.292
STEP: Deleting the pdb to allow a pod to be evicted 11/15/23 06:32:41.332
STEP: Waiting for the pdb to be deleted 11/15/23 06:32:41.358
STEP: Trying to evict the same pod we tried earlier which should now be evictable 11/15/23 06:32:41.373
STEP: Waiting for all pods to be running 11/15/23 06:32:41.374
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Nov 15 06:32:41.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5254" for this suite. 11/15/23 06:32:41.448
------------------------------
â€¢ [SLOW TEST] [8.662 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:32:32.811
    Nov 15 06:32:32.811: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename disruption 11/15/23 06:32:32.812
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:32:32.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:32:32.884
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 11/15/23 06:32:32.895
    STEP: Waiting for the pdb to be processed 11/15/23 06:32:32.93
    STEP: First trying to evict a pod which shouldn't be evictable 11/15/23 06:32:32.974
    STEP: Waiting for all pods to be running 11/15/23 06:32:32.974
    Nov 15 06:32:32.987: INFO: pods: 0 < 3
    Nov 15 06:32:35.000: INFO: running pods: 2 < 3
    STEP: locating a running pod 11/15/23 06:32:37.003
    STEP: Updating the pdb to allow a pod to be evicted 11/15/23 06:32:37.039
    STEP: Waiting for the pdb to be processed 11/15/23 06:32:37.072
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 11/15/23 06:32:39.106
    STEP: Waiting for all pods to be running 11/15/23 06:32:39.107
    STEP: Waiting for the pdb to observed all healthy pods 11/15/23 06:32:39.119
    STEP: Patching the pdb to disallow a pod to be evicted 11/15/23 06:32:39.206
    STEP: Waiting for the pdb to be processed 11/15/23 06:32:39.249
    STEP: Waiting for all pods to be running 11/15/23 06:32:41.279
    STEP: locating a running pod 11/15/23 06:32:41.292
    STEP: Deleting the pdb to allow a pod to be evicted 11/15/23 06:32:41.332
    STEP: Waiting for the pdb to be deleted 11/15/23 06:32:41.358
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 11/15/23 06:32:41.373
    STEP: Waiting for all pods to be running 11/15/23 06:32:41.374
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:32:41.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5254" for this suite. 11/15/23 06:32:41.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:32:41.473
Nov 15 06:32:41.474: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename services 11/15/23 06:32:41.474
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:32:41.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:32:41.546
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-1592 11/15/23 06:32:41.556
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1592 to expose endpoints map[] 11/15/23 06:32:41.603
Nov 15 06:32:41.637: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Nov 15 06:32:42.670: INFO: successfully validated that service multi-endpoint-test in namespace services-1592 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1592 11/15/23 06:32:42.67
Nov 15 06:32:42.696: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1592" to be "running and ready"
Nov 15 06:32:42.707: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.138748ms
Nov 15 06:32:42.707: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:32:44.720: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024301507s
Nov 15 06:32:44.720: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:32:46.724: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.028131632s
Nov 15 06:32:46.724: INFO: The phase of Pod pod1 is Running (Ready = true)
Nov 15 06:32:46.724: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1592 to expose endpoints map[pod1:[100]] 11/15/23 06:32:46.737
Nov 15 06:32:46.780: INFO: successfully validated that service multi-endpoint-test in namespace services-1592 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-1592 11/15/23 06:32:46.78
Nov 15 06:32:46.797: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1592" to be "running and ready"
Nov 15 06:32:46.811: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.04525ms
Nov 15 06:32:46.811: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:32:48.828: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031018876s
Nov 15 06:32:48.828: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:32:50.823: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.0264687s
Nov 15 06:32:50.823: INFO: The phase of Pod pod2 is Running (Ready = true)
Nov 15 06:32:50.823: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1592 to expose endpoints map[pod1:[100] pod2:[101]] 11/15/23 06:32:50.836
Nov 15 06:32:50.886: INFO: successfully validated that service multi-endpoint-test in namespace services-1592 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 11/15/23 06:32:50.886
Nov 15 06:32:50.886: INFO: Creating new exec pod
Nov 15 06:32:50.901: INFO: Waiting up to 5m0s for pod "execpodj54fc" in namespace "services-1592" to be "running"
Nov 15 06:32:50.915: INFO: Pod "execpodj54fc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.965466ms
Nov 15 06:32:52.926: INFO: Pod "execpodj54fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025758386s
Nov 15 06:32:54.928: INFO: Pod "execpodj54fc": Phase="Running", Reason="", readiness=true. Elapsed: 4.027086059s
Nov 15 06:32:54.928: INFO: Pod "execpodj54fc" satisfied condition "running"
Nov 15 06:32:55.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-1592 exec execpodj54fc -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Nov 15 06:32:56.203: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Nov 15 06:32:56.203: INFO: stdout: ""
Nov 15 06:32:56.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-1592 exec execpodj54fc -- /bin/sh -x -c nc -v -z -w 2 172.21.254.106 80'
Nov 15 06:32:56.502: INFO: stderr: "+ nc -v -z -w 2 172.21.254.106 80\nConnection to 172.21.254.106 80 port [tcp/http] succeeded!\n"
Nov 15 06:32:56.502: INFO: stdout: ""
Nov 15 06:32:56.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-1592 exec execpodj54fc -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Nov 15 06:32:56.796: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Nov 15 06:32:56.796: INFO: stdout: ""
Nov 15 06:32:56.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-1592 exec execpodj54fc -- /bin/sh -x -c nc -v -z -w 2 172.21.254.106 81'
Nov 15 06:32:57.090: INFO: stderr: "+ nc -v -z -w 2 172.21.254.106 81\nConnection to 172.21.254.106 81 port [tcp/*] succeeded!\n"
Nov 15 06:32:57.090: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-1592 11/15/23 06:32:57.09
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1592 to expose endpoints map[pod2:[101]] 11/15/23 06:32:57.124
Nov 15 06:32:57.171: INFO: successfully validated that service multi-endpoint-test in namespace services-1592 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-1592 11/15/23 06:32:57.171
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1592 to expose endpoints map[] 11/15/23 06:32:57.207
Nov 15 06:32:57.240: INFO: successfully validated that service multi-endpoint-test in namespace services-1592 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 06:32:57.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1592" for this suite. 11/15/23 06:32:57.321
------------------------------
â€¢ [SLOW TEST] [15.869 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:32:41.473
    Nov 15 06:32:41.474: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename services 11/15/23 06:32:41.474
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:32:41.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:32:41.546
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-1592 11/15/23 06:32:41.556
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1592 to expose endpoints map[] 11/15/23 06:32:41.603
    Nov 15 06:32:41.637: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Nov 15 06:32:42.670: INFO: successfully validated that service multi-endpoint-test in namespace services-1592 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1592 11/15/23 06:32:42.67
    Nov 15 06:32:42.696: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1592" to be "running and ready"
    Nov 15 06:32:42.707: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.138748ms
    Nov 15 06:32:42.707: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:32:44.720: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024301507s
    Nov 15 06:32:44.720: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:32:46.724: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.028131632s
    Nov 15 06:32:46.724: INFO: The phase of Pod pod1 is Running (Ready = true)
    Nov 15 06:32:46.724: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1592 to expose endpoints map[pod1:[100]] 11/15/23 06:32:46.737
    Nov 15 06:32:46.780: INFO: successfully validated that service multi-endpoint-test in namespace services-1592 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-1592 11/15/23 06:32:46.78
    Nov 15 06:32:46.797: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1592" to be "running and ready"
    Nov 15 06:32:46.811: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.04525ms
    Nov 15 06:32:46.811: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:32:48.828: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031018876s
    Nov 15 06:32:48.828: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:32:50.823: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.0264687s
    Nov 15 06:32:50.823: INFO: The phase of Pod pod2 is Running (Ready = true)
    Nov 15 06:32:50.823: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1592 to expose endpoints map[pod1:[100] pod2:[101]] 11/15/23 06:32:50.836
    Nov 15 06:32:50.886: INFO: successfully validated that service multi-endpoint-test in namespace services-1592 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 11/15/23 06:32:50.886
    Nov 15 06:32:50.886: INFO: Creating new exec pod
    Nov 15 06:32:50.901: INFO: Waiting up to 5m0s for pod "execpodj54fc" in namespace "services-1592" to be "running"
    Nov 15 06:32:50.915: INFO: Pod "execpodj54fc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.965466ms
    Nov 15 06:32:52.926: INFO: Pod "execpodj54fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025758386s
    Nov 15 06:32:54.928: INFO: Pod "execpodj54fc": Phase="Running", Reason="", readiness=true. Elapsed: 4.027086059s
    Nov 15 06:32:54.928: INFO: Pod "execpodj54fc" satisfied condition "running"
    Nov 15 06:32:55.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-1592 exec execpodj54fc -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Nov 15 06:32:56.203: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Nov 15 06:32:56.203: INFO: stdout: ""
    Nov 15 06:32:56.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-1592 exec execpodj54fc -- /bin/sh -x -c nc -v -z -w 2 172.21.254.106 80'
    Nov 15 06:32:56.502: INFO: stderr: "+ nc -v -z -w 2 172.21.254.106 80\nConnection to 172.21.254.106 80 port [tcp/http] succeeded!\n"
    Nov 15 06:32:56.502: INFO: stdout: ""
    Nov 15 06:32:56.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-1592 exec execpodj54fc -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Nov 15 06:32:56.796: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Nov 15 06:32:56.796: INFO: stdout: ""
    Nov 15 06:32:56.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-1592 exec execpodj54fc -- /bin/sh -x -c nc -v -z -w 2 172.21.254.106 81'
    Nov 15 06:32:57.090: INFO: stderr: "+ nc -v -z -w 2 172.21.254.106 81\nConnection to 172.21.254.106 81 port [tcp/*] succeeded!\n"
    Nov 15 06:32:57.090: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-1592 11/15/23 06:32:57.09
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1592 to expose endpoints map[pod2:[101]] 11/15/23 06:32:57.124
    Nov 15 06:32:57.171: INFO: successfully validated that service multi-endpoint-test in namespace services-1592 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-1592 11/15/23 06:32:57.171
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1592 to expose endpoints map[] 11/15/23 06:32:57.207
    Nov 15 06:32:57.240: INFO: successfully validated that service multi-endpoint-test in namespace services-1592 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:32:57.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1592" for this suite. 11/15/23 06:32:57.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:32:57.343
Nov 15 06:32:57.343: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename resourcequota 11/15/23 06:32:57.345
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:32:57.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:32:57.415
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 11/15/23 06:32:57.427
STEP: Creating a ResourceQuota 11/15/23 06:33:02.455
STEP: Ensuring resource quota status is calculated 11/15/23 06:33:02.496
STEP: Creating a ReplicaSet 11/15/23 06:33:04.513
STEP: Ensuring resource quota status captures replicaset creation 11/15/23 06:33:04.603
STEP: Deleting a ReplicaSet 11/15/23 06:33:06.616
STEP: Ensuring resource quota status released usage 11/15/23 06:33:06.643
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 06:33:08.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6533" for this suite. 11/15/23 06:33:08.702
------------------------------
â€¢ [SLOW TEST] [11.389 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:32:57.343
    Nov 15 06:32:57.343: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename resourcequota 11/15/23 06:32:57.345
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:32:57.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:32:57.415
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 11/15/23 06:32:57.427
    STEP: Creating a ResourceQuota 11/15/23 06:33:02.455
    STEP: Ensuring resource quota status is calculated 11/15/23 06:33:02.496
    STEP: Creating a ReplicaSet 11/15/23 06:33:04.513
    STEP: Ensuring resource quota status captures replicaset creation 11/15/23 06:33:04.603
    STEP: Deleting a ReplicaSet 11/15/23 06:33:06.616
    STEP: Ensuring resource quota status released usage 11/15/23 06:33:06.643
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:33:08.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6533" for this suite. 11/15/23 06:33:08.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:33:08.735
Nov 15 06:33:08.735: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename pod-network-test 11/15/23 06:33:08.736
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:33:08.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:33:08.852
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-2663 11/15/23 06:33:08.892
STEP: creating a selector 11/15/23 06:33:08.892
STEP: Creating the service pods in kubernetes 11/15/23 06:33:08.892
Nov 15 06:33:08.892: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 15 06:33:09.058: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2663" to be "running and ready"
Nov 15 06:33:09.072: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.328282ms
Nov 15 06:33:09.072: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:33:11.088: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030425641s
Nov 15 06:33:11.088: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:33:13.085: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.027146717s
Nov 15 06:33:13.085: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 06:33:15.086: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.02769503s
Nov 15 06:33:15.086: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 06:33:17.086: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.028238722s
Nov 15 06:33:17.086: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 06:33:19.088: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.030693878s
Nov 15 06:33:19.089: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 06:33:21.085: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.027427353s
Nov 15 06:33:21.085: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 06:33:23.085: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.02764697s
Nov 15 06:33:23.085: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 06:33:25.086: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.027902332s
Nov 15 06:33:25.086: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 06:33:27.086: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.028557733s
Nov 15 06:33:27.086: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 06:33:29.087: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.029101199s
Nov 15 06:33:29.087: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 06:33:31.112: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.054637818s
Nov 15 06:33:31.112: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Nov 15 06:33:31.112: INFO: Pod "netserver-0" satisfied condition "running and ready"
Nov 15 06:33:31.124: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2663" to be "running and ready"
Nov 15 06:33:31.152: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 28.547736ms
Nov 15 06:33:31.152: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Nov 15 06:33:31.152: INFO: Pod "netserver-1" satisfied condition "running and ready"
Nov 15 06:33:31.164: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2663" to be "running and ready"
Nov 15 06:33:31.178: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 13.967989ms
Nov 15 06:33:31.178: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Nov 15 06:33:31.178: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 11/15/23 06:33:31.195
Nov 15 06:33:31.256: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2663" to be "running"
Nov 15 06:33:31.290: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 33.830805ms
Nov 15 06:33:33.305: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.048639753s
Nov 15 06:33:33.305: INFO: Pod "test-container-pod" satisfied condition "running"
Nov 15 06:33:33.317: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2663" to be "running"
Nov 15 06:33:33.328: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 11.438547ms
Nov 15 06:33:33.328: INFO: Pod "host-test-container-pod" satisfied condition "running"
Nov 15 06:33:33.340: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 15 06:33:33.340: INFO: Going to poll 172.30.214.190 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Nov 15 06:33:33.353: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.214.190 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2663 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 06:33:33.353: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 06:33:33.354: INFO: ExecWithOptions: Clientset creation
Nov 15 06:33:33.354: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2663/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.214.190+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 15 06:33:34.559: INFO: Found all 1 expected endpoints: [netserver-0]
Nov 15 06:33:34.559: INFO: Going to poll 172.30.213.153 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Nov 15 06:33:34.572: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.213.153 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2663 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 06:33:34.572: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 06:33:34.572: INFO: ExecWithOptions: Clientset creation
Nov 15 06:33:34.572: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2663/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.213.153+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 15 06:33:35.787: INFO: Found all 1 expected endpoints: [netserver-1]
Nov 15 06:33:35.787: INFO: Going to poll 172.30.10.165 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Nov 15 06:33:35.820: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.10.165 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2663 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 06:33:35.820: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 06:33:35.821: INFO: ExecWithOptions: Clientset creation
Nov 15 06:33:35.821: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2663/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.10.165+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 15 06:33:37.092: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Nov 15 06:33:37.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2663" for this suite. 11/15/23 06:33:37.116
------------------------------
â€¢ [SLOW TEST] [28.415 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:33:08.735
    Nov 15 06:33:08.735: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename pod-network-test 11/15/23 06:33:08.736
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:33:08.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:33:08.852
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-2663 11/15/23 06:33:08.892
    STEP: creating a selector 11/15/23 06:33:08.892
    STEP: Creating the service pods in kubernetes 11/15/23 06:33:08.892
    Nov 15 06:33:08.892: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Nov 15 06:33:09.058: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2663" to be "running and ready"
    Nov 15 06:33:09.072: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.328282ms
    Nov 15 06:33:09.072: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:33:11.088: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030425641s
    Nov 15 06:33:11.088: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:33:13.085: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.027146717s
    Nov 15 06:33:13.085: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 06:33:15.086: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.02769503s
    Nov 15 06:33:15.086: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 06:33:17.086: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.028238722s
    Nov 15 06:33:17.086: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 06:33:19.088: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.030693878s
    Nov 15 06:33:19.089: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 06:33:21.085: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.027427353s
    Nov 15 06:33:21.085: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 06:33:23.085: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.02764697s
    Nov 15 06:33:23.085: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 06:33:25.086: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.027902332s
    Nov 15 06:33:25.086: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 06:33:27.086: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.028557733s
    Nov 15 06:33:27.086: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 06:33:29.087: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.029101199s
    Nov 15 06:33:29.087: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 06:33:31.112: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.054637818s
    Nov 15 06:33:31.112: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Nov 15 06:33:31.112: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Nov 15 06:33:31.124: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2663" to be "running and ready"
    Nov 15 06:33:31.152: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 28.547736ms
    Nov 15 06:33:31.152: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Nov 15 06:33:31.152: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Nov 15 06:33:31.164: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2663" to be "running and ready"
    Nov 15 06:33:31.178: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 13.967989ms
    Nov 15 06:33:31.178: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Nov 15 06:33:31.178: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 11/15/23 06:33:31.195
    Nov 15 06:33:31.256: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2663" to be "running"
    Nov 15 06:33:31.290: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 33.830805ms
    Nov 15 06:33:33.305: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.048639753s
    Nov 15 06:33:33.305: INFO: Pod "test-container-pod" satisfied condition "running"
    Nov 15 06:33:33.317: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2663" to be "running"
    Nov 15 06:33:33.328: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 11.438547ms
    Nov 15 06:33:33.328: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Nov 15 06:33:33.340: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Nov 15 06:33:33.340: INFO: Going to poll 172.30.214.190 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Nov 15 06:33:33.353: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.214.190 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2663 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 06:33:33.353: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 06:33:33.354: INFO: ExecWithOptions: Clientset creation
    Nov 15 06:33:33.354: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2663/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.214.190+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 15 06:33:34.559: INFO: Found all 1 expected endpoints: [netserver-0]
    Nov 15 06:33:34.559: INFO: Going to poll 172.30.213.153 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Nov 15 06:33:34.572: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.213.153 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2663 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 06:33:34.572: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 06:33:34.572: INFO: ExecWithOptions: Clientset creation
    Nov 15 06:33:34.572: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2663/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.213.153+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 15 06:33:35.787: INFO: Found all 1 expected endpoints: [netserver-1]
    Nov 15 06:33:35.787: INFO: Going to poll 172.30.10.165 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Nov 15 06:33:35.820: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.10.165 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2663 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 06:33:35.820: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 06:33:35.821: INFO: ExecWithOptions: Clientset creation
    Nov 15 06:33:35.821: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2663/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.10.165+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 15 06:33:37.092: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:33:37.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2663" for this suite. 11/15/23 06:33:37.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:33:37.153
Nov 15 06:33:37.153: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename pods 11/15/23 06:33:37.154
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:33:37.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:33:37.294
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 11/15/23 06:33:37.314
STEP: submitting the pod to kubernetes 11/15/23 06:33:37.314
STEP: verifying QOS class is set on the pod 11/15/23 06:33:37.344
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Nov 15 06:33:37.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2493" for this suite. 11/15/23 06:33:37.378
------------------------------
â€¢ [0.246 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:33:37.153
    Nov 15 06:33:37.153: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename pods 11/15/23 06:33:37.154
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:33:37.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:33:37.294
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 11/15/23 06:33:37.314
    STEP: submitting the pod to kubernetes 11/15/23 06:33:37.314
    STEP: verifying QOS class is set on the pod 11/15/23 06:33:37.344
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:33:37.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2493" for this suite. 11/15/23 06:33:37.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:33:37.4
Nov 15 06:33:37.400: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename services 11/15/23 06:33:37.401
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:33:37.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:33:37.471
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 11/15/23 06:33:37.486
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 06:33:37.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3418" for this suite. 11/15/23 06:33:37.54
------------------------------
â€¢ [0.161 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:33:37.4
    Nov 15 06:33:37.400: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename services 11/15/23 06:33:37.401
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:33:37.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:33:37.471
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 11/15/23 06:33:37.486
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:33:37.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3418" for this suite. 11/15/23 06:33:37.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:33:37.563
Nov 15 06:33:37.563: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename job 11/15/23 06:33:37.563
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:33:37.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:33:37.642
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 11/15/23 06:33:37.652
W1115 06:33:37.668210      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions 11/15/23 06:33:37.668
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Nov 15 06:33:51.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5782" for this suite. 11/15/23 06:33:51.702
------------------------------
â€¢ [SLOW TEST] [14.162 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:33:37.563
    Nov 15 06:33:37.563: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename job 11/15/23 06:33:37.563
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:33:37.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:33:37.642
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 11/15/23 06:33:37.652
    W1115 06:33:37.668210      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring job reaches completions 11/15/23 06:33:37.668
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:33:51.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5782" for this suite. 11/15/23 06:33:51.702
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:33:51.726
Nov 15 06:33:51.726: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename dns 11/15/23 06:33:51.727
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:33:51.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:33:51.812
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 11/15/23 06:33:51.823
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-810.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-810.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-810.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-810.svc.cluster.local;sleep 1; done
 11/15/23 06:33:51.837
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-810.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-810.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-810.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-810.svc.cluster.local;sleep 1; done
 11/15/23 06:33:51.837
STEP: creating a pod to probe DNS 11/15/23 06:33:51.837
STEP: submitting the pod to kubernetes 11/15/23 06:33:51.837
Nov 15 06:33:51.907: INFO: Waiting up to 15m0s for pod "dns-test-c008341a-b80f-40e8-8b21-3de23c91878a" in namespace "dns-810" to be "running"
Nov 15 06:33:51.926: INFO: Pod "dns-test-c008341a-b80f-40e8-8b21-3de23c91878a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.092578ms
Nov 15 06:33:53.943: INFO: Pod "dns-test-c008341a-b80f-40e8-8b21-3de23c91878a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036091343s
Nov 15 06:33:55.943: INFO: Pod "dns-test-c008341a-b80f-40e8-8b21-3de23c91878a": Phase="Running", Reason="", readiness=true. Elapsed: 4.035565176s
Nov 15 06:33:55.943: INFO: Pod "dns-test-c008341a-b80f-40e8-8b21-3de23c91878a" satisfied condition "running"
STEP: retrieving the pod 11/15/23 06:33:55.943
STEP: looking for the results for each expected name from probers 11/15/23 06:33:55.966
Nov 15 06:33:55.997: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local from pod dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a: the server could not find the requested resource (get pods dns-test-c008341a-b80f-40e8-8b21-3de23c91878a)
Nov 15 06:33:56.017: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local from pod dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a: the server could not find the requested resource (get pods dns-test-c008341a-b80f-40e8-8b21-3de23c91878a)
Nov 15 06:33:56.036: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-810.svc.cluster.local from pod dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a: the server could not find the requested resource (get pods dns-test-c008341a-b80f-40e8-8b21-3de23c91878a)
Nov 15 06:33:56.070: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-810.svc.cluster.local from pod dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a: the server could not find the requested resource (get pods dns-test-c008341a-b80f-40e8-8b21-3de23c91878a)
Nov 15 06:33:56.091: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local from pod dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a: the server could not find the requested resource (get pods dns-test-c008341a-b80f-40e8-8b21-3de23c91878a)
Nov 15 06:33:56.112: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local from pod dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a: the server could not find the requested resource (get pods dns-test-c008341a-b80f-40e8-8b21-3de23c91878a)
Nov 15 06:33:56.134: INFO: Unable to read jessie_udp@dns-test-service-2.dns-810.svc.cluster.local from pod dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a: the server could not find the requested resource (get pods dns-test-c008341a-b80f-40e8-8b21-3de23c91878a)
Nov 15 06:33:56.154: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-810.svc.cluster.local from pod dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a: the server could not find the requested resource (get pods dns-test-c008341a-b80f-40e8-8b21-3de23c91878a)
Nov 15 06:33:56.154: INFO: Lookups using dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local wheezy_udp@dns-test-service-2.dns-810.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-810.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local jessie_udp@dns-test-service-2.dns-810.svc.cluster.local jessie_tcp@dns-test-service-2.dns-810.svc.cluster.local]

Nov 15 06:34:01.363: INFO: DNS probes using dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a succeeded

STEP: deleting the pod 11/15/23 06:34:01.363
STEP: deleting the test headless service 11/15/23 06:34:01.416
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 15 06:34:01.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-810" for this suite. 11/15/23 06:34:01.486
------------------------------
â€¢ [SLOW TEST] [9.779 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:33:51.726
    Nov 15 06:33:51.726: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename dns 11/15/23 06:33:51.727
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:33:51.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:33:51.812
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 11/15/23 06:33:51.823
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-810.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-810.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-810.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-810.svc.cluster.local;sleep 1; done
     11/15/23 06:33:51.837
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-810.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-810.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-810.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-810.svc.cluster.local;sleep 1; done
     11/15/23 06:33:51.837
    STEP: creating a pod to probe DNS 11/15/23 06:33:51.837
    STEP: submitting the pod to kubernetes 11/15/23 06:33:51.837
    Nov 15 06:33:51.907: INFO: Waiting up to 15m0s for pod "dns-test-c008341a-b80f-40e8-8b21-3de23c91878a" in namespace "dns-810" to be "running"
    Nov 15 06:33:51.926: INFO: Pod "dns-test-c008341a-b80f-40e8-8b21-3de23c91878a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.092578ms
    Nov 15 06:33:53.943: INFO: Pod "dns-test-c008341a-b80f-40e8-8b21-3de23c91878a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036091343s
    Nov 15 06:33:55.943: INFO: Pod "dns-test-c008341a-b80f-40e8-8b21-3de23c91878a": Phase="Running", Reason="", readiness=true. Elapsed: 4.035565176s
    Nov 15 06:33:55.943: INFO: Pod "dns-test-c008341a-b80f-40e8-8b21-3de23c91878a" satisfied condition "running"
    STEP: retrieving the pod 11/15/23 06:33:55.943
    STEP: looking for the results for each expected name from probers 11/15/23 06:33:55.966
    Nov 15 06:33:55.997: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local from pod dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a: the server could not find the requested resource (get pods dns-test-c008341a-b80f-40e8-8b21-3de23c91878a)
    Nov 15 06:33:56.017: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local from pod dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a: the server could not find the requested resource (get pods dns-test-c008341a-b80f-40e8-8b21-3de23c91878a)
    Nov 15 06:33:56.036: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-810.svc.cluster.local from pod dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a: the server could not find the requested resource (get pods dns-test-c008341a-b80f-40e8-8b21-3de23c91878a)
    Nov 15 06:33:56.070: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-810.svc.cluster.local from pod dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a: the server could not find the requested resource (get pods dns-test-c008341a-b80f-40e8-8b21-3de23c91878a)
    Nov 15 06:33:56.091: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local from pod dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a: the server could not find the requested resource (get pods dns-test-c008341a-b80f-40e8-8b21-3de23c91878a)
    Nov 15 06:33:56.112: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local from pod dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a: the server could not find the requested resource (get pods dns-test-c008341a-b80f-40e8-8b21-3de23c91878a)
    Nov 15 06:33:56.134: INFO: Unable to read jessie_udp@dns-test-service-2.dns-810.svc.cluster.local from pod dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a: the server could not find the requested resource (get pods dns-test-c008341a-b80f-40e8-8b21-3de23c91878a)
    Nov 15 06:33:56.154: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-810.svc.cluster.local from pod dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a: the server could not find the requested resource (get pods dns-test-c008341a-b80f-40e8-8b21-3de23c91878a)
    Nov 15 06:33:56.154: INFO: Lookups using dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local wheezy_udp@dns-test-service-2.dns-810.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-810.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-810.svc.cluster.local jessie_udp@dns-test-service-2.dns-810.svc.cluster.local jessie_tcp@dns-test-service-2.dns-810.svc.cluster.local]

    Nov 15 06:34:01.363: INFO: DNS probes using dns-810/dns-test-c008341a-b80f-40e8-8b21-3de23c91878a succeeded

    STEP: deleting the pod 11/15/23 06:34:01.363
    STEP: deleting the test headless service 11/15/23 06:34:01.416
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:34:01.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-810" for this suite. 11/15/23 06:34:01.486
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:34:01.506
Nov 15 06:34:01.506: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename deployment 11/15/23 06:34:01.507
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:34:01.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:34:01.577
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Nov 15 06:34:01.620: INFO: Creating deployment "test-recreate-deployment"
Nov 15 06:34:01.637: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Nov 15 06:34:01.674: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Nov 15 06:34:03.700: INFO: Waiting deployment "test-recreate-deployment" to complete
Nov 15 06:34:03.712: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 6, 34, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 34, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 34, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 34, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 06:34:05.726: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Nov 15 06:34:05.753: INFO: Updating deployment test-recreate-deployment
Nov 15 06:34:05.753: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 15 06:34:05.967: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9393  8843a056-4d18-43d4-82dd-05407738d97d 96009 2 2023-11-15 06:34:01 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-11-15 06:34:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:34:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003687bc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-11-15 06:34:05 +0000 UTC,LastTransitionTime:2023-11-15 06:34:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-11-15 06:34:05 +0000 UTC,LastTransitionTime:2023-11-15 06:34:01 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Nov 15 06:34:05.979: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9393  03cb0860-3353-4c1a-b732-452d8be184ea 96008 1 2023-11-15 06:34:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 8843a056-4d18-43d4-82dd-05407738d97d 0xc0045c2090 0xc0045c2091}] [] [{kube-controller-manager Update apps/v1 2023-11-15 06:34:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8843a056-4d18-43d4-82dd-05407738d97d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:34:05 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045c2128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 15 06:34:05.979: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Nov 15 06:34:05.979: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9393  d5103ebe-5e37-4449-90a0-feba2d80e785 95998 2 2023-11-15 06:34:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 8843a056-4d18-43d4-82dd-05407738d97d 0xc003687f77 0xc003687f78}] [] [{kube-controller-manager Update apps/v1 2023-11-15 06:34:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8843a056-4d18-43d4-82dd-05407738d97d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:34:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045c2028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 15 06:34:05.993: INFO: Pod "test-recreate-deployment-cff6dc657-k5z4t" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-k5z4t test-recreate-deployment-cff6dc657- deployment-9393  d9600232-987c-4e32-96a2-c8f3ab893462 96010 0 2023-11-15 06:34:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 03cb0860-3353-4c1a-b732-452d8be184ea 0xc0045c2577 0xc0045c2578}] [] [{kube-controller-manager Update v1 2023-11-15 06:34:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03cb0860-3353-4c1a-b732-452d8be184ea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:34:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7h6zn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7h6zn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c55,c45,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9qr6n,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:34:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:34:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:34:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:34:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:,StartTime:2023-11-15 06:34:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 15 06:34:05.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9393" for this suite. 11/15/23 06:34:06.024
------------------------------
â€¢ [4.538 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:34:01.506
    Nov 15 06:34:01.506: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename deployment 11/15/23 06:34:01.507
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:34:01.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:34:01.577
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Nov 15 06:34:01.620: INFO: Creating deployment "test-recreate-deployment"
    Nov 15 06:34:01.637: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Nov 15 06:34:01.674: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Nov 15 06:34:03.700: INFO: Waiting deployment "test-recreate-deployment" to complete
    Nov 15 06:34:03.712: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 6, 34, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 34, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 34, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 34, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 06:34:05.726: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Nov 15 06:34:05.753: INFO: Updating deployment test-recreate-deployment
    Nov 15 06:34:05.753: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 15 06:34:05.967: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9393  8843a056-4d18-43d4-82dd-05407738d97d 96009 2 2023-11-15 06:34:01 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-11-15 06:34:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:34:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003687bc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-11-15 06:34:05 +0000 UTC,LastTransitionTime:2023-11-15 06:34:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-11-15 06:34:05 +0000 UTC,LastTransitionTime:2023-11-15 06:34:01 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Nov 15 06:34:05.979: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9393  03cb0860-3353-4c1a-b732-452d8be184ea 96008 1 2023-11-15 06:34:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 8843a056-4d18-43d4-82dd-05407738d97d 0xc0045c2090 0xc0045c2091}] [] [{kube-controller-manager Update apps/v1 2023-11-15 06:34:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8843a056-4d18-43d4-82dd-05407738d97d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:34:05 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045c2128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 06:34:05.979: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Nov 15 06:34:05.979: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9393  d5103ebe-5e37-4449-90a0-feba2d80e785 95998 2 2023-11-15 06:34:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 8843a056-4d18-43d4-82dd-05407738d97d 0xc003687f77 0xc003687f78}] [] [{kube-controller-manager Update apps/v1 2023-11-15 06:34:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8843a056-4d18-43d4-82dd-05407738d97d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 06:34:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045c2028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 06:34:05.993: INFO: Pod "test-recreate-deployment-cff6dc657-k5z4t" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-k5z4t test-recreate-deployment-cff6dc657- deployment-9393  d9600232-987c-4e32-96a2-c8f3ab893462 96010 0 2023-11-15 06:34:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 03cb0860-3353-4c1a-b732-452d8be184ea 0xc0045c2577 0xc0045c2578}] [] [{kube-controller-manager Update v1 2023-11-15 06:34:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03cb0860-3353-4c1a-b732-452d8be184ea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 06:34:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7h6zn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7h6zn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c55,c45,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9qr6n,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:34:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:34:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:34:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 06:34:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.88,PodIP:,StartTime:2023-11-15 06:34:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:34:05.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9393" for this suite. 11/15/23 06:34:06.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:34:06.045
Nov 15 06:34:06.045: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename pods 11/15/23 06:34:06.046
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:34:06.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:34:06.111
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 11/15/23 06:34:06.122
STEP: submitting the pod to kubernetes 11/15/23 06:34:06.122
Nov 15 06:34:06.151: INFO: Waiting up to 5m0s for pod "pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445" in namespace "pods-8766" to be "running and ready"
Nov 15 06:34:06.164: INFO: Pod "pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445": Phase="Pending", Reason="", readiness=false. Elapsed: 12.41772ms
Nov 15 06:34:06.164: INFO: The phase of Pod pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:34:08.182: INFO: Pod "pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445": Phase="Running", Reason="", readiness=true. Elapsed: 2.030827677s
Nov 15 06:34:08.182: INFO: The phase of Pod pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445 is Running (Ready = true)
Nov 15 06:34:08.182: INFO: Pod "pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 11/15/23 06:34:08.196
STEP: updating the pod 11/15/23 06:34:08.208
Nov 15 06:34:08.743: INFO: Successfully updated pod "pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445"
Nov 15 06:34:08.743: INFO: Waiting up to 5m0s for pod "pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445" in namespace "pods-8766" to be "running"
Nov 15 06:34:08.758: INFO: Pod "pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445": Phase="Running", Reason="", readiness=true. Elapsed: 15.311309ms
Nov 15 06:34:08.758: INFO: Pod "pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 11/15/23 06:34:08.758
Nov 15 06:34:08.771: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 06:34:08.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8766" for this suite. 11/15/23 06:34:08.803
------------------------------
â€¢ [2.782 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:34:06.045
    Nov 15 06:34:06.045: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename pods 11/15/23 06:34:06.046
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:34:06.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:34:06.111
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 11/15/23 06:34:06.122
    STEP: submitting the pod to kubernetes 11/15/23 06:34:06.122
    Nov 15 06:34:06.151: INFO: Waiting up to 5m0s for pod "pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445" in namespace "pods-8766" to be "running and ready"
    Nov 15 06:34:06.164: INFO: Pod "pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445": Phase="Pending", Reason="", readiness=false. Elapsed: 12.41772ms
    Nov 15 06:34:06.164: INFO: The phase of Pod pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:34:08.182: INFO: Pod "pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445": Phase="Running", Reason="", readiness=true. Elapsed: 2.030827677s
    Nov 15 06:34:08.182: INFO: The phase of Pod pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445 is Running (Ready = true)
    Nov 15 06:34:08.182: INFO: Pod "pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 11/15/23 06:34:08.196
    STEP: updating the pod 11/15/23 06:34:08.208
    Nov 15 06:34:08.743: INFO: Successfully updated pod "pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445"
    Nov 15 06:34:08.743: INFO: Waiting up to 5m0s for pod "pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445" in namespace "pods-8766" to be "running"
    Nov 15 06:34:08.758: INFO: Pod "pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445": Phase="Running", Reason="", readiness=true. Elapsed: 15.311309ms
    Nov 15 06:34:08.758: INFO: Pod "pod-update-1f87a049-7e69-42ee-807a-8d55c0bd1445" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 11/15/23 06:34:08.758
    Nov 15 06:34:08.771: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:34:08.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8766" for this suite. 11/15/23 06:34:08.803
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:34:08.828
Nov 15 06:34:08.828: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename resourcequota 11/15/23 06:34:08.829
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:34:08.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:34:08.896
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 11/15/23 06:34:08.907
STEP: Getting a ResourceQuota 11/15/23 06:34:08.927
STEP: Updating a ResourceQuota 11/15/23 06:34:08.94
STEP: Verifying a ResourceQuota was modified 11/15/23 06:34:08.955
STEP: Deleting a ResourceQuota 11/15/23 06:34:08.969
STEP: Verifying the deleted ResourceQuota 11/15/23 06:34:08.989
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 06:34:09.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7314" for this suite. 11/15/23 06:34:09.061
------------------------------
â€¢ [0.265 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:34:08.828
    Nov 15 06:34:08.828: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename resourcequota 11/15/23 06:34:08.829
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:34:08.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:34:08.896
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 11/15/23 06:34:08.907
    STEP: Getting a ResourceQuota 11/15/23 06:34:08.927
    STEP: Updating a ResourceQuota 11/15/23 06:34:08.94
    STEP: Verifying a ResourceQuota was modified 11/15/23 06:34:08.955
    STEP: Deleting a ResourceQuota 11/15/23 06:34:08.969
    STEP: Verifying the deleted ResourceQuota 11/15/23 06:34:08.989
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:34:09.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7314" for this suite. 11/15/23 06:34:09.061
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:34:09.093
Nov 15 06:34:09.093: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename configmap 11/15/23 06:34:09.094
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:34:09.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:34:09.226
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-5ab45e7c-a3dc-4e1d-a6ac-fbd522e179a9 11/15/23 06:34:09.238
STEP: Creating a pod to test consume configMaps 11/15/23 06:34:09.281
Nov 15 06:34:09.309: INFO: Waiting up to 5m0s for pod "pod-configmaps-a24d15b4-2ad9-48c5-a490-42b15cdbcf8a" in namespace "configmap-1739" to be "Succeeded or Failed"
Nov 15 06:34:09.325: INFO: Pod "pod-configmaps-a24d15b4-2ad9-48c5-a490-42b15cdbcf8a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.379558ms
Nov 15 06:34:11.338: INFO: Pod "pod-configmaps-a24d15b4-2ad9-48c5-a490-42b15cdbcf8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028820711s
Nov 15 06:34:13.338: INFO: Pod "pod-configmaps-a24d15b4-2ad9-48c5-a490-42b15cdbcf8a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028991113s
Nov 15 06:34:15.338: INFO: Pod "pod-configmaps-a24d15b4-2ad9-48c5-a490-42b15cdbcf8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029145025s
STEP: Saw pod success 11/15/23 06:34:15.338
Nov 15 06:34:15.338: INFO: Pod "pod-configmaps-a24d15b4-2ad9-48c5-a490-42b15cdbcf8a" satisfied condition "Succeeded or Failed"
Nov 15 06:34:15.350: INFO: Trying to get logs from node 10.72.152.86 pod pod-configmaps-a24d15b4-2ad9-48c5-a490-42b15cdbcf8a container agnhost-container: <nil>
STEP: delete the pod 11/15/23 06:34:15.41
Nov 15 06:34:15.446: INFO: Waiting for pod pod-configmaps-a24d15b4-2ad9-48c5-a490-42b15cdbcf8a to disappear
Nov 15 06:34:15.466: INFO: Pod pod-configmaps-a24d15b4-2ad9-48c5-a490-42b15cdbcf8a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:34:15.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1739" for this suite. 11/15/23 06:34:15.488
------------------------------
â€¢ [SLOW TEST] [6.420 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:34:09.093
    Nov 15 06:34:09.093: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename configmap 11/15/23 06:34:09.094
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:34:09.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:34:09.226
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-5ab45e7c-a3dc-4e1d-a6ac-fbd522e179a9 11/15/23 06:34:09.238
    STEP: Creating a pod to test consume configMaps 11/15/23 06:34:09.281
    Nov 15 06:34:09.309: INFO: Waiting up to 5m0s for pod "pod-configmaps-a24d15b4-2ad9-48c5-a490-42b15cdbcf8a" in namespace "configmap-1739" to be "Succeeded or Failed"
    Nov 15 06:34:09.325: INFO: Pod "pod-configmaps-a24d15b4-2ad9-48c5-a490-42b15cdbcf8a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.379558ms
    Nov 15 06:34:11.338: INFO: Pod "pod-configmaps-a24d15b4-2ad9-48c5-a490-42b15cdbcf8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028820711s
    Nov 15 06:34:13.338: INFO: Pod "pod-configmaps-a24d15b4-2ad9-48c5-a490-42b15cdbcf8a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028991113s
    Nov 15 06:34:15.338: INFO: Pod "pod-configmaps-a24d15b4-2ad9-48c5-a490-42b15cdbcf8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029145025s
    STEP: Saw pod success 11/15/23 06:34:15.338
    Nov 15 06:34:15.338: INFO: Pod "pod-configmaps-a24d15b4-2ad9-48c5-a490-42b15cdbcf8a" satisfied condition "Succeeded or Failed"
    Nov 15 06:34:15.350: INFO: Trying to get logs from node 10.72.152.86 pod pod-configmaps-a24d15b4-2ad9-48c5-a490-42b15cdbcf8a container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 06:34:15.41
    Nov 15 06:34:15.446: INFO: Waiting for pod pod-configmaps-a24d15b4-2ad9-48c5-a490-42b15cdbcf8a to disappear
    Nov 15 06:34:15.466: INFO: Pod pod-configmaps-a24d15b4-2ad9-48c5-a490-42b15cdbcf8a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:34:15.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1739" for this suite. 11/15/23 06:34:15.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:34:15.518
Nov 15 06:34:15.518: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename resourcequota 11/15/23 06:34:15.519
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:34:15.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:34:15.585
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 11/15/23 06:34:15.594
STEP: Creating a ResourceQuota 11/15/23 06:34:20.614
STEP: Ensuring resource quota status is calculated 11/15/23 06:34:20.629
STEP: Creating a Pod that fits quota 11/15/23 06:34:22.667
STEP: Ensuring ResourceQuota status captures the pod usage 11/15/23 06:34:22.733
STEP: Not allowing a pod to be created that exceeds remaining quota 11/15/23 06:34:24.758
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 11/15/23 06:34:24.767
STEP: Ensuring a pod cannot update its resource requirements 11/15/23 06:34:24.775
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 11/15/23 06:34:24.787
STEP: Deleting the pod 11/15/23 06:34:26.801
STEP: Ensuring resource quota status released the pod usage 11/15/23 06:34:26.827
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 06:34:28.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1879" for this suite. 11/15/23 06:34:28.865
------------------------------
â€¢ [SLOW TEST] [13.366 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:34:15.518
    Nov 15 06:34:15.518: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename resourcequota 11/15/23 06:34:15.519
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:34:15.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:34:15.585
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 11/15/23 06:34:15.594
    STEP: Creating a ResourceQuota 11/15/23 06:34:20.614
    STEP: Ensuring resource quota status is calculated 11/15/23 06:34:20.629
    STEP: Creating a Pod that fits quota 11/15/23 06:34:22.667
    STEP: Ensuring ResourceQuota status captures the pod usage 11/15/23 06:34:22.733
    STEP: Not allowing a pod to be created that exceeds remaining quota 11/15/23 06:34:24.758
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 11/15/23 06:34:24.767
    STEP: Ensuring a pod cannot update its resource requirements 11/15/23 06:34:24.775
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 11/15/23 06:34:24.787
    STEP: Deleting the pod 11/15/23 06:34:26.801
    STEP: Ensuring resource quota status released the pod usage 11/15/23 06:34:26.827
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:34:28.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1879" for this suite. 11/15/23 06:34:28.865
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:34:28.885
Nov 15 06:34:28.885: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename container-probe 11/15/23 06:34:28.886
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:34:28.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:34:28.95
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-1bb617c1-942f-4f39-812f-fbb5bc8016d7 in namespace container-probe-6237 11/15/23 06:34:28.962
Nov 15 06:34:28.990: INFO: Waiting up to 5m0s for pod "liveness-1bb617c1-942f-4f39-812f-fbb5bc8016d7" in namespace "container-probe-6237" to be "not pending"
Nov 15 06:34:29.001: INFO: Pod "liveness-1bb617c1-942f-4f39-812f-fbb5bc8016d7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.182037ms
Nov 15 06:34:31.014: INFO: Pod "liveness-1bb617c1-942f-4f39-812f-fbb5bc8016d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024319285s
Nov 15 06:34:33.015: INFO: Pod "liveness-1bb617c1-942f-4f39-812f-fbb5bc8016d7": Phase="Running", Reason="", readiness=true. Elapsed: 4.02551468s
Nov 15 06:34:33.015: INFO: Pod "liveness-1bb617c1-942f-4f39-812f-fbb5bc8016d7" satisfied condition "not pending"
Nov 15 06:34:33.015: INFO: Started pod liveness-1bb617c1-942f-4f39-812f-fbb5bc8016d7 in namespace container-probe-6237
STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 06:34:33.015
Nov 15 06:34:33.029: INFO: Initial restart count of pod liveness-1bb617c1-942f-4f39-812f-fbb5bc8016d7 is 0
STEP: deleting the pod 11/15/23 06:38:34.871
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 15 06:38:34.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6237" for this suite. 11/15/23 06:38:34.927
------------------------------
â€¢ [SLOW TEST] [246.068 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:34:28.885
    Nov 15 06:34:28.885: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename container-probe 11/15/23 06:34:28.886
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:34:28.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:34:28.95
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-1bb617c1-942f-4f39-812f-fbb5bc8016d7 in namespace container-probe-6237 11/15/23 06:34:28.962
    Nov 15 06:34:28.990: INFO: Waiting up to 5m0s for pod "liveness-1bb617c1-942f-4f39-812f-fbb5bc8016d7" in namespace "container-probe-6237" to be "not pending"
    Nov 15 06:34:29.001: INFO: Pod "liveness-1bb617c1-942f-4f39-812f-fbb5bc8016d7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.182037ms
    Nov 15 06:34:31.014: INFO: Pod "liveness-1bb617c1-942f-4f39-812f-fbb5bc8016d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024319285s
    Nov 15 06:34:33.015: INFO: Pod "liveness-1bb617c1-942f-4f39-812f-fbb5bc8016d7": Phase="Running", Reason="", readiness=true. Elapsed: 4.02551468s
    Nov 15 06:34:33.015: INFO: Pod "liveness-1bb617c1-942f-4f39-812f-fbb5bc8016d7" satisfied condition "not pending"
    Nov 15 06:34:33.015: INFO: Started pod liveness-1bb617c1-942f-4f39-812f-fbb5bc8016d7 in namespace container-probe-6237
    STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 06:34:33.015
    Nov 15 06:34:33.029: INFO: Initial restart count of pod liveness-1bb617c1-942f-4f39-812f-fbb5bc8016d7 is 0
    STEP: deleting the pod 11/15/23 06:38:34.871
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:38:34.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6237" for this suite. 11/15/23 06:38:34.927
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:38:34.955
Nov 15 06:38:34.955: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename resourcequota 11/15/23 06:38:34.956
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:38:35.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:38:35.072
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 11/15/23 06:38:35.083
STEP: Ensuring ResourceQuota status is calculated 11/15/23 06:38:35.1
STEP: Creating a ResourceQuota with not best effort scope 11/15/23 06:38:37.115
STEP: Ensuring ResourceQuota status is calculated 11/15/23 06:38:37.13
STEP: Creating a best-effort pod 11/15/23 06:38:39.145
STEP: Ensuring resource quota with best effort scope captures the pod usage 11/15/23 06:38:39.195
STEP: Ensuring resource quota with not best effort ignored the pod usage 11/15/23 06:38:41.22
STEP: Deleting the pod 11/15/23 06:38:43.234
STEP: Ensuring resource quota status released the pod usage 11/15/23 06:38:43.262
STEP: Creating a not best-effort pod 11/15/23 06:38:45.305
STEP: Ensuring resource quota with not best effort scope captures the pod usage 11/15/23 06:38:45.333
STEP: Ensuring resource quota with best effort scope ignored the pod usage 11/15/23 06:38:47.347
STEP: Deleting the pod 11/15/23 06:38:49.361
STEP: Ensuring resource quota status released the pod usage 11/15/23 06:38:49.396
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 06:38:51.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6314" for this suite. 11/15/23 06:38:51.444
------------------------------
â€¢ [SLOW TEST] [16.527 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:38:34.955
    Nov 15 06:38:34.955: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename resourcequota 11/15/23 06:38:34.956
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:38:35.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:38:35.072
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 11/15/23 06:38:35.083
    STEP: Ensuring ResourceQuota status is calculated 11/15/23 06:38:35.1
    STEP: Creating a ResourceQuota with not best effort scope 11/15/23 06:38:37.115
    STEP: Ensuring ResourceQuota status is calculated 11/15/23 06:38:37.13
    STEP: Creating a best-effort pod 11/15/23 06:38:39.145
    STEP: Ensuring resource quota with best effort scope captures the pod usage 11/15/23 06:38:39.195
    STEP: Ensuring resource quota with not best effort ignored the pod usage 11/15/23 06:38:41.22
    STEP: Deleting the pod 11/15/23 06:38:43.234
    STEP: Ensuring resource quota status released the pod usage 11/15/23 06:38:43.262
    STEP: Creating a not best-effort pod 11/15/23 06:38:45.305
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 11/15/23 06:38:45.333
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 11/15/23 06:38:47.347
    STEP: Deleting the pod 11/15/23 06:38:49.361
    STEP: Ensuring resource quota status released the pod usage 11/15/23 06:38:49.396
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:38:51.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6314" for this suite. 11/15/23 06:38:51.444
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:38:51.488
Nov 15 06:38:51.488: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename gc 11/15/23 06:38:51.489
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:38:51.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:38:51.571
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 11/15/23 06:38:51.586
STEP: delete the rc 11/15/23 06:38:56.627
STEP: wait for all pods to be garbage collected 11/15/23 06:38:56.657
STEP: Gathering metrics 11/15/23 06:39:01.689
W1115 06:39:01.721178      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 15 06:39:01.721: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 15 06:39:01.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7777" for this suite. 11/15/23 06:39:01.739
------------------------------
â€¢ [SLOW TEST] [10.278 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:38:51.488
    Nov 15 06:38:51.488: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename gc 11/15/23 06:38:51.489
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:38:51.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:38:51.571
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 11/15/23 06:38:51.586
    STEP: delete the rc 11/15/23 06:38:56.627
    STEP: wait for all pods to be garbage collected 11/15/23 06:38:56.657
    STEP: Gathering metrics 11/15/23 06:39:01.689
    W1115 06:39:01.721178      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Nov 15 06:39:01.721: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:39:01.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7777" for this suite. 11/15/23 06:39:01.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:39:01.766
Nov 15 06:39:01.767: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename webhook 11/15/23 06:39:01.767
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:39:01.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:39:01.838
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 06:39:01.929
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:39:02.481
STEP: Deploying the webhook pod 11/15/23 06:39:02.507
STEP: Wait for the deployment to be ready 11/15/23 06:39:02.536
Nov 15 06:39:02.569: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 06:39:04.615: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 6, 39, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 39, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 39, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 39, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 06:39:06.632
STEP: Verifying the service has paired with the endpoint 11/15/23 06:39:06.668
Nov 15 06:39:07.671: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 11/15/23 06:39:07.685
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 11/15/23 06:39:07.69
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 11/15/23 06:39:07.69
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 11/15/23 06:39:07.69
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 11/15/23 06:39:07.695
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 11/15/23 06:39:07.695
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 11/15/23 06:39:07.7
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:39:07.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5835" for this suite. 11/15/23 06:39:07.837
STEP: Destroying namespace "webhook-5835-markers" for this suite. 11/15/23 06:39:07.856
------------------------------
â€¢ [SLOW TEST] [6.109 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:39:01.766
    Nov 15 06:39:01.767: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename webhook 11/15/23 06:39:01.767
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:39:01.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:39:01.838
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 06:39:01.929
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:39:02.481
    STEP: Deploying the webhook pod 11/15/23 06:39:02.507
    STEP: Wait for the deployment to be ready 11/15/23 06:39:02.536
    Nov 15 06:39:02.569: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 06:39:04.615: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 6, 39, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 39, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 39, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 39, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 06:39:06.632
    STEP: Verifying the service has paired with the endpoint 11/15/23 06:39:06.668
    Nov 15 06:39:07.671: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 11/15/23 06:39:07.685
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 11/15/23 06:39:07.69
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 11/15/23 06:39:07.69
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 11/15/23 06:39:07.69
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 11/15/23 06:39:07.695
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 11/15/23 06:39:07.695
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 11/15/23 06:39:07.7
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:39:07.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5835" for this suite. 11/15/23 06:39:07.837
    STEP: Destroying namespace "webhook-5835-markers" for this suite. 11/15/23 06:39:07.856
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:39:07.876
Nov 15 06:39:07.876: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename subpath 11/15/23 06:39:07.877
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:39:07.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:39:07.941
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/15/23 06:39:07.951
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-25md 11/15/23 06:39:07.981
STEP: Creating a pod to test atomic-volume-subpath 11/15/23 06:39:07.981
Nov 15 06:39:08.004: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-25md" in namespace "subpath-4959" to be "Succeeded or Failed"
Nov 15 06:39:08.026: INFO: Pod "pod-subpath-test-secret-25md": Phase="Pending", Reason="", readiness=false. Elapsed: 21.642702ms
Nov 15 06:39:10.039: INFO: Pod "pod-subpath-test-secret-25md": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034666443s
Nov 15 06:39:12.039: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 4.034830674s
Nov 15 06:39:14.088: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 6.083694721s
Nov 15 06:39:16.038: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 8.033865832s
Nov 15 06:39:18.039: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 10.035238885s
Nov 15 06:39:20.041: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 12.037206301s
Nov 15 06:39:22.046: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 14.041687935s
Nov 15 06:39:24.040: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 16.035931531s
Nov 15 06:39:26.039: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 18.035147282s
Nov 15 06:39:28.039: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 20.03506964s
Nov 15 06:39:30.041: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 22.037061026s
Nov 15 06:39:32.038: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=false. Elapsed: 24.033546489s
Nov 15 06:39:34.041: INFO: Pod "pod-subpath-test-secret-25md": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.036572749s
STEP: Saw pod success 11/15/23 06:39:34.041
Nov 15 06:39:34.041: INFO: Pod "pod-subpath-test-secret-25md" satisfied condition "Succeeded or Failed"
Nov 15 06:39:34.053: INFO: Trying to get logs from node 10.72.152.86 pod pod-subpath-test-secret-25md container test-container-subpath-secret-25md: <nil>
STEP: delete the pod 11/15/23 06:39:34.136
Nov 15 06:39:34.165: INFO: Waiting for pod pod-subpath-test-secret-25md to disappear
Nov 15 06:39:34.176: INFO: Pod pod-subpath-test-secret-25md no longer exists
STEP: Deleting pod pod-subpath-test-secret-25md 11/15/23 06:39:34.176
Nov 15 06:39:34.176: INFO: Deleting pod "pod-subpath-test-secret-25md" in namespace "subpath-4959"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Nov 15 06:39:34.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4959" for this suite. 11/15/23 06:39:34.206
------------------------------
â€¢ [SLOW TEST] [26.348 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:39:07.876
    Nov 15 06:39:07.876: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename subpath 11/15/23 06:39:07.877
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:39:07.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:39:07.941
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/15/23 06:39:07.951
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-25md 11/15/23 06:39:07.981
    STEP: Creating a pod to test atomic-volume-subpath 11/15/23 06:39:07.981
    Nov 15 06:39:08.004: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-25md" in namespace "subpath-4959" to be "Succeeded or Failed"
    Nov 15 06:39:08.026: INFO: Pod "pod-subpath-test-secret-25md": Phase="Pending", Reason="", readiness=false. Elapsed: 21.642702ms
    Nov 15 06:39:10.039: INFO: Pod "pod-subpath-test-secret-25md": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034666443s
    Nov 15 06:39:12.039: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 4.034830674s
    Nov 15 06:39:14.088: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 6.083694721s
    Nov 15 06:39:16.038: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 8.033865832s
    Nov 15 06:39:18.039: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 10.035238885s
    Nov 15 06:39:20.041: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 12.037206301s
    Nov 15 06:39:22.046: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 14.041687935s
    Nov 15 06:39:24.040: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 16.035931531s
    Nov 15 06:39:26.039: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 18.035147282s
    Nov 15 06:39:28.039: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 20.03506964s
    Nov 15 06:39:30.041: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=true. Elapsed: 22.037061026s
    Nov 15 06:39:32.038: INFO: Pod "pod-subpath-test-secret-25md": Phase="Running", Reason="", readiness=false. Elapsed: 24.033546489s
    Nov 15 06:39:34.041: INFO: Pod "pod-subpath-test-secret-25md": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.036572749s
    STEP: Saw pod success 11/15/23 06:39:34.041
    Nov 15 06:39:34.041: INFO: Pod "pod-subpath-test-secret-25md" satisfied condition "Succeeded or Failed"
    Nov 15 06:39:34.053: INFO: Trying to get logs from node 10.72.152.86 pod pod-subpath-test-secret-25md container test-container-subpath-secret-25md: <nil>
    STEP: delete the pod 11/15/23 06:39:34.136
    Nov 15 06:39:34.165: INFO: Waiting for pod pod-subpath-test-secret-25md to disappear
    Nov 15 06:39:34.176: INFO: Pod pod-subpath-test-secret-25md no longer exists
    STEP: Deleting pod pod-subpath-test-secret-25md 11/15/23 06:39:34.176
    Nov 15 06:39:34.176: INFO: Deleting pod "pod-subpath-test-secret-25md" in namespace "subpath-4959"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:39:34.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4959" for this suite. 11/15/23 06:39:34.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:39:34.228
Nov 15 06:39:34.228: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename cronjob 11/15/23 06:39:34.23
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:39:34.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:39:34.305
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 11/15/23 06:39:34.316
STEP: Ensuring a job is scheduled 11/15/23 06:39:34.336
STEP: Ensuring exactly one is scheduled 11/15/23 06:40:00.354
STEP: Ensuring exactly one running job exists by listing jobs explicitly 11/15/23 06:40:00.369
STEP: Ensuring no more jobs are scheduled 11/15/23 06:40:00.381
STEP: Removing cronjob 11/15/23 06:45:00.413
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Nov 15 06:45:00.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5140" for this suite. 11/15/23 06:45:00.457
------------------------------
â€¢ [SLOW TEST] [326.251 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:39:34.228
    Nov 15 06:39:34.228: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename cronjob 11/15/23 06:39:34.23
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:39:34.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:39:34.305
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 11/15/23 06:39:34.316
    STEP: Ensuring a job is scheduled 11/15/23 06:39:34.336
    STEP: Ensuring exactly one is scheduled 11/15/23 06:40:00.354
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 11/15/23 06:40:00.369
    STEP: Ensuring no more jobs are scheduled 11/15/23 06:40:00.381
    STEP: Removing cronjob 11/15/23 06:45:00.413
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:45:00.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5140" for this suite. 11/15/23 06:45:00.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:45:00.48
Nov 15 06:45:00.480: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename pods 11/15/23 06:45:00.481
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:45:00.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:45:00.561
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 11/15/23 06:45:00.571
Nov 15 06:45:00.602: INFO: Waiting up to 5m0s for pod "pod-hostip-f50cedd9-8f29-40c7-84ac-8637e388296b" in namespace "pods-8396" to be "running and ready"
Nov 15 06:45:00.620: INFO: Pod "pod-hostip-f50cedd9-8f29-40c7-84ac-8637e388296b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.700979ms
Nov 15 06:45:00.620: INFO: The phase of Pod pod-hostip-f50cedd9-8f29-40c7-84ac-8637e388296b is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:45:02.640: INFO: Pod "pod-hostip-f50cedd9-8f29-40c7-84ac-8637e388296b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037985053s
Nov 15 06:45:02.640: INFO: The phase of Pod pod-hostip-f50cedd9-8f29-40c7-84ac-8637e388296b is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:45:04.632: INFO: Pod "pod-hostip-f50cedd9-8f29-40c7-84ac-8637e388296b": Phase="Running", Reason="", readiness=true. Elapsed: 4.030140049s
Nov 15 06:45:04.632: INFO: The phase of Pod pod-hostip-f50cedd9-8f29-40c7-84ac-8637e388296b is Running (Ready = true)
Nov 15 06:45:04.632: INFO: Pod "pod-hostip-f50cedd9-8f29-40c7-84ac-8637e388296b" satisfied condition "running and ready"
Nov 15 06:45:04.683: INFO: Pod pod-hostip-f50cedd9-8f29-40c7-84ac-8637e388296b has hostIP: 10.72.152.81
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 06:45:04.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8396" for this suite. 11/15/23 06:45:04.731
------------------------------
â€¢ [4.272 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:45:00.48
    Nov 15 06:45:00.480: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename pods 11/15/23 06:45:00.481
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:45:00.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:45:00.561
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 11/15/23 06:45:00.571
    Nov 15 06:45:00.602: INFO: Waiting up to 5m0s for pod "pod-hostip-f50cedd9-8f29-40c7-84ac-8637e388296b" in namespace "pods-8396" to be "running and ready"
    Nov 15 06:45:00.620: INFO: Pod "pod-hostip-f50cedd9-8f29-40c7-84ac-8637e388296b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.700979ms
    Nov 15 06:45:00.620: INFO: The phase of Pod pod-hostip-f50cedd9-8f29-40c7-84ac-8637e388296b is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:45:02.640: INFO: Pod "pod-hostip-f50cedd9-8f29-40c7-84ac-8637e388296b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037985053s
    Nov 15 06:45:02.640: INFO: The phase of Pod pod-hostip-f50cedd9-8f29-40c7-84ac-8637e388296b is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:45:04.632: INFO: Pod "pod-hostip-f50cedd9-8f29-40c7-84ac-8637e388296b": Phase="Running", Reason="", readiness=true. Elapsed: 4.030140049s
    Nov 15 06:45:04.632: INFO: The phase of Pod pod-hostip-f50cedd9-8f29-40c7-84ac-8637e388296b is Running (Ready = true)
    Nov 15 06:45:04.632: INFO: Pod "pod-hostip-f50cedd9-8f29-40c7-84ac-8637e388296b" satisfied condition "running and ready"
    Nov 15 06:45:04.683: INFO: Pod pod-hostip-f50cedd9-8f29-40c7-84ac-8637e388296b has hostIP: 10.72.152.81
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:45:04.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8396" for this suite. 11/15/23 06:45:04.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:45:04.755
Nov 15 06:45:04.755: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename cronjob 11/15/23 06:45:04.756
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:45:04.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:45:04.823
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 11/15/23 06:45:04.833
W1115 06:45:04.853080      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring no jobs are scheduled 11/15/23 06:45:04.853
STEP: Ensuring no job exists by listing jobs explicitly 11/15/23 06:50:04.883
STEP: Removing cronjob 11/15/23 06:50:04.894
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Nov 15 06:50:04.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-779" for this suite. 11/15/23 06:50:04.944
------------------------------
â€¢ [SLOW TEST] [300.216 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:45:04.755
    Nov 15 06:45:04.755: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename cronjob 11/15/23 06:45:04.756
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:45:04.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:45:04.823
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 11/15/23 06:45:04.833
    W1115 06:45:04.853080      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring no jobs are scheduled 11/15/23 06:45:04.853
    STEP: Ensuring no job exists by listing jobs explicitly 11/15/23 06:50:04.883
    STEP: Removing cronjob 11/15/23 06:50:04.894
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:50:04.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-779" for this suite. 11/15/23 06:50:04.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:50:04.971
Nov 15 06:50:04.971: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename secrets 11/15/23 06:50:04.972
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:05.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:05.066
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-8ecccb89-0c55-4033-93e8-b7854dd27238 11/15/23 06:50:05.076
STEP: Creating a pod to test consume secrets 11/15/23 06:50:05.091
Nov 15 06:50:05.132: INFO: Waiting up to 5m0s for pod "pod-secrets-64744197-82b8-4dbf-8753-673ae30fe2e1" in namespace "secrets-8528" to be "Succeeded or Failed"
Nov 15 06:50:05.143: INFO: Pod "pod-secrets-64744197-82b8-4dbf-8753-673ae30fe2e1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.697362ms
Nov 15 06:50:07.155: INFO: Pod "pod-secrets-64744197-82b8-4dbf-8753-673ae30fe2e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022958006s
Nov 15 06:50:09.156: INFO: Pod "pod-secrets-64744197-82b8-4dbf-8753-673ae30fe2e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023300406s
Nov 15 06:50:11.157: INFO: Pod "pod-secrets-64744197-82b8-4dbf-8753-673ae30fe2e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024561584s
STEP: Saw pod success 11/15/23 06:50:11.157
Nov 15 06:50:11.158: INFO: Pod "pod-secrets-64744197-82b8-4dbf-8753-673ae30fe2e1" satisfied condition "Succeeded or Failed"
Nov 15 06:50:11.169: INFO: Trying to get logs from node 10.72.152.86 pod pod-secrets-64744197-82b8-4dbf-8753-673ae30fe2e1 container secret-volume-test: <nil>
STEP: delete the pod 11/15/23 06:50:11.231
Nov 15 06:50:11.265: INFO: Waiting for pod pod-secrets-64744197-82b8-4dbf-8753-673ae30fe2e1 to disappear
Nov 15 06:50:11.275: INFO: Pod pod-secrets-64744197-82b8-4dbf-8753-673ae30fe2e1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 06:50:11.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8528" for this suite. 11/15/23 06:50:11.295
------------------------------
â€¢ [SLOW TEST] [6.343 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:50:04.971
    Nov 15 06:50:04.971: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename secrets 11/15/23 06:50:04.972
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:05.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:05.066
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-8ecccb89-0c55-4033-93e8-b7854dd27238 11/15/23 06:50:05.076
    STEP: Creating a pod to test consume secrets 11/15/23 06:50:05.091
    Nov 15 06:50:05.132: INFO: Waiting up to 5m0s for pod "pod-secrets-64744197-82b8-4dbf-8753-673ae30fe2e1" in namespace "secrets-8528" to be "Succeeded or Failed"
    Nov 15 06:50:05.143: INFO: Pod "pod-secrets-64744197-82b8-4dbf-8753-673ae30fe2e1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.697362ms
    Nov 15 06:50:07.155: INFO: Pod "pod-secrets-64744197-82b8-4dbf-8753-673ae30fe2e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022958006s
    Nov 15 06:50:09.156: INFO: Pod "pod-secrets-64744197-82b8-4dbf-8753-673ae30fe2e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023300406s
    Nov 15 06:50:11.157: INFO: Pod "pod-secrets-64744197-82b8-4dbf-8753-673ae30fe2e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024561584s
    STEP: Saw pod success 11/15/23 06:50:11.157
    Nov 15 06:50:11.158: INFO: Pod "pod-secrets-64744197-82b8-4dbf-8753-673ae30fe2e1" satisfied condition "Succeeded or Failed"
    Nov 15 06:50:11.169: INFO: Trying to get logs from node 10.72.152.86 pod pod-secrets-64744197-82b8-4dbf-8753-673ae30fe2e1 container secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 06:50:11.231
    Nov 15 06:50:11.265: INFO: Waiting for pod pod-secrets-64744197-82b8-4dbf-8753-673ae30fe2e1 to disappear
    Nov 15 06:50:11.275: INFO: Pod pod-secrets-64744197-82b8-4dbf-8753-673ae30fe2e1 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:50:11.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8528" for this suite. 11/15/23 06:50:11.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:50:11.316
Nov 15 06:50:11.316: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename namespaces 11/15/23 06:50:11.318
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:11.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:11.381
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-grq8j" 11/15/23 06:50:11.39
Nov 15 06:50:11.471: INFO: Namespace "e2e-ns-grq8j-9691" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-grq8j-9691" 11/15/23 06:50:11.471
Nov 15 06:50:11.503: INFO: Namespace "e2e-ns-grq8j-9691" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-grq8j-9691" 11/15/23 06:50:11.503
Nov 15 06:50:11.536: INFO: Namespace "e2e-ns-grq8j-9691" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:50:11.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8155" for this suite. 11/15/23 06:50:11.559
STEP: Destroying namespace "e2e-ns-grq8j-9691" for this suite. 11/15/23 06:50:11.581
------------------------------
â€¢ [0.285 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:50:11.316
    Nov 15 06:50:11.316: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename namespaces 11/15/23 06:50:11.318
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:11.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:11.381
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-grq8j" 11/15/23 06:50:11.39
    Nov 15 06:50:11.471: INFO: Namespace "e2e-ns-grq8j-9691" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-grq8j-9691" 11/15/23 06:50:11.471
    Nov 15 06:50:11.503: INFO: Namespace "e2e-ns-grq8j-9691" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-grq8j-9691" 11/15/23 06:50:11.503
    Nov 15 06:50:11.536: INFO: Namespace "e2e-ns-grq8j-9691" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:50:11.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8155" for this suite. 11/15/23 06:50:11.559
    STEP: Destroying namespace "e2e-ns-grq8j-9691" for this suite. 11/15/23 06:50:11.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:50:11.603
Nov 15 06:50:11.603: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubectl 11/15/23 06:50:11.605
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:11.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:11.69
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 11/15/23 06:50:11.699
Nov 15 06:50:11.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-6875 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Nov 15 06:50:11.828: INFO: stderr: ""
Nov 15 06:50:11.828: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 11/15/23 06:50:11.828
Nov 15 06:50:11.828: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Nov 15 06:50:11.828: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6875" to be "running and ready, or succeeded"
Nov 15 06:50:11.841: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 13.098498ms
Nov 15 06:50:11.841: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.72.152.86' to be 'Running' but was 'Pending'
Nov 15 06:50:13.857: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.029102751s
Nov 15 06:50:13.857: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Nov 15 06:50:13.857: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 11/15/23 06:50:13.857
Nov 15 06:50:13.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-6875 logs logs-generator logs-generator'
Nov 15 06:50:13.973: INFO: stderr: ""
Nov 15 06:50:13.973: INFO: stdout: "I1115 06:50:13.182783       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/nz4m 267\nI1115 06:50:13.382957       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/r2zw 374\nI1115 06:50:13.583035       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/896p 339\nI1115 06:50:13.783703       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/6ckw 578\n"
Nov 15 06:50:15.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-6875 logs logs-generator logs-generator'
Nov 15 06:50:16.153: INFO: stderr: ""
Nov 15 06:50:16.153: INFO: stdout: "I1115 06:50:13.182783       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/nz4m 267\nI1115 06:50:13.382957       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/r2zw 374\nI1115 06:50:13.583035       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/896p 339\nI1115 06:50:13.783703       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/6ckw 578\nI1115 06:50:13.982994       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/xsz 476\nI1115 06:50:14.183218       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/56ng 493\nI1115 06:50:14.383484       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/v72 372\nI1115 06:50:14.583170       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/jgq 411\nI1115 06:50:14.783625       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/xgrj 477\nI1115 06:50:14.982950       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/tk2 405\nI1115 06:50:15.183363       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/9jvf 415\nI1115 06:50:15.383767       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/f6s 201\nI1115 06:50:15.583141       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/hdcq 550\nI1115 06:50:15.783542       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/d69 595\nI1115 06:50:15.994983       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/rw7 325\n"
STEP: limiting log lines 11/15/23 06:50:16.153
Nov 15 06:50:16.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-6875 logs logs-generator logs-generator --tail=1'
Nov 15 06:50:16.312: INFO: stderr: ""
Nov 15 06:50:16.312: INFO: stdout: "I1115 06:50:16.182937       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/2np 426\n"
Nov 15 06:50:16.312: INFO: got output "I1115 06:50:16.182937       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/2np 426\n"
STEP: limiting log bytes 11/15/23 06:50:16.312
Nov 15 06:50:16.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-6875 logs logs-generator logs-generator --limit-bytes=1'
Nov 15 06:50:16.456: INFO: stderr: ""
Nov 15 06:50:16.456: INFO: stdout: "I"
Nov 15 06:50:16.456: INFO: got output "I"
STEP: exposing timestamps 11/15/23 06:50:16.456
Nov 15 06:50:16.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-6875 logs logs-generator logs-generator --tail=1 --timestamps'
Nov 15 06:50:16.597: INFO: stderr: ""
Nov 15 06:50:16.597: INFO: stdout: "2023-11-15T00:50:16.583238702-06:00 I1115 06:50:16.583153       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/n2hs 575\n"
Nov 15 06:50:16.597: INFO: got output "2023-11-15T00:50:16.583238702-06:00 I1115 06:50:16.583153       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/n2hs 575\n"
STEP: restricting to a time range 11/15/23 06:50:16.597
Nov 15 06:50:19.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-6875 logs logs-generator logs-generator --since=1s'
Nov 15 06:50:19.240: INFO: stderr: ""
Nov 15 06:50:19.240: INFO: stdout: "I1115 06:50:18.382933       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/bqng 243\nI1115 06:50:18.583286       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/xbq 314\nI1115 06:50:18.783767       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/5sx 424\nI1115 06:50:18.986737       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/pr4 347\nI1115 06:50:19.183140       1 logs_generator.go:76] 30 POST /api/v1/namespaces/kube-system/pods/98x 506\n"
Nov 15 06:50:19.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-6875 logs logs-generator logs-generator --since=24h'
Nov 15 06:50:19.385: INFO: stderr: ""
Nov 15 06:50:19.385: INFO: stdout: "I1115 06:50:13.182783       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/nz4m 267\nI1115 06:50:13.382957       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/r2zw 374\nI1115 06:50:13.583035       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/896p 339\nI1115 06:50:13.783703       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/6ckw 578\nI1115 06:50:13.982994       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/xsz 476\nI1115 06:50:14.183218       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/56ng 493\nI1115 06:50:14.383484       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/v72 372\nI1115 06:50:14.583170       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/jgq 411\nI1115 06:50:14.783625       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/xgrj 477\nI1115 06:50:14.982950       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/tk2 405\nI1115 06:50:15.183363       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/9jvf 415\nI1115 06:50:15.383767       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/f6s 201\nI1115 06:50:15.583141       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/hdcq 550\nI1115 06:50:15.783542       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/d69 595\nI1115 06:50:15.994983       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/rw7 325\nI1115 06:50:16.182937       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/2np 426\nI1115 06:50:16.387700       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/d5s 214\nI1115 06:50:16.583153       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/n2hs 575\nI1115 06:50:16.783583       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/flx 319\nI1115 06:50:16.982932       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/4s9 543\nI1115 06:50:17.183338       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/24q 559\nI1115 06:50:17.383794       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/v6d 368\nI1115 06:50:17.583389       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/tsm 257\nI1115 06:50:17.782836       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/tdpr 422\nI1115 06:50:17.983270       1 logs_generator.go:76] 24 POST /api/v1/namespaces/default/pods/5pk 269\nI1115 06:50:18.183696       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/g78 315\nI1115 06:50:18.382933       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/bqng 243\nI1115 06:50:18.583286       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/xbq 314\nI1115 06:50:18.783767       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/5sx 424\nI1115 06:50:18.986737       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/pr4 347\nI1115 06:50:19.183140       1 logs_generator.go:76] 30 POST /api/v1/namespaces/kube-system/pods/98x 506\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Nov 15 06:50:19.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-6875 delete pod logs-generator'
Nov 15 06:50:20.739: INFO: stderr: ""
Nov 15 06:50:20.739: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 06:50:20.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6875" for this suite. 11/15/23 06:50:20.759
------------------------------
â€¢ [SLOW TEST] [9.176 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:50:11.603
    Nov 15 06:50:11.603: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubectl 11/15/23 06:50:11.605
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:11.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:11.69
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 11/15/23 06:50:11.699
    Nov 15 06:50:11.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-6875 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Nov 15 06:50:11.828: INFO: stderr: ""
    Nov 15 06:50:11.828: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 11/15/23 06:50:11.828
    Nov 15 06:50:11.828: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Nov 15 06:50:11.828: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6875" to be "running and ready, or succeeded"
    Nov 15 06:50:11.841: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 13.098498ms
    Nov 15 06:50:11.841: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.72.152.86' to be 'Running' but was 'Pending'
    Nov 15 06:50:13.857: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.029102751s
    Nov 15 06:50:13.857: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Nov 15 06:50:13.857: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 11/15/23 06:50:13.857
    Nov 15 06:50:13.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-6875 logs logs-generator logs-generator'
    Nov 15 06:50:13.973: INFO: stderr: ""
    Nov 15 06:50:13.973: INFO: stdout: "I1115 06:50:13.182783       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/nz4m 267\nI1115 06:50:13.382957       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/r2zw 374\nI1115 06:50:13.583035       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/896p 339\nI1115 06:50:13.783703       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/6ckw 578\n"
    Nov 15 06:50:15.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-6875 logs logs-generator logs-generator'
    Nov 15 06:50:16.153: INFO: stderr: ""
    Nov 15 06:50:16.153: INFO: stdout: "I1115 06:50:13.182783       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/nz4m 267\nI1115 06:50:13.382957       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/r2zw 374\nI1115 06:50:13.583035       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/896p 339\nI1115 06:50:13.783703       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/6ckw 578\nI1115 06:50:13.982994       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/xsz 476\nI1115 06:50:14.183218       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/56ng 493\nI1115 06:50:14.383484       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/v72 372\nI1115 06:50:14.583170       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/jgq 411\nI1115 06:50:14.783625       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/xgrj 477\nI1115 06:50:14.982950       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/tk2 405\nI1115 06:50:15.183363       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/9jvf 415\nI1115 06:50:15.383767       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/f6s 201\nI1115 06:50:15.583141       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/hdcq 550\nI1115 06:50:15.783542       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/d69 595\nI1115 06:50:15.994983       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/rw7 325\n"
    STEP: limiting log lines 11/15/23 06:50:16.153
    Nov 15 06:50:16.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-6875 logs logs-generator logs-generator --tail=1'
    Nov 15 06:50:16.312: INFO: stderr: ""
    Nov 15 06:50:16.312: INFO: stdout: "I1115 06:50:16.182937       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/2np 426\n"
    Nov 15 06:50:16.312: INFO: got output "I1115 06:50:16.182937       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/2np 426\n"
    STEP: limiting log bytes 11/15/23 06:50:16.312
    Nov 15 06:50:16.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-6875 logs logs-generator logs-generator --limit-bytes=1'
    Nov 15 06:50:16.456: INFO: stderr: ""
    Nov 15 06:50:16.456: INFO: stdout: "I"
    Nov 15 06:50:16.456: INFO: got output "I"
    STEP: exposing timestamps 11/15/23 06:50:16.456
    Nov 15 06:50:16.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-6875 logs logs-generator logs-generator --tail=1 --timestamps'
    Nov 15 06:50:16.597: INFO: stderr: ""
    Nov 15 06:50:16.597: INFO: stdout: "2023-11-15T00:50:16.583238702-06:00 I1115 06:50:16.583153       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/n2hs 575\n"
    Nov 15 06:50:16.597: INFO: got output "2023-11-15T00:50:16.583238702-06:00 I1115 06:50:16.583153       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/n2hs 575\n"
    STEP: restricting to a time range 11/15/23 06:50:16.597
    Nov 15 06:50:19.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-6875 logs logs-generator logs-generator --since=1s'
    Nov 15 06:50:19.240: INFO: stderr: ""
    Nov 15 06:50:19.240: INFO: stdout: "I1115 06:50:18.382933       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/bqng 243\nI1115 06:50:18.583286       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/xbq 314\nI1115 06:50:18.783767       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/5sx 424\nI1115 06:50:18.986737       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/pr4 347\nI1115 06:50:19.183140       1 logs_generator.go:76] 30 POST /api/v1/namespaces/kube-system/pods/98x 506\n"
    Nov 15 06:50:19.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-6875 logs logs-generator logs-generator --since=24h'
    Nov 15 06:50:19.385: INFO: stderr: ""
    Nov 15 06:50:19.385: INFO: stdout: "I1115 06:50:13.182783       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/nz4m 267\nI1115 06:50:13.382957       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/r2zw 374\nI1115 06:50:13.583035       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/896p 339\nI1115 06:50:13.783703       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/6ckw 578\nI1115 06:50:13.982994       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/xsz 476\nI1115 06:50:14.183218       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/56ng 493\nI1115 06:50:14.383484       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/v72 372\nI1115 06:50:14.583170       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/jgq 411\nI1115 06:50:14.783625       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/xgrj 477\nI1115 06:50:14.982950       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/tk2 405\nI1115 06:50:15.183363       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/9jvf 415\nI1115 06:50:15.383767       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/f6s 201\nI1115 06:50:15.583141       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/hdcq 550\nI1115 06:50:15.783542       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/d69 595\nI1115 06:50:15.994983       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/rw7 325\nI1115 06:50:16.182937       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/2np 426\nI1115 06:50:16.387700       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/d5s 214\nI1115 06:50:16.583153       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/n2hs 575\nI1115 06:50:16.783583       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/flx 319\nI1115 06:50:16.982932       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/4s9 543\nI1115 06:50:17.183338       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/24q 559\nI1115 06:50:17.383794       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/v6d 368\nI1115 06:50:17.583389       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/tsm 257\nI1115 06:50:17.782836       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/tdpr 422\nI1115 06:50:17.983270       1 logs_generator.go:76] 24 POST /api/v1/namespaces/default/pods/5pk 269\nI1115 06:50:18.183696       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/g78 315\nI1115 06:50:18.382933       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/bqng 243\nI1115 06:50:18.583286       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/xbq 314\nI1115 06:50:18.783767       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/5sx 424\nI1115 06:50:18.986737       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/pr4 347\nI1115 06:50:19.183140       1 logs_generator.go:76] 30 POST /api/v1/namespaces/kube-system/pods/98x 506\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Nov 15 06:50:19.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-6875 delete pod logs-generator'
    Nov 15 06:50:20.739: INFO: stderr: ""
    Nov 15 06:50:20.739: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:50:20.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6875" for this suite. 11/15/23 06:50:20.759
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:50:20.782
Nov 15 06:50:20.782: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename dns 11/15/23 06:50:20.783
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:20.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:20.851
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 11/15/23 06:50:20.862
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6953.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6953.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6953.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6953.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6953.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6953.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 160.170.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.170.160_udp@PTR;check="$$(dig +tcp +noall +answer +search 160.170.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.170.160_tcp@PTR;sleep 1; done
 11/15/23 06:50:20.926
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6953.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6953.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6953.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6953.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6953.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6953.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 160.170.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.170.160_udp@PTR;check="$$(dig +tcp +noall +answer +search 160.170.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.170.160_tcp@PTR;sleep 1; done
 11/15/23 06:50:20.926
STEP: creating a pod to probe DNS 11/15/23 06:50:20.926
STEP: submitting the pod to kubernetes 11/15/23 06:50:20.926
Nov 15 06:50:20.952: INFO: Waiting up to 15m0s for pod "dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8" in namespace "dns-6953" to be "running"
Nov 15 06:50:20.964: INFO: Pod "dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.57197ms
Nov 15 06:50:22.979: INFO: Pod "dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026384667s
Nov 15 06:50:24.976: INFO: Pod "dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8": Phase="Running", Reason="", readiness=true. Elapsed: 4.023351168s
Nov 15 06:50:24.976: INFO: Pod "dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8" satisfied condition "running"
STEP: retrieving the pod 11/15/23 06:50:24.976
STEP: looking for the results for each expected name from probers 11/15/23 06:50:24.986
Nov 15 06:50:25.015: INFO: Unable to read wheezy_udp@dns-test-service.dns-6953.svc.cluster.local from pod dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8: the server could not find the requested resource (get pods dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8)
Nov 15 06:50:25.035: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6953.svc.cluster.local from pod dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8: the server could not find the requested resource (get pods dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8)
Nov 15 06:50:25.055: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local from pod dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8: the server could not find the requested resource (get pods dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8)
Nov 15 06:50:25.075: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local from pod dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8: the server could not find the requested resource (get pods dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8)
Nov 15 06:50:25.173: INFO: Unable to read jessie_udp@dns-test-service.dns-6953.svc.cluster.local from pod dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8: the server could not find the requested resource (get pods dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8)
Nov 15 06:50:25.191: INFO: Unable to read jessie_tcp@dns-test-service.dns-6953.svc.cluster.local from pod dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8: the server could not find the requested resource (get pods dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8)
Nov 15 06:50:25.210: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local from pod dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8: the server could not find the requested resource (get pods dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8)
Nov 15 06:50:25.229: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local from pod dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8: the server could not find the requested resource (get pods dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8)
Nov 15 06:50:25.301: INFO: Lookups using dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8 failed for: [wheezy_udp@dns-test-service.dns-6953.svc.cluster.local wheezy_tcp@dns-test-service.dns-6953.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local jessie_udp@dns-test-service.dns-6953.svc.cluster.local jessie_tcp@dns-test-service.dns-6953.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local]

Nov 15 06:50:30.608: INFO: DNS probes using dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8 succeeded

STEP: deleting the pod 11/15/23 06:50:30.608
STEP: deleting the test service 11/15/23 06:50:30.639
STEP: deleting the test headless service 11/15/23 06:50:30.695
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 15 06:50:30.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6953" for this suite. 11/15/23 06:50:30.745
------------------------------
â€¢ [SLOW TEST] [9.981 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:50:20.782
    Nov 15 06:50:20.782: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename dns 11/15/23 06:50:20.783
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:20.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:20.851
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 11/15/23 06:50:20.862
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6953.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6953.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6953.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6953.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6953.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6953.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 160.170.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.170.160_udp@PTR;check="$$(dig +tcp +noall +answer +search 160.170.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.170.160_tcp@PTR;sleep 1; done
     11/15/23 06:50:20.926
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6953.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6953.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6953.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6953.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6953.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6953.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6953.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 160.170.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.170.160_udp@PTR;check="$$(dig +tcp +noall +answer +search 160.170.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.170.160_tcp@PTR;sleep 1; done
     11/15/23 06:50:20.926
    STEP: creating a pod to probe DNS 11/15/23 06:50:20.926
    STEP: submitting the pod to kubernetes 11/15/23 06:50:20.926
    Nov 15 06:50:20.952: INFO: Waiting up to 15m0s for pod "dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8" in namespace "dns-6953" to be "running"
    Nov 15 06:50:20.964: INFO: Pod "dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.57197ms
    Nov 15 06:50:22.979: INFO: Pod "dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026384667s
    Nov 15 06:50:24.976: INFO: Pod "dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8": Phase="Running", Reason="", readiness=true. Elapsed: 4.023351168s
    Nov 15 06:50:24.976: INFO: Pod "dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8" satisfied condition "running"
    STEP: retrieving the pod 11/15/23 06:50:24.976
    STEP: looking for the results for each expected name from probers 11/15/23 06:50:24.986
    Nov 15 06:50:25.015: INFO: Unable to read wheezy_udp@dns-test-service.dns-6953.svc.cluster.local from pod dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8: the server could not find the requested resource (get pods dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8)
    Nov 15 06:50:25.035: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6953.svc.cluster.local from pod dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8: the server could not find the requested resource (get pods dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8)
    Nov 15 06:50:25.055: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local from pod dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8: the server could not find the requested resource (get pods dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8)
    Nov 15 06:50:25.075: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local from pod dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8: the server could not find the requested resource (get pods dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8)
    Nov 15 06:50:25.173: INFO: Unable to read jessie_udp@dns-test-service.dns-6953.svc.cluster.local from pod dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8: the server could not find the requested resource (get pods dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8)
    Nov 15 06:50:25.191: INFO: Unable to read jessie_tcp@dns-test-service.dns-6953.svc.cluster.local from pod dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8: the server could not find the requested resource (get pods dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8)
    Nov 15 06:50:25.210: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local from pod dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8: the server could not find the requested resource (get pods dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8)
    Nov 15 06:50:25.229: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local from pod dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8: the server could not find the requested resource (get pods dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8)
    Nov 15 06:50:25.301: INFO: Lookups using dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8 failed for: [wheezy_udp@dns-test-service.dns-6953.svc.cluster.local wheezy_tcp@dns-test-service.dns-6953.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local jessie_udp@dns-test-service.dns-6953.svc.cluster.local jessie_tcp@dns-test-service.dns-6953.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6953.svc.cluster.local]

    Nov 15 06:50:30.608: INFO: DNS probes using dns-6953/dns-test-c2fbaf6c-c38d-46b1-93d8-6260b02ce0b8 succeeded

    STEP: deleting the pod 11/15/23 06:50:30.608
    STEP: deleting the test service 11/15/23 06:50:30.639
    STEP: deleting the test headless service 11/15/23 06:50:30.695
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:50:30.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6953" for this suite. 11/15/23 06:50:30.745
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:50:30.764
Nov 15 06:50:30.765: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename gc 11/15/23 06:50:30.766
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:30.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:30.834
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 11/15/23 06:50:30.844
W1115 06:50:30.864431      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the Deployment to create new ReplicaSet 11/15/23 06:50:30.864
STEP: delete the deployment 11/15/23 06:50:30.875
STEP: wait for all rs to be garbage collected 11/15/23 06:50:30.91
STEP: expected 0 rs, got 1 rs 11/15/23 06:50:30.921
STEP: expected 0 pods, got 2 pods 11/15/23 06:50:30.933
STEP: Gathering metrics 11/15/23 06:50:31.485
W1115 06:50:31.508260      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 15 06:50:31.508: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 15 06:50:31.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4272" for this suite. 11/15/23 06:50:31.523
------------------------------
â€¢ [0.777 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:50:30.764
    Nov 15 06:50:30.765: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename gc 11/15/23 06:50:30.766
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:30.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:30.834
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 11/15/23 06:50:30.844
    W1115 06:50:30.864431      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the Deployment to create new ReplicaSet 11/15/23 06:50:30.864
    STEP: delete the deployment 11/15/23 06:50:30.875
    STEP: wait for all rs to be garbage collected 11/15/23 06:50:30.91
    STEP: expected 0 rs, got 1 rs 11/15/23 06:50:30.921
    STEP: expected 0 pods, got 2 pods 11/15/23 06:50:30.933
    STEP: Gathering metrics 11/15/23 06:50:31.485
    W1115 06:50:31.508260      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Nov 15 06:50:31.508: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:50:31.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4272" for this suite. 11/15/23 06:50:31.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:50:31.553
Nov 15 06:50:31.553: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename sched-pred 11/15/23 06:50:31.554
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:31.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:31.644
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Nov 15 06:50:31.655: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 15 06:50:31.692: INFO: Waiting for terminating namespaces to be deleted...
Nov 15 06:50:31.718: INFO: 
Logging pods the apiserver thinks is on node 10.72.152.81 before test
Nov 15 06:50:31.763: INFO: calico-node-p5wd4 from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.763: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 06:50:31.763: INFO: calico-typha-76d9767bd5-985rd from calico-system started at 2023-11-15 03:39:41 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.763: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 06:50:31.763: INFO: ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-dl6kq from ibm-system started at 2023-11-15 03:48:09 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.763: INFO: 	Container ibm-cloud-provider-ip-158-176-89-155 ready: true, restart count 0
Nov 15 06:50:31.763: INFO: ibm-keepalived-watcher-qb8hn from kube-system started at 2023-11-15 03:38:31 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.763: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 06:50:31.763: INFO: ibm-master-proxy-static-10.72.152.81 from kube-system started at 2023-11-15 03:38:18 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.763: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 06:50:31.763: INFO: 	Container pause ready: true, restart count 0
Nov 15 06:50:31.763: INFO: ibmcloud-block-storage-driver-z7fmw from kube-system started at 2023-11-15 03:38:36 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.763: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 06:50:31.763: INFO: vpn-56cd75f85d-qwzcc from kube-system started at 2023-11-15 03:40:24 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.763: INFO: 	Container vpn ready: true, restart count 0
Nov 15 06:50:31.763: INFO: tuned-rb6qn from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.763: INFO: 	Container tuned ready: true, restart count 0
Nov 15 06:50:31.763: INFO: cluster-storage-operator-6cf6b595c7-m7mfz from openshift-cluster-storage-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.763: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Nov 15 06:50:31.763: INFO: csi-snapshot-controller-857d54544d-mwb9h from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.763: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 06:50:31.763: INFO: csi-snapshot-controller-operator-56df7685c7-vnvd2 from openshift-cluster-storage-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.763: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Nov 15 06:50:31.763: INFO: csi-snapshot-webhook-586f5c484d-qg5n7 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.763: INFO: 	Container webhook ready: true, restart count 0
Nov 15 06:50:31.763: INFO: console-68d6458867-wlg2q from openshift-console started at 2023-11-15 03:47:25 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.763: INFO: 	Container console ready: true, restart count 0
Nov 15 06:50:31.763: INFO: downloads-7bb648f846-tvqt6 from openshift-console started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.763: INFO: 	Container download-server ready: true, restart count 0
Nov 15 06:50:31.763: INFO: dns-default-njlcg from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container dns ready: true, restart count 0
Nov 15 06:50:31.764: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.764: INFO: node-resolver-6lxcm from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 15 06:50:31.764: INFO: cluster-image-registry-operator-64994bbb4-6twjh from openshift-image-registry started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Nov 15 06:50:31.764: INFO: node-ca-897k2 from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container node-ca ready: true, restart count 0
Nov 15 06:50:31.764: INFO: registry-pvc-permissions-lw2bj from openshift-image-registry started at 2023-11-15 03:46:43 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container pvc-permissions ready: false, restart count 0
Nov 15 06:50:31.764: INFO: ingress-canary-jqtjt from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 15 06:50:31.764: INFO: router-default-56777c97d6-szb4s from openshift-ingress started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container router ready: true, restart count 0
Nov 15 06:50:31.764: INFO: insights-operator-85b688b59d-v47wz from openshift-insights started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container insights-operator ready: true, restart count 1
Nov 15 06:50:31.764: INFO: openshift-kube-proxy-p8p2b from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 15 06:50:31.764: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.764: INFO: migrator-697dd4cbc5-kk7zn from openshift-kube-storage-version-migrator started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container migrator ready: true, restart count 0
Nov 15 06:50:31.764: INFO: certified-operators-lqtq6 from openshift-marketplace started at 2023-11-15 03:42:42 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container registry-server ready: true, restart count 0
Nov 15 06:50:31.764: INFO: community-operators-jpmzt from openshift-marketplace started at 2023-11-15 05:06:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container registry-server ready: true, restart count 0
Nov 15 06:50:31.764: INFO: marketplace-operator-55cc9f5b6b-xpbw5 from openshift-marketplace started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container marketplace-operator ready: true, restart count 0
Nov 15 06:50:31.764: INFO: redhat-marketplace-xp5nc from openshift-marketplace started at 2023-11-15 03:42:42 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container registry-server ready: true, restart count 0
Nov 15 06:50:31.764: INFO: redhat-operators-bzltn from openshift-marketplace started at 2023-11-15 04:07:58 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container registry-server ready: true, restart count 0
Nov 15 06:50:31.764: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-11-15 03:46:14 +0000 UTC (6 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container alertmanager ready: true, restart count 1
Nov 15 06:50:31.764: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 15 06:50:31.764: INFO: 	Container config-reloader ready: true, restart count 0
Nov 15 06:50:31.764: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.764: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Nov 15 06:50:31.764: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 15 06:50:31.764: INFO: node-exporter-67458 from openshift-monitoring started at 2023-11-15 03:44:49 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.764: INFO: 	Container node-exporter ready: true, restart count 0
Nov 15 06:50:31.764: INFO: prometheus-adapter-5f5bb574db-r4lkm from openshift-monitoring started at 2023-11-15 03:44:53 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 15 06:50:31.764: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-11-15 03:46:20 +0000 UTC (6 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container config-reloader ready: true, restart count 0
Nov 15 06:50:31.764: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.764: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Nov 15 06:50:31.764: INFO: 	Container prometheus ready: true, restart count 0
Nov 15 06:50:31.764: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 15 06:50:31.764: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 15 06:50:31.764: INFO: prometheus-operator-admission-webhook-c78bf8f99-gxfh4 from openshift-monitoring started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Nov 15 06:50:31.764: INFO: telemeter-client-77c946bb95-nsnds from openshift-monitoring started at 2023-11-15 03:46:15 +0000 UTC (3 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.764: INFO: 	Container reload ready: true, restart count 0
Nov 15 06:50:31.764: INFO: 	Container telemeter-client ready: true, restart count 0
Nov 15 06:50:31.764: INFO: multus-9tc6x from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container kube-multus ready: true, restart count 0
Nov 15 06:50:31.764: INFO: multus-additional-cni-plugins-j2jvc from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 15 06:50:31.764: INFO: network-metrics-daemon-g5fgz from openshift-multus started at 2023-11-15 03:38:47 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.764: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 15 06:50:31.764: INFO: network-check-target-6pv9x from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 15 06:50:31.764: INFO: catalog-operator-798697959c-24lbd from openshift-operator-lifecycle-manager started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container catalog-operator ready: true, restart count 0
Nov 15 06:50:31.764: INFO: collect-profiles-28333815-xxvbf from openshift-operator-lifecycle-manager started at 2023-11-15 06:15:00 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 15 06:50:31.764: INFO: olm-operator-846bf6bd78-fzxr5 from openshift-operator-lifecycle-manager started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container olm-operator ready: true, restart count 0
Nov 15 06:50:31.764: INFO: package-server-manager-5b666bf8fd-5v7rb from openshift-operator-lifecycle-manager started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container package-server-manager ready: true, restart count 0
Nov 15 06:50:31.764: INFO: packageserver-758b547fc-f9wwb from openshift-operator-lifecycle-manager started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container packageserver ready: true, restart count 0
Nov 15 06:50:31.764: INFO: service-ca-operator-74cb5c9cf5-fqgj5 from openshift-service-ca-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container service-ca-operator ready: true, restart count 1
Nov 15 06:50:31.764: INFO: service-ca-78fb97bb77-qcz4b from openshift-service-ca started at 2023-11-15 03:41:45 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container service-ca-controller ready: true, restart count 0
Nov 15 06:50:31.764: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-qvnnx from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.764: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 06:50:31.764: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 06:50:31.764: INFO: 
Logging pods the apiserver thinks is on node 10.72.152.86 before test
Nov 15 06:50:31.810: INFO: calico-node-cmgfg from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 06:50:31.810: INFO: calico-typha-76d9767bd5-tx2mp from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 06:50:31.810: INFO: ibm-keepalived-watcher-qclxn from kube-system started at 2023-11-15 03:38:36 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 06:50:31.810: INFO: ibm-master-proxy-static-10.72.152.86 from kube-system started at 2023-11-15 03:38:24 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container pause ready: true, restart count 0
Nov 15 06:50:31.810: INFO: ibmcloud-block-storage-driver-m8gq4 from kube-system started at 2023-11-15 03:38:43 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 06:50:31.810: INFO: cluster-node-tuning-operator-5f77b58f7-t8gf6 from openshift-cluster-node-tuning-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Nov 15 06:50:31.810: INFO: tuned-5qzzn from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container tuned ready: true, restart count 0
Nov 15 06:50:31.810: INFO: cluster-samples-operator-65684cb854-h7n8t from openshift-cluster-samples-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Nov 15 06:50:31.810: INFO: csi-snapshot-controller-857d54544d-qbhf6 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 06:50:31.810: INFO: csi-snapshot-webhook-586f5c484d-vkpg9 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container webhook ready: true, restart count 0
Nov 15 06:50:31.810: INFO: console-operator-769f9748fb-7tfcc from openshift-console-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container console-operator ready: true, restart count 1
Nov 15 06:50:31.810: INFO: 	Container conversion-webhook-server ready: true, restart count 3
Nov 15 06:50:31.810: INFO: downloads-7bb648f846-sr7nb from openshift-console started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container download-server ready: true, restart count 0
Nov 15 06:50:31.810: INFO: dns-operator-dd9c9c896-9gwtp from openshift-dns-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container dns-operator ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.810: INFO: dns-default-24qc6 from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container dns ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.810: INFO: node-resolver-d54kt from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 15 06:50:31.810: INFO: node-ca-w7vhg from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container node-ca ready: true, restart count 0
Nov 15 06:50:31.810: INFO: ingress-canary-ttx6m from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 15 06:50:31.810: INFO: ingress-operator-6d4d6975f7-qtm2n from openshift-ingress-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container ingress-operator ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.810: INFO: openshift-kube-proxy-x5hq9 from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.810: INFO: kube-storage-version-migrator-operator-5d88b7484-8m2cc from openshift-kube-storage-version-migrator-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Nov 15 06:50:31.810: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-11-15 03:46:47 +0000 UTC (6 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container alertmanager ready: true, restart count 1
Nov 15 06:50:31.810: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container config-reloader ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 15 06:50:31.810: INFO: cluster-monitoring-operator-868f9b56cf-xrfz7 from openshift-monitoring started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Nov 15 06:50:31.810: INFO: kube-state-metrics-f8d796647-f4rgj from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (3 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 15 06:50:31.810: INFO: node-exporter-n68zb from openshift-monitoring started at 2023-11-15 03:44:49 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container node-exporter ready: true, restart count 0
Nov 15 06:50:31.810: INFO: openshift-state-metrics-69bb697b65-6bcg7 from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (3 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Nov 15 06:50:31.810: INFO: prometheus-adapter-5f5bb574db-569xs from openshift-monitoring started at 2023-11-15 03:44:53 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 15 06:50:31.810: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-11-15 03:46:38 +0000 UTC (6 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container config-reloader ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container prometheus ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 15 06:50:31.810: INFO: prometheus-operator-6fcb4d4c46-t74rt from openshift-monitoring started at 2023-11-15 03:44:41 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 15 06:50:31.810: INFO: thanos-querier-56b7586647-8m7mp from openshift-monitoring started at 2023-11-15 03:44:57 +0000 UTC (6 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container thanos-query ready: true, restart count 0
Nov 15 06:50:31.810: INFO: multus-additional-cni-plugins-z6xx7 from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 15 06:50:31.810: INFO: multus-admission-controller-6b76dd856b-6qmnp from openshift-multus started at 2023-11-15 03:44:37 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 15 06:50:31.810: INFO: multus-cltwv from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container kube-multus ready: true, restart count 0
Nov 15 06:50:31.810: INFO: network-metrics-daemon-zbz9n from openshift-multus started at 2023-11-15 03:38:47 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 15 06:50:31.810: INFO: network-check-target-gnph7 from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 15 06:50:31.810: INFO: collect-profiles-28333830-78s47 from openshift-operator-lifecycle-manager started at 2023-11-15 06:30:00 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 15 06:50:31.810: INFO: collect-profiles-28333845-ts5cg from openshift-operator-lifecycle-manager started at 2023-11-15 06:45:00 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 15 06:50:31.810: INFO: packageserver-758b547fc-65qc5 from openshift-operator-lifecycle-manager started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container packageserver ready: true, restart count 0
Nov 15 06:50:31.810: INFO: sonobuoy from sonobuoy started at 2023-11-15 05:52:01 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 15 06:50:31.810: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-m6885 from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 06:50:31.810: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-11-15 03:41:49 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.810: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 15 06:50:31.810: INFO: 
Logging pods the apiserver thinks is on node 10.72.152.88 before test
Nov 15 06:50:31.853: INFO: calico-kube-controllers-5dd9d87465-759pm from calico-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 15 06:50:31.853: INFO: calico-node-lmf6x from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 06:50:31.853: INFO: managed-storage-validation-webhooks-7457bf6687-9ds5w from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Nov 15 06:50:31.853: INFO: managed-storage-validation-webhooks-7457bf6687-h522x from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Nov 15 06:50:31.853: INFO: managed-storage-validation-webhooks-7457bf6687-phdgp from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Nov 15 06:50:31.853: INFO: ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-vbxww from ibm-system started at 2023-11-15 03:48:09 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container ibm-cloud-provider-ip-158-176-89-155 ready: true, restart count 0
Nov 15 06:50:31.853: INFO: ibm-file-plugin-5fcf7fb495-xmdts from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 15 06:50:31.853: INFO: ibm-keepalived-watcher-5jx26 from kube-system started at 2023-11-15 03:38:12 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 06:50:31.853: INFO: ibm-master-proxy-static-10.72.152.88 from kube-system started at 2023-11-15 03:38:04 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 06:50:31.853: INFO: 	Container pause ready: true, restart count 0
Nov 15 06:50:31.853: INFO: ibm-storage-metrics-agent-84fbdc746-5sv68 from kube-system started at 2023-11-15 03:39:51 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Nov 15 06:50:31.853: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Nov 15 06:50:31.853: INFO: ibm-storage-watcher-7445c988b-8ngdm from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 15 06:50:31.853: INFO: ibmcloud-block-storage-driver-9t8rj from kube-system started at 2023-11-15 03:38:17 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 06:50:31.853: INFO: ibmcloud-block-storage-plugin-5774687565-gj9xn from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Nov 15 06:50:31.853: INFO: tuned-q894n from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container tuned ready: true, restart count 0
Nov 15 06:50:31.853: INFO: console-68d6458867-krfqd from openshift-console started at 2023-11-15 03:47:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container console ready: true, restart count 0
Nov 15 06:50:31.853: INFO: dns-default-lmngx from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container dns ready: true, restart count 0
Nov 15 06:50:31.853: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.853: INFO: node-resolver-hwp5f from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 15 06:50:31.853: INFO: image-registry-f74f764d8-w48k4 from openshift-image-registry started at 2023-11-15 03:46:35 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container registry ready: true, restart count 0
Nov 15 06:50:31.853: INFO: node-ca-wrqvd from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container node-ca ready: true, restart count 0
Nov 15 06:50:31.853: INFO: ingress-canary-wvl4p from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.853: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 15 06:50:31.853: INFO: router-default-56777c97d6-4bnqq from openshift-ingress started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.854: INFO: 	Container router ready: true, restart count 0
Nov 15 06:50:31.854: INFO: openshift-kube-proxy-tzknx from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.854: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 15 06:50:31.854: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.854: INFO: node-exporter-2452k from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.854: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.854: INFO: 	Container node-exporter ready: true, restart count 0
Nov 15 06:50:31.854: INFO: prometheus-operator-admission-webhook-c78bf8f99-x8vcz from openshift-monitoring started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.854: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Nov 15 06:50:31.854: INFO: thanos-querier-56b7586647-qr75b from openshift-monitoring started at 2023-11-15 03:44:58 +0000 UTC (6 container statuses recorded)
Nov 15 06:50:31.854: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.854: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 15 06:50:31.854: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Nov 15 06:50:31.854: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 15 06:50:31.854: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 15 06:50:31.854: INFO: 	Container thanos-query ready: true, restart count 0
Nov 15 06:50:31.854: INFO: multus-additional-cni-plugins-lcnxr from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.854: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 15 06:50:31.854: INFO: multus-admission-controller-6b76dd856b-6zft2 from openshift-multus started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.854: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.854: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 15 06:50:31.854: INFO: multus-vqg9w from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.854: INFO: 	Container kube-multus ready: true, restart count 0
Nov 15 06:50:31.854: INFO: network-metrics-daemon-9mrwp from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.854: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 06:50:31.854: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 15 06:50:31.854: INFO: network-check-source-5f9c5566b6-grf5l from openshift-network-diagnostics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.854: INFO: 	Container check-endpoints ready: true, restart count 0
Nov 15 06:50:31.854: INFO: network-check-target-6rz7p from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.854: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 15 06:50:31.854: INFO: network-operator-847f47449c-j9glm from openshift-network-operator started at 2023-11-15 03:38:21 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.854: INFO: 	Container network-operator ready: true, restart count 1
Nov 15 06:50:31.854: INFO: metrics-667b585fc4-d5fdk from openshift-roks-metrics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.854: INFO: 	Container metrics ready: true, restart count 5
Nov 15 06:50:31.854: INFO: push-gateway-7f9447c646-5mjcp from openshift-roks-metrics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.854: INFO: 	Container push-gateway ready: true, restart count 0
Nov 15 06:50:31.854: INFO: sonobuoy-e2e-job-4fb3cc1d32834aa9 from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.854: INFO: 	Container e2e ready: true, restart count 0
Nov 15 06:50:31.854: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 06:50:31.854: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-ddv8z from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
Nov 15 06:50:31.854: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 06:50:31.854: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 06:50:31.854: INFO: tigera-operator-7dbcb4fb45-rn78j from tigera-operator started at 2023-11-15 03:38:21 +0000 UTC (1 container statuses recorded)
Nov 15 06:50:31.854: INFO: 	Container tigera-operator ready: true, restart count 2
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node 10.72.152.81 11/15/23 06:50:31.931
STEP: verifying the node has the label node 10.72.152.86 11/15/23 06:50:31.968
STEP: verifying the node has the label node 10.72.152.88 11/15/23 06:50:32.005
Nov 15 06:50:32.111: INFO: Pod calico-kube-controllers-5dd9d87465-759pm requesting resource cpu=10m on Node 10.72.152.88
Nov 15 06:50:32.111: INFO: Pod calico-node-cmgfg requesting resource cpu=250m on Node 10.72.152.86
Nov 15 06:50:32.111: INFO: Pod calico-node-lmf6x requesting resource cpu=250m on Node 10.72.152.88
Nov 15 06:50:32.111: INFO: Pod calico-node-p5wd4 requesting resource cpu=250m on Node 10.72.152.81
Nov 15 06:50:32.111: INFO: Pod calico-typha-76d9767bd5-985rd requesting resource cpu=250m on Node 10.72.152.81
Nov 15 06:50:32.111: INFO: Pod calico-typha-76d9767bd5-tx2mp requesting resource cpu=250m on Node 10.72.152.86
Nov 15 06:50:32.111: INFO: Pod managed-storage-validation-webhooks-7457bf6687-9ds5w requesting resource cpu=10m on Node 10.72.152.88
Nov 15 06:50:32.111: INFO: Pod managed-storage-validation-webhooks-7457bf6687-h522x requesting resource cpu=10m on Node 10.72.152.88
Nov 15 06:50:32.111: INFO: Pod managed-storage-validation-webhooks-7457bf6687-phdgp requesting resource cpu=10m on Node 10.72.152.88
Nov 15 06:50:32.111: INFO: Pod ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-dl6kq requesting resource cpu=5m on Node 10.72.152.81
Nov 15 06:50:32.111: INFO: Pod ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-vbxww requesting resource cpu=5m on Node 10.72.152.88
Nov 15 06:50:32.111: INFO: Pod ibm-file-plugin-5fcf7fb495-xmdts requesting resource cpu=50m on Node 10.72.152.88
Nov 15 06:50:32.111: INFO: Pod ibm-keepalived-watcher-5jx26 requesting resource cpu=5m on Node 10.72.152.88
Nov 15 06:50:32.111: INFO: Pod ibm-keepalived-watcher-qb8hn requesting resource cpu=5m on Node 10.72.152.81
Nov 15 06:50:32.111: INFO: Pod ibm-keepalived-watcher-qclxn requesting resource cpu=5m on Node 10.72.152.86
Nov 15 06:50:32.111: INFO: Pod ibm-master-proxy-static-10.72.152.81 requesting resource cpu=26m on Node 10.72.152.81
Nov 15 06:50:32.111: INFO: Pod ibm-master-proxy-static-10.72.152.86 requesting resource cpu=26m on Node 10.72.152.86
Nov 15 06:50:32.111: INFO: Pod ibm-master-proxy-static-10.72.152.88 requesting resource cpu=26m on Node 10.72.152.88
Nov 15 06:50:32.111: INFO: Pod ibm-storage-metrics-agent-84fbdc746-5sv68 requesting resource cpu=60m on Node 10.72.152.88
Nov 15 06:50:32.111: INFO: Pod ibm-storage-watcher-7445c988b-8ngdm requesting resource cpu=50m on Node 10.72.152.88
Nov 15 06:50:32.111: INFO: Pod ibmcloud-block-storage-driver-9t8rj requesting resource cpu=50m on Node 10.72.152.88
Nov 15 06:50:32.111: INFO: Pod ibmcloud-block-storage-driver-m8gq4 requesting resource cpu=50m on Node 10.72.152.86
Nov 15 06:50:32.111: INFO: Pod ibmcloud-block-storage-driver-z7fmw requesting resource cpu=50m on Node 10.72.152.81
Nov 15 06:50:32.111: INFO: Pod ibmcloud-block-storage-plugin-5774687565-gj9xn requesting resource cpu=50m on Node 10.72.152.88
Nov 15 06:50:32.111: INFO: Pod vpn-56cd75f85d-qwzcc requesting resource cpu=5m on Node 10.72.152.81
Nov 15 06:50:32.111: INFO: Pod cluster-node-tuning-operator-5f77b58f7-t8gf6 requesting resource cpu=10m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod tuned-5qzzn requesting resource cpu=10m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod tuned-q894n requesting resource cpu=10m on Node 10.72.152.88
Nov 15 06:50:32.112: INFO: Pod tuned-rb6qn requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod cluster-samples-operator-65684cb854-h7n8t requesting resource cpu=20m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod cluster-storage-operator-6cf6b595c7-m7mfz requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod csi-snapshot-controller-857d54544d-mwb9h requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod csi-snapshot-controller-857d54544d-qbhf6 requesting resource cpu=10m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod csi-snapshot-controller-operator-56df7685c7-vnvd2 requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod csi-snapshot-webhook-586f5c484d-qg5n7 requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod csi-snapshot-webhook-586f5c484d-vkpg9 requesting resource cpu=10m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod console-operator-769f9748fb-7tfcc requesting resource cpu=20m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod console-68d6458867-krfqd requesting resource cpu=10m on Node 10.72.152.88
Nov 15 06:50:32.112: INFO: Pod console-68d6458867-wlg2q requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod downloads-7bb648f846-sr7nb requesting resource cpu=10m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod downloads-7bb648f846-tvqt6 requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod dns-operator-dd9c9c896-9gwtp requesting resource cpu=20m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod dns-default-24qc6 requesting resource cpu=60m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod dns-default-lmngx requesting resource cpu=60m on Node 10.72.152.88
Nov 15 06:50:32.112: INFO: Pod dns-default-njlcg requesting resource cpu=60m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod node-resolver-6lxcm requesting resource cpu=5m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod node-resolver-d54kt requesting resource cpu=5m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod node-resolver-hwp5f requesting resource cpu=5m on Node 10.72.152.88
Nov 15 06:50:32.112: INFO: Pod cluster-image-registry-operator-64994bbb4-6twjh requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod image-registry-f74f764d8-w48k4 requesting resource cpu=100m on Node 10.72.152.88
Nov 15 06:50:32.112: INFO: Pod node-ca-897k2 requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod node-ca-w7vhg requesting resource cpu=10m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod node-ca-wrqvd requesting resource cpu=10m on Node 10.72.152.88
Nov 15 06:50:32.112: INFO: Pod ingress-canary-jqtjt requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod ingress-canary-ttx6m requesting resource cpu=10m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod ingress-canary-wvl4p requesting resource cpu=10m on Node 10.72.152.88
Nov 15 06:50:32.112: INFO: Pod ingress-operator-6d4d6975f7-qtm2n requesting resource cpu=20m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod router-default-56777c97d6-4bnqq requesting resource cpu=100m on Node 10.72.152.88
Nov 15 06:50:32.112: INFO: Pod router-default-56777c97d6-szb4s requesting resource cpu=100m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod insights-operator-85b688b59d-v47wz requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod openshift-kube-proxy-p8p2b requesting resource cpu=110m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod openshift-kube-proxy-tzknx requesting resource cpu=110m on Node 10.72.152.88
Nov 15 06:50:32.112: INFO: Pod openshift-kube-proxy-x5hq9 requesting resource cpu=110m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod kube-storage-version-migrator-operator-5d88b7484-8m2cc requesting resource cpu=10m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod migrator-697dd4cbc5-kk7zn requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod certified-operators-lqtq6 requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod community-operators-jpmzt requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod marketplace-operator-55cc9f5b6b-xpbw5 requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod redhat-marketplace-xp5nc requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod redhat-operators-bzltn requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod cluster-monitoring-operator-868f9b56cf-xrfz7 requesting resource cpu=10m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod kube-state-metrics-f8d796647-f4rgj requesting resource cpu=4m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod node-exporter-2452k requesting resource cpu=9m on Node 10.72.152.88
Nov 15 06:50:32.112: INFO: Pod node-exporter-67458 requesting resource cpu=9m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod node-exporter-n68zb requesting resource cpu=9m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod openshift-state-metrics-69bb697b65-6bcg7 requesting resource cpu=3m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod prometheus-adapter-5f5bb574db-569xs requesting resource cpu=1m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod prometheus-adapter-5f5bb574db-r4lkm requesting resource cpu=1m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node 10.72.152.86
Nov 15 06:50:32.112: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node 10.72.152.81
Nov 15 06:50:32.112: INFO: Pod prometheus-operator-6fcb4d4c46-t74rt requesting resource cpu=6m on Node 10.72.152.86
Nov 15 06:50:32.113: INFO: Pod prometheus-operator-admission-webhook-c78bf8f99-gxfh4 requesting resource cpu=5m on Node 10.72.152.81
Nov 15 06:50:32.113: INFO: Pod prometheus-operator-admission-webhook-c78bf8f99-x8vcz requesting resource cpu=5m on Node 10.72.152.88
Nov 15 06:50:32.113: INFO: Pod telemeter-client-77c946bb95-nsnds requesting resource cpu=3m on Node 10.72.152.81
Nov 15 06:50:32.113: INFO: Pod thanos-querier-56b7586647-8m7mp requesting resource cpu=15m on Node 10.72.152.86
Nov 15 06:50:32.113: INFO: Pod thanos-querier-56b7586647-qr75b requesting resource cpu=15m on Node 10.72.152.88
Nov 15 06:50:32.113: INFO: Pod multus-9tc6x requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.113: INFO: Pod multus-additional-cni-plugins-j2jvc requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.113: INFO: Pod multus-additional-cni-plugins-lcnxr requesting resource cpu=10m on Node 10.72.152.88
Nov 15 06:50:32.113: INFO: Pod multus-additional-cni-plugins-z6xx7 requesting resource cpu=10m on Node 10.72.152.86
Nov 15 06:50:32.113: INFO: Pod multus-admission-controller-6b76dd856b-6qmnp requesting resource cpu=20m on Node 10.72.152.86
Nov 15 06:50:32.113: INFO: Pod multus-admission-controller-6b76dd856b-6zft2 requesting resource cpu=20m on Node 10.72.152.88
Nov 15 06:50:32.113: INFO: Pod multus-cltwv requesting resource cpu=10m on Node 10.72.152.86
Nov 15 06:50:32.113: INFO: Pod multus-vqg9w requesting resource cpu=10m on Node 10.72.152.88
Nov 15 06:50:32.113: INFO: Pod network-metrics-daemon-9mrwp requesting resource cpu=20m on Node 10.72.152.88
Nov 15 06:50:32.113: INFO: Pod network-metrics-daemon-g5fgz requesting resource cpu=20m on Node 10.72.152.81
Nov 15 06:50:32.113: INFO: Pod network-metrics-daemon-zbz9n requesting resource cpu=20m on Node 10.72.152.86
Nov 15 06:50:32.113: INFO: Pod network-check-source-5f9c5566b6-grf5l requesting resource cpu=10m on Node 10.72.152.88
Nov 15 06:50:32.113: INFO: Pod network-check-target-6pv9x requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.113: INFO: Pod network-check-target-6rz7p requesting resource cpu=10m on Node 10.72.152.88
Nov 15 06:50:32.113: INFO: Pod network-check-target-gnph7 requesting resource cpu=10m on Node 10.72.152.86
Nov 15 06:50:32.113: INFO: Pod network-operator-847f47449c-j9glm requesting resource cpu=10m on Node 10.72.152.88
Nov 15 06:50:32.113: INFO: Pod catalog-operator-798697959c-24lbd requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.113: INFO: Pod olm-operator-846bf6bd78-fzxr5 requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.113: INFO: Pod package-server-manager-5b666bf8fd-5v7rb requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.113: INFO: Pod packageserver-758b547fc-65qc5 requesting resource cpu=10m on Node 10.72.152.86
Nov 15 06:50:32.113: INFO: Pod packageserver-758b547fc-f9wwb requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.113: INFO: Pod metrics-667b585fc4-d5fdk requesting resource cpu=10m on Node 10.72.152.88
Nov 15 06:50:32.113: INFO: Pod push-gateway-7f9447c646-5mjcp requesting resource cpu=10m on Node 10.72.152.88
Nov 15 06:50:32.113: INFO: Pod service-ca-operator-74cb5c9cf5-fqgj5 requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.113: INFO: Pod service-ca-78fb97bb77-qcz4b requesting resource cpu=10m on Node 10.72.152.81
Nov 15 06:50:32.113: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.72.152.86
Nov 15 06:50:32.113: INFO: Pod sonobuoy-e2e-job-4fb3cc1d32834aa9 requesting resource cpu=0m on Node 10.72.152.88
Nov 15 06:50:32.113: INFO: Pod sonobuoy-systemd-logs-daemon-set-997738c51257402b-ddv8z requesting resource cpu=0m on Node 10.72.152.88
Nov 15 06:50:32.113: INFO: Pod sonobuoy-systemd-logs-daemon-set-997738c51257402b-m6885 requesting resource cpu=0m on Node 10.72.152.86
Nov 15 06:50:32.113: INFO: Pod sonobuoy-systemd-logs-daemon-set-997738c51257402b-qvnnx requesting resource cpu=0m on Node 10.72.152.81
Nov 15 06:50:32.113: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.72.152.86
Nov 15 06:50:32.113: INFO: Pod tigera-operator-7dbcb4fb45-rn78j requesting resource cpu=100m on Node 10.72.152.88
STEP: Starting Pods to consume most of the cluster CPU. 11/15/23 06:50:32.113
Nov 15 06:50:32.113: INFO: Creating a pod which consumes cpu=1863m on Node 10.72.152.81
Nov 15 06:50:32.135: INFO: Creating a pod which consumes cpu=1947m on Node 10.72.152.86
Nov 15 06:50:32.150: INFO: Creating a pod which consumes cpu=1869m on Node 10.72.152.88
Nov 15 06:50:32.177: INFO: Waiting up to 5m0s for pod "filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96" in namespace "sched-pred-5694" to be "running"
Nov 15 06:50:32.187: INFO: Pod "filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96": Phase="Pending", Reason="", readiness=false. Elapsed: 9.804298ms
Nov 15 06:50:34.200: INFO: Pod "filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023163337s
Nov 15 06:50:36.200: INFO: Pod "filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96": Phase="Running", Reason="", readiness=true. Elapsed: 4.022812101s
Nov 15 06:50:36.200: INFO: Pod "filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96" satisfied condition "running"
Nov 15 06:50:36.200: INFO: Waiting up to 5m0s for pod "filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81" in namespace "sched-pred-5694" to be "running"
Nov 15 06:50:36.211: INFO: Pod "filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81": Phase="Running", Reason="", readiness=true. Elapsed: 11.59589ms
Nov 15 06:50:36.211: INFO: Pod "filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81" satisfied condition "running"
Nov 15 06:50:36.211: INFO: Waiting up to 5m0s for pod "filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5" in namespace "sched-pred-5694" to be "running"
Nov 15 06:50:36.223: INFO: Pod "filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5": Phase="Running", Reason="", readiness=true. Elapsed: 11.370366ms
Nov 15 06:50:36.223: INFO: Pod "filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 11/15/23 06:50:36.223
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5.1797b937748175d9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5694/filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5 to 10.72.152.88] 11/15/23 06:50:36.236
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5.1797b937a62971b8], Reason = [AddedInterface], Message = [Add eth0 [172.30.10.183/32] from k8s-pod-network] 11/15/23 06:50:36.236
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5.1797b937b32f3f4e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 11/15/23 06:50:36.236
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5.1797b937bd1f0490], Reason = [Created], Message = [Created container filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5] 11/15/23 06:50:36.236
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5.1797b937be3b9920], Reason = [Started], Message = [Started container filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5] 11/15/23 06:50:36.236
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96.1797b937722d4428], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5694/filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96 to 10.72.152.81] 11/15/23 06:50:36.236
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96.1797b937befbe274], Reason = [AddedInterface], Message = [Add eth0 [172.30.214.164/32] from k8s-pod-network] 11/15/23 06:50:36.236
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96.1797b937cd42a940], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 11/15/23 06:50:36.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96.1797b937d89928ac], Reason = [Created], Message = [Created container filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96] 11/15/23 06:50:36.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96.1797b937d9aa10ba], Reason = [Started], Message = [Started container filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96] 11/15/23 06:50:36.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81.1797b93773312925], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5694/filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81 to 10.72.152.86] 11/15/23 06:50:36.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81.1797b937a6bad993], Reason = [AddedInterface], Message = [Add eth0 [172.30.213.191/32] from k8s-pod-network] 11/15/23 06:50:36.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81.1797b937b3e82f34], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 11/15/23 06:50:36.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81.1797b937bf965e37], Reason = [Created], Message = [Created container filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81] 11/15/23 06:50:36.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81.1797b937c0bb7a8a], Reason = [Started], Message = [Started container filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81] 11/15/23 06:50:36.237
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1797b9386715f92b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 11/15/23 06:50:36.269
STEP: removing the label node off the node 10.72.152.81 11/15/23 06:50:37.276
STEP: verifying the node doesn't have the label node 11/15/23 06:50:37.311
STEP: removing the label node off the node 10.72.152.86 11/15/23 06:50:37.323
STEP: verifying the node doesn't have the label node 11/15/23 06:50:37.356
STEP: removing the label node off the node 10.72.152.88 11/15/23 06:50:37.375
STEP: verifying the node doesn't have the label node 11/15/23 06:50:37.407
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:50:37.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5694" for this suite. 11/15/23 06:50:37.458
------------------------------
â€¢ [SLOW TEST] [5.926 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:50:31.553
    Nov 15 06:50:31.553: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename sched-pred 11/15/23 06:50:31.554
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:31.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:31.644
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Nov 15 06:50:31.655: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Nov 15 06:50:31.692: INFO: Waiting for terminating namespaces to be deleted...
    Nov 15 06:50:31.718: INFO: 
    Logging pods the apiserver thinks is on node 10.72.152.81 before test
    Nov 15 06:50:31.763: INFO: calico-node-p5wd4 from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.763: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 06:50:31.763: INFO: calico-typha-76d9767bd5-985rd from calico-system started at 2023-11-15 03:39:41 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.763: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 06:50:31.763: INFO: ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-dl6kq from ibm-system started at 2023-11-15 03:48:09 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.763: INFO: 	Container ibm-cloud-provider-ip-158-176-89-155 ready: true, restart count 0
    Nov 15 06:50:31.763: INFO: ibm-keepalived-watcher-qb8hn from kube-system started at 2023-11-15 03:38:31 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.763: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 06:50:31.763: INFO: ibm-master-proxy-static-10.72.152.81 from kube-system started at 2023-11-15 03:38:18 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.763: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 06:50:31.763: INFO: 	Container pause ready: true, restart count 0
    Nov 15 06:50:31.763: INFO: ibmcloud-block-storage-driver-z7fmw from kube-system started at 2023-11-15 03:38:36 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.763: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 06:50:31.763: INFO: vpn-56cd75f85d-qwzcc from kube-system started at 2023-11-15 03:40:24 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.763: INFO: 	Container vpn ready: true, restart count 0
    Nov 15 06:50:31.763: INFO: tuned-rb6qn from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.763: INFO: 	Container tuned ready: true, restart count 0
    Nov 15 06:50:31.763: INFO: cluster-storage-operator-6cf6b595c7-m7mfz from openshift-cluster-storage-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.763: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Nov 15 06:50:31.763: INFO: csi-snapshot-controller-857d54544d-mwb9h from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.763: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 06:50:31.763: INFO: csi-snapshot-controller-operator-56df7685c7-vnvd2 from openshift-cluster-storage-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.763: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Nov 15 06:50:31.763: INFO: csi-snapshot-webhook-586f5c484d-qg5n7 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.763: INFO: 	Container webhook ready: true, restart count 0
    Nov 15 06:50:31.763: INFO: console-68d6458867-wlg2q from openshift-console started at 2023-11-15 03:47:25 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.763: INFO: 	Container console ready: true, restart count 0
    Nov 15 06:50:31.763: INFO: downloads-7bb648f846-tvqt6 from openshift-console started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.763: INFO: 	Container download-server ready: true, restart count 0
    Nov 15 06:50:31.763: INFO: dns-default-njlcg from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container dns ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: node-resolver-6lxcm from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: cluster-image-registry-operator-64994bbb4-6twjh from openshift-image-registry started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: node-ca-897k2 from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container node-ca ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: registry-pvc-permissions-lw2bj from openshift-image-registry started at 2023-11-15 03:46:43 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container pvc-permissions ready: false, restart count 0
    Nov 15 06:50:31.764: INFO: ingress-canary-jqtjt from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: router-default-56777c97d6-szb4s from openshift-ingress started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container router ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: insights-operator-85b688b59d-v47wz from openshift-insights started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container insights-operator ready: true, restart count 1
    Nov 15 06:50:31.764: INFO: openshift-kube-proxy-p8p2b from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: migrator-697dd4cbc5-kk7zn from openshift-kube-storage-version-migrator started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container migrator ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: certified-operators-lqtq6 from openshift-marketplace started at 2023-11-15 03:42:42 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container registry-server ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: community-operators-jpmzt from openshift-marketplace started at 2023-11-15 05:06:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container registry-server ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: marketplace-operator-55cc9f5b6b-xpbw5 from openshift-marketplace started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container marketplace-operator ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: redhat-marketplace-xp5nc from openshift-marketplace started at 2023-11-15 03:42:42 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container registry-server ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: redhat-operators-bzltn from openshift-marketplace started at 2023-11-15 04:07:58 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container registry-server ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-11-15 03:46:14 +0000 UTC (6 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container alertmanager ready: true, restart count 1
    Nov 15 06:50:31.764: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: node-exporter-67458 from openshift-monitoring started at 2023-11-15 03:44:49 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: prometheus-adapter-5f5bb574db-r4lkm from openshift-monitoring started at 2023-11-15 03:44:53 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-11-15 03:46:20 +0000 UTC (6 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: 	Container prometheus ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: prometheus-operator-admission-webhook-c78bf8f99-gxfh4 from openshift-monitoring started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: telemeter-client-77c946bb95-nsnds from openshift-monitoring started at 2023-11-15 03:46:15 +0000 UTC (3 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: 	Container reload ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: 	Container telemeter-client ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: multus-9tc6x from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: multus-additional-cni-plugins-j2jvc from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: network-metrics-daemon-g5fgz from openshift-multus started at 2023-11-15 03:38:47 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: network-check-target-6pv9x from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container network-check-target-container ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: catalog-operator-798697959c-24lbd from openshift-operator-lifecycle-manager started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container catalog-operator ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: collect-profiles-28333815-xxvbf from openshift-operator-lifecycle-manager started at 2023-11-15 06:15:00 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container collect-profiles ready: false, restart count 0
    Nov 15 06:50:31.764: INFO: olm-operator-846bf6bd78-fzxr5 from openshift-operator-lifecycle-manager started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container olm-operator ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: package-server-manager-5b666bf8fd-5v7rb from openshift-operator-lifecycle-manager started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container package-server-manager ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: packageserver-758b547fc-f9wwb from openshift-operator-lifecycle-manager started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container packageserver ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: service-ca-operator-74cb5c9cf5-fqgj5 from openshift-service-ca-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container service-ca-operator ready: true, restart count 1
    Nov 15 06:50:31.764: INFO: service-ca-78fb97bb77-qcz4b from openshift-service-ca started at 2023-11-15 03:41:45 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container service-ca-controller ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-qvnnx from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.764: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 06:50:31.764: INFO: 
    Logging pods the apiserver thinks is on node 10.72.152.86 before test
    Nov 15 06:50:31.810: INFO: calico-node-cmgfg from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: calico-typha-76d9767bd5-tx2mp from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: ibm-keepalived-watcher-qclxn from kube-system started at 2023-11-15 03:38:36 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: ibm-master-proxy-static-10.72.152.86 from kube-system started at 2023-11-15 03:38:24 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container pause ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: ibmcloud-block-storage-driver-m8gq4 from kube-system started at 2023-11-15 03:38:43 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: cluster-node-tuning-operator-5f77b58f7-t8gf6 from openshift-cluster-node-tuning-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: tuned-5qzzn from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container tuned ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: cluster-samples-operator-65684cb854-h7n8t from openshift-cluster-samples-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: csi-snapshot-controller-857d54544d-qbhf6 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: csi-snapshot-webhook-586f5c484d-vkpg9 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container webhook ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: console-operator-769f9748fb-7tfcc from openshift-console-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container console-operator ready: true, restart count 1
    Nov 15 06:50:31.810: INFO: 	Container conversion-webhook-server ready: true, restart count 3
    Nov 15 06:50:31.810: INFO: downloads-7bb648f846-sr7nb from openshift-console started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container download-server ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: dns-operator-dd9c9c896-9gwtp from openshift-dns-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container dns-operator ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: dns-default-24qc6 from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container dns ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: node-resolver-d54kt from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: node-ca-w7vhg from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container node-ca ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: ingress-canary-ttx6m from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: ingress-operator-6d4d6975f7-qtm2n from openshift-ingress-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container ingress-operator ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: openshift-kube-proxy-x5hq9 from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: kube-storage-version-migrator-operator-5d88b7484-8m2cc from openshift-kube-storage-version-migrator-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Nov 15 06:50:31.810: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-11-15 03:46:47 +0000 UTC (6 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container alertmanager ready: true, restart count 1
    Nov 15 06:50:31.810: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: cluster-monitoring-operator-868f9b56cf-xrfz7 from openshift-monitoring started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: kube-state-metrics-f8d796647-f4rgj from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (3 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: node-exporter-n68zb from openshift-monitoring started at 2023-11-15 03:44:49 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: openshift-state-metrics-69bb697b65-6bcg7 from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (3 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: prometheus-adapter-5f5bb574db-569xs from openshift-monitoring started at 2023-11-15 03:44:53 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-11-15 03:46:38 +0000 UTC (6 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container prometheus ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: prometheus-operator-6fcb4d4c46-t74rt from openshift-monitoring started at 2023-11-15 03:44:41 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container prometheus-operator ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: thanos-querier-56b7586647-8m7mp from openshift-monitoring started at 2023-11-15 03:44:57 +0000 UTC (6 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container oauth-proxy ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container thanos-query ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: multus-additional-cni-plugins-z6xx7 from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: multus-admission-controller-6b76dd856b-6qmnp from openshift-multus started at 2023-11-15 03:44:37 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: multus-cltwv from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: network-metrics-daemon-zbz9n from openshift-multus started at 2023-11-15 03:38:47 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: network-check-target-gnph7 from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container network-check-target-container ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: collect-profiles-28333830-78s47 from openshift-operator-lifecycle-manager started at 2023-11-15 06:30:00 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container collect-profiles ready: false, restart count 0
    Nov 15 06:50:31.810: INFO: collect-profiles-28333845-ts5cg from openshift-operator-lifecycle-manager started at 2023-11-15 06:45:00 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container collect-profiles ready: false, restart count 0
    Nov 15 06:50:31.810: INFO: packageserver-758b547fc-65qc5 from openshift-operator-lifecycle-manager started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container packageserver ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: sonobuoy from sonobuoy started at 2023-11-15 05:52:01 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-m6885 from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-11-15 03:41:49 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.810: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    Nov 15 06:50:31.810: INFO: 
    Logging pods the apiserver thinks is on node 10.72.152.88 before test
    Nov 15 06:50:31.853: INFO: calico-kube-controllers-5dd9d87465-759pm from calico-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: calico-node-lmf6x from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: managed-storage-validation-webhooks-7457bf6687-9ds5w from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: managed-storage-validation-webhooks-7457bf6687-h522x from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: managed-storage-validation-webhooks-7457bf6687-phdgp from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-vbxww from ibm-system started at 2023-11-15 03:48:09 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container ibm-cloud-provider-ip-158-176-89-155 ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: ibm-file-plugin-5fcf7fb495-xmdts from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: ibm-keepalived-watcher-5jx26 from kube-system started at 2023-11-15 03:38:12 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: ibm-master-proxy-static-10.72.152.88 from kube-system started at 2023-11-15 03:38:04 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: 	Container pause ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: ibm-storage-metrics-agent-84fbdc746-5sv68 from kube-system started at 2023-11-15 03:39:51 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: ibm-storage-watcher-7445c988b-8ngdm from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: ibmcloud-block-storage-driver-9t8rj from kube-system started at 2023-11-15 03:38:17 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: ibmcloud-block-storage-plugin-5774687565-gj9xn from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: tuned-q894n from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container tuned ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: console-68d6458867-krfqd from openshift-console started at 2023-11-15 03:47:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container console ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: dns-default-lmngx from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container dns ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: node-resolver-hwp5f from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: image-registry-f74f764d8-w48k4 from openshift-image-registry started at 2023-11-15 03:46:35 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container registry ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: node-ca-wrqvd from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container node-ca ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: ingress-canary-wvl4p from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.853: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Nov 15 06:50:31.853: INFO: router-default-56777c97d6-4bnqq from openshift-ingress started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.854: INFO: 	Container router ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: openshift-kube-proxy-tzknx from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.854: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: node-exporter-2452k from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.854: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: prometheus-operator-admission-webhook-c78bf8f99-x8vcz from openshift-monitoring started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.854: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: thanos-querier-56b7586647-qr75b from openshift-monitoring started at 2023-11-15 03:44:58 +0000 UTC (6 container statuses recorded)
    Nov 15 06:50:31.854: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: 	Container oauth-proxy ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: 	Container thanos-query ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: multus-additional-cni-plugins-lcnxr from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.854: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: multus-admission-controller-6b76dd856b-6zft2 from openshift-multus started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.854: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: multus-vqg9w from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.854: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: network-metrics-daemon-9mrwp from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.854: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: network-check-source-5f9c5566b6-grf5l from openshift-network-diagnostics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.854: INFO: 	Container check-endpoints ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: network-check-target-6rz7p from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.854: INFO: 	Container network-check-target-container ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: network-operator-847f47449c-j9glm from openshift-network-operator started at 2023-11-15 03:38:21 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.854: INFO: 	Container network-operator ready: true, restart count 1
    Nov 15 06:50:31.854: INFO: metrics-667b585fc4-d5fdk from openshift-roks-metrics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.854: INFO: 	Container metrics ready: true, restart count 5
    Nov 15 06:50:31.854: INFO: push-gateway-7f9447c646-5mjcp from openshift-roks-metrics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.854: INFO: 	Container push-gateway ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: sonobuoy-e2e-job-4fb3cc1d32834aa9 from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.854: INFO: 	Container e2e ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-ddv8z from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
    Nov 15 06:50:31.854: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 06:50:31.854: INFO: tigera-operator-7dbcb4fb45-rn78j from tigera-operator started at 2023-11-15 03:38:21 +0000 UTC (1 container statuses recorded)
    Nov 15 06:50:31.854: INFO: 	Container tigera-operator ready: true, restart count 2
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node 10.72.152.81 11/15/23 06:50:31.931
    STEP: verifying the node has the label node 10.72.152.86 11/15/23 06:50:31.968
    STEP: verifying the node has the label node 10.72.152.88 11/15/23 06:50:32.005
    Nov 15 06:50:32.111: INFO: Pod calico-kube-controllers-5dd9d87465-759pm requesting resource cpu=10m on Node 10.72.152.88
    Nov 15 06:50:32.111: INFO: Pod calico-node-cmgfg requesting resource cpu=250m on Node 10.72.152.86
    Nov 15 06:50:32.111: INFO: Pod calico-node-lmf6x requesting resource cpu=250m on Node 10.72.152.88
    Nov 15 06:50:32.111: INFO: Pod calico-node-p5wd4 requesting resource cpu=250m on Node 10.72.152.81
    Nov 15 06:50:32.111: INFO: Pod calico-typha-76d9767bd5-985rd requesting resource cpu=250m on Node 10.72.152.81
    Nov 15 06:50:32.111: INFO: Pod calico-typha-76d9767bd5-tx2mp requesting resource cpu=250m on Node 10.72.152.86
    Nov 15 06:50:32.111: INFO: Pod managed-storage-validation-webhooks-7457bf6687-9ds5w requesting resource cpu=10m on Node 10.72.152.88
    Nov 15 06:50:32.111: INFO: Pod managed-storage-validation-webhooks-7457bf6687-h522x requesting resource cpu=10m on Node 10.72.152.88
    Nov 15 06:50:32.111: INFO: Pod managed-storage-validation-webhooks-7457bf6687-phdgp requesting resource cpu=10m on Node 10.72.152.88
    Nov 15 06:50:32.111: INFO: Pod ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-dl6kq requesting resource cpu=5m on Node 10.72.152.81
    Nov 15 06:50:32.111: INFO: Pod ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-vbxww requesting resource cpu=5m on Node 10.72.152.88
    Nov 15 06:50:32.111: INFO: Pod ibm-file-plugin-5fcf7fb495-xmdts requesting resource cpu=50m on Node 10.72.152.88
    Nov 15 06:50:32.111: INFO: Pod ibm-keepalived-watcher-5jx26 requesting resource cpu=5m on Node 10.72.152.88
    Nov 15 06:50:32.111: INFO: Pod ibm-keepalived-watcher-qb8hn requesting resource cpu=5m on Node 10.72.152.81
    Nov 15 06:50:32.111: INFO: Pod ibm-keepalived-watcher-qclxn requesting resource cpu=5m on Node 10.72.152.86
    Nov 15 06:50:32.111: INFO: Pod ibm-master-proxy-static-10.72.152.81 requesting resource cpu=26m on Node 10.72.152.81
    Nov 15 06:50:32.111: INFO: Pod ibm-master-proxy-static-10.72.152.86 requesting resource cpu=26m on Node 10.72.152.86
    Nov 15 06:50:32.111: INFO: Pod ibm-master-proxy-static-10.72.152.88 requesting resource cpu=26m on Node 10.72.152.88
    Nov 15 06:50:32.111: INFO: Pod ibm-storage-metrics-agent-84fbdc746-5sv68 requesting resource cpu=60m on Node 10.72.152.88
    Nov 15 06:50:32.111: INFO: Pod ibm-storage-watcher-7445c988b-8ngdm requesting resource cpu=50m on Node 10.72.152.88
    Nov 15 06:50:32.111: INFO: Pod ibmcloud-block-storage-driver-9t8rj requesting resource cpu=50m on Node 10.72.152.88
    Nov 15 06:50:32.111: INFO: Pod ibmcloud-block-storage-driver-m8gq4 requesting resource cpu=50m on Node 10.72.152.86
    Nov 15 06:50:32.111: INFO: Pod ibmcloud-block-storage-driver-z7fmw requesting resource cpu=50m on Node 10.72.152.81
    Nov 15 06:50:32.111: INFO: Pod ibmcloud-block-storage-plugin-5774687565-gj9xn requesting resource cpu=50m on Node 10.72.152.88
    Nov 15 06:50:32.111: INFO: Pod vpn-56cd75f85d-qwzcc requesting resource cpu=5m on Node 10.72.152.81
    Nov 15 06:50:32.111: INFO: Pod cluster-node-tuning-operator-5f77b58f7-t8gf6 requesting resource cpu=10m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod tuned-5qzzn requesting resource cpu=10m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod tuned-q894n requesting resource cpu=10m on Node 10.72.152.88
    Nov 15 06:50:32.112: INFO: Pod tuned-rb6qn requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod cluster-samples-operator-65684cb854-h7n8t requesting resource cpu=20m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod cluster-storage-operator-6cf6b595c7-m7mfz requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod csi-snapshot-controller-857d54544d-mwb9h requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod csi-snapshot-controller-857d54544d-qbhf6 requesting resource cpu=10m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod csi-snapshot-controller-operator-56df7685c7-vnvd2 requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod csi-snapshot-webhook-586f5c484d-qg5n7 requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod csi-snapshot-webhook-586f5c484d-vkpg9 requesting resource cpu=10m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod console-operator-769f9748fb-7tfcc requesting resource cpu=20m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod console-68d6458867-krfqd requesting resource cpu=10m on Node 10.72.152.88
    Nov 15 06:50:32.112: INFO: Pod console-68d6458867-wlg2q requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod downloads-7bb648f846-sr7nb requesting resource cpu=10m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod downloads-7bb648f846-tvqt6 requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod dns-operator-dd9c9c896-9gwtp requesting resource cpu=20m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod dns-default-24qc6 requesting resource cpu=60m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod dns-default-lmngx requesting resource cpu=60m on Node 10.72.152.88
    Nov 15 06:50:32.112: INFO: Pod dns-default-njlcg requesting resource cpu=60m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod node-resolver-6lxcm requesting resource cpu=5m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod node-resolver-d54kt requesting resource cpu=5m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod node-resolver-hwp5f requesting resource cpu=5m on Node 10.72.152.88
    Nov 15 06:50:32.112: INFO: Pod cluster-image-registry-operator-64994bbb4-6twjh requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod image-registry-f74f764d8-w48k4 requesting resource cpu=100m on Node 10.72.152.88
    Nov 15 06:50:32.112: INFO: Pod node-ca-897k2 requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod node-ca-w7vhg requesting resource cpu=10m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod node-ca-wrqvd requesting resource cpu=10m on Node 10.72.152.88
    Nov 15 06:50:32.112: INFO: Pod ingress-canary-jqtjt requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod ingress-canary-ttx6m requesting resource cpu=10m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod ingress-canary-wvl4p requesting resource cpu=10m on Node 10.72.152.88
    Nov 15 06:50:32.112: INFO: Pod ingress-operator-6d4d6975f7-qtm2n requesting resource cpu=20m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod router-default-56777c97d6-4bnqq requesting resource cpu=100m on Node 10.72.152.88
    Nov 15 06:50:32.112: INFO: Pod router-default-56777c97d6-szb4s requesting resource cpu=100m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod insights-operator-85b688b59d-v47wz requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod openshift-kube-proxy-p8p2b requesting resource cpu=110m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod openshift-kube-proxy-tzknx requesting resource cpu=110m on Node 10.72.152.88
    Nov 15 06:50:32.112: INFO: Pod openshift-kube-proxy-x5hq9 requesting resource cpu=110m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod kube-storage-version-migrator-operator-5d88b7484-8m2cc requesting resource cpu=10m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod migrator-697dd4cbc5-kk7zn requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod certified-operators-lqtq6 requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod community-operators-jpmzt requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod marketplace-operator-55cc9f5b6b-xpbw5 requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod redhat-marketplace-xp5nc requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod redhat-operators-bzltn requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod cluster-monitoring-operator-868f9b56cf-xrfz7 requesting resource cpu=10m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod kube-state-metrics-f8d796647-f4rgj requesting resource cpu=4m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod node-exporter-2452k requesting resource cpu=9m on Node 10.72.152.88
    Nov 15 06:50:32.112: INFO: Pod node-exporter-67458 requesting resource cpu=9m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod node-exporter-n68zb requesting resource cpu=9m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod openshift-state-metrics-69bb697b65-6bcg7 requesting resource cpu=3m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod prometheus-adapter-5f5bb574db-569xs requesting resource cpu=1m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod prometheus-adapter-5f5bb574db-r4lkm requesting resource cpu=1m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node 10.72.152.86
    Nov 15 06:50:32.112: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node 10.72.152.81
    Nov 15 06:50:32.112: INFO: Pod prometheus-operator-6fcb4d4c46-t74rt requesting resource cpu=6m on Node 10.72.152.86
    Nov 15 06:50:32.113: INFO: Pod prometheus-operator-admission-webhook-c78bf8f99-gxfh4 requesting resource cpu=5m on Node 10.72.152.81
    Nov 15 06:50:32.113: INFO: Pod prometheus-operator-admission-webhook-c78bf8f99-x8vcz requesting resource cpu=5m on Node 10.72.152.88
    Nov 15 06:50:32.113: INFO: Pod telemeter-client-77c946bb95-nsnds requesting resource cpu=3m on Node 10.72.152.81
    Nov 15 06:50:32.113: INFO: Pod thanos-querier-56b7586647-8m7mp requesting resource cpu=15m on Node 10.72.152.86
    Nov 15 06:50:32.113: INFO: Pod thanos-querier-56b7586647-qr75b requesting resource cpu=15m on Node 10.72.152.88
    Nov 15 06:50:32.113: INFO: Pod multus-9tc6x requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.113: INFO: Pod multus-additional-cni-plugins-j2jvc requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.113: INFO: Pod multus-additional-cni-plugins-lcnxr requesting resource cpu=10m on Node 10.72.152.88
    Nov 15 06:50:32.113: INFO: Pod multus-additional-cni-plugins-z6xx7 requesting resource cpu=10m on Node 10.72.152.86
    Nov 15 06:50:32.113: INFO: Pod multus-admission-controller-6b76dd856b-6qmnp requesting resource cpu=20m on Node 10.72.152.86
    Nov 15 06:50:32.113: INFO: Pod multus-admission-controller-6b76dd856b-6zft2 requesting resource cpu=20m on Node 10.72.152.88
    Nov 15 06:50:32.113: INFO: Pod multus-cltwv requesting resource cpu=10m on Node 10.72.152.86
    Nov 15 06:50:32.113: INFO: Pod multus-vqg9w requesting resource cpu=10m on Node 10.72.152.88
    Nov 15 06:50:32.113: INFO: Pod network-metrics-daemon-9mrwp requesting resource cpu=20m on Node 10.72.152.88
    Nov 15 06:50:32.113: INFO: Pod network-metrics-daemon-g5fgz requesting resource cpu=20m on Node 10.72.152.81
    Nov 15 06:50:32.113: INFO: Pod network-metrics-daemon-zbz9n requesting resource cpu=20m on Node 10.72.152.86
    Nov 15 06:50:32.113: INFO: Pod network-check-source-5f9c5566b6-grf5l requesting resource cpu=10m on Node 10.72.152.88
    Nov 15 06:50:32.113: INFO: Pod network-check-target-6pv9x requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.113: INFO: Pod network-check-target-6rz7p requesting resource cpu=10m on Node 10.72.152.88
    Nov 15 06:50:32.113: INFO: Pod network-check-target-gnph7 requesting resource cpu=10m on Node 10.72.152.86
    Nov 15 06:50:32.113: INFO: Pod network-operator-847f47449c-j9glm requesting resource cpu=10m on Node 10.72.152.88
    Nov 15 06:50:32.113: INFO: Pod catalog-operator-798697959c-24lbd requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.113: INFO: Pod olm-operator-846bf6bd78-fzxr5 requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.113: INFO: Pod package-server-manager-5b666bf8fd-5v7rb requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.113: INFO: Pod packageserver-758b547fc-65qc5 requesting resource cpu=10m on Node 10.72.152.86
    Nov 15 06:50:32.113: INFO: Pod packageserver-758b547fc-f9wwb requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.113: INFO: Pod metrics-667b585fc4-d5fdk requesting resource cpu=10m on Node 10.72.152.88
    Nov 15 06:50:32.113: INFO: Pod push-gateway-7f9447c646-5mjcp requesting resource cpu=10m on Node 10.72.152.88
    Nov 15 06:50:32.113: INFO: Pod service-ca-operator-74cb5c9cf5-fqgj5 requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.113: INFO: Pod service-ca-78fb97bb77-qcz4b requesting resource cpu=10m on Node 10.72.152.81
    Nov 15 06:50:32.113: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.72.152.86
    Nov 15 06:50:32.113: INFO: Pod sonobuoy-e2e-job-4fb3cc1d32834aa9 requesting resource cpu=0m on Node 10.72.152.88
    Nov 15 06:50:32.113: INFO: Pod sonobuoy-systemd-logs-daemon-set-997738c51257402b-ddv8z requesting resource cpu=0m on Node 10.72.152.88
    Nov 15 06:50:32.113: INFO: Pod sonobuoy-systemd-logs-daemon-set-997738c51257402b-m6885 requesting resource cpu=0m on Node 10.72.152.86
    Nov 15 06:50:32.113: INFO: Pod sonobuoy-systemd-logs-daemon-set-997738c51257402b-qvnnx requesting resource cpu=0m on Node 10.72.152.81
    Nov 15 06:50:32.113: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.72.152.86
    Nov 15 06:50:32.113: INFO: Pod tigera-operator-7dbcb4fb45-rn78j requesting resource cpu=100m on Node 10.72.152.88
    STEP: Starting Pods to consume most of the cluster CPU. 11/15/23 06:50:32.113
    Nov 15 06:50:32.113: INFO: Creating a pod which consumes cpu=1863m on Node 10.72.152.81
    Nov 15 06:50:32.135: INFO: Creating a pod which consumes cpu=1947m on Node 10.72.152.86
    Nov 15 06:50:32.150: INFO: Creating a pod which consumes cpu=1869m on Node 10.72.152.88
    Nov 15 06:50:32.177: INFO: Waiting up to 5m0s for pod "filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96" in namespace "sched-pred-5694" to be "running"
    Nov 15 06:50:32.187: INFO: Pod "filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96": Phase="Pending", Reason="", readiness=false. Elapsed: 9.804298ms
    Nov 15 06:50:34.200: INFO: Pod "filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023163337s
    Nov 15 06:50:36.200: INFO: Pod "filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96": Phase="Running", Reason="", readiness=true. Elapsed: 4.022812101s
    Nov 15 06:50:36.200: INFO: Pod "filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96" satisfied condition "running"
    Nov 15 06:50:36.200: INFO: Waiting up to 5m0s for pod "filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81" in namespace "sched-pred-5694" to be "running"
    Nov 15 06:50:36.211: INFO: Pod "filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81": Phase="Running", Reason="", readiness=true. Elapsed: 11.59589ms
    Nov 15 06:50:36.211: INFO: Pod "filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81" satisfied condition "running"
    Nov 15 06:50:36.211: INFO: Waiting up to 5m0s for pod "filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5" in namespace "sched-pred-5694" to be "running"
    Nov 15 06:50:36.223: INFO: Pod "filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5": Phase="Running", Reason="", readiness=true. Elapsed: 11.370366ms
    Nov 15 06:50:36.223: INFO: Pod "filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 11/15/23 06:50:36.223
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5.1797b937748175d9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5694/filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5 to 10.72.152.88] 11/15/23 06:50:36.236
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5.1797b937a62971b8], Reason = [AddedInterface], Message = [Add eth0 [172.30.10.183/32] from k8s-pod-network] 11/15/23 06:50:36.236
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5.1797b937b32f3f4e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 11/15/23 06:50:36.236
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5.1797b937bd1f0490], Reason = [Created], Message = [Created container filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5] 11/15/23 06:50:36.236
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5.1797b937be3b9920], Reason = [Started], Message = [Started container filler-pod-2050577e-35b7-4a2d-abf9-3e63877a2be5] 11/15/23 06:50:36.236
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96.1797b937722d4428], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5694/filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96 to 10.72.152.81] 11/15/23 06:50:36.236
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96.1797b937befbe274], Reason = [AddedInterface], Message = [Add eth0 [172.30.214.164/32] from k8s-pod-network] 11/15/23 06:50:36.236
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96.1797b937cd42a940], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 11/15/23 06:50:36.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96.1797b937d89928ac], Reason = [Created], Message = [Created container filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96] 11/15/23 06:50:36.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96.1797b937d9aa10ba], Reason = [Started], Message = [Started container filler-pod-57cd4121-6475-478a-acc5-b3f2781b9a96] 11/15/23 06:50:36.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81.1797b93773312925], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5694/filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81 to 10.72.152.86] 11/15/23 06:50:36.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81.1797b937a6bad993], Reason = [AddedInterface], Message = [Add eth0 [172.30.213.191/32] from k8s-pod-network] 11/15/23 06:50:36.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81.1797b937b3e82f34], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 11/15/23 06:50:36.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81.1797b937bf965e37], Reason = [Created], Message = [Created container filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81] 11/15/23 06:50:36.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81.1797b937c0bb7a8a], Reason = [Started], Message = [Started container filler-pod-97ca137d-40d3-4f3c-afcd-52947b8fcc81] 11/15/23 06:50:36.237
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1797b9386715f92b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 11/15/23 06:50:36.269
    STEP: removing the label node off the node 10.72.152.81 11/15/23 06:50:37.276
    STEP: verifying the node doesn't have the label node 11/15/23 06:50:37.311
    STEP: removing the label node off the node 10.72.152.86 11/15/23 06:50:37.323
    STEP: verifying the node doesn't have the label node 11/15/23 06:50:37.356
    STEP: removing the label node off the node 10.72.152.88 11/15/23 06:50:37.375
    STEP: verifying the node doesn't have the label node 11/15/23 06:50:37.407
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:50:37.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5694" for this suite. 11/15/23 06:50:37.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:50:37.481
Nov 15 06:50:37.481: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubectl 11/15/23 06:50:37.482
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:37.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:37.556
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 11/15/23 06:50:37.566
Nov 15 06:50:37.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9665 create -f -'
Nov 15 06:50:38.021: INFO: stderr: ""
Nov 15 06:50:38.021: INFO: stdout: "pod/pause created\n"
Nov 15 06:50:38.021: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov 15 06:50:38.021: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9665" to be "running and ready"
Nov 15 06:50:38.032: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 11.199259ms
Nov 15 06:50:38.032: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.72.152.81' to be 'Running' but was 'Pending'
Nov 15 06:50:40.045: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023807426s
Nov 15 06:50:40.045: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.72.152.81' to be 'Running' but was 'Pending'
Nov 15 06:50:42.048: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.026469656s
Nov 15 06:50:42.048: INFO: Pod "pause" satisfied condition "running and ready"
Nov 15 06:50:42.048: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 11/15/23 06:50:42.048
Nov 15 06:50:42.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9665 label pods pause testing-label=testing-label-value'
Nov 15 06:50:42.188: INFO: stderr: ""
Nov 15 06:50:42.188: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 11/15/23 06:50:42.188
Nov 15 06:50:42.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9665 get pod pause -L testing-label'
Nov 15 06:50:42.310: INFO: stderr: ""
Nov 15 06:50:42.311: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod 11/15/23 06:50:42.311
Nov 15 06:50:42.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9665 label pods pause testing-label-'
Nov 15 06:50:42.457: INFO: stderr: ""
Nov 15 06:50:42.457: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 11/15/23 06:50:42.457
Nov 15 06:50:42.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9665 get pod pause -L testing-label'
Nov 15 06:50:42.557: INFO: stderr: ""
Nov 15 06:50:42.557: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 11/15/23 06:50:42.557
Nov 15 06:50:42.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9665 delete --grace-period=0 --force -f -'
Nov 15 06:50:42.704: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 06:50:42.704: INFO: stdout: "pod \"pause\" force deleted\n"
Nov 15 06:50:42.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9665 get rc,svc -l name=pause --no-headers'
Nov 15 06:50:42.843: INFO: stderr: "No resources found in kubectl-9665 namespace.\n"
Nov 15 06:50:42.843: INFO: stdout: ""
Nov 15 06:50:42.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9665 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 15 06:50:42.955: INFO: stderr: ""
Nov 15 06:50:42.955: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 06:50:42.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9665" for this suite. 11/15/23 06:50:42.977
------------------------------
â€¢ [SLOW TEST] [5.525 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:50:37.481
    Nov 15 06:50:37.481: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubectl 11/15/23 06:50:37.482
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:37.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:37.556
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 11/15/23 06:50:37.566
    Nov 15 06:50:37.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9665 create -f -'
    Nov 15 06:50:38.021: INFO: stderr: ""
    Nov 15 06:50:38.021: INFO: stdout: "pod/pause created\n"
    Nov 15 06:50:38.021: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Nov 15 06:50:38.021: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9665" to be "running and ready"
    Nov 15 06:50:38.032: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 11.199259ms
    Nov 15 06:50:38.032: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.72.152.81' to be 'Running' but was 'Pending'
    Nov 15 06:50:40.045: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023807426s
    Nov 15 06:50:40.045: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.72.152.81' to be 'Running' but was 'Pending'
    Nov 15 06:50:42.048: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.026469656s
    Nov 15 06:50:42.048: INFO: Pod "pause" satisfied condition "running and ready"
    Nov 15 06:50:42.048: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 11/15/23 06:50:42.048
    Nov 15 06:50:42.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9665 label pods pause testing-label=testing-label-value'
    Nov 15 06:50:42.188: INFO: stderr: ""
    Nov 15 06:50:42.188: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 11/15/23 06:50:42.188
    Nov 15 06:50:42.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9665 get pod pause -L testing-label'
    Nov 15 06:50:42.310: INFO: stderr: ""
    Nov 15 06:50:42.311: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 11/15/23 06:50:42.311
    Nov 15 06:50:42.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9665 label pods pause testing-label-'
    Nov 15 06:50:42.457: INFO: stderr: ""
    Nov 15 06:50:42.457: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 11/15/23 06:50:42.457
    Nov 15 06:50:42.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9665 get pod pause -L testing-label'
    Nov 15 06:50:42.557: INFO: stderr: ""
    Nov 15 06:50:42.557: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 11/15/23 06:50:42.557
    Nov 15 06:50:42.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9665 delete --grace-period=0 --force -f -'
    Nov 15 06:50:42.704: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 15 06:50:42.704: INFO: stdout: "pod \"pause\" force deleted\n"
    Nov 15 06:50:42.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9665 get rc,svc -l name=pause --no-headers'
    Nov 15 06:50:42.843: INFO: stderr: "No resources found in kubectl-9665 namespace.\n"
    Nov 15 06:50:42.843: INFO: stdout: ""
    Nov 15 06:50:42.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-9665 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Nov 15 06:50:42.955: INFO: stderr: ""
    Nov 15 06:50:42.955: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:50:42.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9665" for this suite. 11/15/23 06:50:42.977
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:50:43.007
Nov 15 06:50:43.007: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename secrets 11/15/23 06:50:43.008
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:43.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:43.07
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-6645/secret-test-2a0e3686-abe1-475c-866d-52536453d316 11/15/23 06:50:43.08
STEP: Creating a pod to test consume secrets 11/15/23 06:50:43.094
Nov 15 06:50:43.120: INFO: Waiting up to 5m0s for pod "pod-configmaps-aaad9e8e-6d6c-4323-90b7-2a8b64701027" in namespace "secrets-6645" to be "Succeeded or Failed"
Nov 15 06:50:43.137: INFO: Pod "pod-configmaps-aaad9e8e-6d6c-4323-90b7-2a8b64701027": Phase="Pending", Reason="", readiness=false. Elapsed: 16.846092ms
Nov 15 06:50:45.150: INFO: Pod "pod-configmaps-aaad9e8e-6d6c-4323-90b7-2a8b64701027": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029317651s
Nov 15 06:50:47.155: INFO: Pod "pod-configmaps-aaad9e8e-6d6c-4323-90b7-2a8b64701027": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034890115s
Nov 15 06:50:49.149: INFO: Pod "pod-configmaps-aaad9e8e-6d6c-4323-90b7-2a8b64701027": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028134654s
STEP: Saw pod success 11/15/23 06:50:49.149
Nov 15 06:50:49.149: INFO: Pod "pod-configmaps-aaad9e8e-6d6c-4323-90b7-2a8b64701027" satisfied condition "Succeeded or Failed"
Nov 15 06:50:49.160: INFO: Trying to get logs from node 10.72.152.81 pod pod-configmaps-aaad9e8e-6d6c-4323-90b7-2a8b64701027 container env-test: <nil>
STEP: delete the pod 11/15/23 06:50:49.227
Nov 15 06:50:49.318: INFO: Waiting for pod pod-configmaps-aaad9e8e-6d6c-4323-90b7-2a8b64701027 to disappear
Nov 15 06:50:49.329: INFO: Pod pod-configmaps-aaad9e8e-6d6c-4323-90b7-2a8b64701027 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 06:50:49.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6645" for this suite. 11/15/23 06:50:49.353
------------------------------
â€¢ [SLOW TEST] [6.367 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:50:43.007
    Nov 15 06:50:43.007: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename secrets 11/15/23 06:50:43.008
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:43.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:43.07
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-6645/secret-test-2a0e3686-abe1-475c-866d-52536453d316 11/15/23 06:50:43.08
    STEP: Creating a pod to test consume secrets 11/15/23 06:50:43.094
    Nov 15 06:50:43.120: INFO: Waiting up to 5m0s for pod "pod-configmaps-aaad9e8e-6d6c-4323-90b7-2a8b64701027" in namespace "secrets-6645" to be "Succeeded or Failed"
    Nov 15 06:50:43.137: INFO: Pod "pod-configmaps-aaad9e8e-6d6c-4323-90b7-2a8b64701027": Phase="Pending", Reason="", readiness=false. Elapsed: 16.846092ms
    Nov 15 06:50:45.150: INFO: Pod "pod-configmaps-aaad9e8e-6d6c-4323-90b7-2a8b64701027": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029317651s
    Nov 15 06:50:47.155: INFO: Pod "pod-configmaps-aaad9e8e-6d6c-4323-90b7-2a8b64701027": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034890115s
    Nov 15 06:50:49.149: INFO: Pod "pod-configmaps-aaad9e8e-6d6c-4323-90b7-2a8b64701027": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028134654s
    STEP: Saw pod success 11/15/23 06:50:49.149
    Nov 15 06:50:49.149: INFO: Pod "pod-configmaps-aaad9e8e-6d6c-4323-90b7-2a8b64701027" satisfied condition "Succeeded or Failed"
    Nov 15 06:50:49.160: INFO: Trying to get logs from node 10.72.152.81 pod pod-configmaps-aaad9e8e-6d6c-4323-90b7-2a8b64701027 container env-test: <nil>
    STEP: delete the pod 11/15/23 06:50:49.227
    Nov 15 06:50:49.318: INFO: Waiting for pod pod-configmaps-aaad9e8e-6d6c-4323-90b7-2a8b64701027 to disappear
    Nov 15 06:50:49.329: INFO: Pod pod-configmaps-aaad9e8e-6d6c-4323-90b7-2a8b64701027 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:50:49.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6645" for this suite. 11/15/23 06:50:49.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:50:49.375
Nov 15 06:50:49.375: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename downward-api 11/15/23 06:50:49.376
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:49.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:49.443
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 11/15/23 06:50:49.453
Nov 15 06:50:49.498: INFO: Waiting up to 5m0s for pod "downward-api-7f907c8a-55bf-41da-a017-4a2bf3b0c54b" in namespace "downward-api-8066" to be "Succeeded or Failed"
Nov 15 06:50:49.534: INFO: Pod "downward-api-7f907c8a-55bf-41da-a017-4a2bf3b0c54b": Phase="Pending", Reason="", readiness=false. Elapsed: 36.23603ms
Nov 15 06:50:51.546: INFO: Pod "downward-api-7f907c8a-55bf-41da-a017-4a2bf3b0c54b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04829358s
Nov 15 06:50:53.548: INFO: Pod "downward-api-7f907c8a-55bf-41da-a017-4a2bf3b0c54b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050526985s
Nov 15 06:50:55.552: INFO: Pod "downward-api-7f907c8a-55bf-41da-a017-4a2bf3b0c54b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054126645s
STEP: Saw pod success 11/15/23 06:50:55.552
Nov 15 06:50:55.552: INFO: Pod "downward-api-7f907c8a-55bf-41da-a017-4a2bf3b0c54b" satisfied condition "Succeeded or Failed"
Nov 15 06:50:55.564: INFO: Trying to get logs from node 10.72.152.81 pod downward-api-7f907c8a-55bf-41da-a017-4a2bf3b0c54b container dapi-container: <nil>
STEP: delete the pod 11/15/23 06:50:55.66
Nov 15 06:50:55.716: INFO: Waiting for pod downward-api-7f907c8a-55bf-41da-a017-4a2bf3b0c54b to disappear
Nov 15 06:50:55.740: INFO: Pod downward-api-7f907c8a-55bf-41da-a017-4a2bf3b0c54b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Nov 15 06:50:55.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8066" for this suite. 11/15/23 06:50:55.774
------------------------------
â€¢ [SLOW TEST] [6.452 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:50:49.375
    Nov 15 06:50:49.375: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename downward-api 11/15/23 06:50:49.376
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:49.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:49.443
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 11/15/23 06:50:49.453
    Nov 15 06:50:49.498: INFO: Waiting up to 5m0s for pod "downward-api-7f907c8a-55bf-41da-a017-4a2bf3b0c54b" in namespace "downward-api-8066" to be "Succeeded or Failed"
    Nov 15 06:50:49.534: INFO: Pod "downward-api-7f907c8a-55bf-41da-a017-4a2bf3b0c54b": Phase="Pending", Reason="", readiness=false. Elapsed: 36.23603ms
    Nov 15 06:50:51.546: INFO: Pod "downward-api-7f907c8a-55bf-41da-a017-4a2bf3b0c54b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04829358s
    Nov 15 06:50:53.548: INFO: Pod "downward-api-7f907c8a-55bf-41da-a017-4a2bf3b0c54b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050526985s
    Nov 15 06:50:55.552: INFO: Pod "downward-api-7f907c8a-55bf-41da-a017-4a2bf3b0c54b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054126645s
    STEP: Saw pod success 11/15/23 06:50:55.552
    Nov 15 06:50:55.552: INFO: Pod "downward-api-7f907c8a-55bf-41da-a017-4a2bf3b0c54b" satisfied condition "Succeeded or Failed"
    Nov 15 06:50:55.564: INFO: Trying to get logs from node 10.72.152.81 pod downward-api-7f907c8a-55bf-41da-a017-4a2bf3b0c54b container dapi-container: <nil>
    STEP: delete the pod 11/15/23 06:50:55.66
    Nov 15 06:50:55.716: INFO: Waiting for pod downward-api-7f907c8a-55bf-41da-a017-4a2bf3b0c54b to disappear
    Nov 15 06:50:55.740: INFO: Pod downward-api-7f907c8a-55bf-41da-a017-4a2bf3b0c54b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:50:55.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8066" for this suite. 11/15/23 06:50:55.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:50:55.828
Nov 15 06:50:55.828: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename namespaces 11/15/23 06:50:55.829
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:55.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:55.904
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 11/15/23 06:50:55.932
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:55.999
STEP: Creating a pod in the namespace 11/15/23 06:50:56.027
STEP: Waiting for the pod to have running status 11/15/23 06:50:56.079
Nov 15 06:50:56.079: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-5050" to be "running"
Nov 15 06:50:56.090: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.894594ms
Nov 15 06:50:58.124: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.044445869s
Nov 15 06:50:58.124: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 11/15/23 06:50:58.124
STEP: Waiting for the namespace to be removed. 11/15/23 06:50:58.163
STEP: Recreating the namespace 11/15/23 06:51:13.175
STEP: Verifying there are no pods in the namespace 11/15/23 06:51:13.235
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:51:13.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5243" for this suite. 11/15/23 06:51:13.273
STEP: Destroying namespace "nsdeletetest-5050" for this suite. 11/15/23 06:51:13.301
Nov 15 06:51:13.320: INFO: Namespace nsdeletetest-5050 was already deleted
STEP: Destroying namespace "nsdeletetest-1169" for this suite. 11/15/23 06:51:13.32
------------------------------
â€¢ [SLOW TEST] [17.520 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:50:55.828
    Nov 15 06:50:55.828: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename namespaces 11/15/23 06:50:55.829
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:55.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:50:55.904
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 11/15/23 06:50:55.932
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:50:55.999
    STEP: Creating a pod in the namespace 11/15/23 06:50:56.027
    STEP: Waiting for the pod to have running status 11/15/23 06:50:56.079
    Nov 15 06:50:56.079: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-5050" to be "running"
    Nov 15 06:50:56.090: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.894594ms
    Nov 15 06:50:58.124: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.044445869s
    Nov 15 06:50:58.124: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 11/15/23 06:50:58.124
    STEP: Waiting for the namespace to be removed. 11/15/23 06:50:58.163
    STEP: Recreating the namespace 11/15/23 06:51:13.175
    STEP: Verifying there are no pods in the namespace 11/15/23 06:51:13.235
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:51:13.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5243" for this suite. 11/15/23 06:51:13.273
    STEP: Destroying namespace "nsdeletetest-5050" for this suite. 11/15/23 06:51:13.301
    Nov 15 06:51:13.320: INFO: Namespace nsdeletetest-5050 was already deleted
    STEP: Destroying namespace "nsdeletetest-1169" for this suite. 11/15/23 06:51:13.32
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:51:13.349
Nov 15 06:51:13.349: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename var-expansion 11/15/23 06:51:13.351
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:51:13.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:51:13.421
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 11/15/23 06:51:13.431
Nov 15 06:51:13.465: INFO: Waiting up to 2m0s for pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778" in namespace "var-expansion-16" to be "running"
Nov 15 06:51:13.481: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 16.347306ms
Nov 15 06:51:15.496: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031293084s
Nov 15 06:51:17.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029947713s
Nov 15 06:51:19.496: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031271448s
Nov 15 06:51:21.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029794748s
Nov 15 06:51:23.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029090852s
Nov 15 06:51:25.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 12.030208143s
Nov 15 06:51:27.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 14.028826215s
Nov 15 06:51:29.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 16.029111715s
Nov 15 06:51:31.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 18.029715778s
Nov 15 06:51:33.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 20.029585492s
Nov 15 06:51:35.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 22.02944956s
Nov 15 06:51:37.503: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 24.037644211s
Nov 15 06:51:39.501: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 26.035542611s
Nov 15 06:51:41.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 28.030273481s
Nov 15 06:51:43.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 30.029440986s
Nov 15 06:51:45.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 32.030409837s
Nov 15 06:51:47.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 34.029435225s
Nov 15 06:51:49.503: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 36.037526195s
Nov 15 06:51:51.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 38.030084768s
Nov 15 06:51:53.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 40.029952474s
Nov 15 06:51:55.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 42.028908773s
Nov 15 06:51:57.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 44.029162874s
Nov 15 06:51:59.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 46.030108028s
Nov 15 06:52:01.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 48.030274489s
Nov 15 06:52:03.493: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 50.028265684s
Nov 15 06:52:05.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 52.029788934s
Nov 15 06:52:07.498: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 54.032974279s
Nov 15 06:52:09.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 56.029982372s
Nov 15 06:52:11.501: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 58.035879638s
Nov 15 06:52:13.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.029487024s
Nov 15 06:52:15.493: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.028307735s
Nov 15 06:52:17.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.028626054s
Nov 15 06:52:19.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.029857697s
Nov 15 06:52:21.493: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.028012491s
Nov 15 06:52:23.496: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.030673159s
Nov 15 06:52:25.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.029331275s
Nov 15 06:52:27.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.029567101s
Nov 15 06:52:29.493: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.02816249s
Nov 15 06:52:31.496: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.031030698s
Nov 15 06:52:33.511: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.046329854s
Nov 15 06:52:35.504: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.038766937s
Nov 15 06:52:37.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.028677994s
Nov 15 06:52:39.493: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.027629204s
Nov 15 06:52:41.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.02865767s
Nov 15 06:52:43.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.028653722s
Nov 15 06:52:45.506: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.041318779s
Nov 15 06:52:47.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.029620833s
Nov 15 06:52:49.498: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.033062899s
Nov 15 06:52:51.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.028912564s
Nov 15 06:52:53.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.029560841s
Nov 15 06:52:55.496: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.030493879s
Nov 15 06:52:57.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.029594548s
Nov 15 06:52:59.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.028920257s
Nov 15 06:53:01.497: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.032047909s
Nov 15 06:53:03.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.028908449s
Nov 15 06:53:05.497: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.031868769s
Nov 15 06:53:07.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.028681592s
Nov 15 06:53:09.493: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.028092213s
Nov 15 06:53:11.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.029311127s
Nov 15 06:53:13.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.029417609s
Nov 15 06:53:13.505: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.040254068s
STEP: updating the pod 11/15/23 06:53:13.505
Nov 15 06:53:14.037: INFO: Successfully updated pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778"
STEP: waiting for pod running 11/15/23 06:53:14.037
Nov 15 06:53:14.038: INFO: Waiting up to 2m0s for pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778" in namespace "var-expansion-16" to be "running"
Nov 15 06:53:14.049: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 11.205887ms
Nov 15 06:53:16.073: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Running", Reason="", readiness=true. Elapsed: 2.034565923s
Nov 15 06:53:16.073: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778" satisfied condition "running"
STEP: deleting the pod gracefully 11/15/23 06:53:16.073
Nov 15 06:53:16.073: INFO: Deleting pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778" in namespace "var-expansion-16"
Nov 15 06:53:16.091: INFO: Wait up to 5m0s for pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 15 06:53:48.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-16" for this suite. 11/15/23 06:53:48.136
------------------------------
â€¢ [SLOW TEST] [154.806 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:51:13.349
    Nov 15 06:51:13.349: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename var-expansion 11/15/23 06:51:13.351
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:51:13.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:51:13.421
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 11/15/23 06:51:13.431
    Nov 15 06:51:13.465: INFO: Waiting up to 2m0s for pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778" in namespace "var-expansion-16" to be "running"
    Nov 15 06:51:13.481: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 16.347306ms
    Nov 15 06:51:15.496: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031293084s
    Nov 15 06:51:17.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029947713s
    Nov 15 06:51:19.496: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031271448s
    Nov 15 06:51:21.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029794748s
    Nov 15 06:51:23.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029090852s
    Nov 15 06:51:25.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 12.030208143s
    Nov 15 06:51:27.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 14.028826215s
    Nov 15 06:51:29.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 16.029111715s
    Nov 15 06:51:31.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 18.029715778s
    Nov 15 06:51:33.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 20.029585492s
    Nov 15 06:51:35.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 22.02944956s
    Nov 15 06:51:37.503: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 24.037644211s
    Nov 15 06:51:39.501: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 26.035542611s
    Nov 15 06:51:41.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 28.030273481s
    Nov 15 06:51:43.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 30.029440986s
    Nov 15 06:51:45.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 32.030409837s
    Nov 15 06:51:47.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 34.029435225s
    Nov 15 06:51:49.503: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 36.037526195s
    Nov 15 06:51:51.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 38.030084768s
    Nov 15 06:51:53.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 40.029952474s
    Nov 15 06:51:55.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 42.028908773s
    Nov 15 06:51:57.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 44.029162874s
    Nov 15 06:51:59.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 46.030108028s
    Nov 15 06:52:01.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 48.030274489s
    Nov 15 06:52:03.493: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 50.028265684s
    Nov 15 06:52:05.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 52.029788934s
    Nov 15 06:52:07.498: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 54.032974279s
    Nov 15 06:52:09.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 56.029982372s
    Nov 15 06:52:11.501: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 58.035879638s
    Nov 15 06:52:13.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.029487024s
    Nov 15 06:52:15.493: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.028307735s
    Nov 15 06:52:17.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.028626054s
    Nov 15 06:52:19.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.029857697s
    Nov 15 06:52:21.493: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.028012491s
    Nov 15 06:52:23.496: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.030673159s
    Nov 15 06:52:25.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.029331275s
    Nov 15 06:52:27.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.029567101s
    Nov 15 06:52:29.493: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.02816249s
    Nov 15 06:52:31.496: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.031030698s
    Nov 15 06:52:33.511: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.046329854s
    Nov 15 06:52:35.504: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.038766937s
    Nov 15 06:52:37.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.028677994s
    Nov 15 06:52:39.493: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.027629204s
    Nov 15 06:52:41.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.02865767s
    Nov 15 06:52:43.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.028653722s
    Nov 15 06:52:45.506: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.041318779s
    Nov 15 06:52:47.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.029620833s
    Nov 15 06:52:49.498: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.033062899s
    Nov 15 06:52:51.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.028912564s
    Nov 15 06:52:53.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.029560841s
    Nov 15 06:52:55.496: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.030493879s
    Nov 15 06:52:57.495: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.029594548s
    Nov 15 06:52:59.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.028920257s
    Nov 15 06:53:01.497: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.032047909s
    Nov 15 06:53:03.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.028908449s
    Nov 15 06:53:05.497: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.031868769s
    Nov 15 06:53:07.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.028681592s
    Nov 15 06:53:09.493: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.028092213s
    Nov 15 06:53:11.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.029311127s
    Nov 15 06:53:13.494: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.029417609s
    Nov 15 06:53:13.505: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.040254068s
    STEP: updating the pod 11/15/23 06:53:13.505
    Nov 15 06:53:14.037: INFO: Successfully updated pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778"
    STEP: waiting for pod running 11/15/23 06:53:14.037
    Nov 15 06:53:14.038: INFO: Waiting up to 2m0s for pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778" in namespace "var-expansion-16" to be "running"
    Nov 15 06:53:14.049: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Pending", Reason="", readiness=false. Elapsed: 11.205887ms
    Nov 15 06:53:16.073: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778": Phase="Running", Reason="", readiness=true. Elapsed: 2.034565923s
    Nov 15 06:53:16.073: INFO: Pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778" satisfied condition "running"
    STEP: deleting the pod gracefully 11/15/23 06:53:16.073
    Nov 15 06:53:16.073: INFO: Deleting pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778" in namespace "var-expansion-16"
    Nov 15 06:53:16.091: INFO: Wait up to 5m0s for pod "var-expansion-8ce86e5c-6602-484b-b35b-a1c24a122778" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:53:48.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-16" for this suite. 11/15/23 06:53:48.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:53:48.159
Nov 15 06:53:48.159: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename configmap 11/15/23 06:53:48.16
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:53:48.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:53:48.225
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
Nov 15 06:53:48.252: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-37c724f0-b1ee-484b-a705-aec80321ddba 11/15/23 06:53:48.252
STEP: Creating the pod 11/15/23 06:53:48.264
Nov 15 06:53:48.289: INFO: Waiting up to 5m0s for pod "pod-configmaps-ebe8ec50-5075-43ef-b4e9-e075484a28e9" in namespace "configmap-9316" to be "running and ready"
Nov 15 06:53:48.303: INFO: Pod "pod-configmaps-ebe8ec50-5075-43ef-b4e9-e075484a28e9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.79535ms
Nov 15 06:53:48.303: INFO: The phase of Pod pod-configmaps-ebe8ec50-5075-43ef-b4e9-e075484a28e9 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:53:50.317: INFO: Pod "pod-configmaps-ebe8ec50-5075-43ef-b4e9-e075484a28e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.028053019s
Nov 15 06:53:50.317: INFO: The phase of Pod pod-configmaps-ebe8ec50-5075-43ef-b4e9-e075484a28e9 is Running (Ready = true)
Nov 15 06:53:50.317: INFO: Pod "pod-configmaps-ebe8ec50-5075-43ef-b4e9-e075484a28e9" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-37c724f0-b1ee-484b-a705-aec80321ddba 11/15/23 06:53:50.53
STEP: waiting to observe update in volume 11/15/23 06:53:50.543
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 06:53:52.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9316" for this suite. 11/15/23 06:53:52.632
------------------------------
â€¢ [4.493 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:53:48.159
    Nov 15 06:53:48.159: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename configmap 11/15/23 06:53:48.16
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:53:48.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:53:48.225
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    Nov 15 06:53:48.252: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name configmap-test-upd-37c724f0-b1ee-484b-a705-aec80321ddba 11/15/23 06:53:48.252
    STEP: Creating the pod 11/15/23 06:53:48.264
    Nov 15 06:53:48.289: INFO: Waiting up to 5m0s for pod "pod-configmaps-ebe8ec50-5075-43ef-b4e9-e075484a28e9" in namespace "configmap-9316" to be "running and ready"
    Nov 15 06:53:48.303: INFO: Pod "pod-configmaps-ebe8ec50-5075-43ef-b4e9-e075484a28e9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.79535ms
    Nov 15 06:53:48.303: INFO: The phase of Pod pod-configmaps-ebe8ec50-5075-43ef-b4e9-e075484a28e9 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:53:50.317: INFO: Pod "pod-configmaps-ebe8ec50-5075-43ef-b4e9-e075484a28e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.028053019s
    Nov 15 06:53:50.317: INFO: The phase of Pod pod-configmaps-ebe8ec50-5075-43ef-b4e9-e075484a28e9 is Running (Ready = true)
    Nov 15 06:53:50.317: INFO: Pod "pod-configmaps-ebe8ec50-5075-43ef-b4e9-e075484a28e9" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-37c724f0-b1ee-484b-a705-aec80321ddba 11/15/23 06:53:50.53
    STEP: waiting to observe update in volume 11/15/23 06:53:50.543
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:53:52.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9316" for this suite. 11/15/23 06:53:52.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:53:52.653
Nov 15 06:53:52.653: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename statefulset 11/15/23 06:53:52.654
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:53:52.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:53:52.716
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1102 11/15/23 06:53:52.728
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Nov 15 06:53:52.774: INFO: Found 0 stateful pods, waiting for 1
Nov 15 06:54:02.787: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 11/15/23 06:54:02.823
W1115 06:54:02.858980      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Nov 15 06:54:02.885: INFO: Found 1 stateful pods, waiting for 2
Nov 15 06:54:12.900: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=false
Nov 15 06:54:22.908: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 06:54:22.908: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 11/15/23 06:54:22.942
STEP: Delete all of the StatefulSets 11/15/23 06:54:22.975
STEP: Verify that StatefulSets have been deleted 11/15/23 06:54:23.014
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 15 06:54:23.054: INFO: Deleting all statefulset in ns statefulset-1102
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 15 06:54:23.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1102" for this suite. 11/15/23 06:54:23.127
------------------------------
â€¢ [SLOW TEST] [30.494 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:53:52.653
    Nov 15 06:53:52.653: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename statefulset 11/15/23 06:53:52.654
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:53:52.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:53:52.716
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1102 11/15/23 06:53:52.728
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Nov 15 06:53:52.774: INFO: Found 0 stateful pods, waiting for 1
    Nov 15 06:54:02.787: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 11/15/23 06:54:02.823
    W1115 06:54:02.858980      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Nov 15 06:54:02.885: INFO: Found 1 stateful pods, waiting for 2
    Nov 15 06:54:12.900: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Nov 15 06:54:22.908: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 06:54:22.908: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 11/15/23 06:54:22.942
    STEP: Delete all of the StatefulSets 11/15/23 06:54:22.975
    STEP: Verify that StatefulSets have been deleted 11/15/23 06:54:23.014
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 15 06:54:23.054: INFO: Deleting all statefulset in ns statefulset-1102
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:54:23.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1102" for this suite. 11/15/23 06:54:23.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:54:23.149
Nov 15 06:54:23.149: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename pods 11/15/23 06:54:23.151
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:54:23.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:54:23.23
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 11/15/23 06:54:23.24
Nov 15 06:54:23.267: INFO: Waiting up to 5m0s for pod "pod-4pgbf" in namespace "pods-8062" to be "running"
Nov 15 06:54:23.285: INFO: Pod "pod-4pgbf": Phase="Pending", Reason="", readiness=false. Elapsed: 17.353271ms
Nov 15 06:54:25.297: INFO: Pod "pod-4pgbf": Phase="Running", Reason="", readiness=true. Elapsed: 2.029424275s
Nov 15 06:54:25.297: INFO: Pod "pod-4pgbf" satisfied condition "running"
STEP: patching /status 11/15/23 06:54:25.297
Nov 15 06:54:25.315: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 06:54:25.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8062" for this suite. 11/15/23 06:54:25.334
------------------------------
â€¢ [2.205 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:54:23.149
    Nov 15 06:54:23.149: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename pods 11/15/23 06:54:23.151
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:54:23.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:54:23.23
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 11/15/23 06:54:23.24
    Nov 15 06:54:23.267: INFO: Waiting up to 5m0s for pod "pod-4pgbf" in namespace "pods-8062" to be "running"
    Nov 15 06:54:23.285: INFO: Pod "pod-4pgbf": Phase="Pending", Reason="", readiness=false. Elapsed: 17.353271ms
    Nov 15 06:54:25.297: INFO: Pod "pod-4pgbf": Phase="Running", Reason="", readiness=true. Elapsed: 2.029424275s
    Nov 15 06:54:25.297: INFO: Pod "pod-4pgbf" satisfied condition "running"
    STEP: patching /status 11/15/23 06:54:25.297
    Nov 15 06:54:25.315: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:54:25.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8062" for this suite. 11/15/23 06:54:25.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:54:25.358
Nov 15 06:54:25.358: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename secrets 11/15/23 06:54:25.359
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:54:25.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:54:25.468
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-123dbc90-f61f-4672-987d-645ab6a6964e 11/15/23 06:54:25.486
STEP: Creating a pod to test consume secrets 11/15/23 06:54:25.507
Nov 15 06:54:25.537: INFO: Waiting up to 5m0s for pod "pod-secrets-827e4f50-5412-49a7-abc7-6e43608b97ad" in namespace "secrets-8037" to be "Succeeded or Failed"
Nov 15 06:54:25.554: INFO: Pod "pod-secrets-827e4f50-5412-49a7-abc7-6e43608b97ad": Phase="Pending", Reason="", readiness=false. Elapsed: 16.795975ms
Nov 15 06:54:27.566: INFO: Pod "pod-secrets-827e4f50-5412-49a7-abc7-6e43608b97ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028975378s
Nov 15 06:54:29.568: INFO: Pod "pod-secrets-827e4f50-5412-49a7-abc7-6e43608b97ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03091438s
STEP: Saw pod success 11/15/23 06:54:29.568
Nov 15 06:54:29.569: INFO: Pod "pod-secrets-827e4f50-5412-49a7-abc7-6e43608b97ad" satisfied condition "Succeeded or Failed"
Nov 15 06:54:29.580: INFO: Trying to get logs from node 10.72.152.86 pod pod-secrets-827e4f50-5412-49a7-abc7-6e43608b97ad container secret-volume-test: <nil>
STEP: delete the pod 11/15/23 06:54:29.609
Nov 15 06:54:29.638: INFO: Waiting for pod pod-secrets-827e4f50-5412-49a7-abc7-6e43608b97ad to disappear
Nov 15 06:54:29.649: INFO: Pod pod-secrets-827e4f50-5412-49a7-abc7-6e43608b97ad no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 06:54:29.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8037" for this suite. 11/15/23 06:54:29.67
------------------------------
â€¢ [4.336 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:54:25.358
    Nov 15 06:54:25.358: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename secrets 11/15/23 06:54:25.359
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:54:25.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:54:25.468
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-123dbc90-f61f-4672-987d-645ab6a6964e 11/15/23 06:54:25.486
    STEP: Creating a pod to test consume secrets 11/15/23 06:54:25.507
    Nov 15 06:54:25.537: INFO: Waiting up to 5m0s for pod "pod-secrets-827e4f50-5412-49a7-abc7-6e43608b97ad" in namespace "secrets-8037" to be "Succeeded or Failed"
    Nov 15 06:54:25.554: INFO: Pod "pod-secrets-827e4f50-5412-49a7-abc7-6e43608b97ad": Phase="Pending", Reason="", readiness=false. Elapsed: 16.795975ms
    Nov 15 06:54:27.566: INFO: Pod "pod-secrets-827e4f50-5412-49a7-abc7-6e43608b97ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028975378s
    Nov 15 06:54:29.568: INFO: Pod "pod-secrets-827e4f50-5412-49a7-abc7-6e43608b97ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03091438s
    STEP: Saw pod success 11/15/23 06:54:29.568
    Nov 15 06:54:29.569: INFO: Pod "pod-secrets-827e4f50-5412-49a7-abc7-6e43608b97ad" satisfied condition "Succeeded or Failed"
    Nov 15 06:54:29.580: INFO: Trying to get logs from node 10.72.152.86 pod pod-secrets-827e4f50-5412-49a7-abc7-6e43608b97ad container secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 06:54:29.609
    Nov 15 06:54:29.638: INFO: Waiting for pod pod-secrets-827e4f50-5412-49a7-abc7-6e43608b97ad to disappear
    Nov 15 06:54:29.649: INFO: Pod pod-secrets-827e4f50-5412-49a7-abc7-6e43608b97ad no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:54:29.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8037" for this suite. 11/15/23 06:54:29.67
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:54:29.696
Nov 15 06:54:29.696: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename emptydir-wrapper 11/15/23 06:54:29.697
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:54:29.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:54:29.771
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Nov 15 06:54:29.845: INFO: Waiting up to 5m0s for pod "pod-secrets-ba381697-e90d-4d4f-87e1-068bb6af7b00" in namespace "emptydir-wrapper-3414" to be "running and ready"
Nov 15 06:54:29.856: INFO: Pod "pod-secrets-ba381697-e90d-4d4f-87e1-068bb6af7b00": Phase="Pending", Reason="", readiness=false. Elapsed: 10.967666ms
Nov 15 06:54:29.856: INFO: The phase of Pod pod-secrets-ba381697-e90d-4d4f-87e1-068bb6af7b00 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:54:31.883: INFO: Pod "pod-secrets-ba381697-e90d-4d4f-87e1-068bb6af7b00": Phase="Running", Reason="", readiness=true. Elapsed: 2.038071062s
Nov 15 06:54:31.883: INFO: The phase of Pod pod-secrets-ba381697-e90d-4d4f-87e1-068bb6af7b00 is Running (Ready = true)
Nov 15 06:54:31.883: INFO: Pod "pod-secrets-ba381697-e90d-4d4f-87e1-068bb6af7b00" satisfied condition "running and ready"
STEP: Cleaning up the secret 11/15/23 06:54:31.924
STEP: Cleaning up the configmap 11/15/23 06:54:31.943
STEP: Cleaning up the pod 11/15/23 06:54:32.008
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 06:54:32.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-3414" for this suite. 11/15/23 06:54:32.079
------------------------------
â€¢ [2.435 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:54:29.696
    Nov 15 06:54:29.696: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename emptydir-wrapper 11/15/23 06:54:29.697
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:54:29.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:54:29.771
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Nov 15 06:54:29.845: INFO: Waiting up to 5m0s for pod "pod-secrets-ba381697-e90d-4d4f-87e1-068bb6af7b00" in namespace "emptydir-wrapper-3414" to be "running and ready"
    Nov 15 06:54:29.856: INFO: Pod "pod-secrets-ba381697-e90d-4d4f-87e1-068bb6af7b00": Phase="Pending", Reason="", readiness=false. Elapsed: 10.967666ms
    Nov 15 06:54:29.856: INFO: The phase of Pod pod-secrets-ba381697-e90d-4d4f-87e1-068bb6af7b00 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:54:31.883: INFO: Pod "pod-secrets-ba381697-e90d-4d4f-87e1-068bb6af7b00": Phase="Running", Reason="", readiness=true. Elapsed: 2.038071062s
    Nov 15 06:54:31.883: INFO: The phase of Pod pod-secrets-ba381697-e90d-4d4f-87e1-068bb6af7b00 is Running (Ready = true)
    Nov 15 06:54:31.883: INFO: Pod "pod-secrets-ba381697-e90d-4d4f-87e1-068bb6af7b00" satisfied condition "running and ready"
    STEP: Cleaning up the secret 11/15/23 06:54:31.924
    STEP: Cleaning up the configmap 11/15/23 06:54:31.943
    STEP: Cleaning up the pod 11/15/23 06:54:32.008
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:54:32.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-3414" for this suite. 11/15/23 06:54:32.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:54:32.131
Nov 15 06:54:32.131: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename crd-watch 11/15/23 06:54:32.132
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:54:32.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:54:32.203
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Nov 15 06:54:32.213: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Creating first CR  11/15/23 06:54:34.821
Nov 15 06:54:34.834: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T06:54:34Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T06:54:34Z]] name:name1 resourceVersion:104303 uid:83fa1f7c-f1ba-4733-ac1b-f9863e733abf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 11/15/23 06:54:44.836
Nov 15 06:54:44.851: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T06:54:44Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T06:54:44Z]] name:name2 resourceVersion:104411 uid:30b94a6b-e3af-4d20-bed3-9cb325296946] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 11/15/23 06:54:54.853
Nov 15 06:54:54.869: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T06:54:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T06:54:54Z]] name:name1 resourceVersion:104461 uid:83fa1f7c-f1ba-4733-ac1b-f9863e733abf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 11/15/23 06:55:04.87
Nov 15 06:55:04.885: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T06:54:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T06:55:04Z]] name:name2 resourceVersion:104512 uid:30b94a6b-e3af-4d20-bed3-9cb325296946] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 11/15/23 06:55:14.887
Nov 15 06:55:14.908: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T06:54:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T06:54:54Z]] name:name1 resourceVersion:104556 uid:83fa1f7c-f1ba-4733-ac1b-f9863e733abf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 11/15/23 06:55:24.91
Nov 15 06:55:24.930: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T06:54:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T06:55:04Z]] name:name2 resourceVersion:104605 uid:30b94a6b-e3af-4d20-bed3-9cb325296946] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:55:35.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-3165" for this suite. 11/15/23 06:55:35.483
------------------------------
â€¢ [SLOW TEST] [63.371 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:54:32.131
    Nov 15 06:54:32.131: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename crd-watch 11/15/23 06:54:32.132
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:54:32.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:54:32.203
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Nov 15 06:54:32.213: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Creating first CR  11/15/23 06:54:34.821
    Nov 15 06:54:34.834: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T06:54:34Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T06:54:34Z]] name:name1 resourceVersion:104303 uid:83fa1f7c-f1ba-4733-ac1b-f9863e733abf] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 11/15/23 06:54:44.836
    Nov 15 06:54:44.851: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T06:54:44Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T06:54:44Z]] name:name2 resourceVersion:104411 uid:30b94a6b-e3af-4d20-bed3-9cb325296946] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 11/15/23 06:54:54.853
    Nov 15 06:54:54.869: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T06:54:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T06:54:54Z]] name:name1 resourceVersion:104461 uid:83fa1f7c-f1ba-4733-ac1b-f9863e733abf] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 11/15/23 06:55:04.87
    Nov 15 06:55:04.885: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T06:54:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T06:55:04Z]] name:name2 resourceVersion:104512 uid:30b94a6b-e3af-4d20-bed3-9cb325296946] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 11/15/23 06:55:14.887
    Nov 15 06:55:14.908: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T06:54:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T06:54:54Z]] name:name1 resourceVersion:104556 uid:83fa1f7c-f1ba-4733-ac1b-f9863e733abf] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 11/15/23 06:55:24.91
    Nov 15 06:55:24.930: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T06:54:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T06:55:04Z]] name:name2 resourceVersion:104605 uid:30b94a6b-e3af-4d20-bed3-9cb325296946] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:55:35.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-3165" for this suite. 11/15/23 06:55:35.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:55:35.505
Nov 15 06:55:35.505: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename resourcequota 11/15/23 06:55:35.506
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:55:35.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:55:35.571
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 11/15/23 06:55:35.581
STEP: Counting existing ResourceQuota 11/15/23 06:55:41.611
STEP: Creating a ResourceQuota 11/15/23 06:55:46.623
STEP: Ensuring resource quota status is calculated 11/15/23 06:55:46.638
STEP: Creating a Secret 11/15/23 06:55:48.652
STEP: Ensuring resource quota status captures secret creation 11/15/23 06:55:48.679
STEP: Deleting a secret 11/15/23 06:55:50.694
STEP: Ensuring resource quota status released usage 11/15/23 06:55:50.713
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 06:55:52.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2205" for this suite. 11/15/23 06:55:52.747
------------------------------
â€¢ [SLOW TEST] [17.265 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:55:35.505
    Nov 15 06:55:35.505: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename resourcequota 11/15/23 06:55:35.506
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:55:35.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:55:35.571
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 11/15/23 06:55:35.581
    STEP: Counting existing ResourceQuota 11/15/23 06:55:41.611
    STEP: Creating a ResourceQuota 11/15/23 06:55:46.623
    STEP: Ensuring resource quota status is calculated 11/15/23 06:55:46.638
    STEP: Creating a Secret 11/15/23 06:55:48.652
    STEP: Ensuring resource quota status captures secret creation 11/15/23 06:55:48.679
    STEP: Deleting a secret 11/15/23 06:55:50.694
    STEP: Ensuring resource quota status released usage 11/15/23 06:55:50.713
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:55:52.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2205" for this suite. 11/15/23 06:55:52.747
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:55:52.772
Nov 15 06:55:52.772: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename statefulset 11/15/23 06:55:52.773
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:55:52.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:55:52.858
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7064 11/15/23 06:55:52.868
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-7064 11/15/23 06:55:52.882
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7064 11/15/23 06:55:52.904
Nov 15 06:55:52.919: INFO: Found 0 stateful pods, waiting for 1
Nov 15 06:56:02.932: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 11/15/23 06:56:02.932
Nov 15 06:56:02.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7064 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 06:56:03.248: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 06:56:03.248: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 06:56:03.248: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 15 06:56:03.260: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 15 06:56:13.272: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 06:56:13.272: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 06:56:13.350: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Nov 15 06:56:13.350: INFO: ss-0  10.72.152.86  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:55:52 +0000 UTC  }]
Nov 15 06:56:13.350: INFO: 
Nov 15 06:56:13.350: INFO: StatefulSet ss has not reached scale 3, at 1
Nov 15 06:56:14.363: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.984817738s
Nov 15 06:56:15.378: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.972507738s
Nov 15 06:56:16.392: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.956843457s
Nov 15 06:56:17.404: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.944022593s
Nov 15 06:56:18.424: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.929118126s
Nov 15 06:56:19.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.911940465s
Nov 15 06:56:20.452: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.897546052s
Nov 15 06:56:21.471: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.883414833s
Nov 15 06:56:22.484: INFO: Verifying statefulset ss doesn't scale past 3 for another 865.094437ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7064 11/15/23 06:56:23.484
Nov 15 06:56:23.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7064 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 15 06:56:23.784: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 15 06:56:23.784: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 15 06:56:23.784: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 15 06:56:23.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7064 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 15 06:56:24.138: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 15 06:56:24.138: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 15 06:56:24.138: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 15 06:56:24.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7064 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 15 06:56:24.442: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 15 06:56:24.442: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 15 06:56:24.442: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 15 06:56:24.454: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 06:56:24.454: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 06:56:24.454: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 11/15/23 06:56:24.454
Nov 15 06:56:24.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7064 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 06:56:24.833: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 06:56:24.833: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 06:56:24.833: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 15 06:56:24.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7064 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 06:56:25.172: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 06:56:25.172: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 06:56:25.172: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 15 06:56:25.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7064 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 06:56:25.440: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 06:56:25.440: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 06:56:25.440: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 15 06:56:25.440: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 06:56:25.460: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Nov 15 06:56:35.492: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 06:56:35.492: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 06:56:35.492: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 06:56:35.541: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Nov 15 06:56:35.541: INFO: ss-0  10.72.152.86  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:55:52 +0000 UTC  }]
Nov 15 06:56:35.541: INFO: ss-1  10.72.152.88  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:13 +0000 UTC  }]
Nov 15 06:56:35.541: INFO: ss-2  10.72.152.81  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:13 +0000 UTC  }]
Nov 15 06:56:35.541: INFO: 
Nov 15 06:56:35.541: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 15 06:56:36.558: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Nov 15 06:56:36.558: INFO: ss-0  10.72.152.86  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:55:52 +0000 UTC  }]
Nov 15 06:56:36.558: INFO: ss-1  10.72.152.88  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:13 +0000 UTC  }]
Nov 15 06:56:36.558: INFO: ss-2  10.72.152.81  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:13 +0000 UTC  }]
Nov 15 06:56:36.558: INFO: 
Nov 15 06:56:36.558: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 15 06:56:37.570: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.967954788s
Nov 15 06:56:38.584: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.955872681s
Nov 15 06:56:39.597: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.942677122s
Nov 15 06:56:40.609: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.928817381s
Nov 15 06:56:41.633: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.905895494s
Nov 15 06:56:42.645: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.893703015s
Nov 15 06:56:43.658: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.881561693s
Nov 15 06:56:44.670: INFO: Verifying statefulset ss doesn't scale past 0 for another 868.540201ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7064 11/15/23 06:56:45.67
Nov 15 06:56:45.683: INFO: Scaling statefulset ss to 0
Nov 15 06:56:45.727: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 15 06:56:45.742: INFO: Deleting all statefulset in ns statefulset-7064
Nov 15 06:56:45.758: INFO: Scaling statefulset ss to 0
Nov 15 06:56:45.802: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 06:56:45.817: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 15 06:56:45.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7064" for this suite. 11/15/23 06:56:45.898
------------------------------
â€¢ [SLOW TEST] [53.146 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:55:52.772
    Nov 15 06:55:52.772: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename statefulset 11/15/23 06:55:52.773
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:55:52.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:55:52.858
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7064 11/15/23 06:55:52.868
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-7064 11/15/23 06:55:52.882
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7064 11/15/23 06:55:52.904
    Nov 15 06:55:52.919: INFO: Found 0 stateful pods, waiting for 1
    Nov 15 06:56:02.932: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 11/15/23 06:56:02.932
    Nov 15 06:56:02.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7064 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 06:56:03.248: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 06:56:03.248: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 06:56:03.248: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 15 06:56:03.260: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Nov 15 06:56:13.272: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Nov 15 06:56:13.272: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 06:56:13.350: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Nov 15 06:56:13.350: INFO: ss-0  10.72.152.86  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:55:52 +0000 UTC  }]
    Nov 15 06:56:13.350: INFO: 
    Nov 15 06:56:13.350: INFO: StatefulSet ss has not reached scale 3, at 1
    Nov 15 06:56:14.363: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.984817738s
    Nov 15 06:56:15.378: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.972507738s
    Nov 15 06:56:16.392: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.956843457s
    Nov 15 06:56:17.404: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.944022593s
    Nov 15 06:56:18.424: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.929118126s
    Nov 15 06:56:19.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.911940465s
    Nov 15 06:56:20.452: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.897546052s
    Nov 15 06:56:21.471: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.883414833s
    Nov 15 06:56:22.484: INFO: Verifying statefulset ss doesn't scale past 3 for another 865.094437ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7064 11/15/23 06:56:23.484
    Nov 15 06:56:23.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7064 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 15 06:56:23.784: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 15 06:56:23.784: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 15 06:56:23.784: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 15 06:56:23.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7064 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 15 06:56:24.138: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Nov 15 06:56:24.138: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 15 06:56:24.138: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 15 06:56:24.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7064 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 15 06:56:24.442: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Nov 15 06:56:24.442: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 15 06:56:24.442: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 15 06:56:24.454: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 06:56:24.454: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 06:56:24.454: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 11/15/23 06:56:24.454
    Nov 15 06:56:24.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7064 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 06:56:24.833: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 06:56:24.833: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 06:56:24.833: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 15 06:56:24.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7064 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 06:56:25.172: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 06:56:25.172: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 06:56:25.172: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 15 06:56:25.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7064 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 06:56:25.440: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 06:56:25.440: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 06:56:25.440: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 15 06:56:25.440: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 06:56:25.460: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Nov 15 06:56:35.492: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Nov 15 06:56:35.492: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Nov 15 06:56:35.492: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Nov 15 06:56:35.541: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Nov 15 06:56:35.541: INFO: ss-0  10.72.152.86  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:55:52 +0000 UTC  }]
    Nov 15 06:56:35.541: INFO: ss-1  10.72.152.88  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:13 +0000 UTC  }]
    Nov 15 06:56:35.541: INFO: ss-2  10.72.152.81  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:13 +0000 UTC  }]
    Nov 15 06:56:35.541: INFO: 
    Nov 15 06:56:35.541: INFO: StatefulSet ss has not reached scale 0, at 3
    Nov 15 06:56:36.558: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Nov 15 06:56:36.558: INFO: ss-0  10.72.152.86  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:55:52 +0000 UTC  }]
    Nov 15 06:56:36.558: INFO: ss-1  10.72.152.88  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:13 +0000 UTC  }]
    Nov 15 06:56:36.558: INFO: ss-2  10.72.152.81  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 06:56:13 +0000 UTC  }]
    Nov 15 06:56:36.558: INFO: 
    Nov 15 06:56:36.558: INFO: StatefulSet ss has not reached scale 0, at 3
    Nov 15 06:56:37.570: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.967954788s
    Nov 15 06:56:38.584: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.955872681s
    Nov 15 06:56:39.597: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.942677122s
    Nov 15 06:56:40.609: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.928817381s
    Nov 15 06:56:41.633: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.905895494s
    Nov 15 06:56:42.645: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.893703015s
    Nov 15 06:56:43.658: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.881561693s
    Nov 15 06:56:44.670: INFO: Verifying statefulset ss doesn't scale past 0 for another 868.540201ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7064 11/15/23 06:56:45.67
    Nov 15 06:56:45.683: INFO: Scaling statefulset ss to 0
    Nov 15 06:56:45.727: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 15 06:56:45.742: INFO: Deleting all statefulset in ns statefulset-7064
    Nov 15 06:56:45.758: INFO: Scaling statefulset ss to 0
    Nov 15 06:56:45.802: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 06:56:45.817: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:56:45.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7064" for this suite. 11/15/23 06:56:45.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:56:45.919
Nov 15 06:56:45.919: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename pods 11/15/23 06:56:45.92
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:56:45.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:56:45.995
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Nov 15 06:56:46.035: INFO: Waiting up to 5m0s for pod "server-envvars-b46cc45d-a98d-40c4-8c34-ed8901235ff9" in namespace "pods-3387" to be "running and ready"
Nov 15 06:56:46.048: INFO: Pod "server-envvars-b46cc45d-a98d-40c4-8c34-ed8901235ff9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.473693ms
Nov 15 06:56:46.048: INFO: The phase of Pod server-envvars-b46cc45d-a98d-40c4-8c34-ed8901235ff9 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:56:48.060: INFO: Pod "server-envvars-b46cc45d-a98d-40c4-8c34-ed8901235ff9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025878272s
Nov 15 06:56:48.061: INFO: The phase of Pod server-envvars-b46cc45d-a98d-40c4-8c34-ed8901235ff9 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 06:56:50.075: INFO: Pod "server-envvars-b46cc45d-a98d-40c4-8c34-ed8901235ff9": Phase="Running", Reason="", readiness=true. Elapsed: 4.040117977s
Nov 15 06:56:50.075: INFO: The phase of Pod server-envvars-b46cc45d-a98d-40c4-8c34-ed8901235ff9 is Running (Ready = true)
Nov 15 06:56:50.075: INFO: Pod "server-envvars-b46cc45d-a98d-40c4-8c34-ed8901235ff9" satisfied condition "running and ready"
Nov 15 06:56:50.186: INFO: Waiting up to 5m0s for pod "client-envvars-bc96c286-c050-4b59-9cd6-2934cec2a744" in namespace "pods-3387" to be "Succeeded or Failed"
Nov 15 06:56:50.199: INFO: Pod "client-envvars-bc96c286-c050-4b59-9cd6-2934cec2a744": Phase="Pending", Reason="", readiness=false. Elapsed: 12.182461ms
Nov 15 06:56:52.211: INFO: Pod "client-envvars-bc96c286-c050-4b59-9cd6-2934cec2a744": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024858037s
Nov 15 06:56:54.211: INFO: Pod "client-envvars-bc96c286-c050-4b59-9cd6-2934cec2a744": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024819477s
STEP: Saw pod success 11/15/23 06:56:54.211
Nov 15 06:56:54.212: INFO: Pod "client-envvars-bc96c286-c050-4b59-9cd6-2934cec2a744" satisfied condition "Succeeded or Failed"
Nov 15 06:56:54.222: INFO: Trying to get logs from node 10.72.152.81 pod client-envvars-bc96c286-c050-4b59-9cd6-2934cec2a744 container env3cont: <nil>
STEP: delete the pod 11/15/23 06:56:54.298
Nov 15 06:56:54.330: INFO: Waiting for pod client-envvars-bc96c286-c050-4b59-9cd6-2934cec2a744 to disappear
Nov 15 06:56:54.341: INFO: Pod client-envvars-bc96c286-c050-4b59-9cd6-2934cec2a744 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 06:56:54.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3387" for this suite. 11/15/23 06:56:54.365
------------------------------
â€¢ [SLOW TEST] [8.476 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:56:45.919
    Nov 15 06:56:45.919: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename pods 11/15/23 06:56:45.92
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:56:45.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:56:45.995
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Nov 15 06:56:46.035: INFO: Waiting up to 5m0s for pod "server-envvars-b46cc45d-a98d-40c4-8c34-ed8901235ff9" in namespace "pods-3387" to be "running and ready"
    Nov 15 06:56:46.048: INFO: Pod "server-envvars-b46cc45d-a98d-40c4-8c34-ed8901235ff9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.473693ms
    Nov 15 06:56:46.048: INFO: The phase of Pod server-envvars-b46cc45d-a98d-40c4-8c34-ed8901235ff9 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:56:48.060: INFO: Pod "server-envvars-b46cc45d-a98d-40c4-8c34-ed8901235ff9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025878272s
    Nov 15 06:56:48.061: INFO: The phase of Pod server-envvars-b46cc45d-a98d-40c4-8c34-ed8901235ff9 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 06:56:50.075: INFO: Pod "server-envvars-b46cc45d-a98d-40c4-8c34-ed8901235ff9": Phase="Running", Reason="", readiness=true. Elapsed: 4.040117977s
    Nov 15 06:56:50.075: INFO: The phase of Pod server-envvars-b46cc45d-a98d-40c4-8c34-ed8901235ff9 is Running (Ready = true)
    Nov 15 06:56:50.075: INFO: Pod "server-envvars-b46cc45d-a98d-40c4-8c34-ed8901235ff9" satisfied condition "running and ready"
    Nov 15 06:56:50.186: INFO: Waiting up to 5m0s for pod "client-envvars-bc96c286-c050-4b59-9cd6-2934cec2a744" in namespace "pods-3387" to be "Succeeded or Failed"
    Nov 15 06:56:50.199: INFO: Pod "client-envvars-bc96c286-c050-4b59-9cd6-2934cec2a744": Phase="Pending", Reason="", readiness=false. Elapsed: 12.182461ms
    Nov 15 06:56:52.211: INFO: Pod "client-envvars-bc96c286-c050-4b59-9cd6-2934cec2a744": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024858037s
    Nov 15 06:56:54.211: INFO: Pod "client-envvars-bc96c286-c050-4b59-9cd6-2934cec2a744": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024819477s
    STEP: Saw pod success 11/15/23 06:56:54.211
    Nov 15 06:56:54.212: INFO: Pod "client-envvars-bc96c286-c050-4b59-9cd6-2934cec2a744" satisfied condition "Succeeded or Failed"
    Nov 15 06:56:54.222: INFO: Trying to get logs from node 10.72.152.81 pod client-envvars-bc96c286-c050-4b59-9cd6-2934cec2a744 container env3cont: <nil>
    STEP: delete the pod 11/15/23 06:56:54.298
    Nov 15 06:56:54.330: INFO: Waiting for pod client-envvars-bc96c286-c050-4b59-9cd6-2934cec2a744 to disappear
    Nov 15 06:56:54.341: INFO: Pod client-envvars-bc96c286-c050-4b59-9cd6-2934cec2a744 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:56:54.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3387" for this suite. 11/15/23 06:56:54.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:56:54.396
Nov 15 06:56:54.396: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename webhook 11/15/23 06:56:54.397
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:56:54.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:56:54.53
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 06:56:54.586
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:56:55.022
STEP: Deploying the webhook pod 11/15/23 06:56:55.047
STEP: Wait for the deployment to be ready 11/15/23 06:56:55.077
Nov 15 06:56:55.107: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 06:56:57.147: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 6, 56, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 56, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 56, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 56, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 06:56:59.191
STEP: Verifying the service has paired with the endpoint 11/15/23 06:56:59.254
Nov 15 06:57:00.258: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 11/15/23 06:57:00.289
STEP: create a namespace for the webhook 11/15/23 06:57:00.415
STEP: create a configmap should be unconditionally rejected by the webhook 11/15/23 06:57:00.451
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:57:00.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6082" for this suite. 11/15/23 06:57:00.743
STEP: Destroying namespace "webhook-6082-markers" for this suite. 11/15/23 06:57:00.773
------------------------------
â€¢ [SLOW TEST] [6.400 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:56:54.396
    Nov 15 06:56:54.396: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename webhook 11/15/23 06:56:54.397
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:56:54.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:56:54.53
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 06:56:54.586
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 06:56:55.022
    STEP: Deploying the webhook pod 11/15/23 06:56:55.047
    STEP: Wait for the deployment to be ready 11/15/23 06:56:55.077
    Nov 15 06:56:55.107: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 06:56:57.147: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 6, 56, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 56, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 6, 56, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 6, 56, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 06:56:59.191
    STEP: Verifying the service has paired with the endpoint 11/15/23 06:56:59.254
    Nov 15 06:57:00.258: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 11/15/23 06:57:00.289
    STEP: create a namespace for the webhook 11/15/23 06:57:00.415
    STEP: create a configmap should be unconditionally rejected by the webhook 11/15/23 06:57:00.451
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:57:00.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6082" for this suite. 11/15/23 06:57:00.743
    STEP: Destroying namespace "webhook-6082-markers" for this suite. 11/15/23 06:57:00.773
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:57:00.796
Nov 15 06:57:00.796: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename proxy 11/15/23 06:57:00.797
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:57:00.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:57:00.875
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 11/15/23 06:57:00.927
STEP: creating replication controller proxy-service-xmgpc in namespace proxy-6595 11/15/23 06:57:00.927
I1115 06:57:00.956618      22 runners.go:193] Created replication controller with name: proxy-service-xmgpc, namespace: proxy-6595, replica count: 1
I1115 06:57:02.008604      22 runners.go:193] proxy-service-xmgpc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1115 06:57:03.009342      22 runners.go:193] proxy-service-xmgpc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1115 06:57:04.010518      22 runners.go:193] proxy-service-xmgpc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 06:57:04.023: INFO: setup took 3.13348229s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 11/15/23 06:57:04.023
Nov 15 06:57:04.057: INFO: (0) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 33.651581ms)
Nov 15 06:57:04.059: INFO: (0) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 35.228781ms)
Nov 15 06:57:04.059: INFO: (0) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 35.58639ms)
Nov 15 06:57:04.059: INFO: (0) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 36.01765ms)
Nov 15 06:57:04.059: INFO: (0) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 36.35712ms)
Nov 15 06:57:04.061: INFO: (0) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 37.327089ms)
Nov 15 06:57:04.065: INFO: (0) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 41.774856ms)
Nov 15 06:57:04.066: INFO: (0) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 42.604355ms)
Nov 15 06:57:04.066: INFO: (0) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 42.580855ms)
Nov 15 06:57:04.066: INFO: (0) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 43.288667ms)
Nov 15 06:57:04.066: INFO: (0) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 43.455459ms)
Nov 15 06:57:04.075: INFO: (0) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 52.245155ms)
Nov 15 06:57:04.075: INFO: (0) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 52.400886ms)
Nov 15 06:57:04.078: INFO: (0) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 54.200965ms)
Nov 15 06:57:04.078: INFO: (0) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 54.436878ms)
Nov 15 06:57:04.087: INFO: (0) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 63.115223ms)
Nov 15 06:57:04.103: INFO: (1) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 16.78148ms)
Nov 15 06:57:04.108: INFO: (1) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 21.000554ms)
Nov 15 06:57:04.110: INFO: (1) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 23.429684ms)
Nov 15 06:57:04.110: INFO: (1) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 23.696544ms)
Nov 15 06:57:04.111: INFO: (1) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 24.418247ms)
Nov 15 06:57:04.111: INFO: (1) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 24.450647ms)
Nov 15 06:57:04.111: INFO: (1) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 24.563734ms)
Nov 15 06:57:04.112: INFO: (1) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 24.856652ms)
Nov 15 06:57:04.112: INFO: (1) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 24.967614ms)
Nov 15 06:57:04.112: INFO: (1) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 25.199435ms)
Nov 15 06:57:04.116: INFO: (1) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 29.513269ms)
Nov 15 06:57:04.118: INFO: (1) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 31.706353ms)
Nov 15 06:57:04.119: INFO: (1) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 31.873998ms)
Nov 15 06:57:04.119: INFO: (1) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 32.39343ms)
Nov 15 06:57:04.119: INFO: (1) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 32.702395ms)
Nov 15 06:57:04.120: INFO: (1) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 33.008365ms)
Nov 15 06:57:04.138: INFO: (2) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 18.216149ms)
Nov 15 06:57:04.138: INFO: (2) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 18.107519ms)
Nov 15 06:57:04.140: INFO: (2) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 19.006322ms)
Nov 15 06:57:04.140: INFO: (2) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 19.497697ms)
Nov 15 06:57:04.141: INFO: (2) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 20.953487ms)
Nov 15 06:57:04.141: INFO: (2) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 20.165625ms)
Nov 15 06:57:04.141: INFO: (2) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 21.175727ms)
Nov 15 06:57:04.141: INFO: (2) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 20.594537ms)
Nov 15 06:57:04.142: INFO: (2) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 21.123348ms)
Nov 15 06:57:04.142: INFO: (2) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 20.688942ms)
Nov 15 06:57:04.147: INFO: (2) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 26.164606ms)
Nov 15 06:57:04.147: INFO: (2) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 26.208414ms)
Nov 15 06:57:04.155: INFO: (2) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 34.501483ms)
Nov 15 06:57:04.156: INFO: (2) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 35.052251ms)
Nov 15 06:57:04.156: INFO: (2) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 35.609856ms)
Nov 15 06:57:04.156: INFO: (2) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 35.558154ms)
Nov 15 06:57:04.176: INFO: (3) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 20.174197ms)
Nov 15 06:57:04.178: INFO: (3) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 22.090151ms)
Nov 15 06:57:04.179: INFO: (3) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 22.589693ms)
Nov 15 06:57:04.179: INFO: (3) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 22.567229ms)
Nov 15 06:57:04.179: INFO: (3) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 22.597403ms)
Nov 15 06:57:04.179: INFO: (3) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 22.911342ms)
Nov 15 06:57:04.180: INFO: (3) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 23.319561ms)
Nov 15 06:57:04.180: INFO: (3) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 23.396646ms)
Nov 15 06:57:04.180: INFO: (3) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 23.450692ms)
Nov 15 06:57:04.180: INFO: (3) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 23.481669ms)
Nov 15 06:57:04.185: INFO: (3) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 28.679107ms)
Nov 15 06:57:04.187: INFO: (3) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 30.449926ms)
Nov 15 06:57:04.188: INFO: (3) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 31.210531ms)
Nov 15 06:57:04.188: INFO: (3) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 31.221399ms)
Nov 15 06:57:04.188: INFO: (3) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 32.19547ms)
Nov 15 06:57:04.189: INFO: (3) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 32.110088ms)
Nov 15 06:57:04.206: INFO: (4) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 17.649114ms)
Nov 15 06:57:04.211: INFO: (4) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 21.93686ms)
Nov 15 06:57:04.211: INFO: (4) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 22.121059ms)
Nov 15 06:57:04.211: INFO: (4) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 22.172693ms)
Nov 15 06:57:04.212: INFO: (4) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 22.931221ms)
Nov 15 06:57:04.212: INFO: (4) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 22.600761ms)
Nov 15 06:57:04.212: INFO: (4) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 23.09164ms)
Nov 15 06:57:04.212: INFO: (4) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 23.421433ms)
Nov 15 06:57:04.212: INFO: (4) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 23.120666ms)
Nov 15 06:57:04.213: INFO: (4) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 23.658303ms)
Nov 15 06:57:04.216: INFO: (4) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 27.225243ms)
Nov 15 06:57:04.219: INFO: (4) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 30.416544ms)
Nov 15 06:57:04.220: INFO: (4) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 30.702872ms)
Nov 15 06:57:04.220: INFO: (4) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 30.877871ms)
Nov 15 06:57:04.220: INFO: (4) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 31.03747ms)
Nov 15 06:57:04.220: INFO: (4) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 31.286069ms)
Nov 15 06:57:04.241: INFO: (5) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 20.519355ms)
Nov 15 06:57:04.245: INFO: (5) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 24.308641ms)
Nov 15 06:57:04.245: INFO: (5) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 25.046707ms)
Nov 15 06:57:04.245: INFO: (5) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 24.965374ms)
Nov 15 06:57:04.246: INFO: (5) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 25.802963ms)
Nov 15 06:57:04.246: INFO: (5) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 25.893409ms)
Nov 15 06:57:04.247: INFO: (5) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 26.203531ms)
Nov 15 06:57:04.247: INFO: (5) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 26.196144ms)
Nov 15 06:57:04.247: INFO: (5) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 26.223762ms)
Nov 15 06:57:04.247: INFO: (5) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 26.455384ms)
Nov 15 06:57:04.252: INFO: (5) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 31.945864ms)
Nov 15 06:57:04.254: INFO: (5) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 33.425253ms)
Nov 15 06:57:04.254: INFO: (5) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 33.675973ms)
Nov 15 06:57:04.254: INFO: (5) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 33.559424ms)
Nov 15 06:57:04.254: INFO: (5) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 33.662131ms)
Nov 15 06:57:04.254: INFO: (5) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 33.733769ms)
Nov 15 06:57:04.272: INFO: (6) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 18.198745ms)
Nov 15 06:57:04.276: INFO: (6) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 21.121205ms)
Nov 15 06:57:04.276: INFO: (6) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 21.553363ms)
Nov 15 06:57:04.277: INFO: (6) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 22.169809ms)
Nov 15 06:57:04.277: INFO: (6) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 22.334553ms)
Nov 15 06:57:04.278: INFO: (6) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 22.700497ms)
Nov 15 06:57:04.278: INFO: (6) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 22.990076ms)
Nov 15 06:57:04.278: INFO: (6) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 23.141671ms)
Nov 15 06:57:04.278: INFO: (6) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 23.452752ms)
Nov 15 06:57:04.279: INFO: (6) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 23.955254ms)
Nov 15 06:57:04.284: INFO: (6) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 28.990055ms)
Nov 15 06:57:04.284: INFO: (6) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 28.996053ms)
Nov 15 06:57:04.285: INFO: (6) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 30.443148ms)
Nov 15 06:57:04.285: INFO: (6) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 31.150776ms)
Nov 15 06:57:04.286: INFO: (6) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 30.783923ms)
Nov 15 06:57:04.288: INFO: (6) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 33.367446ms)
Nov 15 06:57:04.306: INFO: (7) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 17.05404ms)
Nov 15 06:57:04.306: INFO: (7) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 17.628228ms)
Nov 15 06:57:04.312: INFO: (7) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 23.786262ms)
Nov 15 06:57:04.313: INFO: (7) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 24.092522ms)
Nov 15 06:57:04.313: INFO: (7) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 24.877859ms)
Nov 15 06:57:04.313: INFO: (7) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 24.655856ms)
Nov 15 06:57:04.313: INFO: (7) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 24.000226ms)
Nov 15 06:57:04.313: INFO: (7) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 24.94983ms)
Nov 15 06:57:04.313: INFO: (7) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 24.359793ms)
Nov 15 06:57:04.313: INFO: (7) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 24.355786ms)
Nov 15 06:57:04.318: INFO: (7) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 29.24264ms)
Nov 15 06:57:04.319: INFO: (7) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 30.357178ms)
Nov 15 06:57:04.319: INFO: (7) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 31.295445ms)
Nov 15 06:57:04.319: INFO: (7) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 30.282308ms)
Nov 15 06:57:04.320: INFO: (7) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 30.712991ms)
Nov 15 06:57:04.320: INFO: (7) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 31.20356ms)
Nov 15 06:57:04.338: INFO: (8) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 18.548137ms)
Nov 15 06:57:04.343: INFO: (8) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 23.313107ms)
Nov 15 06:57:04.343: INFO: (8) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 23.371389ms)
Nov 15 06:57:04.344: INFO: (8) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 23.490225ms)
Nov 15 06:57:04.344: INFO: (8) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 23.379181ms)
Nov 15 06:57:04.344: INFO: (8) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 23.685976ms)
Nov 15 06:57:04.344: INFO: (8) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 24.034716ms)
Nov 15 06:57:04.344: INFO: (8) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 24.016381ms)
Nov 15 06:57:04.344: INFO: (8) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 24.048108ms)
Nov 15 06:57:04.344: INFO: (8) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 24.055089ms)
Nov 15 06:57:04.350: INFO: (8) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 29.951351ms)
Nov 15 06:57:04.351: INFO: (8) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 31.107027ms)
Nov 15 06:57:04.354: INFO: (8) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 34.325116ms)
Nov 15 06:57:04.354: INFO: (8) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 33.778156ms)
Nov 15 06:57:04.355: INFO: (8) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 34.494342ms)
Nov 15 06:57:04.355: INFO: (8) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 34.364418ms)
Nov 15 06:57:04.375: INFO: (9) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 20.112044ms)
Nov 15 06:57:04.383: INFO: (9) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 28.452922ms)
Nov 15 06:57:04.383: INFO: (9) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 28.552382ms)
Nov 15 06:57:04.383: INFO: (9) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 27.796002ms)
Nov 15 06:57:04.383: INFO: (9) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 28.057196ms)
Nov 15 06:57:04.383: INFO: (9) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 28.466512ms)
Nov 15 06:57:04.384: INFO: (9) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 28.455868ms)
Nov 15 06:57:04.384: INFO: (9) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 28.719782ms)
Nov 15 06:57:04.384: INFO: (9) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 28.74305ms)
Nov 15 06:57:04.384: INFO: (9) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 29.138274ms)
Nov 15 06:57:04.391: INFO: (9) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 35.886014ms)
Nov 15 06:57:04.391: INFO: (9) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 36.039655ms)
Nov 15 06:57:04.391: INFO: (9) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 36.164516ms)
Nov 15 06:57:04.391: INFO: (9) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 36.185792ms)
Nov 15 06:57:04.391: INFO: (9) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 36.375933ms)
Nov 15 06:57:04.391: INFO: (9) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 36.756841ms)
Nov 15 06:57:04.411: INFO: (10) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 18.719697ms)
Nov 15 06:57:04.414: INFO: (10) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 22.275581ms)
Nov 15 06:57:04.414: INFO: (10) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 21.768392ms)
Nov 15 06:57:04.414: INFO: (10) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 21.999464ms)
Nov 15 06:57:04.415: INFO: (10) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 22.671156ms)
Nov 15 06:57:04.415: INFO: (10) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 23.011406ms)
Nov 15 06:57:04.415: INFO: (10) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 23.028614ms)
Nov 15 06:57:04.415: INFO: (10) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 22.825279ms)
Nov 15 06:57:04.415: INFO: (10) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 22.778161ms)
Nov 15 06:57:04.415: INFO: (10) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 23.077599ms)
Nov 15 06:57:04.432: INFO: (10) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 39.959637ms)
Nov 15 06:57:04.432: INFO: (10) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 39.640383ms)
Nov 15 06:57:04.432: INFO: (10) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 39.773856ms)
Nov 15 06:57:04.432: INFO: (10) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 39.773856ms)
Nov 15 06:57:04.432: INFO: (10) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 39.506993ms)
Nov 15 06:57:04.432: INFO: (10) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 40.466826ms)
Nov 15 06:57:04.454: INFO: (11) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 20.734864ms)
Nov 15 06:57:04.460: INFO: (11) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 28.497673ms)
Nov 15 06:57:04.461: INFO: (11) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 26.704489ms)
Nov 15 06:57:04.461: INFO: (11) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 27.264272ms)
Nov 15 06:57:04.461: INFO: (11) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 26.864637ms)
Nov 15 06:57:04.461: INFO: (11) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 27.528878ms)
Nov 15 06:57:04.461: INFO: (11) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 27.51876ms)
Nov 15 06:57:04.461: INFO: (11) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 28.424871ms)
Nov 15 06:57:04.461: INFO: (11) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 28.582596ms)
Nov 15 06:57:04.461: INFO: (11) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 27.947065ms)
Nov 15 06:57:04.463: INFO: (11) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 28.937372ms)
Nov 15 06:57:04.468: INFO: (11) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 34.786532ms)
Nov 15 06:57:04.468: INFO: (11) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 34.666769ms)
Nov 15 06:57:04.468: INFO: (11) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 34.772231ms)
Nov 15 06:57:04.471: INFO: (11) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 37.970248ms)
Nov 15 06:57:04.472: INFO: (11) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 38.430622ms)
Nov 15 06:57:04.488: INFO: (12) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 16.524792ms)
Nov 15 06:57:04.491: INFO: (12) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 19.242052ms)
Nov 15 06:57:04.492: INFO: (12) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 20.418522ms)
Nov 15 06:57:04.493: INFO: (12) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 19.93473ms)
Nov 15 06:57:04.493: INFO: (12) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 21.091644ms)
Nov 15 06:57:04.493: INFO: (12) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 20.513152ms)
Nov 15 06:57:04.493: INFO: (12) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 20.947599ms)
Nov 15 06:57:04.493: INFO: (12) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 21.191144ms)
Nov 15 06:57:04.493: INFO: (12) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 20.633407ms)
Nov 15 06:57:04.493: INFO: (12) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 20.717409ms)
Nov 15 06:57:04.500: INFO: (12) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 27.011459ms)
Nov 15 06:57:04.502: INFO: (12) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 29.940958ms)
Nov 15 06:57:04.502: INFO: (12) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 29.766616ms)
Nov 15 06:57:04.502: INFO: (12) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 29.648133ms)
Nov 15 06:57:04.502: INFO: (12) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 30.936996ms)
Nov 15 06:57:04.502: INFO: (12) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 30.037147ms)
Nov 15 06:57:04.523: INFO: (13) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 20.623804ms)
Nov 15 06:57:04.525: INFO: (13) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 21.500105ms)
Nov 15 06:57:04.525: INFO: (13) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 21.702909ms)
Nov 15 06:57:04.525: INFO: (13) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 22.219219ms)
Nov 15 06:57:04.525: INFO: (13) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 22.274173ms)
Nov 15 06:57:04.526: INFO: (13) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 22.779736ms)
Nov 15 06:57:04.526: INFO: (13) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 23.523505ms)
Nov 15 06:57:04.527: INFO: (13) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 23.76313ms)
Nov 15 06:57:04.527: INFO: (13) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 23.513272ms)
Nov 15 06:57:04.527: INFO: (13) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 24.262024ms)
Nov 15 06:57:04.531: INFO: (13) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 28.937666ms)
Nov 15 06:57:04.533: INFO: (13) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 30.521946ms)
Nov 15 06:57:04.534: INFO: (13) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 31.26257ms)
Nov 15 06:57:04.537: INFO: (13) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 34.700226ms)
Nov 15 06:57:04.537: INFO: (13) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 34.49692ms)
Nov 15 06:57:04.537: INFO: (13) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 34.250388ms)
Nov 15 06:57:04.568: INFO: (14) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 29.817833ms)
Nov 15 06:57:04.568: INFO: (14) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 28.92381ms)
Nov 15 06:57:04.568: INFO: (14) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 29.51681ms)
Nov 15 06:57:04.568: INFO: (14) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 30.127154ms)
Nov 15 06:57:04.568: INFO: (14) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 30.144587ms)
Nov 15 06:57:04.568: INFO: (14) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 30.048887ms)
Nov 15 06:57:04.569: INFO: (14) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 30.994829ms)
Nov 15 06:57:04.569: INFO: (14) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 31.007656ms)
Nov 15 06:57:04.569: INFO: (14) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 30.332451ms)
Nov 15 06:57:04.569: INFO: (14) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 30.500923ms)
Nov 15 06:57:04.573: INFO: (14) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 34.523096ms)
Nov 15 06:57:04.575: INFO: (14) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 36.797967ms)
Nov 15 06:57:04.578: INFO: (14) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 40.344441ms)
Nov 15 06:57:04.578: INFO: (14) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 40.916915ms)
Nov 15 06:57:04.579: INFO: (14) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 40.691061ms)
Nov 15 06:57:04.579: INFO: (14) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 40.673126ms)
Nov 15 06:57:04.608: INFO: (15) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 28.067262ms)
Nov 15 06:57:04.608: INFO: (15) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 28.761336ms)
Nov 15 06:57:04.608: INFO: (15) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 28.743031ms)
Nov 15 06:57:04.608: INFO: (15) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 28.989029ms)
Nov 15 06:57:04.609: INFO: (15) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 29.94598ms)
Nov 15 06:57:04.609: INFO: (15) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 29.616813ms)
Nov 15 06:57:04.609: INFO: (15) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 29.90692ms)
Nov 15 06:57:04.609: INFO: (15) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 30.036467ms)
Nov 15 06:57:04.610: INFO: (15) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 30.743923ms)
Nov 15 06:57:04.610: INFO: (15) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 31.0456ms)
Nov 15 06:57:04.617: INFO: (15) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 37.541968ms)
Nov 15 06:57:04.622: INFO: (15) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 42.80043ms)
Nov 15 06:57:04.622: INFO: (15) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 42.972306ms)
Nov 15 06:57:04.623: INFO: (15) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 43.158729ms)
Nov 15 06:57:04.623: INFO: (15) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 43.488882ms)
Nov 15 06:57:04.623: INFO: (15) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 43.556072ms)
Nov 15 06:57:04.641: INFO: (16) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 17.749984ms)
Nov 15 06:57:04.647: INFO: (16) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 23.259728ms)
Nov 15 06:57:04.647: INFO: (16) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 23.470713ms)
Nov 15 06:57:04.648: INFO: (16) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 24.290476ms)
Nov 15 06:57:04.648: INFO: (16) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 24.44566ms)
Nov 15 06:57:04.648: INFO: (16) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 24.661215ms)
Nov 15 06:57:04.650: INFO: (16) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 26.278549ms)
Nov 15 06:57:04.650: INFO: (16) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 26.305052ms)
Nov 15 06:57:04.650: INFO: (16) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 26.486246ms)
Nov 15 06:57:04.650: INFO: (16) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 26.410154ms)
Nov 15 06:57:04.652: INFO: (16) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 27.902786ms)
Nov 15 06:57:04.656: INFO: (16) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 32.445634ms)
Nov 15 06:57:04.656: INFO: (16) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 32.705918ms)
Nov 15 06:57:04.657: INFO: (16) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 33.336294ms)
Nov 15 06:57:04.657: INFO: (16) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 33.283571ms)
Nov 15 06:57:04.657: INFO: (16) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 33.382429ms)
Nov 15 06:57:04.677: INFO: (17) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 19.541532ms)
Nov 15 06:57:04.677: INFO: (17) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 19.702912ms)
Nov 15 06:57:04.677: INFO: (17) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 19.49794ms)
Nov 15 06:57:04.678: INFO: (17) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 21.089964ms)
Nov 15 06:57:04.678: INFO: (17) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 21.058255ms)
Nov 15 06:57:04.679: INFO: (17) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 21.812014ms)
Nov 15 06:57:04.679: INFO: (17) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 21.617183ms)
Nov 15 06:57:04.680: INFO: (17) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 22.308118ms)
Nov 15 06:57:04.680: INFO: (17) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 22.675304ms)
Nov 15 06:57:04.680: INFO: (17) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 22.711865ms)
Nov 15 06:57:04.683: INFO: (17) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 25.915978ms)
Nov 15 06:57:04.701: INFO: (17) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 43.583213ms)
Nov 15 06:57:04.701: INFO: (17) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 43.395554ms)
Nov 15 06:57:04.701: INFO: (17) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 44.01749ms)
Nov 15 06:57:04.701: INFO: (17) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 43.838706ms)
Nov 15 06:57:04.702: INFO: (17) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 44.157331ms)
Nov 15 06:57:04.720: INFO: (18) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 18.123424ms)
Nov 15 06:57:04.725: INFO: (18) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 23.167351ms)
Nov 15 06:57:04.726: INFO: (18) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 23.403966ms)
Nov 15 06:57:04.726: INFO: (18) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 23.763989ms)
Nov 15 06:57:04.726: INFO: (18) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 23.800307ms)
Nov 15 06:57:04.726: INFO: (18) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 23.969317ms)
Nov 15 06:57:04.726: INFO: (18) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 24.149737ms)
Nov 15 06:57:04.726: INFO: (18) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 24.329236ms)
Nov 15 06:57:04.726: INFO: (18) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 24.711259ms)
Nov 15 06:57:04.727: INFO: (18) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 24.924709ms)
Nov 15 06:57:04.731: INFO: (18) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 29.596721ms)
Nov 15 06:57:04.733: INFO: (18) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 30.542724ms)
Nov 15 06:57:04.734: INFO: (18) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 32.089222ms)
Nov 15 06:57:04.735: INFO: (18) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 32.550895ms)
Nov 15 06:57:04.735: INFO: (18) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 32.349791ms)
Nov 15 06:57:04.735: INFO: (18) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 32.729184ms)
Nov 15 06:57:04.759: INFO: (19) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 24.039584ms)
Nov 15 06:57:04.759: INFO: (19) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 24.114258ms)
Nov 15 06:57:04.759: INFO: (19) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 24.022178ms)
Nov 15 06:57:04.759: INFO: (19) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 24.250622ms)
Nov 15 06:57:04.760: INFO: (19) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 24.868352ms)
Nov 15 06:57:04.760: INFO: (19) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 24.750775ms)
Nov 15 06:57:04.760: INFO: (19) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 24.759491ms)
Nov 15 06:57:04.760: INFO: (19) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 25.208172ms)
Nov 15 06:57:04.760: INFO: (19) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 25.095083ms)
Nov 15 06:57:04.760: INFO: (19) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 25.331211ms)
Nov 15 06:57:04.760: INFO: (19) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 25.516057ms)
Nov 15 06:57:04.766: INFO: (19) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 30.635237ms)
Nov 15 06:57:04.767: INFO: (19) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 31.802392ms)
Nov 15 06:57:04.767: INFO: (19) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 31.585411ms)
Nov 15 06:57:04.768: INFO: (19) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 33.201604ms)
Nov 15 06:57:04.768: INFO: (19) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 32.799147ms)
STEP: deleting ReplicationController proxy-service-xmgpc in namespace proxy-6595, will wait for the garbage collector to delete the pods 11/15/23 06:57:04.768
Nov 15 06:57:04.861: INFO: Deleting ReplicationController proxy-service-xmgpc took: 27.099427ms
Nov 15 06:57:04.962: INFO: Terminating ReplicationController proxy-service-xmgpc pods took: 100.579108ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Nov 15 06:57:07.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6595" for this suite. 11/15/23 06:57:07.684
------------------------------
â€¢ [SLOW TEST] [6.909 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:57:00.796
    Nov 15 06:57:00.796: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename proxy 11/15/23 06:57:00.797
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:57:00.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:57:00.875
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 11/15/23 06:57:00.927
    STEP: creating replication controller proxy-service-xmgpc in namespace proxy-6595 11/15/23 06:57:00.927
    I1115 06:57:00.956618      22 runners.go:193] Created replication controller with name: proxy-service-xmgpc, namespace: proxy-6595, replica count: 1
    I1115 06:57:02.008604      22 runners.go:193] proxy-service-xmgpc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1115 06:57:03.009342      22 runners.go:193] proxy-service-xmgpc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1115 06:57:04.010518      22 runners.go:193] proxy-service-xmgpc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 15 06:57:04.023: INFO: setup took 3.13348229s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 11/15/23 06:57:04.023
    Nov 15 06:57:04.057: INFO: (0) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 33.651581ms)
    Nov 15 06:57:04.059: INFO: (0) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 35.228781ms)
    Nov 15 06:57:04.059: INFO: (0) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 35.58639ms)
    Nov 15 06:57:04.059: INFO: (0) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 36.01765ms)
    Nov 15 06:57:04.059: INFO: (0) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 36.35712ms)
    Nov 15 06:57:04.061: INFO: (0) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 37.327089ms)
    Nov 15 06:57:04.065: INFO: (0) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 41.774856ms)
    Nov 15 06:57:04.066: INFO: (0) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 42.604355ms)
    Nov 15 06:57:04.066: INFO: (0) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 42.580855ms)
    Nov 15 06:57:04.066: INFO: (0) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 43.288667ms)
    Nov 15 06:57:04.066: INFO: (0) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 43.455459ms)
    Nov 15 06:57:04.075: INFO: (0) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 52.245155ms)
    Nov 15 06:57:04.075: INFO: (0) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 52.400886ms)
    Nov 15 06:57:04.078: INFO: (0) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 54.200965ms)
    Nov 15 06:57:04.078: INFO: (0) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 54.436878ms)
    Nov 15 06:57:04.087: INFO: (0) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 63.115223ms)
    Nov 15 06:57:04.103: INFO: (1) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 16.78148ms)
    Nov 15 06:57:04.108: INFO: (1) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 21.000554ms)
    Nov 15 06:57:04.110: INFO: (1) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 23.429684ms)
    Nov 15 06:57:04.110: INFO: (1) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 23.696544ms)
    Nov 15 06:57:04.111: INFO: (1) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 24.418247ms)
    Nov 15 06:57:04.111: INFO: (1) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 24.450647ms)
    Nov 15 06:57:04.111: INFO: (1) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 24.563734ms)
    Nov 15 06:57:04.112: INFO: (1) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 24.856652ms)
    Nov 15 06:57:04.112: INFO: (1) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 24.967614ms)
    Nov 15 06:57:04.112: INFO: (1) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 25.199435ms)
    Nov 15 06:57:04.116: INFO: (1) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 29.513269ms)
    Nov 15 06:57:04.118: INFO: (1) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 31.706353ms)
    Nov 15 06:57:04.119: INFO: (1) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 31.873998ms)
    Nov 15 06:57:04.119: INFO: (1) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 32.39343ms)
    Nov 15 06:57:04.119: INFO: (1) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 32.702395ms)
    Nov 15 06:57:04.120: INFO: (1) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 33.008365ms)
    Nov 15 06:57:04.138: INFO: (2) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 18.216149ms)
    Nov 15 06:57:04.138: INFO: (2) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 18.107519ms)
    Nov 15 06:57:04.140: INFO: (2) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 19.006322ms)
    Nov 15 06:57:04.140: INFO: (2) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 19.497697ms)
    Nov 15 06:57:04.141: INFO: (2) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 20.953487ms)
    Nov 15 06:57:04.141: INFO: (2) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 20.165625ms)
    Nov 15 06:57:04.141: INFO: (2) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 21.175727ms)
    Nov 15 06:57:04.141: INFO: (2) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 20.594537ms)
    Nov 15 06:57:04.142: INFO: (2) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 21.123348ms)
    Nov 15 06:57:04.142: INFO: (2) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 20.688942ms)
    Nov 15 06:57:04.147: INFO: (2) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 26.164606ms)
    Nov 15 06:57:04.147: INFO: (2) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 26.208414ms)
    Nov 15 06:57:04.155: INFO: (2) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 34.501483ms)
    Nov 15 06:57:04.156: INFO: (2) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 35.052251ms)
    Nov 15 06:57:04.156: INFO: (2) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 35.609856ms)
    Nov 15 06:57:04.156: INFO: (2) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 35.558154ms)
    Nov 15 06:57:04.176: INFO: (3) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 20.174197ms)
    Nov 15 06:57:04.178: INFO: (3) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 22.090151ms)
    Nov 15 06:57:04.179: INFO: (3) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 22.589693ms)
    Nov 15 06:57:04.179: INFO: (3) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 22.567229ms)
    Nov 15 06:57:04.179: INFO: (3) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 22.597403ms)
    Nov 15 06:57:04.179: INFO: (3) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 22.911342ms)
    Nov 15 06:57:04.180: INFO: (3) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 23.319561ms)
    Nov 15 06:57:04.180: INFO: (3) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 23.396646ms)
    Nov 15 06:57:04.180: INFO: (3) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 23.450692ms)
    Nov 15 06:57:04.180: INFO: (3) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 23.481669ms)
    Nov 15 06:57:04.185: INFO: (3) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 28.679107ms)
    Nov 15 06:57:04.187: INFO: (3) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 30.449926ms)
    Nov 15 06:57:04.188: INFO: (3) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 31.210531ms)
    Nov 15 06:57:04.188: INFO: (3) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 31.221399ms)
    Nov 15 06:57:04.188: INFO: (3) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 32.19547ms)
    Nov 15 06:57:04.189: INFO: (3) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 32.110088ms)
    Nov 15 06:57:04.206: INFO: (4) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 17.649114ms)
    Nov 15 06:57:04.211: INFO: (4) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 21.93686ms)
    Nov 15 06:57:04.211: INFO: (4) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 22.121059ms)
    Nov 15 06:57:04.211: INFO: (4) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 22.172693ms)
    Nov 15 06:57:04.212: INFO: (4) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 22.931221ms)
    Nov 15 06:57:04.212: INFO: (4) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 22.600761ms)
    Nov 15 06:57:04.212: INFO: (4) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 23.09164ms)
    Nov 15 06:57:04.212: INFO: (4) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 23.421433ms)
    Nov 15 06:57:04.212: INFO: (4) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 23.120666ms)
    Nov 15 06:57:04.213: INFO: (4) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 23.658303ms)
    Nov 15 06:57:04.216: INFO: (4) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 27.225243ms)
    Nov 15 06:57:04.219: INFO: (4) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 30.416544ms)
    Nov 15 06:57:04.220: INFO: (4) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 30.702872ms)
    Nov 15 06:57:04.220: INFO: (4) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 30.877871ms)
    Nov 15 06:57:04.220: INFO: (4) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 31.03747ms)
    Nov 15 06:57:04.220: INFO: (4) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 31.286069ms)
    Nov 15 06:57:04.241: INFO: (5) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 20.519355ms)
    Nov 15 06:57:04.245: INFO: (5) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 24.308641ms)
    Nov 15 06:57:04.245: INFO: (5) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 25.046707ms)
    Nov 15 06:57:04.245: INFO: (5) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 24.965374ms)
    Nov 15 06:57:04.246: INFO: (5) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 25.802963ms)
    Nov 15 06:57:04.246: INFO: (5) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 25.893409ms)
    Nov 15 06:57:04.247: INFO: (5) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 26.203531ms)
    Nov 15 06:57:04.247: INFO: (5) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 26.196144ms)
    Nov 15 06:57:04.247: INFO: (5) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 26.223762ms)
    Nov 15 06:57:04.247: INFO: (5) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 26.455384ms)
    Nov 15 06:57:04.252: INFO: (5) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 31.945864ms)
    Nov 15 06:57:04.254: INFO: (5) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 33.425253ms)
    Nov 15 06:57:04.254: INFO: (5) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 33.675973ms)
    Nov 15 06:57:04.254: INFO: (5) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 33.559424ms)
    Nov 15 06:57:04.254: INFO: (5) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 33.662131ms)
    Nov 15 06:57:04.254: INFO: (5) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 33.733769ms)
    Nov 15 06:57:04.272: INFO: (6) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 18.198745ms)
    Nov 15 06:57:04.276: INFO: (6) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 21.121205ms)
    Nov 15 06:57:04.276: INFO: (6) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 21.553363ms)
    Nov 15 06:57:04.277: INFO: (6) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 22.169809ms)
    Nov 15 06:57:04.277: INFO: (6) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 22.334553ms)
    Nov 15 06:57:04.278: INFO: (6) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 22.700497ms)
    Nov 15 06:57:04.278: INFO: (6) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 22.990076ms)
    Nov 15 06:57:04.278: INFO: (6) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 23.141671ms)
    Nov 15 06:57:04.278: INFO: (6) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 23.452752ms)
    Nov 15 06:57:04.279: INFO: (6) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 23.955254ms)
    Nov 15 06:57:04.284: INFO: (6) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 28.990055ms)
    Nov 15 06:57:04.284: INFO: (6) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 28.996053ms)
    Nov 15 06:57:04.285: INFO: (6) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 30.443148ms)
    Nov 15 06:57:04.285: INFO: (6) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 31.150776ms)
    Nov 15 06:57:04.286: INFO: (6) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 30.783923ms)
    Nov 15 06:57:04.288: INFO: (6) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 33.367446ms)
    Nov 15 06:57:04.306: INFO: (7) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 17.05404ms)
    Nov 15 06:57:04.306: INFO: (7) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 17.628228ms)
    Nov 15 06:57:04.312: INFO: (7) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 23.786262ms)
    Nov 15 06:57:04.313: INFO: (7) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 24.092522ms)
    Nov 15 06:57:04.313: INFO: (7) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 24.877859ms)
    Nov 15 06:57:04.313: INFO: (7) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 24.655856ms)
    Nov 15 06:57:04.313: INFO: (7) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 24.000226ms)
    Nov 15 06:57:04.313: INFO: (7) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 24.94983ms)
    Nov 15 06:57:04.313: INFO: (7) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 24.359793ms)
    Nov 15 06:57:04.313: INFO: (7) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 24.355786ms)
    Nov 15 06:57:04.318: INFO: (7) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 29.24264ms)
    Nov 15 06:57:04.319: INFO: (7) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 30.357178ms)
    Nov 15 06:57:04.319: INFO: (7) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 31.295445ms)
    Nov 15 06:57:04.319: INFO: (7) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 30.282308ms)
    Nov 15 06:57:04.320: INFO: (7) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 30.712991ms)
    Nov 15 06:57:04.320: INFO: (7) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 31.20356ms)
    Nov 15 06:57:04.338: INFO: (8) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 18.548137ms)
    Nov 15 06:57:04.343: INFO: (8) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 23.313107ms)
    Nov 15 06:57:04.343: INFO: (8) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 23.371389ms)
    Nov 15 06:57:04.344: INFO: (8) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 23.490225ms)
    Nov 15 06:57:04.344: INFO: (8) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 23.379181ms)
    Nov 15 06:57:04.344: INFO: (8) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 23.685976ms)
    Nov 15 06:57:04.344: INFO: (8) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 24.034716ms)
    Nov 15 06:57:04.344: INFO: (8) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 24.016381ms)
    Nov 15 06:57:04.344: INFO: (8) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 24.048108ms)
    Nov 15 06:57:04.344: INFO: (8) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 24.055089ms)
    Nov 15 06:57:04.350: INFO: (8) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 29.951351ms)
    Nov 15 06:57:04.351: INFO: (8) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 31.107027ms)
    Nov 15 06:57:04.354: INFO: (8) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 34.325116ms)
    Nov 15 06:57:04.354: INFO: (8) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 33.778156ms)
    Nov 15 06:57:04.355: INFO: (8) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 34.494342ms)
    Nov 15 06:57:04.355: INFO: (8) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 34.364418ms)
    Nov 15 06:57:04.375: INFO: (9) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 20.112044ms)
    Nov 15 06:57:04.383: INFO: (9) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 28.452922ms)
    Nov 15 06:57:04.383: INFO: (9) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 28.552382ms)
    Nov 15 06:57:04.383: INFO: (9) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 27.796002ms)
    Nov 15 06:57:04.383: INFO: (9) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 28.057196ms)
    Nov 15 06:57:04.383: INFO: (9) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 28.466512ms)
    Nov 15 06:57:04.384: INFO: (9) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 28.455868ms)
    Nov 15 06:57:04.384: INFO: (9) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 28.719782ms)
    Nov 15 06:57:04.384: INFO: (9) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 28.74305ms)
    Nov 15 06:57:04.384: INFO: (9) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 29.138274ms)
    Nov 15 06:57:04.391: INFO: (9) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 35.886014ms)
    Nov 15 06:57:04.391: INFO: (9) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 36.039655ms)
    Nov 15 06:57:04.391: INFO: (9) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 36.164516ms)
    Nov 15 06:57:04.391: INFO: (9) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 36.185792ms)
    Nov 15 06:57:04.391: INFO: (9) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 36.375933ms)
    Nov 15 06:57:04.391: INFO: (9) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 36.756841ms)
    Nov 15 06:57:04.411: INFO: (10) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 18.719697ms)
    Nov 15 06:57:04.414: INFO: (10) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 22.275581ms)
    Nov 15 06:57:04.414: INFO: (10) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 21.768392ms)
    Nov 15 06:57:04.414: INFO: (10) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 21.999464ms)
    Nov 15 06:57:04.415: INFO: (10) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 22.671156ms)
    Nov 15 06:57:04.415: INFO: (10) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 23.011406ms)
    Nov 15 06:57:04.415: INFO: (10) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 23.028614ms)
    Nov 15 06:57:04.415: INFO: (10) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 22.825279ms)
    Nov 15 06:57:04.415: INFO: (10) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 22.778161ms)
    Nov 15 06:57:04.415: INFO: (10) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 23.077599ms)
    Nov 15 06:57:04.432: INFO: (10) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 39.959637ms)
    Nov 15 06:57:04.432: INFO: (10) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 39.640383ms)
    Nov 15 06:57:04.432: INFO: (10) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 39.773856ms)
    Nov 15 06:57:04.432: INFO: (10) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 39.773856ms)
    Nov 15 06:57:04.432: INFO: (10) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 39.506993ms)
    Nov 15 06:57:04.432: INFO: (10) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 40.466826ms)
    Nov 15 06:57:04.454: INFO: (11) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 20.734864ms)
    Nov 15 06:57:04.460: INFO: (11) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 28.497673ms)
    Nov 15 06:57:04.461: INFO: (11) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 26.704489ms)
    Nov 15 06:57:04.461: INFO: (11) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 27.264272ms)
    Nov 15 06:57:04.461: INFO: (11) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 26.864637ms)
    Nov 15 06:57:04.461: INFO: (11) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 27.528878ms)
    Nov 15 06:57:04.461: INFO: (11) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 27.51876ms)
    Nov 15 06:57:04.461: INFO: (11) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 28.424871ms)
    Nov 15 06:57:04.461: INFO: (11) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 28.582596ms)
    Nov 15 06:57:04.461: INFO: (11) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 27.947065ms)
    Nov 15 06:57:04.463: INFO: (11) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 28.937372ms)
    Nov 15 06:57:04.468: INFO: (11) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 34.786532ms)
    Nov 15 06:57:04.468: INFO: (11) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 34.666769ms)
    Nov 15 06:57:04.468: INFO: (11) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 34.772231ms)
    Nov 15 06:57:04.471: INFO: (11) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 37.970248ms)
    Nov 15 06:57:04.472: INFO: (11) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 38.430622ms)
    Nov 15 06:57:04.488: INFO: (12) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 16.524792ms)
    Nov 15 06:57:04.491: INFO: (12) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 19.242052ms)
    Nov 15 06:57:04.492: INFO: (12) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 20.418522ms)
    Nov 15 06:57:04.493: INFO: (12) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 19.93473ms)
    Nov 15 06:57:04.493: INFO: (12) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 21.091644ms)
    Nov 15 06:57:04.493: INFO: (12) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 20.513152ms)
    Nov 15 06:57:04.493: INFO: (12) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 20.947599ms)
    Nov 15 06:57:04.493: INFO: (12) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 21.191144ms)
    Nov 15 06:57:04.493: INFO: (12) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 20.633407ms)
    Nov 15 06:57:04.493: INFO: (12) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 20.717409ms)
    Nov 15 06:57:04.500: INFO: (12) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 27.011459ms)
    Nov 15 06:57:04.502: INFO: (12) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 29.940958ms)
    Nov 15 06:57:04.502: INFO: (12) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 29.766616ms)
    Nov 15 06:57:04.502: INFO: (12) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 29.648133ms)
    Nov 15 06:57:04.502: INFO: (12) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 30.936996ms)
    Nov 15 06:57:04.502: INFO: (12) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 30.037147ms)
    Nov 15 06:57:04.523: INFO: (13) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 20.623804ms)
    Nov 15 06:57:04.525: INFO: (13) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 21.500105ms)
    Nov 15 06:57:04.525: INFO: (13) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 21.702909ms)
    Nov 15 06:57:04.525: INFO: (13) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 22.219219ms)
    Nov 15 06:57:04.525: INFO: (13) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 22.274173ms)
    Nov 15 06:57:04.526: INFO: (13) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 22.779736ms)
    Nov 15 06:57:04.526: INFO: (13) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 23.523505ms)
    Nov 15 06:57:04.527: INFO: (13) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 23.76313ms)
    Nov 15 06:57:04.527: INFO: (13) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 23.513272ms)
    Nov 15 06:57:04.527: INFO: (13) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 24.262024ms)
    Nov 15 06:57:04.531: INFO: (13) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 28.937666ms)
    Nov 15 06:57:04.533: INFO: (13) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 30.521946ms)
    Nov 15 06:57:04.534: INFO: (13) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 31.26257ms)
    Nov 15 06:57:04.537: INFO: (13) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 34.700226ms)
    Nov 15 06:57:04.537: INFO: (13) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 34.49692ms)
    Nov 15 06:57:04.537: INFO: (13) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 34.250388ms)
    Nov 15 06:57:04.568: INFO: (14) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 29.817833ms)
    Nov 15 06:57:04.568: INFO: (14) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 28.92381ms)
    Nov 15 06:57:04.568: INFO: (14) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 29.51681ms)
    Nov 15 06:57:04.568: INFO: (14) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 30.127154ms)
    Nov 15 06:57:04.568: INFO: (14) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 30.144587ms)
    Nov 15 06:57:04.568: INFO: (14) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 30.048887ms)
    Nov 15 06:57:04.569: INFO: (14) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 30.994829ms)
    Nov 15 06:57:04.569: INFO: (14) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 31.007656ms)
    Nov 15 06:57:04.569: INFO: (14) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 30.332451ms)
    Nov 15 06:57:04.569: INFO: (14) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 30.500923ms)
    Nov 15 06:57:04.573: INFO: (14) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 34.523096ms)
    Nov 15 06:57:04.575: INFO: (14) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 36.797967ms)
    Nov 15 06:57:04.578: INFO: (14) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 40.344441ms)
    Nov 15 06:57:04.578: INFO: (14) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 40.916915ms)
    Nov 15 06:57:04.579: INFO: (14) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 40.691061ms)
    Nov 15 06:57:04.579: INFO: (14) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 40.673126ms)
    Nov 15 06:57:04.608: INFO: (15) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 28.067262ms)
    Nov 15 06:57:04.608: INFO: (15) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 28.761336ms)
    Nov 15 06:57:04.608: INFO: (15) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 28.743031ms)
    Nov 15 06:57:04.608: INFO: (15) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 28.989029ms)
    Nov 15 06:57:04.609: INFO: (15) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 29.94598ms)
    Nov 15 06:57:04.609: INFO: (15) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 29.616813ms)
    Nov 15 06:57:04.609: INFO: (15) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 29.90692ms)
    Nov 15 06:57:04.609: INFO: (15) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 30.036467ms)
    Nov 15 06:57:04.610: INFO: (15) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 30.743923ms)
    Nov 15 06:57:04.610: INFO: (15) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 31.0456ms)
    Nov 15 06:57:04.617: INFO: (15) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 37.541968ms)
    Nov 15 06:57:04.622: INFO: (15) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 42.80043ms)
    Nov 15 06:57:04.622: INFO: (15) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 42.972306ms)
    Nov 15 06:57:04.623: INFO: (15) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 43.158729ms)
    Nov 15 06:57:04.623: INFO: (15) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 43.488882ms)
    Nov 15 06:57:04.623: INFO: (15) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 43.556072ms)
    Nov 15 06:57:04.641: INFO: (16) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 17.749984ms)
    Nov 15 06:57:04.647: INFO: (16) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 23.259728ms)
    Nov 15 06:57:04.647: INFO: (16) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 23.470713ms)
    Nov 15 06:57:04.648: INFO: (16) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 24.290476ms)
    Nov 15 06:57:04.648: INFO: (16) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 24.44566ms)
    Nov 15 06:57:04.648: INFO: (16) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 24.661215ms)
    Nov 15 06:57:04.650: INFO: (16) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 26.278549ms)
    Nov 15 06:57:04.650: INFO: (16) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 26.305052ms)
    Nov 15 06:57:04.650: INFO: (16) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 26.486246ms)
    Nov 15 06:57:04.650: INFO: (16) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 26.410154ms)
    Nov 15 06:57:04.652: INFO: (16) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 27.902786ms)
    Nov 15 06:57:04.656: INFO: (16) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 32.445634ms)
    Nov 15 06:57:04.656: INFO: (16) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 32.705918ms)
    Nov 15 06:57:04.657: INFO: (16) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 33.336294ms)
    Nov 15 06:57:04.657: INFO: (16) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 33.283571ms)
    Nov 15 06:57:04.657: INFO: (16) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 33.382429ms)
    Nov 15 06:57:04.677: INFO: (17) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 19.541532ms)
    Nov 15 06:57:04.677: INFO: (17) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 19.702912ms)
    Nov 15 06:57:04.677: INFO: (17) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 19.49794ms)
    Nov 15 06:57:04.678: INFO: (17) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 21.089964ms)
    Nov 15 06:57:04.678: INFO: (17) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 21.058255ms)
    Nov 15 06:57:04.679: INFO: (17) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 21.812014ms)
    Nov 15 06:57:04.679: INFO: (17) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 21.617183ms)
    Nov 15 06:57:04.680: INFO: (17) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 22.308118ms)
    Nov 15 06:57:04.680: INFO: (17) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 22.675304ms)
    Nov 15 06:57:04.680: INFO: (17) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 22.711865ms)
    Nov 15 06:57:04.683: INFO: (17) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 25.915978ms)
    Nov 15 06:57:04.701: INFO: (17) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 43.583213ms)
    Nov 15 06:57:04.701: INFO: (17) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 43.395554ms)
    Nov 15 06:57:04.701: INFO: (17) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 44.01749ms)
    Nov 15 06:57:04.701: INFO: (17) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 43.838706ms)
    Nov 15 06:57:04.702: INFO: (17) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 44.157331ms)
    Nov 15 06:57:04.720: INFO: (18) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 18.123424ms)
    Nov 15 06:57:04.725: INFO: (18) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 23.167351ms)
    Nov 15 06:57:04.726: INFO: (18) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 23.403966ms)
    Nov 15 06:57:04.726: INFO: (18) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 23.763989ms)
    Nov 15 06:57:04.726: INFO: (18) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 23.800307ms)
    Nov 15 06:57:04.726: INFO: (18) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 23.969317ms)
    Nov 15 06:57:04.726: INFO: (18) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 24.149737ms)
    Nov 15 06:57:04.726: INFO: (18) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 24.329236ms)
    Nov 15 06:57:04.726: INFO: (18) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 24.711259ms)
    Nov 15 06:57:04.727: INFO: (18) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 24.924709ms)
    Nov 15 06:57:04.731: INFO: (18) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 29.596721ms)
    Nov 15 06:57:04.733: INFO: (18) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 30.542724ms)
    Nov 15 06:57:04.734: INFO: (18) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 32.089222ms)
    Nov 15 06:57:04.735: INFO: (18) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 32.550895ms)
    Nov 15 06:57:04.735: INFO: (18) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 32.349791ms)
    Nov 15 06:57:04.735: INFO: (18) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 32.729184ms)
    Nov 15 06:57:04.759: INFO: (19) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">test<... (200; 24.039584ms)
    Nov 15 06:57:04.759: INFO: (19) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 24.114258ms)
    Nov 15 06:57:04.759: INFO: (19) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 24.022178ms)
    Nov 15 06:57:04.759: INFO: (19) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:160/proxy/: foo (200; 24.250622ms)
    Nov 15 06:57:04.760: INFO: (19) /api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/proxy-service-xmgpc-pktxp/proxy/rewriteme">test</a> (200; 24.868352ms)
    Nov 15 06:57:04.760: INFO: (19) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:162/proxy/: bar (200; 24.750775ms)
    Nov 15 06:57:04.760: INFO: (19) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:462/proxy/: tls qux (200; 24.759491ms)
    Nov 15 06:57:04.760: INFO: (19) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:443/proxy/tlsrewritem... (200; 25.208172ms)
    Nov 15 06:57:04.760: INFO: (19) /api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/: <a href="/api/v1/namespaces/proxy-6595/pods/http:proxy-service-xmgpc-pktxp:1080/proxy/rewriteme">... (200; 25.095083ms)
    Nov 15 06:57:04.760: INFO: (19) /api/v1/namespaces/proxy-6595/pods/https:proxy-service-xmgpc-pktxp:460/proxy/: tls baz (200; 25.331211ms)
    Nov 15 06:57:04.760: INFO: (19) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname2/proxy/: tls qux (200; 25.516057ms)
    Nov 15 06:57:04.766: INFO: (19) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname2/proxy/: bar (200; 30.635237ms)
    Nov 15 06:57:04.767: INFO: (19) /api/v1/namespaces/proxy-6595/services/proxy-service-xmgpc:portname1/proxy/: foo (200; 31.802392ms)
    Nov 15 06:57:04.767: INFO: (19) /api/v1/namespaces/proxy-6595/services/https:proxy-service-xmgpc:tlsportname1/proxy/: tls baz (200; 31.585411ms)
    Nov 15 06:57:04.768: INFO: (19) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname2/proxy/: bar (200; 33.201604ms)
    Nov 15 06:57:04.768: INFO: (19) /api/v1/namespaces/proxy-6595/services/http:proxy-service-xmgpc:portname1/proxy/: foo (200; 32.799147ms)
    STEP: deleting ReplicationController proxy-service-xmgpc in namespace proxy-6595, will wait for the garbage collector to delete the pods 11/15/23 06:57:04.768
    Nov 15 06:57:04.861: INFO: Deleting ReplicationController proxy-service-xmgpc took: 27.099427ms
    Nov 15 06:57:04.962: INFO: Terminating ReplicationController proxy-service-xmgpc pods took: 100.579108ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:57:07.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6595" for this suite. 11/15/23 06:57:07.684
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:57:07.707
Nov 15 06:57:07.707: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 06:57:07.708
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:57:07.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:57:07.787
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Nov 15 06:57:07.799: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 06:57:15.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6251" for this suite. 11/15/23 06:57:15.128
------------------------------
â€¢ [SLOW TEST] [7.441 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:57:07.707
    Nov 15 06:57:07.707: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 06:57:07.708
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:57:07.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:57:07.787
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Nov 15 06:57:07.799: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 06:57:15.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6251" for this suite. 11/15/23 06:57:15.128
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 06:57:15.15
Nov 15 06:57:15.151: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename container-probe 11/15/23 06:57:15.151
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:57:15.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:57:15.223
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-c25f34ab-96ee-451d-b5dd-f90a2f2eed96 in namespace container-probe-2394 11/15/23 06:57:15.232
Nov 15 06:57:15.256: INFO: Waiting up to 5m0s for pod "test-webserver-c25f34ab-96ee-451d-b5dd-f90a2f2eed96" in namespace "container-probe-2394" to be "not pending"
Nov 15 06:57:15.270: INFO: Pod "test-webserver-c25f34ab-96ee-451d-b5dd-f90a2f2eed96": Phase="Pending", Reason="", readiness=false. Elapsed: 14.293478ms
Nov 15 06:57:17.284: INFO: Pod "test-webserver-c25f34ab-96ee-451d-b5dd-f90a2f2eed96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028472073s
Nov 15 06:57:19.285: INFO: Pod "test-webserver-c25f34ab-96ee-451d-b5dd-f90a2f2eed96": Phase="Running", Reason="", readiness=true. Elapsed: 4.028555933s
Nov 15 06:57:19.285: INFO: Pod "test-webserver-c25f34ab-96ee-451d-b5dd-f90a2f2eed96" satisfied condition "not pending"
Nov 15 06:57:19.285: INFO: Started pod test-webserver-c25f34ab-96ee-451d-b5dd-f90a2f2eed96 in namespace container-probe-2394
STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 06:57:19.285
Nov 15 06:57:19.296: INFO: Initial restart count of pod test-webserver-c25f34ab-96ee-451d-b5dd-f90a2f2eed96 is 0
STEP: deleting the pod 11/15/23 07:01:21.188
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 15 07:01:21.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2394" for this suite. 11/15/23 07:01:21.239
------------------------------
â€¢ [SLOW TEST] [246.121 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 06:57:15.15
    Nov 15 06:57:15.151: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename container-probe 11/15/23 06:57:15.151
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 06:57:15.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 06:57:15.223
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-c25f34ab-96ee-451d-b5dd-f90a2f2eed96 in namespace container-probe-2394 11/15/23 06:57:15.232
    Nov 15 06:57:15.256: INFO: Waiting up to 5m0s for pod "test-webserver-c25f34ab-96ee-451d-b5dd-f90a2f2eed96" in namespace "container-probe-2394" to be "not pending"
    Nov 15 06:57:15.270: INFO: Pod "test-webserver-c25f34ab-96ee-451d-b5dd-f90a2f2eed96": Phase="Pending", Reason="", readiness=false. Elapsed: 14.293478ms
    Nov 15 06:57:17.284: INFO: Pod "test-webserver-c25f34ab-96ee-451d-b5dd-f90a2f2eed96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028472073s
    Nov 15 06:57:19.285: INFO: Pod "test-webserver-c25f34ab-96ee-451d-b5dd-f90a2f2eed96": Phase="Running", Reason="", readiness=true. Elapsed: 4.028555933s
    Nov 15 06:57:19.285: INFO: Pod "test-webserver-c25f34ab-96ee-451d-b5dd-f90a2f2eed96" satisfied condition "not pending"
    Nov 15 06:57:19.285: INFO: Started pod test-webserver-c25f34ab-96ee-451d-b5dd-f90a2f2eed96 in namespace container-probe-2394
    STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 06:57:19.285
    Nov 15 06:57:19.296: INFO: Initial restart count of pod test-webserver-c25f34ab-96ee-451d-b5dd-f90a2f2eed96 is 0
    STEP: deleting the pod 11/15/23 07:01:21.188
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:01:21.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2394" for this suite. 11/15/23 07:01:21.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:01:21.273
Nov 15 07:01:21.273: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 07:01:21.274
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:01:21.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:01:21.346
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 11/15/23 07:01:21.356
W1115 07:01:21.385214      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Nov 15 07:01:21.385: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b843cdf5-79a9-4c44-aa53-75bc6152eefa" in namespace "projected-2466" to be "Succeeded or Failed"
Nov 15 07:01:21.403: INFO: Pod "downwardapi-volume-b843cdf5-79a9-4c44-aa53-75bc6152eefa": Phase="Pending", Reason="", readiness=false. Elapsed: 18.268069ms
Nov 15 07:01:23.415: INFO: Pod "downwardapi-volume-b843cdf5-79a9-4c44-aa53-75bc6152eefa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030517713s
Nov 15 07:01:25.416: INFO: Pod "downwardapi-volume-b843cdf5-79a9-4c44-aa53-75bc6152eefa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030679163s
Nov 15 07:01:27.416: INFO: Pod "downwardapi-volume-b843cdf5-79a9-4c44-aa53-75bc6152eefa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030945476s
STEP: Saw pod success 11/15/23 07:01:27.416
Nov 15 07:01:27.416: INFO: Pod "downwardapi-volume-b843cdf5-79a9-4c44-aa53-75bc6152eefa" satisfied condition "Succeeded or Failed"
Nov 15 07:01:27.427: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-b843cdf5-79a9-4c44-aa53-75bc6152eefa container client-container: <nil>
STEP: delete the pod 11/15/23 07:01:27.489
Nov 15 07:01:27.516: INFO: Waiting for pod downwardapi-volume-b843cdf5-79a9-4c44-aa53-75bc6152eefa to disappear
Nov 15 07:01:27.530: INFO: Pod downwardapi-volume-b843cdf5-79a9-4c44-aa53-75bc6152eefa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 07:01:27.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2466" for this suite. 11/15/23 07:01:27.549
------------------------------
â€¢ [SLOW TEST] [6.297 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:01:21.273
    Nov 15 07:01:21.273: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 07:01:21.274
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:01:21.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:01:21.346
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 11/15/23 07:01:21.356
    W1115 07:01:21.385214      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Nov 15 07:01:21.385: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b843cdf5-79a9-4c44-aa53-75bc6152eefa" in namespace "projected-2466" to be "Succeeded or Failed"
    Nov 15 07:01:21.403: INFO: Pod "downwardapi-volume-b843cdf5-79a9-4c44-aa53-75bc6152eefa": Phase="Pending", Reason="", readiness=false. Elapsed: 18.268069ms
    Nov 15 07:01:23.415: INFO: Pod "downwardapi-volume-b843cdf5-79a9-4c44-aa53-75bc6152eefa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030517713s
    Nov 15 07:01:25.416: INFO: Pod "downwardapi-volume-b843cdf5-79a9-4c44-aa53-75bc6152eefa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030679163s
    Nov 15 07:01:27.416: INFO: Pod "downwardapi-volume-b843cdf5-79a9-4c44-aa53-75bc6152eefa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030945476s
    STEP: Saw pod success 11/15/23 07:01:27.416
    Nov 15 07:01:27.416: INFO: Pod "downwardapi-volume-b843cdf5-79a9-4c44-aa53-75bc6152eefa" satisfied condition "Succeeded or Failed"
    Nov 15 07:01:27.427: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-b843cdf5-79a9-4c44-aa53-75bc6152eefa container client-container: <nil>
    STEP: delete the pod 11/15/23 07:01:27.489
    Nov 15 07:01:27.516: INFO: Waiting for pod downwardapi-volume-b843cdf5-79a9-4c44-aa53-75bc6152eefa to disappear
    Nov 15 07:01:27.530: INFO: Pod downwardapi-volume-b843cdf5-79a9-4c44-aa53-75bc6152eefa no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:01:27.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2466" for this suite. 11/15/23 07:01:27.549
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:01:27.571
Nov 15 07:01:27.571: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename emptydir 11/15/23 07:01:27.572
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:01:27.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:01:27.642
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 11/15/23 07:01:27.653
Nov 15 07:01:27.681: INFO: Waiting up to 5m0s for pod "pod-09533c43-4656-4938-b67b-79bd52d2c3cf" in namespace "emptydir-4413" to be "Succeeded or Failed"
Nov 15 07:01:27.693: INFO: Pod "pod-09533c43-4656-4938-b67b-79bd52d2c3cf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.29666ms
Nov 15 07:01:29.704: INFO: Pod "pod-09533c43-4656-4938-b67b-79bd52d2c3cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022397466s
Nov 15 07:01:31.705: INFO: Pod "pod-09533c43-4656-4938-b67b-79bd52d2c3cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023946863s
Nov 15 07:01:33.705: INFO: Pod "pod-09533c43-4656-4938-b67b-79bd52d2c3cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023266017s
STEP: Saw pod success 11/15/23 07:01:33.705
Nov 15 07:01:33.705: INFO: Pod "pod-09533c43-4656-4938-b67b-79bd52d2c3cf" satisfied condition "Succeeded or Failed"
Nov 15 07:01:33.718: INFO: Trying to get logs from node 10.72.152.86 pod pod-09533c43-4656-4938-b67b-79bd52d2c3cf container test-container: <nil>
STEP: delete the pod 11/15/23 07:01:33.745
Nov 15 07:01:33.776: INFO: Waiting for pod pod-09533c43-4656-4938-b67b-79bd52d2c3cf to disappear
Nov 15 07:01:33.786: INFO: Pod pod-09533c43-4656-4938-b67b-79bd52d2c3cf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 07:01:33.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4413" for this suite. 11/15/23 07:01:33.81
------------------------------
â€¢ [SLOW TEST] [6.258 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:01:27.571
    Nov 15 07:01:27.571: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename emptydir 11/15/23 07:01:27.572
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:01:27.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:01:27.642
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 11/15/23 07:01:27.653
    Nov 15 07:01:27.681: INFO: Waiting up to 5m0s for pod "pod-09533c43-4656-4938-b67b-79bd52d2c3cf" in namespace "emptydir-4413" to be "Succeeded or Failed"
    Nov 15 07:01:27.693: INFO: Pod "pod-09533c43-4656-4938-b67b-79bd52d2c3cf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.29666ms
    Nov 15 07:01:29.704: INFO: Pod "pod-09533c43-4656-4938-b67b-79bd52d2c3cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022397466s
    Nov 15 07:01:31.705: INFO: Pod "pod-09533c43-4656-4938-b67b-79bd52d2c3cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023946863s
    Nov 15 07:01:33.705: INFO: Pod "pod-09533c43-4656-4938-b67b-79bd52d2c3cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023266017s
    STEP: Saw pod success 11/15/23 07:01:33.705
    Nov 15 07:01:33.705: INFO: Pod "pod-09533c43-4656-4938-b67b-79bd52d2c3cf" satisfied condition "Succeeded or Failed"
    Nov 15 07:01:33.718: INFO: Trying to get logs from node 10.72.152.86 pod pod-09533c43-4656-4938-b67b-79bd52d2c3cf container test-container: <nil>
    STEP: delete the pod 11/15/23 07:01:33.745
    Nov 15 07:01:33.776: INFO: Waiting for pod pod-09533c43-4656-4938-b67b-79bd52d2c3cf to disappear
    Nov 15 07:01:33.786: INFO: Pod pod-09533c43-4656-4938-b67b-79bd52d2c3cf no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:01:33.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4413" for this suite. 11/15/23 07:01:33.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:01:33.831
Nov 15 07:01:33.831: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename services 11/15/23 07:01:33.832
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:01:33.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:01:33.903
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-7638 11/15/23 07:01:33.913
STEP: creating service affinity-nodeport in namespace services-7638 11/15/23 07:01:33.913
STEP: creating replication controller affinity-nodeport in namespace services-7638 11/15/23 07:01:33.982
I1115 07:01:34.007831      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7638, replica count: 3
I1115 07:01:37.058439      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 07:01:37.099: INFO: Creating new exec pod
Nov 15 07:01:37.123: INFO: Waiting up to 5m0s for pod "execpod-affinitybkxhb" in namespace "services-7638" to be "running"
Nov 15 07:01:37.133: INFO: Pod "execpod-affinitybkxhb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.430383ms
Nov 15 07:01:39.146: INFO: Pod "execpod-affinitybkxhb": Phase="Running", Reason="", readiness=true. Elapsed: 2.023566251s
Nov 15 07:01:39.146: INFO: Pod "execpod-affinitybkxhb" satisfied condition "running"
Nov 15 07:01:40.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7638 exec execpod-affinitybkxhb -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Nov 15 07:01:40.494: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Nov 15 07:01:40.494: INFO: stdout: ""
Nov 15 07:01:40.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7638 exec execpod-affinitybkxhb -- /bin/sh -x -c nc -v -z -w 2 172.21.245.91 80'
Nov 15 07:01:40.816: INFO: stderr: "+ nc -v -z -w 2 172.21.245.91 80\nConnection to 172.21.245.91 80 port [tcp/http] succeeded!\n"
Nov 15 07:01:40.816: INFO: stdout: ""
Nov 15 07:01:40.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7638 exec execpod-affinitybkxhb -- /bin/sh -x -c nc -v -z -w 2 10.72.152.86 30018'
Nov 15 07:01:41.136: INFO: stderr: "+ nc -v -z -w 2 10.72.152.86 30018\nConnection to 10.72.152.86 30018 port [tcp/*] succeeded!\n"
Nov 15 07:01:41.136: INFO: stdout: ""
Nov 15 07:01:41.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7638 exec execpod-affinitybkxhb -- /bin/sh -x -c nc -v -z -w 2 10.72.152.81 30018'
Nov 15 07:01:41.418: INFO: stderr: "+ nc -v -z -w 2 10.72.152.81 30018\nConnection to 10.72.152.81 30018 port [tcp/*] succeeded!\n"
Nov 15 07:01:41.418: INFO: stdout: ""
Nov 15 07:01:41.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7638 exec execpod-affinitybkxhb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.72.152.81:30018/ ; done'
Nov 15 07:01:41.863: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n"
Nov 15 07:01:41.863: INFO: stdout: "\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4"
Nov 15 07:01:41.863: INFO: Received response from host: affinity-nodeport-42wv4
Nov 15 07:01:41.863: INFO: Received response from host: affinity-nodeport-42wv4
Nov 15 07:01:41.863: INFO: Received response from host: affinity-nodeport-42wv4
Nov 15 07:01:41.863: INFO: Received response from host: affinity-nodeport-42wv4
Nov 15 07:01:41.863: INFO: Received response from host: affinity-nodeport-42wv4
Nov 15 07:01:41.863: INFO: Received response from host: affinity-nodeport-42wv4
Nov 15 07:01:41.863: INFO: Received response from host: affinity-nodeport-42wv4
Nov 15 07:01:41.863: INFO: Received response from host: affinity-nodeport-42wv4
Nov 15 07:01:41.863: INFO: Received response from host: affinity-nodeport-42wv4
Nov 15 07:01:41.864: INFO: Received response from host: affinity-nodeport-42wv4
Nov 15 07:01:41.864: INFO: Received response from host: affinity-nodeport-42wv4
Nov 15 07:01:41.864: INFO: Received response from host: affinity-nodeport-42wv4
Nov 15 07:01:41.864: INFO: Received response from host: affinity-nodeport-42wv4
Nov 15 07:01:41.864: INFO: Received response from host: affinity-nodeport-42wv4
Nov 15 07:01:41.864: INFO: Received response from host: affinity-nodeport-42wv4
Nov 15 07:01:41.864: INFO: Received response from host: affinity-nodeport-42wv4
Nov 15 07:01:41.864: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7638, will wait for the garbage collector to delete the pods 11/15/23 07:01:41.89
Nov 15 07:01:41.981: INFO: Deleting ReplicationController affinity-nodeport took: 26.628037ms
Nov 15 07:01:42.081: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.431929ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 07:01:45.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7638" for this suite. 11/15/23 07:01:45.132
------------------------------
â€¢ [SLOW TEST] [11.320 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:01:33.831
    Nov 15 07:01:33.831: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename services 11/15/23 07:01:33.832
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:01:33.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:01:33.903
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-7638 11/15/23 07:01:33.913
    STEP: creating service affinity-nodeport in namespace services-7638 11/15/23 07:01:33.913
    STEP: creating replication controller affinity-nodeport in namespace services-7638 11/15/23 07:01:33.982
    I1115 07:01:34.007831      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7638, replica count: 3
    I1115 07:01:37.058439      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 15 07:01:37.099: INFO: Creating new exec pod
    Nov 15 07:01:37.123: INFO: Waiting up to 5m0s for pod "execpod-affinitybkxhb" in namespace "services-7638" to be "running"
    Nov 15 07:01:37.133: INFO: Pod "execpod-affinitybkxhb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.430383ms
    Nov 15 07:01:39.146: INFO: Pod "execpod-affinitybkxhb": Phase="Running", Reason="", readiness=true. Elapsed: 2.023566251s
    Nov 15 07:01:39.146: INFO: Pod "execpod-affinitybkxhb" satisfied condition "running"
    Nov 15 07:01:40.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7638 exec execpod-affinitybkxhb -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Nov 15 07:01:40.494: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Nov 15 07:01:40.494: INFO: stdout: ""
    Nov 15 07:01:40.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7638 exec execpod-affinitybkxhb -- /bin/sh -x -c nc -v -z -w 2 172.21.245.91 80'
    Nov 15 07:01:40.816: INFO: stderr: "+ nc -v -z -w 2 172.21.245.91 80\nConnection to 172.21.245.91 80 port [tcp/http] succeeded!\n"
    Nov 15 07:01:40.816: INFO: stdout: ""
    Nov 15 07:01:40.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7638 exec execpod-affinitybkxhb -- /bin/sh -x -c nc -v -z -w 2 10.72.152.86 30018'
    Nov 15 07:01:41.136: INFO: stderr: "+ nc -v -z -w 2 10.72.152.86 30018\nConnection to 10.72.152.86 30018 port [tcp/*] succeeded!\n"
    Nov 15 07:01:41.136: INFO: stdout: ""
    Nov 15 07:01:41.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7638 exec execpod-affinitybkxhb -- /bin/sh -x -c nc -v -z -w 2 10.72.152.81 30018'
    Nov 15 07:01:41.418: INFO: stderr: "+ nc -v -z -w 2 10.72.152.81 30018\nConnection to 10.72.152.81 30018 port [tcp/*] succeeded!\n"
    Nov 15 07:01:41.418: INFO: stdout: ""
    Nov 15 07:01:41.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7638 exec execpod-affinitybkxhb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.72.152.81:30018/ ; done'
    Nov 15 07:01:41.863: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30018/\n"
    Nov 15 07:01:41.863: INFO: stdout: "\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4\naffinity-nodeport-42wv4"
    Nov 15 07:01:41.863: INFO: Received response from host: affinity-nodeport-42wv4
    Nov 15 07:01:41.863: INFO: Received response from host: affinity-nodeport-42wv4
    Nov 15 07:01:41.863: INFO: Received response from host: affinity-nodeport-42wv4
    Nov 15 07:01:41.863: INFO: Received response from host: affinity-nodeport-42wv4
    Nov 15 07:01:41.863: INFO: Received response from host: affinity-nodeport-42wv4
    Nov 15 07:01:41.863: INFO: Received response from host: affinity-nodeport-42wv4
    Nov 15 07:01:41.863: INFO: Received response from host: affinity-nodeport-42wv4
    Nov 15 07:01:41.863: INFO: Received response from host: affinity-nodeport-42wv4
    Nov 15 07:01:41.863: INFO: Received response from host: affinity-nodeport-42wv4
    Nov 15 07:01:41.864: INFO: Received response from host: affinity-nodeport-42wv4
    Nov 15 07:01:41.864: INFO: Received response from host: affinity-nodeport-42wv4
    Nov 15 07:01:41.864: INFO: Received response from host: affinity-nodeport-42wv4
    Nov 15 07:01:41.864: INFO: Received response from host: affinity-nodeport-42wv4
    Nov 15 07:01:41.864: INFO: Received response from host: affinity-nodeport-42wv4
    Nov 15 07:01:41.864: INFO: Received response from host: affinity-nodeport-42wv4
    Nov 15 07:01:41.864: INFO: Received response from host: affinity-nodeport-42wv4
    Nov 15 07:01:41.864: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-7638, will wait for the garbage collector to delete the pods 11/15/23 07:01:41.89
    Nov 15 07:01:41.981: INFO: Deleting ReplicationController affinity-nodeport took: 26.628037ms
    Nov 15 07:01:42.081: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.431929ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:01:45.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7638" for this suite. 11/15/23 07:01:45.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:01:45.152
Nov 15 07:01:45.152: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename emptydir 11/15/23 07:01:45.154
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:01:45.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:01:45.232
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 11/15/23 07:01:45.243
Nov 15 07:01:45.272: INFO: Waiting up to 5m0s for pod "pod-66694af6-c83e-4595-b5d7-5b7f39909153" in namespace "emptydir-1749" to be "Succeeded or Failed"
Nov 15 07:01:45.284: INFO: Pod "pod-66694af6-c83e-4595-b5d7-5b7f39909153": Phase="Pending", Reason="", readiness=false. Elapsed: 12.764645ms
Nov 15 07:01:47.297: INFO: Pod "pod-66694af6-c83e-4595-b5d7-5b7f39909153": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025521416s
Nov 15 07:01:49.298: INFO: Pod "pod-66694af6-c83e-4595-b5d7-5b7f39909153": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026217202s
STEP: Saw pod success 11/15/23 07:01:49.298
Nov 15 07:01:49.298: INFO: Pod "pod-66694af6-c83e-4595-b5d7-5b7f39909153" satisfied condition "Succeeded or Failed"
Nov 15 07:01:49.309: INFO: Trying to get logs from node 10.72.152.86 pod pod-66694af6-c83e-4595-b5d7-5b7f39909153 container test-container: <nil>
STEP: delete the pod 11/15/23 07:01:49.337
Nov 15 07:01:49.365: INFO: Waiting for pod pod-66694af6-c83e-4595-b5d7-5b7f39909153 to disappear
Nov 15 07:01:49.376: INFO: Pod pod-66694af6-c83e-4595-b5d7-5b7f39909153 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 07:01:49.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1749" for this suite. 11/15/23 07:01:49.395
------------------------------
â€¢ [4.264 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:01:45.152
    Nov 15 07:01:45.152: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename emptydir 11/15/23 07:01:45.154
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:01:45.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:01:45.232
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 11/15/23 07:01:45.243
    Nov 15 07:01:45.272: INFO: Waiting up to 5m0s for pod "pod-66694af6-c83e-4595-b5d7-5b7f39909153" in namespace "emptydir-1749" to be "Succeeded or Failed"
    Nov 15 07:01:45.284: INFO: Pod "pod-66694af6-c83e-4595-b5d7-5b7f39909153": Phase="Pending", Reason="", readiness=false. Elapsed: 12.764645ms
    Nov 15 07:01:47.297: INFO: Pod "pod-66694af6-c83e-4595-b5d7-5b7f39909153": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025521416s
    Nov 15 07:01:49.298: INFO: Pod "pod-66694af6-c83e-4595-b5d7-5b7f39909153": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026217202s
    STEP: Saw pod success 11/15/23 07:01:49.298
    Nov 15 07:01:49.298: INFO: Pod "pod-66694af6-c83e-4595-b5d7-5b7f39909153" satisfied condition "Succeeded or Failed"
    Nov 15 07:01:49.309: INFO: Trying to get logs from node 10.72.152.86 pod pod-66694af6-c83e-4595-b5d7-5b7f39909153 container test-container: <nil>
    STEP: delete the pod 11/15/23 07:01:49.337
    Nov 15 07:01:49.365: INFO: Waiting for pod pod-66694af6-c83e-4595-b5d7-5b7f39909153 to disappear
    Nov 15 07:01:49.376: INFO: Pod pod-66694af6-c83e-4595-b5d7-5b7f39909153 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:01:49.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1749" for this suite. 11/15/23 07:01:49.395
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:01:49.416
Nov 15 07:01:49.416: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename endpointslice 11/15/23 07:01:49.417
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:01:49.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:01:49.484
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Nov 15 07:01:53.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-6320" for this suite. 11/15/23 07:01:53.74
------------------------------
â€¢ [4.344 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:01:49.416
    Nov 15 07:01:49.416: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename endpointslice 11/15/23 07:01:49.417
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:01:49.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:01:49.484
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:01:53.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-6320" for this suite. 11/15/23 07:01:53.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:01:53.763
Nov 15 07:01:53.763: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename endpointslicemirroring 11/15/23 07:01:53.764
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:01:53.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:01:53.837
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 11/15/23 07:01:53.901
Nov 15 07:01:53.942: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 11/15/23 07:01:55.957
STEP: mirroring deletion of a custom Endpoint 11/15/23 07:01:56.001
Nov 15 07:01:56.062: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Nov 15 07:01:58.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-5631" for this suite. 11/15/23 07:01:58.097
------------------------------
â€¢ [4.354 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:01:53.763
    Nov 15 07:01:53.763: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename endpointslicemirroring 11/15/23 07:01:53.764
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:01:53.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:01:53.837
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 11/15/23 07:01:53.901
    Nov 15 07:01:53.942: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 11/15/23 07:01:55.957
    STEP: mirroring deletion of a custom Endpoint 11/15/23 07:01:56.001
    Nov 15 07:01:56.062: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:01:58.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-5631" for this suite. 11/15/23 07:01:58.097
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:01:58.118
Nov 15 07:01:58.118: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename crd-webhook 11/15/23 07:01:58.119
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:01:58.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:01:58.185
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 11/15/23 07:01:58.196
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 11/15/23 07:01:58.507
STEP: Deploying the custom resource conversion webhook pod 11/15/23 07:01:58.544
STEP: Wait for the deployment to be ready 11/15/23 07:01:58.577
Nov 15 07:01:58.600: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Nov 15 07:02:00.662: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 1, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 1, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 1, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 1, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 07:02:02.677
STEP: Verifying the service has paired with the endpoint 11/15/23 07:02:02.712
Nov 15 07:02:03.713: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Nov 15 07:02:03.726: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Creating a v1 custom resource 11/15/23 07:02:06.454
STEP: Create a v2 custom resource 11/15/23 07:02:06.495
STEP: List CRs in v1 11/15/23 07:02:06.631
STEP: List CRs in v2 11/15/23 07:02:06.667
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:02:07.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-6679" for this suite. 11/15/23 07:02:07.447
------------------------------
â€¢ [SLOW TEST] [9.367 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:01:58.118
    Nov 15 07:01:58.118: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename crd-webhook 11/15/23 07:01:58.119
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:01:58.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:01:58.185
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 11/15/23 07:01:58.196
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 11/15/23 07:01:58.507
    STEP: Deploying the custom resource conversion webhook pod 11/15/23 07:01:58.544
    STEP: Wait for the deployment to be ready 11/15/23 07:01:58.577
    Nov 15 07:01:58.600: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    Nov 15 07:02:00.662: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 1, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 1, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 1, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 1, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 07:02:02.677
    STEP: Verifying the service has paired with the endpoint 11/15/23 07:02:02.712
    Nov 15 07:02:03.713: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Nov 15 07:02:03.726: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Creating a v1 custom resource 11/15/23 07:02:06.454
    STEP: Create a v2 custom resource 11/15/23 07:02:06.495
    STEP: List CRs in v1 11/15/23 07:02:06.631
    STEP: List CRs in v2 11/15/23 07:02:06.667
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:02:07.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-6679" for this suite. 11/15/23 07:02:07.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:02:07.486
Nov 15 07:02:07.486: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename namespaces 11/15/23 07:02:07.487
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:07.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:07.553
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 11/15/23 07:02:07.575
STEP: patching the Namespace 11/15/23 07:02:07.661
STEP: get the Namespace and ensuring it has the label 11/15/23 07:02:07.676
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:02:07.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4342" for this suite. 11/15/23 07:02:07.721
STEP: Destroying namespace "nspatchtest-54226ddf-dc06-41a8-bd21-359c4e5e0539-6714" for this suite. 11/15/23 07:02:07.753
------------------------------
â€¢ [0.298 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:02:07.486
    Nov 15 07:02:07.486: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename namespaces 11/15/23 07:02:07.487
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:07.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:07.553
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 11/15/23 07:02:07.575
    STEP: patching the Namespace 11/15/23 07:02:07.661
    STEP: get the Namespace and ensuring it has the label 11/15/23 07:02:07.676
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:02:07.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4342" for this suite. 11/15/23 07:02:07.721
    STEP: Destroying namespace "nspatchtest-54226ddf-dc06-41a8-bd21-359c4e5e0539-6714" for this suite. 11/15/23 07:02:07.753
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:02:07.785
Nov 15 07:02:07.785: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename gc 11/15/23 07:02:07.786
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:07.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:07.869
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 11/15/23 07:02:07.886
STEP: Wait for the Deployment to create new ReplicaSet 11/15/23 07:02:07.901
STEP: delete the deployment 11/15/23 07:02:08.444
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 11/15/23 07:02:08.468
STEP: Gathering metrics 11/15/23 07:02:09.042
W1115 07:02:09.066608      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 15 07:02:09.066: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 15 07:02:09.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2290" for this suite. 11/15/23 07:02:09.08
------------------------------
â€¢ [1.317 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:02:07.785
    Nov 15 07:02:07.785: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename gc 11/15/23 07:02:07.786
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:07.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:07.869
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 11/15/23 07:02:07.886
    STEP: Wait for the Deployment to create new ReplicaSet 11/15/23 07:02:07.901
    STEP: delete the deployment 11/15/23 07:02:08.444
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 11/15/23 07:02:08.468
    STEP: Gathering metrics 11/15/23 07:02:09.042
    W1115 07:02:09.066608      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Nov 15 07:02:09.066: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:02:09.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2290" for this suite. 11/15/23 07:02:09.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:02:09.109
Nov 15 07:02:09.110: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename services 11/15/23 07:02:09.11
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:09.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:09.183
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7495 11/15/23 07:02:09.194
STEP: changing the ExternalName service to type=NodePort 11/15/23 07:02:09.207
STEP: creating replication controller externalname-service in namespace services-7495 11/15/23 07:02:09.306
I1115 07:02:09.331218      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7495, replica count: 2
I1115 07:02:12.382647      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 07:02:12.382: INFO: Creating new exec pod
Nov 15 07:02:12.406: INFO: Waiting up to 5m0s for pod "execpod6f6fh" in namespace "services-7495" to be "running"
Nov 15 07:02:12.418: INFO: Pod "execpod6f6fh": Phase="Pending", Reason="", readiness=false. Elapsed: 11.887952ms
Nov 15 07:02:14.434: INFO: Pod "execpod6f6fh": Phase="Running", Reason="", readiness=true. Elapsed: 2.027602705s
Nov 15 07:02:14.434: INFO: Pod "execpod6f6fh" satisfied condition "running"
Nov 15 07:02:15.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7495 exec execpod6f6fh -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Nov 15 07:02:15.834: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 15 07:02:15.834: INFO: stdout: ""
Nov 15 07:02:15.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7495 exec execpod6f6fh -- /bin/sh -x -c nc -v -z -w 2 172.21.247.96 80'
Nov 15 07:02:16.141: INFO: stderr: "+ nc -v -z -w 2 172.21.247.96 80\nConnection to 172.21.247.96 80 port [tcp/http] succeeded!\n"
Nov 15 07:02:16.141: INFO: stdout: ""
Nov 15 07:02:16.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7495 exec execpod6f6fh -- /bin/sh -x -c nc -v -z -w 2 10.72.152.86 32326'
Nov 15 07:02:16.400: INFO: stderr: "+ nc -v -z -w 2 10.72.152.86 32326\nConnection to 10.72.152.86 32326 port [tcp/*] succeeded!\n"
Nov 15 07:02:16.400: INFO: stdout: ""
Nov 15 07:02:16.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7495 exec execpod6f6fh -- /bin/sh -x -c nc -v -z -w 2 10.72.152.81 32326'
Nov 15 07:02:16.704: INFO: stderr: "+ nc -v -z -w 2 10.72.152.81 32326\nConnection to 10.72.152.81 32326 port [tcp/*] succeeded!\n"
Nov 15 07:02:16.704: INFO: stdout: ""
Nov 15 07:02:16.704: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 07:02:16.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7495" for this suite. 11/15/23 07:02:16.801
------------------------------
â€¢ [SLOW TEST] [7.711 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:02:09.109
    Nov 15 07:02:09.110: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename services 11/15/23 07:02:09.11
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:09.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:09.183
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7495 11/15/23 07:02:09.194
    STEP: changing the ExternalName service to type=NodePort 11/15/23 07:02:09.207
    STEP: creating replication controller externalname-service in namespace services-7495 11/15/23 07:02:09.306
    I1115 07:02:09.331218      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7495, replica count: 2
    I1115 07:02:12.382647      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 15 07:02:12.382: INFO: Creating new exec pod
    Nov 15 07:02:12.406: INFO: Waiting up to 5m0s for pod "execpod6f6fh" in namespace "services-7495" to be "running"
    Nov 15 07:02:12.418: INFO: Pod "execpod6f6fh": Phase="Pending", Reason="", readiness=false. Elapsed: 11.887952ms
    Nov 15 07:02:14.434: INFO: Pod "execpod6f6fh": Phase="Running", Reason="", readiness=true. Elapsed: 2.027602705s
    Nov 15 07:02:14.434: INFO: Pod "execpod6f6fh" satisfied condition "running"
    Nov 15 07:02:15.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7495 exec execpod6f6fh -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Nov 15 07:02:15.834: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Nov 15 07:02:15.834: INFO: stdout: ""
    Nov 15 07:02:15.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7495 exec execpod6f6fh -- /bin/sh -x -c nc -v -z -w 2 172.21.247.96 80'
    Nov 15 07:02:16.141: INFO: stderr: "+ nc -v -z -w 2 172.21.247.96 80\nConnection to 172.21.247.96 80 port [tcp/http] succeeded!\n"
    Nov 15 07:02:16.141: INFO: stdout: ""
    Nov 15 07:02:16.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7495 exec execpod6f6fh -- /bin/sh -x -c nc -v -z -w 2 10.72.152.86 32326'
    Nov 15 07:02:16.400: INFO: stderr: "+ nc -v -z -w 2 10.72.152.86 32326\nConnection to 10.72.152.86 32326 port [tcp/*] succeeded!\n"
    Nov 15 07:02:16.400: INFO: stdout: ""
    Nov 15 07:02:16.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7495 exec execpod6f6fh -- /bin/sh -x -c nc -v -z -w 2 10.72.152.81 32326'
    Nov 15 07:02:16.704: INFO: stderr: "+ nc -v -z -w 2 10.72.152.81 32326\nConnection to 10.72.152.81 32326 port [tcp/*] succeeded!\n"
    Nov 15 07:02:16.704: INFO: stdout: ""
    Nov 15 07:02:16.704: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:02:16.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7495" for this suite. 11/15/23 07:02:16.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:02:16.822
Nov 15 07:02:16.822: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename events 11/15/23 07:02:16.823
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:16.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:16.907
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 11/15/23 07:02:16.917
STEP: listing events in all namespaces 11/15/23 07:02:16.951
STEP: listing events in test namespace 11/15/23 07:02:17.036
STEP: listing events with field selection filtering on source 11/15/23 07:02:17.05
STEP: listing events with field selection filtering on reportingController 11/15/23 07:02:17.065
STEP: getting the test event 11/15/23 07:02:17.079
STEP: patching the test event 11/15/23 07:02:17.093
STEP: getting the test event 11/15/23 07:02:17.122
STEP: updating the test event 11/15/23 07:02:17.138
STEP: getting the test event 11/15/23 07:02:17.169
STEP: deleting the test event 11/15/23 07:02:17.181
STEP: listing events in all namespaces 11/15/23 07:02:17.215
STEP: listing events in test namespace 11/15/23 07:02:17.286
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Nov 15 07:02:17.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4726" for this suite. 11/15/23 07:02:17.319
------------------------------
â€¢ [0.519 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:02:16.822
    Nov 15 07:02:16.822: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename events 11/15/23 07:02:16.823
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:16.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:16.907
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 11/15/23 07:02:16.917
    STEP: listing events in all namespaces 11/15/23 07:02:16.951
    STEP: listing events in test namespace 11/15/23 07:02:17.036
    STEP: listing events with field selection filtering on source 11/15/23 07:02:17.05
    STEP: listing events with field selection filtering on reportingController 11/15/23 07:02:17.065
    STEP: getting the test event 11/15/23 07:02:17.079
    STEP: patching the test event 11/15/23 07:02:17.093
    STEP: getting the test event 11/15/23 07:02:17.122
    STEP: updating the test event 11/15/23 07:02:17.138
    STEP: getting the test event 11/15/23 07:02:17.169
    STEP: deleting the test event 11/15/23 07:02:17.181
    STEP: listing events in all namespaces 11/15/23 07:02:17.215
    STEP: listing events in test namespace 11/15/23 07:02:17.286
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:02:17.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4726" for this suite. 11/15/23 07:02:17.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:02:17.347
Nov 15 07:02:17.347: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 07:02:17.348
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:17.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:17.421
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-d49b76f1-10de-4726-9217-0ba281523900 11/15/23 07:02:17.431
STEP: Creating a pod to test consume secrets 11/15/23 07:02:17.467
Nov 15 07:02:17.508: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b3bbb4a0-b491-4408-a653-eac95108cb3d" in namespace "projected-5369" to be "Succeeded or Failed"
Nov 15 07:02:17.527: INFO: Pod "pod-projected-secrets-b3bbb4a0-b491-4408-a653-eac95108cb3d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.395057ms
Nov 15 07:02:19.540: INFO: Pod "pod-projected-secrets-b3bbb4a0-b491-4408-a653-eac95108cb3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031780861s
Nov 15 07:02:21.538: INFO: Pod "pod-projected-secrets-b3bbb4a0-b491-4408-a653-eac95108cb3d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030143269s
Nov 15 07:02:23.539: INFO: Pod "pod-projected-secrets-b3bbb4a0-b491-4408-a653-eac95108cb3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030960299s
STEP: Saw pod success 11/15/23 07:02:23.539
Nov 15 07:02:23.539: INFO: Pod "pod-projected-secrets-b3bbb4a0-b491-4408-a653-eac95108cb3d" satisfied condition "Succeeded or Failed"
Nov 15 07:02:23.550: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-secrets-b3bbb4a0-b491-4408-a653-eac95108cb3d container projected-secret-volume-test: <nil>
STEP: delete the pod 11/15/23 07:02:23.58
Nov 15 07:02:23.613: INFO: Waiting for pod pod-projected-secrets-b3bbb4a0-b491-4408-a653-eac95108cb3d to disappear
Nov 15 07:02:23.625: INFO: Pod pod-projected-secrets-b3bbb4a0-b491-4408-a653-eac95108cb3d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 15 07:02:23.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5369" for this suite. 11/15/23 07:02:23.642
------------------------------
â€¢ [SLOW TEST] [6.317 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:02:17.347
    Nov 15 07:02:17.347: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 07:02:17.348
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:17.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:17.421
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-d49b76f1-10de-4726-9217-0ba281523900 11/15/23 07:02:17.431
    STEP: Creating a pod to test consume secrets 11/15/23 07:02:17.467
    Nov 15 07:02:17.508: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b3bbb4a0-b491-4408-a653-eac95108cb3d" in namespace "projected-5369" to be "Succeeded or Failed"
    Nov 15 07:02:17.527: INFO: Pod "pod-projected-secrets-b3bbb4a0-b491-4408-a653-eac95108cb3d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.395057ms
    Nov 15 07:02:19.540: INFO: Pod "pod-projected-secrets-b3bbb4a0-b491-4408-a653-eac95108cb3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031780861s
    Nov 15 07:02:21.538: INFO: Pod "pod-projected-secrets-b3bbb4a0-b491-4408-a653-eac95108cb3d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030143269s
    Nov 15 07:02:23.539: INFO: Pod "pod-projected-secrets-b3bbb4a0-b491-4408-a653-eac95108cb3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030960299s
    STEP: Saw pod success 11/15/23 07:02:23.539
    Nov 15 07:02:23.539: INFO: Pod "pod-projected-secrets-b3bbb4a0-b491-4408-a653-eac95108cb3d" satisfied condition "Succeeded or Failed"
    Nov 15 07:02:23.550: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-secrets-b3bbb4a0-b491-4408-a653-eac95108cb3d container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 07:02:23.58
    Nov 15 07:02:23.613: INFO: Waiting for pod pod-projected-secrets-b3bbb4a0-b491-4408-a653-eac95108cb3d to disappear
    Nov 15 07:02:23.625: INFO: Pod pod-projected-secrets-b3bbb4a0-b491-4408-a653-eac95108cb3d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:02:23.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5369" for this suite. 11/15/23 07:02:23.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:02:23.667
Nov 15 07:02:23.667: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 07:02:23.668
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:23.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:23.737
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Nov 15 07:02:23.748: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/15/23 07:02:29.432
Nov 15 07:02:29.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-9982 --namespace=crd-publish-openapi-9982 create -f -'
Nov 15 07:02:31.131: INFO: stderr: ""
Nov 15 07:02:31.131: INFO: stdout: "e2e-test-crd-publish-openapi-4753-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 15 07:02:31.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-9982 --namespace=crd-publish-openapi-9982 delete e2e-test-crd-publish-openapi-4753-crds test-cr'
Nov 15 07:02:31.369: INFO: stderr: ""
Nov 15 07:02:31.369: INFO: stdout: "e2e-test-crd-publish-openapi-4753-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Nov 15 07:02:31.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-9982 --namespace=crd-publish-openapi-9982 apply -f -'
Nov 15 07:02:32.794: INFO: stderr: ""
Nov 15 07:02:32.794: INFO: stdout: "e2e-test-crd-publish-openapi-4753-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 15 07:02:32.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-9982 --namespace=crd-publish-openapi-9982 delete e2e-test-crd-publish-openapi-4753-crds test-cr'
Nov 15 07:02:32.952: INFO: stderr: ""
Nov 15 07:02:32.952: INFO: stdout: "e2e-test-crd-publish-openapi-4753-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 11/15/23 07:02:32.952
Nov 15 07:02:32.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-9982 explain e2e-test-crd-publish-openapi-4753-crds'
Nov 15 07:02:34.198: INFO: stderr: ""
Nov 15 07:02:34.198: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4753-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:02:39.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9982" for this suite. 11/15/23 07:02:39.159
------------------------------
â€¢ [SLOW TEST] [15.520 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:02:23.667
    Nov 15 07:02:23.667: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 07:02:23.668
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:23.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:23.737
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Nov 15 07:02:23.748: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/15/23 07:02:29.432
    Nov 15 07:02:29.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-9982 --namespace=crd-publish-openapi-9982 create -f -'
    Nov 15 07:02:31.131: INFO: stderr: ""
    Nov 15 07:02:31.131: INFO: stdout: "e2e-test-crd-publish-openapi-4753-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Nov 15 07:02:31.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-9982 --namespace=crd-publish-openapi-9982 delete e2e-test-crd-publish-openapi-4753-crds test-cr'
    Nov 15 07:02:31.369: INFO: stderr: ""
    Nov 15 07:02:31.369: INFO: stdout: "e2e-test-crd-publish-openapi-4753-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Nov 15 07:02:31.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-9982 --namespace=crd-publish-openapi-9982 apply -f -'
    Nov 15 07:02:32.794: INFO: stderr: ""
    Nov 15 07:02:32.794: INFO: stdout: "e2e-test-crd-publish-openapi-4753-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Nov 15 07:02:32.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-9982 --namespace=crd-publish-openapi-9982 delete e2e-test-crd-publish-openapi-4753-crds test-cr'
    Nov 15 07:02:32.952: INFO: stderr: ""
    Nov 15 07:02:32.952: INFO: stdout: "e2e-test-crd-publish-openapi-4753-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 11/15/23 07:02:32.952
    Nov 15 07:02:32.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-9982 explain e2e-test-crd-publish-openapi-4753-crds'
    Nov 15 07:02:34.198: INFO: stderr: ""
    Nov 15 07:02:34.198: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4753-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:02:39.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9982" for this suite. 11/15/23 07:02:39.159
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:02:39.187
Nov 15 07:02:39.187: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename endpointslice 11/15/23 07:02:39.188
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:39.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:39.262
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Nov 15 07:02:39.348: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
Nov 15 07:02:39.348: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Nov 15 07:02:39.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5385" for this suite. 11/15/23 07:02:39.385
------------------------------
â€¢ [0.227 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:02:39.187
    Nov 15 07:02:39.187: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename endpointslice 11/15/23 07:02:39.188
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:39.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:39.262
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Nov 15 07:02:39.348: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
    Nov 15 07:02:39.348: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:02:39.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5385" for this suite. 11/15/23 07:02:39.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:02:39.415
Nov 15 07:02:39.416: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename replication-controller 11/15/23 07:02:39.417
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:39.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:39.491
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-lczcz" 11/15/23 07:02:39.513
W1115 07:02:39.541562      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Nov 15 07:02:39.541: INFO: Get Replication Controller "e2e-rc-lczcz" to confirm replicas
Nov 15 07:02:40.561: INFO: Get Replication Controller "e2e-rc-lczcz" to confirm replicas
Nov 15 07:02:40.591: INFO: Found 1 replicas for "e2e-rc-lczcz" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-lczcz" 11/15/23 07:02:40.591
STEP: Updating a scale subresource 11/15/23 07:02:40.639
STEP: Verifying replicas where modified for replication controller "e2e-rc-lczcz" 11/15/23 07:02:40.658
Nov 15 07:02:40.658: INFO: Get Replication Controller "e2e-rc-lczcz" to confirm replicas
Nov 15 07:02:41.675: INFO: Get Replication Controller "e2e-rc-lczcz" to confirm replicas
Nov 15 07:02:41.693: INFO: Found 2 replicas for "e2e-rc-lczcz" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Nov 15 07:02:41.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4140" for this suite. 11/15/23 07:02:41.728
------------------------------
â€¢ [2.339 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:02:39.415
    Nov 15 07:02:39.416: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename replication-controller 11/15/23 07:02:39.417
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:39.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:39.491
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-lczcz" 11/15/23 07:02:39.513
    W1115 07:02:39.541562      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Nov 15 07:02:39.541: INFO: Get Replication Controller "e2e-rc-lczcz" to confirm replicas
    Nov 15 07:02:40.561: INFO: Get Replication Controller "e2e-rc-lczcz" to confirm replicas
    Nov 15 07:02:40.591: INFO: Found 1 replicas for "e2e-rc-lczcz" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-lczcz" 11/15/23 07:02:40.591
    STEP: Updating a scale subresource 11/15/23 07:02:40.639
    STEP: Verifying replicas where modified for replication controller "e2e-rc-lczcz" 11/15/23 07:02:40.658
    Nov 15 07:02:40.658: INFO: Get Replication Controller "e2e-rc-lczcz" to confirm replicas
    Nov 15 07:02:41.675: INFO: Get Replication Controller "e2e-rc-lczcz" to confirm replicas
    Nov 15 07:02:41.693: INFO: Found 2 replicas for "e2e-rc-lczcz" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:02:41.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4140" for this suite. 11/15/23 07:02:41.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:02:41.759
Nov 15 07:02:41.760: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 07:02:41.761
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:41.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:41.833
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 11/15/23 07:02:41.847
Nov 15 07:02:41.927: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c793f678-9c40-4dae-bd13-2da93202fb17" in namespace "projected-4093" to be "Succeeded or Failed"
Nov 15 07:02:41.972: INFO: Pod "downwardapi-volume-c793f678-9c40-4dae-bd13-2da93202fb17": Phase="Pending", Reason="", readiness=false. Elapsed: 44.749221ms
Nov 15 07:02:43.990: INFO: Pod "downwardapi-volume-c793f678-9c40-4dae-bd13-2da93202fb17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063471042s
Nov 15 07:02:45.993: INFO: Pod "downwardapi-volume-c793f678-9c40-4dae-bd13-2da93202fb17": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066011669s
Nov 15 07:02:48.004: INFO: Pod "downwardapi-volume-c793f678-9c40-4dae-bd13-2da93202fb17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.077185539s
STEP: Saw pod success 11/15/23 07:02:48.004
Nov 15 07:02:48.004: INFO: Pod "downwardapi-volume-c793f678-9c40-4dae-bd13-2da93202fb17" satisfied condition "Succeeded or Failed"
Nov 15 07:02:48.049: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-c793f678-9c40-4dae-bd13-2da93202fb17 container client-container: <nil>
STEP: delete the pod 11/15/23 07:02:48.12
Nov 15 07:02:48.184: INFO: Waiting for pod downwardapi-volume-c793f678-9c40-4dae-bd13-2da93202fb17 to disappear
Nov 15 07:02:48.202: INFO: Pod downwardapi-volume-c793f678-9c40-4dae-bd13-2da93202fb17 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 07:02:48.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4093" for this suite. 11/15/23 07:02:48.237
------------------------------
â€¢ [SLOW TEST] [6.508 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:02:41.759
    Nov 15 07:02:41.760: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 07:02:41.761
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:41.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:41.833
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 11/15/23 07:02:41.847
    Nov 15 07:02:41.927: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c793f678-9c40-4dae-bd13-2da93202fb17" in namespace "projected-4093" to be "Succeeded or Failed"
    Nov 15 07:02:41.972: INFO: Pod "downwardapi-volume-c793f678-9c40-4dae-bd13-2da93202fb17": Phase="Pending", Reason="", readiness=false. Elapsed: 44.749221ms
    Nov 15 07:02:43.990: INFO: Pod "downwardapi-volume-c793f678-9c40-4dae-bd13-2da93202fb17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063471042s
    Nov 15 07:02:45.993: INFO: Pod "downwardapi-volume-c793f678-9c40-4dae-bd13-2da93202fb17": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066011669s
    Nov 15 07:02:48.004: INFO: Pod "downwardapi-volume-c793f678-9c40-4dae-bd13-2da93202fb17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.077185539s
    STEP: Saw pod success 11/15/23 07:02:48.004
    Nov 15 07:02:48.004: INFO: Pod "downwardapi-volume-c793f678-9c40-4dae-bd13-2da93202fb17" satisfied condition "Succeeded or Failed"
    Nov 15 07:02:48.049: INFO: Trying to get logs from node 10.72.152.86 pod downwardapi-volume-c793f678-9c40-4dae-bd13-2da93202fb17 container client-container: <nil>
    STEP: delete the pod 11/15/23 07:02:48.12
    Nov 15 07:02:48.184: INFO: Waiting for pod downwardapi-volume-c793f678-9c40-4dae-bd13-2da93202fb17 to disappear
    Nov 15 07:02:48.202: INFO: Pod downwardapi-volume-c793f678-9c40-4dae-bd13-2da93202fb17 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:02:48.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4093" for this suite. 11/15/23 07:02:48.237
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:02:48.269
Nov 15 07:02:48.269: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename webhook 11/15/23 07:02:48.27
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:48.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:48.337
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 07:02:48.422
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 07:02:48.683
STEP: Deploying the webhook pod 11/15/23 07:02:48.743
STEP: Wait for the deployment to be ready 11/15/23 07:02:48.78
Nov 15 07:02:48.823: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 07:02:50.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 2, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 2, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 2, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 2, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 07:02:52.88
STEP: Verifying the service has paired with the endpoint 11/15/23 07:02:52.913
Nov 15 07:02:53.914: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 11/15/23 07:02:53.935
STEP: Creating a custom resource definition that should be denied by the webhook 11/15/23 07:02:54.012
Nov 15 07:02:54.012: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:02:54.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-412" for this suite. 11/15/23 07:02:54.261
STEP: Destroying namespace "webhook-412-markers" for this suite. 11/15/23 07:02:54.29
------------------------------
â€¢ [SLOW TEST] [6.051 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:02:48.269
    Nov 15 07:02:48.269: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename webhook 11/15/23 07:02:48.27
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:48.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:48.337
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 07:02:48.422
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 07:02:48.683
    STEP: Deploying the webhook pod 11/15/23 07:02:48.743
    STEP: Wait for the deployment to be ready 11/15/23 07:02:48.78
    Nov 15 07:02:48.823: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 07:02:50.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 2, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 2, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 2, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 2, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 07:02:52.88
    STEP: Verifying the service has paired with the endpoint 11/15/23 07:02:52.913
    Nov 15 07:02:53.914: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 11/15/23 07:02:53.935
    STEP: Creating a custom resource definition that should be denied by the webhook 11/15/23 07:02:54.012
    Nov 15 07:02:54.012: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:02:54.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-412" for this suite. 11/15/23 07:02:54.261
    STEP: Destroying namespace "webhook-412-markers" for this suite. 11/15/23 07:02:54.29
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:02:54.321
Nov 15 07:02:54.321: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename svc-latency 11/15/23 07:02:54.322
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:54.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:54.407
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Nov 15 07:02:54.420: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5415 11/15/23 07:02:54.421
I1115 07:02:54.442032      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5415, replica count: 1
I1115 07:02:55.492740      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1115 07:02:56.492983      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1115 07:02:57.493858      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 07:02:57.632: INFO: Created: latency-svc-h79x2
Nov 15 07:02:57.649: INFO: Got endpoints: latency-svc-h79x2 [53.749033ms]
Nov 15 07:02:57.702: INFO: Created: latency-svc-6tztk
Nov 15 07:02:57.714: INFO: Created: latency-svc-s9kbk
Nov 15 07:02:57.723: INFO: Got endpoints: latency-svc-6tztk [73.230018ms]
Nov 15 07:02:57.724: INFO: Created: latency-svc-fmjnl
Nov 15 07:02:57.735: INFO: Got endpoints: latency-svc-s9kbk [84.714594ms]
Nov 15 07:02:57.735: INFO: Created: latency-svc-vhgw5
Nov 15 07:02:57.745: INFO: Got endpoints: latency-svc-fmjnl [95.112616ms]
Nov 15 07:02:57.771: INFO: Got endpoints: latency-svc-vhgw5 [120.038673ms]
Nov 15 07:02:57.773: INFO: Created: latency-svc-47b4z
Nov 15 07:02:57.785: INFO: Created: latency-svc-qmqzh
Nov 15 07:02:57.787: INFO: Created: latency-svc-rckpw
Nov 15 07:02:57.802: INFO: Got endpoints: latency-svc-qmqzh [150.358284ms]
Nov 15 07:02:57.803: INFO: Got endpoints: latency-svc-47b4z [151.034533ms]
Nov 15 07:02:57.807: INFO: Got endpoints: latency-svc-rckpw [155.111352ms]
Nov 15 07:02:57.809: INFO: Created: latency-svc-fqrj8
Nov 15 07:02:57.820: INFO: Created: latency-svc-ch7pf
Nov 15 07:02:57.824: INFO: Got endpoints: latency-svc-fqrj8 [170.975316ms]
Nov 15 07:02:57.832: INFO: Created: latency-svc-929hd
Nov 15 07:02:57.834: INFO: Got endpoints: latency-svc-ch7pf [181.196403ms]
Nov 15 07:02:57.845: INFO: Created: latency-svc-z9xmd
Nov 15 07:02:57.847: INFO: Got endpoints: latency-svc-929hd [193.909341ms]
Nov 15 07:02:57.858: INFO: Created: latency-svc-8llp9
Nov 15 07:02:57.859: INFO: Got endpoints: latency-svc-z9xmd [205.568544ms]
Nov 15 07:02:57.872: INFO: Got endpoints: latency-svc-8llp9 [218.754782ms]
Nov 15 07:02:58.034: INFO: Created: latency-svc-pvrjk
Nov 15 07:02:58.035: INFO: Created: latency-svc-2m8dk
Nov 15 07:02:58.035: INFO: Created: latency-svc-9958b
Nov 15 07:02:58.035: INFO: Created: latency-svc-zt7wr
Nov 15 07:02:58.035: INFO: Created: latency-svc-bhf5f
Nov 15 07:02:58.037: INFO: Created: latency-svc-pncfz
Nov 15 07:02:58.037: INFO: Created: latency-svc-69mc6
Nov 15 07:02:58.037: INFO: Created: latency-svc-49tl5
Nov 15 07:02:58.038: INFO: Created: latency-svc-4z5gk
Nov 15 07:02:58.038: INFO: Created: latency-svc-vbfh5
Nov 15 07:02:58.038: INFO: Created: latency-svc-jvtwc
Nov 15 07:02:58.039: INFO: Created: latency-svc-fdmhg
Nov 15 07:02:58.039: INFO: Created: latency-svc-f9vsb
Nov 15 07:02:58.039: INFO: Created: latency-svc-d6bgt
Nov 15 07:02:58.040: INFO: Created: latency-svc-7jkbj
Nov 15 07:02:58.050: INFO: Got endpoints: latency-svc-9958b [327.507208ms]
Nov 15 07:02:58.051: INFO: Got endpoints: latency-svc-2m8dk [192.788032ms]
Nov 15 07:02:58.052: INFO: Got endpoints: latency-svc-zt7wr [399.365682ms]
Nov 15 07:02:58.053: INFO: Got endpoints: latency-svc-pvrjk [218.214708ms]
Nov 15 07:02:58.061: INFO: Got endpoints: latency-svc-7jkbj [316.194244ms]
Nov 15 07:02:58.078: INFO: Got endpoints: latency-svc-69mc6 [275.185096ms]
Nov 15 07:02:58.078: INFO: Got endpoints: latency-svc-bhf5f [307.114313ms]
Nov 15 07:02:58.078: INFO: Got endpoints: latency-svc-f9vsb [275.392309ms]
Nov 15 07:02:58.078: INFO: Got endpoints: latency-svc-pncfz [270.901452ms]
Nov 15 07:02:58.084: INFO: Got endpoints: latency-svc-49tl5 [259.889419ms]
Nov 15 07:02:58.089: INFO: Got endpoints: latency-svc-vbfh5 [242.006236ms]
Nov 15 07:02:58.090: INFO: Created: latency-svc-4zmkh
Nov 15 07:02:58.091: INFO: Got endpoints: latency-svc-jvtwc [356.275239ms]
Nov 15 07:02:58.093: INFO: Got endpoints: latency-svc-4z5gk [220.494598ms]
Nov 15 07:02:58.093: INFO: Got endpoints: latency-svc-d6bgt [440.281511ms]
Nov 15 07:02:58.101: INFO: Got endpoints: latency-svc-fdmhg [448.41999ms]
Nov 15 07:02:58.102: INFO: Created: latency-svc-c65nf
Nov 15 07:02:58.109: INFO: Got endpoints: latency-svc-4zmkh [58.017897ms]
Nov 15 07:02:58.114: INFO: Created: latency-svc-rtkdt
Nov 15 07:02:58.126: INFO: Got endpoints: latency-svc-c65nf [73.467642ms]
Nov 15 07:02:58.127: INFO: Created: latency-svc-jxvxg
Nov 15 07:02:58.134: INFO: Got endpoints: latency-svc-rtkdt [81.861681ms]
Nov 15 07:02:58.141: INFO: Created: latency-svc-jxhn9
Nov 15 07:02:58.151: INFO: Got endpoints: latency-svc-jxvxg [99.986851ms]
Nov 15 07:02:58.171: INFO: Created: latency-svc-4p822
Nov 15 07:02:58.171: INFO: Got endpoints: latency-svc-jxhn9 [109.665747ms]
Nov 15 07:02:58.189: INFO: Got endpoints: latency-svc-4p822 [110.490729ms]
Nov 15 07:02:58.191: INFO: Created: latency-svc-rcht7
Nov 15 07:02:58.199: INFO: Created: latency-svc-jqh5m
Nov 15 07:02:58.208: INFO: Got endpoints: latency-svc-rcht7 [129.852159ms]
Nov 15 07:02:58.210: INFO: Created: latency-svc-hwp4g
Nov 15 07:02:58.216: INFO: Got endpoints: latency-svc-jqh5m [137.538059ms]
Nov 15 07:02:58.224: INFO: Created: latency-svc-ct4kb
Nov 15 07:02:58.229: INFO: Got endpoints: latency-svc-hwp4g [150.87797ms]
Nov 15 07:02:58.231: INFO: Created: latency-svc-shslb
Nov 15 07:02:58.241: INFO: Got endpoints: latency-svc-ct4kb [157.128472ms]
Nov 15 07:02:58.242: INFO: Created: latency-svc-d77zd
Nov 15 07:02:58.249: INFO: Created: latency-svc-pbr9c
Nov 15 07:02:58.250: INFO: Got endpoints: latency-svc-shslb [160.547069ms]
Nov 15 07:02:58.261: INFO: Got endpoints: latency-svc-d77zd [169.277047ms]
Nov 15 07:02:58.263: INFO: Created: latency-svc-52rdj
Nov 15 07:02:58.268: INFO: Got endpoints: latency-svc-pbr9c [175.706099ms]
Nov 15 07:02:58.281: INFO: Got endpoints: latency-svc-52rdj [188.633885ms]
Nov 15 07:02:58.437: INFO: Created: latency-svc-kzmln
Nov 15 07:02:58.442: INFO: Created: latency-svc-d8f5d
Nov 15 07:02:58.442: INFO: Created: latency-svc-dzbpz
Nov 15 07:02:58.442: INFO: Created: latency-svc-p6m49
Nov 15 07:02:58.443: INFO: Created: latency-svc-sqpqs
Nov 15 07:02:58.443: INFO: Created: latency-svc-mswqs
Nov 15 07:02:58.443: INFO: Created: latency-svc-fdjf7
Nov 15 07:02:58.443: INFO: Created: latency-svc-rwgq2
Nov 15 07:02:58.443: INFO: Created: latency-svc-9kf54
Nov 15 07:02:58.445: INFO: Created: latency-svc-dhm6m
Nov 15 07:02:58.446: INFO: Created: latency-svc-gz525
Nov 15 07:02:58.446: INFO: Created: latency-svc-htgmk
Nov 15 07:02:58.447: INFO: Created: latency-svc-swgtd
Nov 15 07:02:58.447: INFO: Created: latency-svc-52rpv
Nov 15 07:02:58.447: INFO: Created: latency-svc-7nsps
Nov 15 07:02:58.461: INFO: Got endpoints: latency-svc-kzmln [200.193685ms]
Nov 15 07:02:58.462: INFO: Got endpoints: latency-svc-swgtd [212.57826ms]
Nov 15 07:02:58.462: INFO: Got endpoints: latency-svc-7nsps [221.700659ms]
Nov 15 07:02:58.463: INFO: Got endpoints: latency-svc-p6m49 [274.08428ms]
Nov 15 07:02:58.463: INFO: Got endpoints: latency-svc-mswqs [354.761595ms]
Nov 15 07:02:58.481: INFO: Got endpoints: latency-svc-fdjf7 [273.041377ms]
Nov 15 07:02:58.481: INFO: Got endpoints: latency-svc-dhm6m [199.691865ms]
Nov 15 07:02:58.483: INFO: Got endpoints: latency-svc-sqpqs [266.737183ms]
Nov 15 07:02:58.483: INFO: Got endpoints: latency-svc-htgmk [253.911578ms]
Nov 15 07:02:58.483: INFO: Got endpoints: latency-svc-52rpv [357.030235ms]
Nov 15 07:02:58.501: INFO: Created: latency-svc-tvs6v
Nov 15 07:02:58.502: INFO: Got endpoints: latency-svc-9kf54 [233.120614ms]
Nov 15 07:02:58.504: INFO: Got endpoints: latency-svc-rwgq2 [402.41095ms]
Nov 15 07:02:58.504: INFO: Got endpoints: latency-svc-gz525 [352.678698ms]
Nov 15 07:02:58.504: INFO: Got endpoints: latency-svc-d8f5d [333.079075ms]
Nov 15 07:02:58.505: INFO: Got endpoints: latency-svc-dzbpz [370.454726ms]
Nov 15 07:02:58.522: INFO: Got endpoints: latency-svc-tvs6v [60.780407ms]
Nov 15 07:02:58.523: INFO: Created: latency-svc-zkkq8
Nov 15 07:02:58.535: INFO: Created: latency-svc-crhrp
Nov 15 07:02:58.535: INFO: Got endpoints: latency-svc-zkkq8 [73.126523ms]
Nov 15 07:02:58.544: INFO: Created: latency-svc-nkdrk
Nov 15 07:02:58.555: INFO: Got endpoints: latency-svc-crhrp [92.653139ms]
Nov 15 07:02:58.558: INFO: Got endpoints: latency-svc-nkdrk [95.970904ms]
Nov 15 07:02:58.584: INFO: Created: latency-svc-fnmpg
Nov 15 07:02:58.602: INFO: Got endpoints: latency-svc-fnmpg [138.432269ms]
Nov 15 07:02:58.749: INFO: Created: latency-svc-t6z4w
Nov 15 07:02:58.753: INFO: Created: latency-svc-hkv8s
Nov 15 07:02:58.754: INFO: Created: latency-svc-zg72h
Nov 15 07:02:58.754: INFO: Created: latency-svc-bgcmz
Nov 15 07:02:58.754: INFO: Created: latency-svc-vlx9z
Nov 15 07:02:58.754: INFO: Created: latency-svc-t8pvh
Nov 15 07:02:58.754: INFO: Created: latency-svc-g85v5
Nov 15 07:02:58.754: INFO: Created: latency-svc-z5p2p
Nov 15 07:02:58.755: INFO: Created: latency-svc-wmfv6
Nov 15 07:02:58.755: INFO: Created: latency-svc-2lggv
Nov 15 07:02:58.755: INFO: Created: latency-svc-ct2j6
Nov 15 07:02:58.755: INFO: Created: latency-svc-4crnb
Nov 15 07:02:58.755: INFO: Created: latency-svc-6ft6h
Nov 15 07:02:58.755: INFO: Created: latency-svc-588kv
Nov 15 07:02:58.755: INFO: Created: latency-svc-rw2t7
Nov 15 07:02:58.793: INFO: Got endpoints: latency-svc-zg72h [237.330873ms]
Nov 15 07:02:58.793: INFO: Got endpoints: latency-svc-t6z4w [311.851274ms]
Nov 15 07:02:58.793: INFO: Got endpoints: latency-svc-hkv8s [234.798391ms]
Nov 15 07:02:58.794: INFO: Got endpoints: latency-svc-t8pvh [258.217036ms]
Nov 15 07:02:58.794: INFO: Got endpoints: latency-svc-vlx9z [310.681478ms]
Nov 15 07:02:58.842: INFO: Got endpoints: latency-svc-4crnb [340.453043ms]
Nov 15 07:02:58.843: INFO: Got endpoints: latency-svc-wmfv6 [362.057968ms]
Nov 15 07:02:58.843: INFO: Got endpoints: latency-svc-g85v5 [359.97673ms]
Nov 15 07:02:58.843: INFO: Got endpoints: latency-svc-588kv [360.550321ms]
Nov 15 07:02:58.843: INFO: Got endpoints: latency-svc-2lggv [241.647813ms]
Nov 15 07:02:58.888: INFO: Created: latency-svc-wv5fg
Nov 15 07:02:58.889: INFO: Created: latency-svc-2m6n9
Nov 15 07:02:58.889: INFO: Created: latency-svc-p8ffs
Nov 15 07:02:58.889: INFO: Got endpoints: latency-svc-2m6n9 [95.259094ms]
Nov 15 07:02:58.892: INFO: Got endpoints: latency-svc-bgcmz [387.761084ms]
Nov 15 07:02:58.892: INFO: Got endpoints: latency-svc-z5p2p [388.208508ms]
Nov 15 07:02:58.892: INFO: Got endpoints: latency-svc-rw2t7 [370.371732ms]
Nov 15 07:02:58.892: INFO: Got endpoints: latency-svc-6ft6h [387.720128ms]
Nov 15 07:02:58.893: INFO: Got endpoints: latency-svc-wv5fg [99.718907ms]
Nov 15 07:02:58.893: INFO: Got endpoints: latency-svc-ct2j6 [388.299103ms]
Nov 15 07:02:58.896: INFO: Created: latency-svc-j4mln
Nov 15 07:02:58.907: INFO: Got endpoints: latency-svc-p8ffs [114.095497ms]
Nov 15 07:02:58.911: INFO: Created: latency-svc-p9vml
Nov 15 07:02:58.914: INFO: Got endpoints: latency-svc-j4mln [119.60431ms]
Nov 15 07:02:58.927: INFO: Got endpoints: latency-svc-p9vml [134.414451ms]
Nov 15 07:02:58.929: INFO: Created: latency-svc-6bn8v
Nov 15 07:02:58.937: INFO: Created: latency-svc-xcshb
Nov 15 07:02:58.946: INFO: Got endpoints: latency-svc-6bn8v [102.546899ms]
Nov 15 07:02:58.954: INFO: Created: latency-svc-wdhrw
Nov 15 07:02:58.956: INFO: Got endpoints: latency-svc-xcshb [113.692002ms]
Nov 15 07:02:58.964: INFO: Created: latency-svc-f2sh2
Nov 15 07:02:58.971: INFO: Got endpoints: latency-svc-wdhrw [127.260093ms]
Nov 15 07:02:58.977: INFO: Created: latency-svc-9vwwg
Nov 15 07:02:59.006: INFO: Created: latency-svc-8l4rv
Nov 15 07:02:59.006: INFO: Created: latency-svc-s47sn
Nov 15 07:02:59.006: INFO: Got endpoints: latency-svc-8l4rv [117.121002ms]
Nov 15 07:02:59.006: INFO: Got endpoints: latency-svc-9vwwg [162.610486ms]
Nov 15 07:02:59.006: INFO: Got endpoints: latency-svc-f2sh2 [163.434091ms]
Nov 15 07:02:59.024: INFO: Created: latency-svc-8xv9k
Nov 15 07:02:59.028: INFO: Got endpoints: latency-svc-s47sn [136.202224ms]
Nov 15 07:02:59.037: INFO: Created: latency-svc-2r6v4
Nov 15 07:02:59.044: INFO: Got endpoints: latency-svc-8xv9k [152.225792ms]
Nov 15 07:02:59.049: INFO: Created: latency-svc-c8kjt
Nov 15 07:02:59.062: INFO: Created: latency-svc-m2mdn
Nov 15 07:02:59.064: INFO: Got endpoints: latency-svc-2r6v4 [171.311509ms]
Nov 15 07:02:59.065: INFO: Got endpoints: latency-svc-c8kjt [172.421836ms]
Nov 15 07:02:59.081: INFO: Created: latency-svc-trzrc
Nov 15 07:02:59.083: INFO: Got endpoints: latency-svc-m2mdn [190.558247ms]
Nov 15 07:02:59.091: INFO: Created: latency-svc-pbw69
Nov 15 07:02:59.122: INFO: Got endpoints: latency-svc-trzrc [229.470679ms]
Nov 15 07:02:59.123: INFO: Got endpoints: latency-svc-pbw69 [215.219799ms]
Nov 15 07:02:59.123: INFO: Created: latency-svc-q2llr
Nov 15 07:02:59.130: INFO: Created: latency-svc-72fbr
Nov 15 07:02:59.140: INFO: Created: latency-svc-m7jb8
Nov 15 07:02:59.142: INFO: Got endpoints: latency-svc-q2llr [228.007845ms]
Nov 15 07:02:59.146: INFO: Got endpoints: latency-svc-72fbr [218.189285ms]
Nov 15 07:02:59.158: INFO: Created: latency-svc-72kt5
Nov 15 07:02:59.164: INFO: Got endpoints: latency-svc-m7jb8 [218.125726ms]
Nov 15 07:02:59.165: INFO: Created: latency-svc-6bppj
Nov 15 07:02:59.201: INFO: Created: latency-svc-mb794
Nov 15 07:02:59.201: INFO: Got endpoints: latency-svc-6bppj [230.704421ms]
Nov 15 07:02:59.202: INFO: Got endpoints: latency-svc-72kt5 [245.593327ms]
Nov 15 07:02:59.212: INFO: Created: latency-svc-8swmk
Nov 15 07:02:59.214: INFO: Got endpoints: latency-svc-mb794 [207.611132ms]
Nov 15 07:02:59.238: INFO: Created: latency-svc-fgzb4
Nov 15 07:02:59.245: INFO: Got endpoints: latency-svc-8swmk [238.542905ms]
Nov 15 07:02:59.260: INFO: Got endpoints: latency-svc-fgzb4 [253.297342ms]
Nov 15 07:02:59.525: INFO: Created: latency-svc-9jmwf
Nov 15 07:02:59.526: INFO: Created: latency-svc-gn224
Nov 15 07:02:59.526: INFO: Created: latency-svc-wktfk
Nov 15 07:02:59.526: INFO: Created: latency-svc-js9ff
Nov 15 07:02:59.557: INFO: Created: latency-svc-zf8pv
Nov 15 07:02:59.557: INFO: Got endpoints: latency-svc-wktfk [355.40265ms]
Nov 15 07:02:59.559: INFO: Got endpoints: latency-svc-js9ff [514.93399ms]
Nov 15 07:02:59.562: INFO: Got endpoints: latency-svc-gn224 [533.282661ms]
Nov 15 07:02:59.562: INFO: Got endpoints: latency-svc-9jmwf [348.309784ms]
Nov 15 07:02:59.606: INFO: Created: latency-svc-mzm4p
Nov 15 07:02:59.606: INFO: Created: latency-svc-rhc6c
Nov 15 07:02:59.606: INFO: Created: latency-svc-5wldq
Nov 15 07:02:59.614: INFO: Got endpoints: latency-svc-zf8pv [369.560081ms]
Nov 15 07:02:59.615: INFO: Created: latency-svc-hn9r6
Nov 15 07:02:59.615: INFO: Created: latency-svc-rb659
Nov 15 07:02:59.615: INFO: Created: latency-svc-6cnz2
Nov 15 07:02:59.615: INFO: Created: latency-svc-hjcgl
Nov 15 07:02:59.615: INFO: Created: latency-svc-fmtwr
Nov 15 07:02:59.615: INFO: Created: latency-svc-9qcrj
Nov 15 07:02:59.615: INFO: Created: latency-svc-d5jjz
Nov 15 07:02:59.632: INFO: Got endpoints: latency-svc-rhc6c [371.936732ms]
Nov 15 07:02:59.641: INFO: Got endpoints: latency-svc-5wldq [557.477596ms]
Nov 15 07:02:59.641: INFO: Got endpoints: latency-svc-mzm4p [577.336444ms]
Nov 15 07:02:59.641: INFO: Got endpoints: latency-svc-hn9r6 [519.097686ms]
Nov 15 07:02:59.641: INFO: Got endpoints: latency-svc-6cnz2 [518.624562ms]
Nov 15 07:02:59.645: INFO: Created: latency-svc-7jfz7
Nov 15 07:02:59.676: INFO: Created: latency-svc-j4n7m
Nov 15 07:02:59.699: INFO: Created: latency-svc-zmpf8
Nov 15 07:02:59.728: INFO: Got endpoints: latency-svc-fmtwr [586.601889ms]
Nov 15 07:02:59.729: INFO: Got endpoints: latency-svc-9qcrj [582.996559ms]
Nov 15 07:02:59.729: INFO: Got endpoints: latency-svc-d5jjz [565.198318ms]
Nov 15 07:02:59.730: INFO: Got endpoints: latency-svc-rb659 [665.489419ms]
Nov 15 07:02:59.731: INFO: Got endpoints: latency-svc-hjcgl [529.274647ms]
Nov 15 07:02:59.749: INFO: Created: latency-svc-cs5d2
Nov 15 07:02:59.750: INFO: Created: latency-svc-ch68q
Nov 15 07:02:59.750: INFO: Created: latency-svc-d4nf4
Nov 15 07:02:59.750: INFO: Got endpoints: latency-svc-7jfz7 [193.471893ms]
Nov 15 07:02:59.751: INFO: Got endpoints: latency-svc-zmpf8 [189.567116ms]
Nov 15 07:02:59.751: INFO: Got endpoints: latency-svc-j4n7m [189.275419ms]
Nov 15 07:02:59.847: INFO: Got endpoints: latency-svc-ch68q [232.911627ms]
Nov 15 07:02:59.848: INFO: Got endpoints: latency-svc-d4nf4 [216.159037ms]
Nov 15 07:02:59.848: INFO: Got endpoints: latency-svc-cs5d2 [286.23138ms]
Nov 15 07:02:59.857: INFO: Created: latency-svc-lxbrk
Nov 15 07:02:59.882: INFO: Got endpoints: latency-svc-lxbrk [240.947096ms]
Nov 15 07:02:59.882: INFO: Created: latency-svc-hsdwt
Nov 15 07:02:59.882: INFO: Created: latency-svc-6qnpk
Nov 15 07:02:59.895: INFO: Got endpoints: latency-svc-hsdwt [253.581707ms]
Nov 15 07:02:59.900: INFO: Got endpoints: latency-svc-6qnpk [259.254721ms]
Nov 15 07:02:59.904: INFO: Created: latency-svc-ptrpw
Nov 15 07:02:59.918: INFO: Created: latency-svc-zgqrh
Nov 15 07:02:59.920: INFO: Got endpoints: latency-svc-ptrpw [277.691532ms]
Nov 15 07:02:59.930: INFO: Created: latency-svc-dbrzn
Nov 15 07:02:59.934: INFO: Got endpoints: latency-svc-zgqrh [204.882924ms]
Nov 15 07:02:59.940: INFO: Created: latency-svc-t8752
Nov 15 07:02:59.964: INFO: Got endpoints: latency-svc-dbrzn [236.170548ms]
Nov 15 07:02:59.970: INFO: Created: latency-svc-2tlsq
Nov 15 07:02:59.973: INFO: Got endpoints: latency-svc-t8752 [244.446886ms]
Nov 15 07:02:59.990: INFO: Got endpoints: latency-svc-2tlsq [256.009117ms]
Nov 15 07:02:59.991: INFO: Created: latency-svc-r4fl8
Nov 15 07:03:00.006: INFO: Got endpoints: latency-svc-r4fl8 [275.649662ms]
Nov 15 07:03:00.007: INFO: Created: latency-svc-pk58s
Nov 15 07:03:00.019: INFO: Created: latency-svc-vw6sf
Nov 15 07:03:00.028: INFO: Got endpoints: latency-svc-pk58s [277.710203ms]
Nov 15 07:03:00.030: INFO: Created: latency-svc-wkpc4
Nov 15 07:03:00.040: INFO: Got endpoints: latency-svc-vw6sf [288.861215ms]
Nov 15 07:03:00.048: INFO: Got endpoints: latency-svc-wkpc4 [296.894539ms]
Nov 15 07:03:00.049: INFO: Created: latency-svc-4cngg
Nov 15 07:03:00.064: INFO: Got endpoints: latency-svc-4cngg [216.895635ms]
Nov 15 07:03:00.065: INFO: Created: latency-svc-tp5gk
Nov 15 07:03:00.077: INFO: Created: latency-svc-cdz85
Nov 15 07:03:00.081: INFO: Got endpoints: latency-svc-tp5gk [233.306079ms]
Nov 15 07:03:00.104: INFO: Got endpoints: latency-svc-cdz85 [255.229526ms]
Nov 15 07:03:00.105: INFO: Created: latency-svc-dbzxn
Nov 15 07:03:00.116: INFO: Created: latency-svc-k2xrv
Nov 15 07:03:00.128: INFO: Got endpoints: latency-svc-dbzxn [246.43752ms]
Nov 15 07:03:00.129: INFO: Created: latency-svc-6n5n9
Nov 15 07:03:00.139: INFO: Got endpoints: latency-svc-k2xrv [243.371069ms]
Nov 15 07:03:00.140: INFO: Created: latency-svc-gjwdb
Nov 15 07:03:00.156: INFO: Got endpoints: latency-svc-6n5n9 [255.679746ms]
Nov 15 07:03:00.157: INFO: Got endpoints: latency-svc-gjwdb [237.528322ms]
Nov 15 07:03:00.163: INFO: Created: latency-svc-mklj8
Nov 15 07:03:00.170: INFO: Created: latency-svc-mnhnh
Nov 15 07:03:00.181: INFO: Got endpoints: latency-svc-mklj8 [247.281272ms]
Nov 15 07:03:00.190: INFO: Got endpoints: latency-svc-mnhnh [225.31851ms]
Nov 15 07:03:00.192: INFO: Created: latency-svc-gkbk8
Nov 15 07:03:00.207: INFO: Created: latency-svc-f55xr
Nov 15 07:03:00.214: INFO: Got endpoints: latency-svc-gkbk8 [240.406371ms]
Nov 15 07:03:00.220: INFO: Created: latency-svc-mt846
Nov 15 07:03:00.223: INFO: Got endpoints: latency-svc-f55xr [233.425635ms]
Nov 15 07:03:00.233: INFO: Created: latency-svc-wbr6h
Nov 15 07:03:00.239: INFO: Got endpoints: latency-svc-mt846 [233.099304ms]
Nov 15 07:03:00.246: INFO: Created: latency-svc-swcrj
Nov 15 07:03:00.252: INFO: Got endpoints: latency-svc-wbr6h [223.804968ms]
Nov 15 07:03:00.261: INFO: Created: latency-svc-2595w
Nov 15 07:03:00.270: INFO: Got endpoints: latency-svc-swcrj [230.309342ms]
Nov 15 07:03:00.273: INFO: Created: latency-svc-vrv9r
Nov 15 07:03:00.283: INFO: Got endpoints: latency-svc-2595w [235.21397ms]
Nov 15 07:03:00.287: INFO: Created: latency-svc-8lpld
Nov 15 07:03:00.288: INFO: Got endpoints: latency-svc-vrv9r [223.683199ms]
Nov 15 07:03:00.298: INFO: Created: latency-svc-fmm9v
Nov 15 07:03:00.306: INFO: Got endpoints: latency-svc-8lpld [224.131365ms]
Nov 15 07:03:00.313: INFO: Created: latency-svc-v8jcw
Nov 15 07:03:00.317: INFO: Got endpoints: latency-svc-fmm9v [213.733183ms]
Nov 15 07:03:00.328: INFO: Created: latency-svc-cqpqk
Nov 15 07:03:00.328: INFO: Got endpoints: latency-svc-v8jcw [200.227718ms]
Nov 15 07:03:00.350: INFO: Created: latency-svc-tsltp
Nov 15 07:03:00.350: INFO: Got endpoints: latency-svc-cqpqk [210.737825ms]
Nov 15 07:03:00.363: INFO: Created: latency-svc-m6dz8
Nov 15 07:03:00.368: INFO: Got endpoints: latency-svc-tsltp [211.701983ms]
Nov 15 07:03:00.377: INFO: Created: latency-svc-7wqbf
Nov 15 07:03:00.387: INFO: Got endpoints: latency-svc-m6dz8 [229.437869ms]
Nov 15 07:03:00.395: INFO: Created: latency-svc-2lx6v
Nov 15 07:03:00.400: INFO: Got endpoints: latency-svc-7wqbf [218.424197ms]
Nov 15 07:03:00.404: INFO: Created: latency-svc-4vhxl
Nov 15 07:03:00.422: INFO: Got endpoints: latency-svc-2lx6v [231.757813ms]
Nov 15 07:03:00.423: INFO: Got endpoints: latency-svc-4vhxl [208.798543ms]
Nov 15 07:03:00.423: INFO: Created: latency-svc-4ddld
Nov 15 07:03:00.431: INFO: Created: latency-svc-9cfqj
Nov 15 07:03:00.443: INFO: Got endpoints: latency-svc-4ddld [219.063093ms]
Nov 15 07:03:00.452: INFO: Got endpoints: latency-svc-9cfqj [212.854524ms]
Nov 15 07:03:00.453: INFO: Created: latency-svc-rcd4h
Nov 15 07:03:00.462: INFO: Created: latency-svc-pqgpp
Nov 15 07:03:00.467: INFO: Got endpoints: latency-svc-rcd4h [215.019254ms]
Nov 15 07:03:00.477: INFO: Created: latency-svc-rrhtv
Nov 15 07:03:00.480: INFO: Got endpoints: latency-svc-pqgpp [210.180344ms]
Nov 15 07:03:00.490: INFO: Created: latency-svc-9sxrv
Nov 15 07:03:00.503: INFO: Got endpoints: latency-svc-rrhtv [219.558754ms]
Nov 15 07:03:00.506: INFO: Created: latency-svc-kb5st
Nov 15 07:03:00.548: INFO: Created: latency-svc-mstw4
Nov 15 07:03:00.548: INFO: Got endpoints: latency-svc-9sxrv [260.012148ms]
Nov 15 07:03:00.556: INFO: Got endpoints: latency-svc-kb5st [249.991634ms]
Nov 15 07:03:00.588: INFO: Got endpoints: latency-svc-mstw4 [270.700416ms]
Nov 15 07:03:00.589: INFO: Created: latency-svc-4zccr
Nov 15 07:03:00.592: INFO: Created: latency-svc-p56pw
Nov 15 07:03:00.592: INFO: Created: latency-svc-96whx
Nov 15 07:03:00.610: INFO: Created: latency-svc-jzthr
Nov 15 07:03:00.612: INFO: Got endpoints: latency-svc-4zccr [283.700106ms]
Nov 15 07:03:00.613: INFO: Got endpoints: latency-svc-p56pw [244.926377ms]
Nov 15 07:03:00.614: INFO: Got endpoints: latency-svc-96whx [263.889158ms]
Nov 15 07:03:00.619: INFO: Created: latency-svc-gnxfx
Nov 15 07:03:00.622: INFO: Got endpoints: latency-svc-jzthr [235.441258ms]
Nov 15 07:03:00.635: INFO: Got endpoints: latency-svc-gnxfx [234.968725ms]
Nov 15 07:03:00.636: INFO: Created: latency-svc-lj7g6
Nov 15 07:03:00.650: INFO: Got endpoints: latency-svc-lj7g6 [228.614831ms]
Nov 15 07:03:00.856: INFO: Created: latency-svc-zk77m
Nov 15 07:03:00.861: INFO: Created: latency-svc-nnqvf
Nov 15 07:03:00.862: INFO: Created: latency-svc-dr78c
Nov 15 07:03:00.862: INFO: Created: latency-svc-4bwq7
Nov 15 07:03:00.862: INFO: Created: latency-svc-ngg6n
Nov 15 07:03:00.863: INFO: Created: latency-svc-wlvrr
Nov 15 07:03:00.863: INFO: Created: latency-svc-l6dm4
Nov 15 07:03:00.864: INFO: Created: latency-svc-jlkg8
Nov 15 07:03:00.864: INFO: Created: latency-svc-rlh6n
Nov 15 07:03:00.865: INFO: Created: latency-svc-dhrnn
Nov 15 07:03:00.870: INFO: Created: latency-svc-6skf5
Nov 15 07:03:00.870: INFO: Created: latency-svc-ns5qx
Nov 15 07:03:00.870: INFO: Created: latency-svc-92rzk
Nov 15 07:03:00.870: INFO: Created: latency-svc-cbt5g
Nov 15 07:03:00.871: INFO: Created: latency-svc-kbxwb
Nov 15 07:03:00.879: INFO: Got endpoints: latency-svc-l6dm4 [426.194486ms]
Nov 15 07:03:00.880: INFO: Got endpoints: latency-svc-zk77m [229.362174ms]
Nov 15 07:03:00.880: INFO: Got endpoints: latency-svc-4bwq7 [412.529586ms]
Nov 15 07:03:00.883: INFO: Got endpoints: latency-svc-jlkg8 [270.753128ms]
Nov 15 07:03:00.885: INFO: Got endpoints: latency-svc-nnqvf [442.002918ms]
Nov 15 07:03:00.903: INFO: Got endpoints: latency-svc-wlvrr [347.828061ms]
Nov 15 07:03:00.903: INFO: Got endpoints: latency-svc-rlh6n [355.257806ms]
Nov 15 07:03:00.904: INFO: Got endpoints: latency-svc-dr78c [423.451887ms]
Nov 15 07:03:00.904: INFO: Got endpoints: latency-svc-ngg6n [400.642762ms]
Nov 15 07:03:00.904: INFO: Got endpoints: latency-svc-dhrnn [481.317906ms]
Nov 15 07:03:00.946: INFO: Got endpoints: latency-svc-6skf5 [324.074577ms]
Nov 15 07:03:00.947: INFO: Created: latency-svc-8qfjf
Nov 15 07:03:00.948: INFO: Got endpoints: latency-svc-cbt5g [334.994827ms]
Nov 15 07:03:00.950: INFO: Got endpoints: latency-svc-ns5qx [361.379637ms]
Nov 15 07:03:00.950: INFO: Got endpoints: latency-svc-kbxwb [314.676897ms]
Nov 15 07:03:00.950: INFO: Got endpoints: latency-svc-92rzk [335.939701ms]
Nov 15 07:03:00.957: INFO: Created: latency-svc-bv7hq
Nov 15 07:03:00.970: INFO: Got endpoints: latency-svc-8qfjf [91.501987ms]
Nov 15 07:03:00.973: INFO: Created: latency-svc-z2s9d
Nov 15 07:03:00.975: INFO: Got endpoints: latency-svc-bv7hq [95.531133ms]
Nov 15 07:03:01.023: INFO: Created: latency-svc-8ggbz
Nov 15 07:03:01.024: INFO: Got endpoints: latency-svc-z2s9d [144.155016ms]
Nov 15 07:03:01.045: INFO: Got endpoints: latency-svc-8ggbz [162.353325ms]
Nov 15 07:03:01.046: INFO: Created: latency-svc-5cm6l
Nov 15 07:03:01.057: INFO: Created: latency-svc-mjg4j
Nov 15 07:03:01.066: INFO: Got endpoints: latency-svc-5cm6l [181.325313ms]
Nov 15 07:03:01.072: INFO: Created: latency-svc-g8rp7
Nov 15 07:03:01.085: INFO: Got endpoints: latency-svc-g8rp7 [181.286411ms]
Nov 15 07:03:01.085: INFO: Got endpoints: latency-svc-mjg4j [181.380356ms]
Nov 15 07:03:01.088: INFO: Created: latency-svc-prmg5
Nov 15 07:03:01.105: INFO: Created: latency-svc-dx6nf
Nov 15 07:03:01.105: INFO: Got endpoints: latency-svc-prmg5 [201.175065ms]
Nov 15 07:03:01.118: INFO: Created: latency-svc-wtvqg
Nov 15 07:03:01.120: INFO: Got endpoints: latency-svc-dx6nf [216.063826ms]
Nov 15 07:03:01.133: INFO: Created: latency-svc-nqkmp
Nov 15 07:03:01.139: INFO: Got endpoints: latency-svc-wtvqg [234.476923ms]
Nov 15 07:03:01.146: INFO: Got endpoints: latency-svc-nqkmp [200.064488ms]
Nov 15 07:03:01.147: INFO: Latencies: [58.017897ms 60.780407ms 73.126523ms 73.230018ms 73.467642ms 81.861681ms 84.714594ms 91.501987ms 92.653139ms 95.112616ms 95.259094ms 95.531133ms 95.970904ms 99.718907ms 99.986851ms 102.546899ms 109.665747ms 110.490729ms 113.692002ms 114.095497ms 117.121002ms 119.60431ms 120.038673ms 127.260093ms 129.852159ms 134.414451ms 136.202224ms 137.538059ms 138.432269ms 144.155016ms 150.358284ms 150.87797ms 151.034533ms 152.225792ms 155.111352ms 157.128472ms 160.547069ms 162.353325ms 162.610486ms 163.434091ms 169.277047ms 170.975316ms 171.311509ms 172.421836ms 175.706099ms 181.196403ms 181.286411ms 181.325313ms 181.380356ms 188.633885ms 189.275419ms 189.567116ms 190.558247ms 192.788032ms 193.471893ms 193.909341ms 199.691865ms 200.064488ms 200.193685ms 200.227718ms 201.175065ms 204.882924ms 205.568544ms 207.611132ms 208.798543ms 210.180344ms 210.737825ms 211.701983ms 212.57826ms 212.854524ms 213.733183ms 215.019254ms 215.219799ms 216.063826ms 216.159037ms 216.895635ms 218.125726ms 218.189285ms 218.214708ms 218.424197ms 218.754782ms 219.063093ms 219.558754ms 220.494598ms 221.700659ms 223.683199ms 223.804968ms 224.131365ms 225.31851ms 228.007845ms 228.614831ms 229.362174ms 229.437869ms 229.470679ms 230.309342ms 230.704421ms 231.757813ms 232.911627ms 233.099304ms 233.120614ms 233.306079ms 233.425635ms 234.476923ms 234.798391ms 234.968725ms 235.21397ms 235.441258ms 236.170548ms 237.330873ms 237.528322ms 238.542905ms 240.406371ms 240.947096ms 241.647813ms 242.006236ms 243.371069ms 244.446886ms 244.926377ms 245.593327ms 246.43752ms 247.281272ms 249.991634ms 253.297342ms 253.581707ms 253.911578ms 255.229526ms 255.679746ms 256.009117ms 258.217036ms 259.254721ms 259.889419ms 260.012148ms 263.889158ms 266.737183ms 270.700416ms 270.753128ms 270.901452ms 273.041377ms 274.08428ms 275.185096ms 275.392309ms 275.649662ms 277.691532ms 277.710203ms 283.700106ms 286.23138ms 288.861215ms 296.894539ms 307.114313ms 310.681478ms 311.851274ms 314.676897ms 316.194244ms 324.074577ms 327.507208ms 333.079075ms 334.994827ms 335.939701ms 340.453043ms 347.828061ms 348.309784ms 352.678698ms 354.761595ms 355.257806ms 355.40265ms 356.275239ms 357.030235ms 359.97673ms 360.550321ms 361.379637ms 362.057968ms 369.560081ms 370.371732ms 370.454726ms 371.936732ms 387.720128ms 387.761084ms 388.208508ms 388.299103ms 399.365682ms 400.642762ms 402.41095ms 412.529586ms 423.451887ms 426.194486ms 440.281511ms 442.002918ms 448.41999ms 481.317906ms 514.93399ms 518.624562ms 519.097686ms 529.274647ms 533.282661ms 557.477596ms 565.198318ms 577.336444ms 582.996559ms 586.601889ms 665.489419ms]
Nov 15 07:03:01.147: INFO: 50 %ile: 233.306079ms
Nov 15 07:03:01.147: INFO: 90 %ile: 400.642762ms
Nov 15 07:03:01.147: INFO: 99 %ile: 586.601889ms
Nov 15 07:03:01.147: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Nov 15 07:03:01.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-5415" for this suite. 11/15/23 07:03:01.18
------------------------------
â€¢ [SLOW TEST] [6.906 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:02:54.321
    Nov 15 07:02:54.321: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename svc-latency 11/15/23 07:02:54.322
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:02:54.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:02:54.407
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Nov 15 07:02:54.420: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-5415 11/15/23 07:02:54.421
    I1115 07:02:54.442032      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5415, replica count: 1
    I1115 07:02:55.492740      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1115 07:02:56.492983      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1115 07:02:57.493858      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 15 07:02:57.632: INFO: Created: latency-svc-h79x2
    Nov 15 07:02:57.649: INFO: Got endpoints: latency-svc-h79x2 [53.749033ms]
    Nov 15 07:02:57.702: INFO: Created: latency-svc-6tztk
    Nov 15 07:02:57.714: INFO: Created: latency-svc-s9kbk
    Nov 15 07:02:57.723: INFO: Got endpoints: latency-svc-6tztk [73.230018ms]
    Nov 15 07:02:57.724: INFO: Created: latency-svc-fmjnl
    Nov 15 07:02:57.735: INFO: Got endpoints: latency-svc-s9kbk [84.714594ms]
    Nov 15 07:02:57.735: INFO: Created: latency-svc-vhgw5
    Nov 15 07:02:57.745: INFO: Got endpoints: latency-svc-fmjnl [95.112616ms]
    Nov 15 07:02:57.771: INFO: Got endpoints: latency-svc-vhgw5 [120.038673ms]
    Nov 15 07:02:57.773: INFO: Created: latency-svc-47b4z
    Nov 15 07:02:57.785: INFO: Created: latency-svc-qmqzh
    Nov 15 07:02:57.787: INFO: Created: latency-svc-rckpw
    Nov 15 07:02:57.802: INFO: Got endpoints: latency-svc-qmqzh [150.358284ms]
    Nov 15 07:02:57.803: INFO: Got endpoints: latency-svc-47b4z [151.034533ms]
    Nov 15 07:02:57.807: INFO: Got endpoints: latency-svc-rckpw [155.111352ms]
    Nov 15 07:02:57.809: INFO: Created: latency-svc-fqrj8
    Nov 15 07:02:57.820: INFO: Created: latency-svc-ch7pf
    Nov 15 07:02:57.824: INFO: Got endpoints: latency-svc-fqrj8 [170.975316ms]
    Nov 15 07:02:57.832: INFO: Created: latency-svc-929hd
    Nov 15 07:02:57.834: INFO: Got endpoints: latency-svc-ch7pf [181.196403ms]
    Nov 15 07:02:57.845: INFO: Created: latency-svc-z9xmd
    Nov 15 07:02:57.847: INFO: Got endpoints: latency-svc-929hd [193.909341ms]
    Nov 15 07:02:57.858: INFO: Created: latency-svc-8llp9
    Nov 15 07:02:57.859: INFO: Got endpoints: latency-svc-z9xmd [205.568544ms]
    Nov 15 07:02:57.872: INFO: Got endpoints: latency-svc-8llp9 [218.754782ms]
    Nov 15 07:02:58.034: INFO: Created: latency-svc-pvrjk
    Nov 15 07:02:58.035: INFO: Created: latency-svc-2m8dk
    Nov 15 07:02:58.035: INFO: Created: latency-svc-9958b
    Nov 15 07:02:58.035: INFO: Created: latency-svc-zt7wr
    Nov 15 07:02:58.035: INFO: Created: latency-svc-bhf5f
    Nov 15 07:02:58.037: INFO: Created: latency-svc-pncfz
    Nov 15 07:02:58.037: INFO: Created: latency-svc-69mc6
    Nov 15 07:02:58.037: INFO: Created: latency-svc-49tl5
    Nov 15 07:02:58.038: INFO: Created: latency-svc-4z5gk
    Nov 15 07:02:58.038: INFO: Created: latency-svc-vbfh5
    Nov 15 07:02:58.038: INFO: Created: latency-svc-jvtwc
    Nov 15 07:02:58.039: INFO: Created: latency-svc-fdmhg
    Nov 15 07:02:58.039: INFO: Created: latency-svc-f9vsb
    Nov 15 07:02:58.039: INFO: Created: latency-svc-d6bgt
    Nov 15 07:02:58.040: INFO: Created: latency-svc-7jkbj
    Nov 15 07:02:58.050: INFO: Got endpoints: latency-svc-9958b [327.507208ms]
    Nov 15 07:02:58.051: INFO: Got endpoints: latency-svc-2m8dk [192.788032ms]
    Nov 15 07:02:58.052: INFO: Got endpoints: latency-svc-zt7wr [399.365682ms]
    Nov 15 07:02:58.053: INFO: Got endpoints: latency-svc-pvrjk [218.214708ms]
    Nov 15 07:02:58.061: INFO: Got endpoints: latency-svc-7jkbj [316.194244ms]
    Nov 15 07:02:58.078: INFO: Got endpoints: latency-svc-69mc6 [275.185096ms]
    Nov 15 07:02:58.078: INFO: Got endpoints: latency-svc-bhf5f [307.114313ms]
    Nov 15 07:02:58.078: INFO: Got endpoints: latency-svc-f9vsb [275.392309ms]
    Nov 15 07:02:58.078: INFO: Got endpoints: latency-svc-pncfz [270.901452ms]
    Nov 15 07:02:58.084: INFO: Got endpoints: latency-svc-49tl5 [259.889419ms]
    Nov 15 07:02:58.089: INFO: Got endpoints: latency-svc-vbfh5 [242.006236ms]
    Nov 15 07:02:58.090: INFO: Created: latency-svc-4zmkh
    Nov 15 07:02:58.091: INFO: Got endpoints: latency-svc-jvtwc [356.275239ms]
    Nov 15 07:02:58.093: INFO: Got endpoints: latency-svc-4z5gk [220.494598ms]
    Nov 15 07:02:58.093: INFO: Got endpoints: latency-svc-d6bgt [440.281511ms]
    Nov 15 07:02:58.101: INFO: Got endpoints: latency-svc-fdmhg [448.41999ms]
    Nov 15 07:02:58.102: INFO: Created: latency-svc-c65nf
    Nov 15 07:02:58.109: INFO: Got endpoints: latency-svc-4zmkh [58.017897ms]
    Nov 15 07:02:58.114: INFO: Created: latency-svc-rtkdt
    Nov 15 07:02:58.126: INFO: Got endpoints: latency-svc-c65nf [73.467642ms]
    Nov 15 07:02:58.127: INFO: Created: latency-svc-jxvxg
    Nov 15 07:02:58.134: INFO: Got endpoints: latency-svc-rtkdt [81.861681ms]
    Nov 15 07:02:58.141: INFO: Created: latency-svc-jxhn9
    Nov 15 07:02:58.151: INFO: Got endpoints: latency-svc-jxvxg [99.986851ms]
    Nov 15 07:02:58.171: INFO: Created: latency-svc-4p822
    Nov 15 07:02:58.171: INFO: Got endpoints: latency-svc-jxhn9 [109.665747ms]
    Nov 15 07:02:58.189: INFO: Got endpoints: latency-svc-4p822 [110.490729ms]
    Nov 15 07:02:58.191: INFO: Created: latency-svc-rcht7
    Nov 15 07:02:58.199: INFO: Created: latency-svc-jqh5m
    Nov 15 07:02:58.208: INFO: Got endpoints: latency-svc-rcht7 [129.852159ms]
    Nov 15 07:02:58.210: INFO: Created: latency-svc-hwp4g
    Nov 15 07:02:58.216: INFO: Got endpoints: latency-svc-jqh5m [137.538059ms]
    Nov 15 07:02:58.224: INFO: Created: latency-svc-ct4kb
    Nov 15 07:02:58.229: INFO: Got endpoints: latency-svc-hwp4g [150.87797ms]
    Nov 15 07:02:58.231: INFO: Created: latency-svc-shslb
    Nov 15 07:02:58.241: INFO: Got endpoints: latency-svc-ct4kb [157.128472ms]
    Nov 15 07:02:58.242: INFO: Created: latency-svc-d77zd
    Nov 15 07:02:58.249: INFO: Created: latency-svc-pbr9c
    Nov 15 07:02:58.250: INFO: Got endpoints: latency-svc-shslb [160.547069ms]
    Nov 15 07:02:58.261: INFO: Got endpoints: latency-svc-d77zd [169.277047ms]
    Nov 15 07:02:58.263: INFO: Created: latency-svc-52rdj
    Nov 15 07:02:58.268: INFO: Got endpoints: latency-svc-pbr9c [175.706099ms]
    Nov 15 07:02:58.281: INFO: Got endpoints: latency-svc-52rdj [188.633885ms]
    Nov 15 07:02:58.437: INFO: Created: latency-svc-kzmln
    Nov 15 07:02:58.442: INFO: Created: latency-svc-d8f5d
    Nov 15 07:02:58.442: INFO: Created: latency-svc-dzbpz
    Nov 15 07:02:58.442: INFO: Created: latency-svc-p6m49
    Nov 15 07:02:58.443: INFO: Created: latency-svc-sqpqs
    Nov 15 07:02:58.443: INFO: Created: latency-svc-mswqs
    Nov 15 07:02:58.443: INFO: Created: latency-svc-fdjf7
    Nov 15 07:02:58.443: INFO: Created: latency-svc-rwgq2
    Nov 15 07:02:58.443: INFO: Created: latency-svc-9kf54
    Nov 15 07:02:58.445: INFO: Created: latency-svc-dhm6m
    Nov 15 07:02:58.446: INFO: Created: latency-svc-gz525
    Nov 15 07:02:58.446: INFO: Created: latency-svc-htgmk
    Nov 15 07:02:58.447: INFO: Created: latency-svc-swgtd
    Nov 15 07:02:58.447: INFO: Created: latency-svc-52rpv
    Nov 15 07:02:58.447: INFO: Created: latency-svc-7nsps
    Nov 15 07:02:58.461: INFO: Got endpoints: latency-svc-kzmln [200.193685ms]
    Nov 15 07:02:58.462: INFO: Got endpoints: latency-svc-swgtd [212.57826ms]
    Nov 15 07:02:58.462: INFO: Got endpoints: latency-svc-7nsps [221.700659ms]
    Nov 15 07:02:58.463: INFO: Got endpoints: latency-svc-p6m49 [274.08428ms]
    Nov 15 07:02:58.463: INFO: Got endpoints: latency-svc-mswqs [354.761595ms]
    Nov 15 07:02:58.481: INFO: Got endpoints: latency-svc-fdjf7 [273.041377ms]
    Nov 15 07:02:58.481: INFO: Got endpoints: latency-svc-dhm6m [199.691865ms]
    Nov 15 07:02:58.483: INFO: Got endpoints: latency-svc-sqpqs [266.737183ms]
    Nov 15 07:02:58.483: INFO: Got endpoints: latency-svc-htgmk [253.911578ms]
    Nov 15 07:02:58.483: INFO: Got endpoints: latency-svc-52rpv [357.030235ms]
    Nov 15 07:02:58.501: INFO: Created: latency-svc-tvs6v
    Nov 15 07:02:58.502: INFO: Got endpoints: latency-svc-9kf54 [233.120614ms]
    Nov 15 07:02:58.504: INFO: Got endpoints: latency-svc-rwgq2 [402.41095ms]
    Nov 15 07:02:58.504: INFO: Got endpoints: latency-svc-gz525 [352.678698ms]
    Nov 15 07:02:58.504: INFO: Got endpoints: latency-svc-d8f5d [333.079075ms]
    Nov 15 07:02:58.505: INFO: Got endpoints: latency-svc-dzbpz [370.454726ms]
    Nov 15 07:02:58.522: INFO: Got endpoints: latency-svc-tvs6v [60.780407ms]
    Nov 15 07:02:58.523: INFO: Created: latency-svc-zkkq8
    Nov 15 07:02:58.535: INFO: Created: latency-svc-crhrp
    Nov 15 07:02:58.535: INFO: Got endpoints: latency-svc-zkkq8 [73.126523ms]
    Nov 15 07:02:58.544: INFO: Created: latency-svc-nkdrk
    Nov 15 07:02:58.555: INFO: Got endpoints: latency-svc-crhrp [92.653139ms]
    Nov 15 07:02:58.558: INFO: Got endpoints: latency-svc-nkdrk [95.970904ms]
    Nov 15 07:02:58.584: INFO: Created: latency-svc-fnmpg
    Nov 15 07:02:58.602: INFO: Got endpoints: latency-svc-fnmpg [138.432269ms]
    Nov 15 07:02:58.749: INFO: Created: latency-svc-t6z4w
    Nov 15 07:02:58.753: INFO: Created: latency-svc-hkv8s
    Nov 15 07:02:58.754: INFO: Created: latency-svc-zg72h
    Nov 15 07:02:58.754: INFO: Created: latency-svc-bgcmz
    Nov 15 07:02:58.754: INFO: Created: latency-svc-vlx9z
    Nov 15 07:02:58.754: INFO: Created: latency-svc-t8pvh
    Nov 15 07:02:58.754: INFO: Created: latency-svc-g85v5
    Nov 15 07:02:58.754: INFO: Created: latency-svc-z5p2p
    Nov 15 07:02:58.755: INFO: Created: latency-svc-wmfv6
    Nov 15 07:02:58.755: INFO: Created: latency-svc-2lggv
    Nov 15 07:02:58.755: INFO: Created: latency-svc-ct2j6
    Nov 15 07:02:58.755: INFO: Created: latency-svc-4crnb
    Nov 15 07:02:58.755: INFO: Created: latency-svc-6ft6h
    Nov 15 07:02:58.755: INFO: Created: latency-svc-588kv
    Nov 15 07:02:58.755: INFO: Created: latency-svc-rw2t7
    Nov 15 07:02:58.793: INFO: Got endpoints: latency-svc-zg72h [237.330873ms]
    Nov 15 07:02:58.793: INFO: Got endpoints: latency-svc-t6z4w [311.851274ms]
    Nov 15 07:02:58.793: INFO: Got endpoints: latency-svc-hkv8s [234.798391ms]
    Nov 15 07:02:58.794: INFO: Got endpoints: latency-svc-t8pvh [258.217036ms]
    Nov 15 07:02:58.794: INFO: Got endpoints: latency-svc-vlx9z [310.681478ms]
    Nov 15 07:02:58.842: INFO: Got endpoints: latency-svc-4crnb [340.453043ms]
    Nov 15 07:02:58.843: INFO: Got endpoints: latency-svc-wmfv6 [362.057968ms]
    Nov 15 07:02:58.843: INFO: Got endpoints: latency-svc-g85v5 [359.97673ms]
    Nov 15 07:02:58.843: INFO: Got endpoints: latency-svc-588kv [360.550321ms]
    Nov 15 07:02:58.843: INFO: Got endpoints: latency-svc-2lggv [241.647813ms]
    Nov 15 07:02:58.888: INFO: Created: latency-svc-wv5fg
    Nov 15 07:02:58.889: INFO: Created: latency-svc-2m6n9
    Nov 15 07:02:58.889: INFO: Created: latency-svc-p8ffs
    Nov 15 07:02:58.889: INFO: Got endpoints: latency-svc-2m6n9 [95.259094ms]
    Nov 15 07:02:58.892: INFO: Got endpoints: latency-svc-bgcmz [387.761084ms]
    Nov 15 07:02:58.892: INFO: Got endpoints: latency-svc-z5p2p [388.208508ms]
    Nov 15 07:02:58.892: INFO: Got endpoints: latency-svc-rw2t7 [370.371732ms]
    Nov 15 07:02:58.892: INFO: Got endpoints: latency-svc-6ft6h [387.720128ms]
    Nov 15 07:02:58.893: INFO: Got endpoints: latency-svc-wv5fg [99.718907ms]
    Nov 15 07:02:58.893: INFO: Got endpoints: latency-svc-ct2j6 [388.299103ms]
    Nov 15 07:02:58.896: INFO: Created: latency-svc-j4mln
    Nov 15 07:02:58.907: INFO: Got endpoints: latency-svc-p8ffs [114.095497ms]
    Nov 15 07:02:58.911: INFO: Created: latency-svc-p9vml
    Nov 15 07:02:58.914: INFO: Got endpoints: latency-svc-j4mln [119.60431ms]
    Nov 15 07:02:58.927: INFO: Got endpoints: latency-svc-p9vml [134.414451ms]
    Nov 15 07:02:58.929: INFO: Created: latency-svc-6bn8v
    Nov 15 07:02:58.937: INFO: Created: latency-svc-xcshb
    Nov 15 07:02:58.946: INFO: Got endpoints: latency-svc-6bn8v [102.546899ms]
    Nov 15 07:02:58.954: INFO: Created: latency-svc-wdhrw
    Nov 15 07:02:58.956: INFO: Got endpoints: latency-svc-xcshb [113.692002ms]
    Nov 15 07:02:58.964: INFO: Created: latency-svc-f2sh2
    Nov 15 07:02:58.971: INFO: Got endpoints: latency-svc-wdhrw [127.260093ms]
    Nov 15 07:02:58.977: INFO: Created: latency-svc-9vwwg
    Nov 15 07:02:59.006: INFO: Created: latency-svc-8l4rv
    Nov 15 07:02:59.006: INFO: Created: latency-svc-s47sn
    Nov 15 07:02:59.006: INFO: Got endpoints: latency-svc-8l4rv [117.121002ms]
    Nov 15 07:02:59.006: INFO: Got endpoints: latency-svc-9vwwg [162.610486ms]
    Nov 15 07:02:59.006: INFO: Got endpoints: latency-svc-f2sh2 [163.434091ms]
    Nov 15 07:02:59.024: INFO: Created: latency-svc-8xv9k
    Nov 15 07:02:59.028: INFO: Got endpoints: latency-svc-s47sn [136.202224ms]
    Nov 15 07:02:59.037: INFO: Created: latency-svc-2r6v4
    Nov 15 07:02:59.044: INFO: Got endpoints: latency-svc-8xv9k [152.225792ms]
    Nov 15 07:02:59.049: INFO: Created: latency-svc-c8kjt
    Nov 15 07:02:59.062: INFO: Created: latency-svc-m2mdn
    Nov 15 07:02:59.064: INFO: Got endpoints: latency-svc-2r6v4 [171.311509ms]
    Nov 15 07:02:59.065: INFO: Got endpoints: latency-svc-c8kjt [172.421836ms]
    Nov 15 07:02:59.081: INFO: Created: latency-svc-trzrc
    Nov 15 07:02:59.083: INFO: Got endpoints: latency-svc-m2mdn [190.558247ms]
    Nov 15 07:02:59.091: INFO: Created: latency-svc-pbw69
    Nov 15 07:02:59.122: INFO: Got endpoints: latency-svc-trzrc [229.470679ms]
    Nov 15 07:02:59.123: INFO: Got endpoints: latency-svc-pbw69 [215.219799ms]
    Nov 15 07:02:59.123: INFO: Created: latency-svc-q2llr
    Nov 15 07:02:59.130: INFO: Created: latency-svc-72fbr
    Nov 15 07:02:59.140: INFO: Created: latency-svc-m7jb8
    Nov 15 07:02:59.142: INFO: Got endpoints: latency-svc-q2llr [228.007845ms]
    Nov 15 07:02:59.146: INFO: Got endpoints: latency-svc-72fbr [218.189285ms]
    Nov 15 07:02:59.158: INFO: Created: latency-svc-72kt5
    Nov 15 07:02:59.164: INFO: Got endpoints: latency-svc-m7jb8 [218.125726ms]
    Nov 15 07:02:59.165: INFO: Created: latency-svc-6bppj
    Nov 15 07:02:59.201: INFO: Created: latency-svc-mb794
    Nov 15 07:02:59.201: INFO: Got endpoints: latency-svc-6bppj [230.704421ms]
    Nov 15 07:02:59.202: INFO: Got endpoints: latency-svc-72kt5 [245.593327ms]
    Nov 15 07:02:59.212: INFO: Created: latency-svc-8swmk
    Nov 15 07:02:59.214: INFO: Got endpoints: latency-svc-mb794 [207.611132ms]
    Nov 15 07:02:59.238: INFO: Created: latency-svc-fgzb4
    Nov 15 07:02:59.245: INFO: Got endpoints: latency-svc-8swmk [238.542905ms]
    Nov 15 07:02:59.260: INFO: Got endpoints: latency-svc-fgzb4 [253.297342ms]
    Nov 15 07:02:59.525: INFO: Created: latency-svc-9jmwf
    Nov 15 07:02:59.526: INFO: Created: latency-svc-gn224
    Nov 15 07:02:59.526: INFO: Created: latency-svc-wktfk
    Nov 15 07:02:59.526: INFO: Created: latency-svc-js9ff
    Nov 15 07:02:59.557: INFO: Created: latency-svc-zf8pv
    Nov 15 07:02:59.557: INFO: Got endpoints: latency-svc-wktfk [355.40265ms]
    Nov 15 07:02:59.559: INFO: Got endpoints: latency-svc-js9ff [514.93399ms]
    Nov 15 07:02:59.562: INFO: Got endpoints: latency-svc-gn224 [533.282661ms]
    Nov 15 07:02:59.562: INFO: Got endpoints: latency-svc-9jmwf [348.309784ms]
    Nov 15 07:02:59.606: INFO: Created: latency-svc-mzm4p
    Nov 15 07:02:59.606: INFO: Created: latency-svc-rhc6c
    Nov 15 07:02:59.606: INFO: Created: latency-svc-5wldq
    Nov 15 07:02:59.614: INFO: Got endpoints: latency-svc-zf8pv [369.560081ms]
    Nov 15 07:02:59.615: INFO: Created: latency-svc-hn9r6
    Nov 15 07:02:59.615: INFO: Created: latency-svc-rb659
    Nov 15 07:02:59.615: INFO: Created: latency-svc-6cnz2
    Nov 15 07:02:59.615: INFO: Created: latency-svc-hjcgl
    Nov 15 07:02:59.615: INFO: Created: latency-svc-fmtwr
    Nov 15 07:02:59.615: INFO: Created: latency-svc-9qcrj
    Nov 15 07:02:59.615: INFO: Created: latency-svc-d5jjz
    Nov 15 07:02:59.632: INFO: Got endpoints: latency-svc-rhc6c [371.936732ms]
    Nov 15 07:02:59.641: INFO: Got endpoints: latency-svc-5wldq [557.477596ms]
    Nov 15 07:02:59.641: INFO: Got endpoints: latency-svc-mzm4p [577.336444ms]
    Nov 15 07:02:59.641: INFO: Got endpoints: latency-svc-hn9r6 [519.097686ms]
    Nov 15 07:02:59.641: INFO: Got endpoints: latency-svc-6cnz2 [518.624562ms]
    Nov 15 07:02:59.645: INFO: Created: latency-svc-7jfz7
    Nov 15 07:02:59.676: INFO: Created: latency-svc-j4n7m
    Nov 15 07:02:59.699: INFO: Created: latency-svc-zmpf8
    Nov 15 07:02:59.728: INFO: Got endpoints: latency-svc-fmtwr [586.601889ms]
    Nov 15 07:02:59.729: INFO: Got endpoints: latency-svc-9qcrj [582.996559ms]
    Nov 15 07:02:59.729: INFO: Got endpoints: latency-svc-d5jjz [565.198318ms]
    Nov 15 07:02:59.730: INFO: Got endpoints: latency-svc-rb659 [665.489419ms]
    Nov 15 07:02:59.731: INFO: Got endpoints: latency-svc-hjcgl [529.274647ms]
    Nov 15 07:02:59.749: INFO: Created: latency-svc-cs5d2
    Nov 15 07:02:59.750: INFO: Created: latency-svc-ch68q
    Nov 15 07:02:59.750: INFO: Created: latency-svc-d4nf4
    Nov 15 07:02:59.750: INFO: Got endpoints: latency-svc-7jfz7 [193.471893ms]
    Nov 15 07:02:59.751: INFO: Got endpoints: latency-svc-zmpf8 [189.567116ms]
    Nov 15 07:02:59.751: INFO: Got endpoints: latency-svc-j4n7m [189.275419ms]
    Nov 15 07:02:59.847: INFO: Got endpoints: latency-svc-ch68q [232.911627ms]
    Nov 15 07:02:59.848: INFO: Got endpoints: latency-svc-d4nf4 [216.159037ms]
    Nov 15 07:02:59.848: INFO: Got endpoints: latency-svc-cs5d2 [286.23138ms]
    Nov 15 07:02:59.857: INFO: Created: latency-svc-lxbrk
    Nov 15 07:02:59.882: INFO: Got endpoints: latency-svc-lxbrk [240.947096ms]
    Nov 15 07:02:59.882: INFO: Created: latency-svc-hsdwt
    Nov 15 07:02:59.882: INFO: Created: latency-svc-6qnpk
    Nov 15 07:02:59.895: INFO: Got endpoints: latency-svc-hsdwt [253.581707ms]
    Nov 15 07:02:59.900: INFO: Got endpoints: latency-svc-6qnpk [259.254721ms]
    Nov 15 07:02:59.904: INFO: Created: latency-svc-ptrpw
    Nov 15 07:02:59.918: INFO: Created: latency-svc-zgqrh
    Nov 15 07:02:59.920: INFO: Got endpoints: latency-svc-ptrpw [277.691532ms]
    Nov 15 07:02:59.930: INFO: Created: latency-svc-dbrzn
    Nov 15 07:02:59.934: INFO: Got endpoints: latency-svc-zgqrh [204.882924ms]
    Nov 15 07:02:59.940: INFO: Created: latency-svc-t8752
    Nov 15 07:02:59.964: INFO: Got endpoints: latency-svc-dbrzn [236.170548ms]
    Nov 15 07:02:59.970: INFO: Created: latency-svc-2tlsq
    Nov 15 07:02:59.973: INFO: Got endpoints: latency-svc-t8752 [244.446886ms]
    Nov 15 07:02:59.990: INFO: Got endpoints: latency-svc-2tlsq [256.009117ms]
    Nov 15 07:02:59.991: INFO: Created: latency-svc-r4fl8
    Nov 15 07:03:00.006: INFO: Got endpoints: latency-svc-r4fl8 [275.649662ms]
    Nov 15 07:03:00.007: INFO: Created: latency-svc-pk58s
    Nov 15 07:03:00.019: INFO: Created: latency-svc-vw6sf
    Nov 15 07:03:00.028: INFO: Got endpoints: latency-svc-pk58s [277.710203ms]
    Nov 15 07:03:00.030: INFO: Created: latency-svc-wkpc4
    Nov 15 07:03:00.040: INFO: Got endpoints: latency-svc-vw6sf [288.861215ms]
    Nov 15 07:03:00.048: INFO: Got endpoints: latency-svc-wkpc4 [296.894539ms]
    Nov 15 07:03:00.049: INFO: Created: latency-svc-4cngg
    Nov 15 07:03:00.064: INFO: Got endpoints: latency-svc-4cngg [216.895635ms]
    Nov 15 07:03:00.065: INFO: Created: latency-svc-tp5gk
    Nov 15 07:03:00.077: INFO: Created: latency-svc-cdz85
    Nov 15 07:03:00.081: INFO: Got endpoints: latency-svc-tp5gk [233.306079ms]
    Nov 15 07:03:00.104: INFO: Got endpoints: latency-svc-cdz85 [255.229526ms]
    Nov 15 07:03:00.105: INFO: Created: latency-svc-dbzxn
    Nov 15 07:03:00.116: INFO: Created: latency-svc-k2xrv
    Nov 15 07:03:00.128: INFO: Got endpoints: latency-svc-dbzxn [246.43752ms]
    Nov 15 07:03:00.129: INFO: Created: latency-svc-6n5n9
    Nov 15 07:03:00.139: INFO: Got endpoints: latency-svc-k2xrv [243.371069ms]
    Nov 15 07:03:00.140: INFO: Created: latency-svc-gjwdb
    Nov 15 07:03:00.156: INFO: Got endpoints: latency-svc-6n5n9 [255.679746ms]
    Nov 15 07:03:00.157: INFO: Got endpoints: latency-svc-gjwdb [237.528322ms]
    Nov 15 07:03:00.163: INFO: Created: latency-svc-mklj8
    Nov 15 07:03:00.170: INFO: Created: latency-svc-mnhnh
    Nov 15 07:03:00.181: INFO: Got endpoints: latency-svc-mklj8 [247.281272ms]
    Nov 15 07:03:00.190: INFO: Got endpoints: latency-svc-mnhnh [225.31851ms]
    Nov 15 07:03:00.192: INFO: Created: latency-svc-gkbk8
    Nov 15 07:03:00.207: INFO: Created: latency-svc-f55xr
    Nov 15 07:03:00.214: INFO: Got endpoints: latency-svc-gkbk8 [240.406371ms]
    Nov 15 07:03:00.220: INFO: Created: latency-svc-mt846
    Nov 15 07:03:00.223: INFO: Got endpoints: latency-svc-f55xr [233.425635ms]
    Nov 15 07:03:00.233: INFO: Created: latency-svc-wbr6h
    Nov 15 07:03:00.239: INFO: Got endpoints: latency-svc-mt846 [233.099304ms]
    Nov 15 07:03:00.246: INFO: Created: latency-svc-swcrj
    Nov 15 07:03:00.252: INFO: Got endpoints: latency-svc-wbr6h [223.804968ms]
    Nov 15 07:03:00.261: INFO: Created: latency-svc-2595w
    Nov 15 07:03:00.270: INFO: Got endpoints: latency-svc-swcrj [230.309342ms]
    Nov 15 07:03:00.273: INFO: Created: latency-svc-vrv9r
    Nov 15 07:03:00.283: INFO: Got endpoints: latency-svc-2595w [235.21397ms]
    Nov 15 07:03:00.287: INFO: Created: latency-svc-8lpld
    Nov 15 07:03:00.288: INFO: Got endpoints: latency-svc-vrv9r [223.683199ms]
    Nov 15 07:03:00.298: INFO: Created: latency-svc-fmm9v
    Nov 15 07:03:00.306: INFO: Got endpoints: latency-svc-8lpld [224.131365ms]
    Nov 15 07:03:00.313: INFO: Created: latency-svc-v8jcw
    Nov 15 07:03:00.317: INFO: Got endpoints: latency-svc-fmm9v [213.733183ms]
    Nov 15 07:03:00.328: INFO: Created: latency-svc-cqpqk
    Nov 15 07:03:00.328: INFO: Got endpoints: latency-svc-v8jcw [200.227718ms]
    Nov 15 07:03:00.350: INFO: Created: latency-svc-tsltp
    Nov 15 07:03:00.350: INFO: Got endpoints: latency-svc-cqpqk [210.737825ms]
    Nov 15 07:03:00.363: INFO: Created: latency-svc-m6dz8
    Nov 15 07:03:00.368: INFO: Got endpoints: latency-svc-tsltp [211.701983ms]
    Nov 15 07:03:00.377: INFO: Created: latency-svc-7wqbf
    Nov 15 07:03:00.387: INFO: Got endpoints: latency-svc-m6dz8 [229.437869ms]
    Nov 15 07:03:00.395: INFO: Created: latency-svc-2lx6v
    Nov 15 07:03:00.400: INFO: Got endpoints: latency-svc-7wqbf [218.424197ms]
    Nov 15 07:03:00.404: INFO: Created: latency-svc-4vhxl
    Nov 15 07:03:00.422: INFO: Got endpoints: latency-svc-2lx6v [231.757813ms]
    Nov 15 07:03:00.423: INFO: Got endpoints: latency-svc-4vhxl [208.798543ms]
    Nov 15 07:03:00.423: INFO: Created: latency-svc-4ddld
    Nov 15 07:03:00.431: INFO: Created: latency-svc-9cfqj
    Nov 15 07:03:00.443: INFO: Got endpoints: latency-svc-4ddld [219.063093ms]
    Nov 15 07:03:00.452: INFO: Got endpoints: latency-svc-9cfqj [212.854524ms]
    Nov 15 07:03:00.453: INFO: Created: latency-svc-rcd4h
    Nov 15 07:03:00.462: INFO: Created: latency-svc-pqgpp
    Nov 15 07:03:00.467: INFO: Got endpoints: latency-svc-rcd4h [215.019254ms]
    Nov 15 07:03:00.477: INFO: Created: latency-svc-rrhtv
    Nov 15 07:03:00.480: INFO: Got endpoints: latency-svc-pqgpp [210.180344ms]
    Nov 15 07:03:00.490: INFO: Created: latency-svc-9sxrv
    Nov 15 07:03:00.503: INFO: Got endpoints: latency-svc-rrhtv [219.558754ms]
    Nov 15 07:03:00.506: INFO: Created: latency-svc-kb5st
    Nov 15 07:03:00.548: INFO: Created: latency-svc-mstw4
    Nov 15 07:03:00.548: INFO: Got endpoints: latency-svc-9sxrv [260.012148ms]
    Nov 15 07:03:00.556: INFO: Got endpoints: latency-svc-kb5st [249.991634ms]
    Nov 15 07:03:00.588: INFO: Got endpoints: latency-svc-mstw4 [270.700416ms]
    Nov 15 07:03:00.589: INFO: Created: latency-svc-4zccr
    Nov 15 07:03:00.592: INFO: Created: latency-svc-p56pw
    Nov 15 07:03:00.592: INFO: Created: latency-svc-96whx
    Nov 15 07:03:00.610: INFO: Created: latency-svc-jzthr
    Nov 15 07:03:00.612: INFO: Got endpoints: latency-svc-4zccr [283.700106ms]
    Nov 15 07:03:00.613: INFO: Got endpoints: latency-svc-p56pw [244.926377ms]
    Nov 15 07:03:00.614: INFO: Got endpoints: latency-svc-96whx [263.889158ms]
    Nov 15 07:03:00.619: INFO: Created: latency-svc-gnxfx
    Nov 15 07:03:00.622: INFO: Got endpoints: latency-svc-jzthr [235.441258ms]
    Nov 15 07:03:00.635: INFO: Got endpoints: latency-svc-gnxfx [234.968725ms]
    Nov 15 07:03:00.636: INFO: Created: latency-svc-lj7g6
    Nov 15 07:03:00.650: INFO: Got endpoints: latency-svc-lj7g6 [228.614831ms]
    Nov 15 07:03:00.856: INFO: Created: latency-svc-zk77m
    Nov 15 07:03:00.861: INFO: Created: latency-svc-nnqvf
    Nov 15 07:03:00.862: INFO: Created: latency-svc-dr78c
    Nov 15 07:03:00.862: INFO: Created: latency-svc-4bwq7
    Nov 15 07:03:00.862: INFO: Created: latency-svc-ngg6n
    Nov 15 07:03:00.863: INFO: Created: latency-svc-wlvrr
    Nov 15 07:03:00.863: INFO: Created: latency-svc-l6dm4
    Nov 15 07:03:00.864: INFO: Created: latency-svc-jlkg8
    Nov 15 07:03:00.864: INFO: Created: latency-svc-rlh6n
    Nov 15 07:03:00.865: INFO: Created: latency-svc-dhrnn
    Nov 15 07:03:00.870: INFO: Created: latency-svc-6skf5
    Nov 15 07:03:00.870: INFO: Created: latency-svc-ns5qx
    Nov 15 07:03:00.870: INFO: Created: latency-svc-92rzk
    Nov 15 07:03:00.870: INFO: Created: latency-svc-cbt5g
    Nov 15 07:03:00.871: INFO: Created: latency-svc-kbxwb
    Nov 15 07:03:00.879: INFO: Got endpoints: latency-svc-l6dm4 [426.194486ms]
    Nov 15 07:03:00.880: INFO: Got endpoints: latency-svc-zk77m [229.362174ms]
    Nov 15 07:03:00.880: INFO: Got endpoints: latency-svc-4bwq7 [412.529586ms]
    Nov 15 07:03:00.883: INFO: Got endpoints: latency-svc-jlkg8 [270.753128ms]
    Nov 15 07:03:00.885: INFO: Got endpoints: latency-svc-nnqvf [442.002918ms]
    Nov 15 07:03:00.903: INFO: Got endpoints: latency-svc-wlvrr [347.828061ms]
    Nov 15 07:03:00.903: INFO: Got endpoints: latency-svc-rlh6n [355.257806ms]
    Nov 15 07:03:00.904: INFO: Got endpoints: latency-svc-dr78c [423.451887ms]
    Nov 15 07:03:00.904: INFO: Got endpoints: latency-svc-ngg6n [400.642762ms]
    Nov 15 07:03:00.904: INFO: Got endpoints: latency-svc-dhrnn [481.317906ms]
    Nov 15 07:03:00.946: INFO: Got endpoints: latency-svc-6skf5 [324.074577ms]
    Nov 15 07:03:00.947: INFO: Created: latency-svc-8qfjf
    Nov 15 07:03:00.948: INFO: Got endpoints: latency-svc-cbt5g [334.994827ms]
    Nov 15 07:03:00.950: INFO: Got endpoints: latency-svc-ns5qx [361.379637ms]
    Nov 15 07:03:00.950: INFO: Got endpoints: latency-svc-kbxwb [314.676897ms]
    Nov 15 07:03:00.950: INFO: Got endpoints: latency-svc-92rzk [335.939701ms]
    Nov 15 07:03:00.957: INFO: Created: latency-svc-bv7hq
    Nov 15 07:03:00.970: INFO: Got endpoints: latency-svc-8qfjf [91.501987ms]
    Nov 15 07:03:00.973: INFO: Created: latency-svc-z2s9d
    Nov 15 07:03:00.975: INFO: Got endpoints: latency-svc-bv7hq [95.531133ms]
    Nov 15 07:03:01.023: INFO: Created: latency-svc-8ggbz
    Nov 15 07:03:01.024: INFO: Got endpoints: latency-svc-z2s9d [144.155016ms]
    Nov 15 07:03:01.045: INFO: Got endpoints: latency-svc-8ggbz [162.353325ms]
    Nov 15 07:03:01.046: INFO: Created: latency-svc-5cm6l
    Nov 15 07:03:01.057: INFO: Created: latency-svc-mjg4j
    Nov 15 07:03:01.066: INFO: Got endpoints: latency-svc-5cm6l [181.325313ms]
    Nov 15 07:03:01.072: INFO: Created: latency-svc-g8rp7
    Nov 15 07:03:01.085: INFO: Got endpoints: latency-svc-g8rp7 [181.286411ms]
    Nov 15 07:03:01.085: INFO: Got endpoints: latency-svc-mjg4j [181.380356ms]
    Nov 15 07:03:01.088: INFO: Created: latency-svc-prmg5
    Nov 15 07:03:01.105: INFO: Created: latency-svc-dx6nf
    Nov 15 07:03:01.105: INFO: Got endpoints: latency-svc-prmg5 [201.175065ms]
    Nov 15 07:03:01.118: INFO: Created: latency-svc-wtvqg
    Nov 15 07:03:01.120: INFO: Got endpoints: latency-svc-dx6nf [216.063826ms]
    Nov 15 07:03:01.133: INFO: Created: latency-svc-nqkmp
    Nov 15 07:03:01.139: INFO: Got endpoints: latency-svc-wtvqg [234.476923ms]
    Nov 15 07:03:01.146: INFO: Got endpoints: latency-svc-nqkmp [200.064488ms]
    Nov 15 07:03:01.147: INFO: Latencies: [58.017897ms 60.780407ms 73.126523ms 73.230018ms 73.467642ms 81.861681ms 84.714594ms 91.501987ms 92.653139ms 95.112616ms 95.259094ms 95.531133ms 95.970904ms 99.718907ms 99.986851ms 102.546899ms 109.665747ms 110.490729ms 113.692002ms 114.095497ms 117.121002ms 119.60431ms 120.038673ms 127.260093ms 129.852159ms 134.414451ms 136.202224ms 137.538059ms 138.432269ms 144.155016ms 150.358284ms 150.87797ms 151.034533ms 152.225792ms 155.111352ms 157.128472ms 160.547069ms 162.353325ms 162.610486ms 163.434091ms 169.277047ms 170.975316ms 171.311509ms 172.421836ms 175.706099ms 181.196403ms 181.286411ms 181.325313ms 181.380356ms 188.633885ms 189.275419ms 189.567116ms 190.558247ms 192.788032ms 193.471893ms 193.909341ms 199.691865ms 200.064488ms 200.193685ms 200.227718ms 201.175065ms 204.882924ms 205.568544ms 207.611132ms 208.798543ms 210.180344ms 210.737825ms 211.701983ms 212.57826ms 212.854524ms 213.733183ms 215.019254ms 215.219799ms 216.063826ms 216.159037ms 216.895635ms 218.125726ms 218.189285ms 218.214708ms 218.424197ms 218.754782ms 219.063093ms 219.558754ms 220.494598ms 221.700659ms 223.683199ms 223.804968ms 224.131365ms 225.31851ms 228.007845ms 228.614831ms 229.362174ms 229.437869ms 229.470679ms 230.309342ms 230.704421ms 231.757813ms 232.911627ms 233.099304ms 233.120614ms 233.306079ms 233.425635ms 234.476923ms 234.798391ms 234.968725ms 235.21397ms 235.441258ms 236.170548ms 237.330873ms 237.528322ms 238.542905ms 240.406371ms 240.947096ms 241.647813ms 242.006236ms 243.371069ms 244.446886ms 244.926377ms 245.593327ms 246.43752ms 247.281272ms 249.991634ms 253.297342ms 253.581707ms 253.911578ms 255.229526ms 255.679746ms 256.009117ms 258.217036ms 259.254721ms 259.889419ms 260.012148ms 263.889158ms 266.737183ms 270.700416ms 270.753128ms 270.901452ms 273.041377ms 274.08428ms 275.185096ms 275.392309ms 275.649662ms 277.691532ms 277.710203ms 283.700106ms 286.23138ms 288.861215ms 296.894539ms 307.114313ms 310.681478ms 311.851274ms 314.676897ms 316.194244ms 324.074577ms 327.507208ms 333.079075ms 334.994827ms 335.939701ms 340.453043ms 347.828061ms 348.309784ms 352.678698ms 354.761595ms 355.257806ms 355.40265ms 356.275239ms 357.030235ms 359.97673ms 360.550321ms 361.379637ms 362.057968ms 369.560081ms 370.371732ms 370.454726ms 371.936732ms 387.720128ms 387.761084ms 388.208508ms 388.299103ms 399.365682ms 400.642762ms 402.41095ms 412.529586ms 423.451887ms 426.194486ms 440.281511ms 442.002918ms 448.41999ms 481.317906ms 514.93399ms 518.624562ms 519.097686ms 529.274647ms 533.282661ms 557.477596ms 565.198318ms 577.336444ms 582.996559ms 586.601889ms 665.489419ms]
    Nov 15 07:03:01.147: INFO: 50 %ile: 233.306079ms
    Nov 15 07:03:01.147: INFO: 90 %ile: 400.642762ms
    Nov 15 07:03:01.147: INFO: 99 %ile: 586.601889ms
    Nov 15 07:03:01.147: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:03:01.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-5415" for this suite. 11/15/23 07:03:01.18
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:03:01.228
Nov 15 07:03:01.228: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 07:03:01.23
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:03:01.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:03:01.298
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Nov 15 07:03:01.320: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/15/23 07:03:06.396
Nov 15 07:03:06.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-4596 --namespace=crd-publish-openapi-4596 create -f -'
Nov 15 07:03:07.940: INFO: stderr: ""
Nov 15 07:03:07.940: INFO: stdout: "e2e-test-crd-publish-openapi-993-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 15 07:03:07.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-4596 --namespace=crd-publish-openapi-4596 delete e2e-test-crd-publish-openapi-993-crds test-cr'
Nov 15 07:03:08.070: INFO: stderr: ""
Nov 15 07:03:08.070: INFO: stdout: "e2e-test-crd-publish-openapi-993-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Nov 15 07:03:08.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-4596 --namespace=crd-publish-openapi-4596 apply -f -'
Nov 15 07:03:09.598: INFO: stderr: ""
Nov 15 07:03:09.598: INFO: stdout: "e2e-test-crd-publish-openapi-993-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 15 07:03:09.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-4596 --namespace=crd-publish-openapi-4596 delete e2e-test-crd-publish-openapi-993-crds test-cr'
Nov 15 07:03:09.849: INFO: stderr: ""
Nov 15 07:03:09.849: INFO: stdout: "e2e-test-crd-publish-openapi-993-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 11/15/23 07:03:09.849
Nov 15 07:03:09.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-4596 explain e2e-test-crd-publish-openapi-993-crds'
Nov 15 07:03:10.295: INFO: stderr: ""
Nov 15 07:03:10.295: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-993-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:03:15.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4596" for this suite. 11/15/23 07:03:15.508
------------------------------
â€¢ [SLOW TEST] [14.306 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:03:01.228
    Nov 15 07:03:01.228: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 07:03:01.23
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:03:01.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:03:01.298
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Nov 15 07:03:01.320: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/15/23 07:03:06.396
    Nov 15 07:03:06.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-4596 --namespace=crd-publish-openapi-4596 create -f -'
    Nov 15 07:03:07.940: INFO: stderr: ""
    Nov 15 07:03:07.940: INFO: stdout: "e2e-test-crd-publish-openapi-993-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Nov 15 07:03:07.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-4596 --namespace=crd-publish-openapi-4596 delete e2e-test-crd-publish-openapi-993-crds test-cr'
    Nov 15 07:03:08.070: INFO: stderr: ""
    Nov 15 07:03:08.070: INFO: stdout: "e2e-test-crd-publish-openapi-993-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Nov 15 07:03:08.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-4596 --namespace=crd-publish-openapi-4596 apply -f -'
    Nov 15 07:03:09.598: INFO: stderr: ""
    Nov 15 07:03:09.598: INFO: stdout: "e2e-test-crd-publish-openapi-993-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Nov 15 07:03:09.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-4596 --namespace=crd-publish-openapi-4596 delete e2e-test-crd-publish-openapi-993-crds test-cr'
    Nov 15 07:03:09.849: INFO: stderr: ""
    Nov 15 07:03:09.849: INFO: stdout: "e2e-test-crd-publish-openapi-993-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 11/15/23 07:03:09.849
    Nov 15 07:03:09.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-4596 explain e2e-test-crd-publish-openapi-993-crds'
    Nov 15 07:03:10.295: INFO: stderr: ""
    Nov 15 07:03:10.295: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-993-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:03:15.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4596" for this suite. 11/15/23 07:03:15.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:03:15.536
Nov 15 07:03:15.537: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 07:03:15.538
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:03:15.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:03:15.608
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-fdf1f803-5f60-4021-b3d8-dace2086a37f 11/15/23 07:03:15.621
STEP: Creating a pod to test consume configMaps 11/15/23 07:03:15.644
Nov 15 07:03:15.707: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cebb8e0c-84d2-4ef0-8116-4b8cf1b19db3" in namespace "projected-2534" to be "Succeeded or Failed"
Nov 15 07:03:15.731: INFO: Pod "pod-projected-configmaps-cebb8e0c-84d2-4ef0-8116-4b8cf1b19db3": Phase="Pending", Reason="", readiness=false. Elapsed: 24.173684ms
Nov 15 07:03:17.752: INFO: Pod "pod-projected-configmaps-cebb8e0c-84d2-4ef0-8116-4b8cf1b19db3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04479037s
Nov 15 07:03:19.752: INFO: Pod "pod-projected-configmaps-cebb8e0c-84d2-4ef0-8116-4b8cf1b19db3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045412469s
STEP: Saw pod success 11/15/23 07:03:19.752
Nov 15 07:03:19.752: INFO: Pod "pod-projected-configmaps-cebb8e0c-84d2-4ef0-8116-4b8cf1b19db3" satisfied condition "Succeeded or Failed"
Nov 15 07:03:19.772: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-configmaps-cebb8e0c-84d2-4ef0-8116-4b8cf1b19db3 container agnhost-container: <nil>
STEP: delete the pod 11/15/23 07:03:19.815
Nov 15 07:03:19.866: INFO: Waiting for pod pod-projected-configmaps-cebb8e0c-84d2-4ef0-8116-4b8cf1b19db3 to disappear
Nov 15 07:03:19.906: INFO: Pod pod-projected-configmaps-cebb8e0c-84d2-4ef0-8116-4b8cf1b19db3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 15 07:03:19.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2534" for this suite. 11/15/23 07:03:19.949
------------------------------
â€¢ [4.441 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:03:15.536
    Nov 15 07:03:15.537: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 07:03:15.538
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:03:15.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:03:15.608
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-fdf1f803-5f60-4021-b3d8-dace2086a37f 11/15/23 07:03:15.621
    STEP: Creating a pod to test consume configMaps 11/15/23 07:03:15.644
    Nov 15 07:03:15.707: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cebb8e0c-84d2-4ef0-8116-4b8cf1b19db3" in namespace "projected-2534" to be "Succeeded or Failed"
    Nov 15 07:03:15.731: INFO: Pod "pod-projected-configmaps-cebb8e0c-84d2-4ef0-8116-4b8cf1b19db3": Phase="Pending", Reason="", readiness=false. Elapsed: 24.173684ms
    Nov 15 07:03:17.752: INFO: Pod "pod-projected-configmaps-cebb8e0c-84d2-4ef0-8116-4b8cf1b19db3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04479037s
    Nov 15 07:03:19.752: INFO: Pod "pod-projected-configmaps-cebb8e0c-84d2-4ef0-8116-4b8cf1b19db3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045412469s
    STEP: Saw pod success 11/15/23 07:03:19.752
    Nov 15 07:03:19.752: INFO: Pod "pod-projected-configmaps-cebb8e0c-84d2-4ef0-8116-4b8cf1b19db3" satisfied condition "Succeeded or Failed"
    Nov 15 07:03:19.772: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-configmaps-cebb8e0c-84d2-4ef0-8116-4b8cf1b19db3 container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 07:03:19.815
    Nov 15 07:03:19.866: INFO: Waiting for pod pod-projected-configmaps-cebb8e0c-84d2-4ef0-8116-4b8cf1b19db3 to disappear
    Nov 15 07:03:19.906: INFO: Pod pod-projected-configmaps-cebb8e0c-84d2-4ef0-8116-4b8cf1b19db3 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:03:19.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2534" for this suite. 11/15/23 07:03:19.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:03:19.982
Nov 15 07:03:19.982: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename disruption 11/15/23 07:03:19.983
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:03:20.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:03:20.061
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 11/15/23 07:03:20.092
STEP: Waiting for all pods to be running 11/15/23 07:03:22.247
Nov 15 07:03:22.274: INFO: running pods: 0 < 3
Nov 15 07:03:24.325: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Nov 15 07:03:26.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1917" for this suite. 11/15/23 07:03:26.343
------------------------------
â€¢ [SLOW TEST] [6.390 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:03:19.982
    Nov 15 07:03:19.982: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename disruption 11/15/23 07:03:19.983
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:03:20.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:03:20.061
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 11/15/23 07:03:20.092
    STEP: Waiting for all pods to be running 11/15/23 07:03:22.247
    Nov 15 07:03:22.274: INFO: running pods: 0 < 3
    Nov 15 07:03:24.325: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:03:26.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1917" for this suite. 11/15/23 07:03:26.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:03:26.381
Nov 15 07:03:26.381: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename tables 11/15/23 07:03:26.382
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:03:26.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:03:26.485
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Nov 15 07:03:26.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-2458" for this suite. 11/15/23 07:03:26.592
------------------------------
â€¢ [0.245 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:03:26.381
    Nov 15 07:03:26.381: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename tables 11/15/23 07:03:26.382
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:03:26.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:03:26.485
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:03:26.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-2458" for this suite. 11/15/23 07:03:26.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:03:26.628
Nov 15 07:03:26.628: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename pods 11/15/23 07:03:26.629
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:03:26.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:03:26.743
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Nov 15 07:03:26.758: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: creating the pod 11/15/23 07:03:26.759
STEP: submitting the pod to kubernetes 11/15/23 07:03:26.759
Nov 15 07:03:26.829: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-eed8ff53-f317-44a4-930b-513a53c072ca" in namespace "pods-9004" to be "running and ready"
Nov 15 07:03:26.845: INFO: Pod "pod-logs-websocket-eed8ff53-f317-44a4-930b-513a53c072ca": Phase="Pending", Reason="", readiness=false. Elapsed: 15.898866ms
Nov 15 07:03:26.846: INFO: The phase of Pod pod-logs-websocket-eed8ff53-f317-44a4-930b-513a53c072ca is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:03:28.865: INFO: Pod "pod-logs-websocket-eed8ff53-f317-44a4-930b-513a53c072ca": Phase="Running", Reason="", readiness=true. Elapsed: 2.035921621s
Nov 15 07:03:28.865: INFO: The phase of Pod pod-logs-websocket-eed8ff53-f317-44a4-930b-513a53c072ca is Running (Ready = true)
Nov 15 07:03:28.866: INFO: Pod "pod-logs-websocket-eed8ff53-f317-44a4-930b-513a53c072ca" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 07:03:28.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9004" for this suite. 11/15/23 07:03:29.004
------------------------------
â€¢ [2.404 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:03:26.628
    Nov 15 07:03:26.628: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename pods 11/15/23 07:03:26.629
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:03:26.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:03:26.743
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Nov 15 07:03:26.758: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: creating the pod 11/15/23 07:03:26.759
    STEP: submitting the pod to kubernetes 11/15/23 07:03:26.759
    Nov 15 07:03:26.829: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-eed8ff53-f317-44a4-930b-513a53c072ca" in namespace "pods-9004" to be "running and ready"
    Nov 15 07:03:26.845: INFO: Pod "pod-logs-websocket-eed8ff53-f317-44a4-930b-513a53c072ca": Phase="Pending", Reason="", readiness=false. Elapsed: 15.898866ms
    Nov 15 07:03:26.846: INFO: The phase of Pod pod-logs-websocket-eed8ff53-f317-44a4-930b-513a53c072ca is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:03:28.865: INFO: Pod "pod-logs-websocket-eed8ff53-f317-44a4-930b-513a53c072ca": Phase="Running", Reason="", readiness=true. Elapsed: 2.035921621s
    Nov 15 07:03:28.865: INFO: The phase of Pod pod-logs-websocket-eed8ff53-f317-44a4-930b-513a53c072ca is Running (Ready = true)
    Nov 15 07:03:28.866: INFO: Pod "pod-logs-websocket-eed8ff53-f317-44a4-930b-513a53c072ca" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:03:28.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9004" for this suite. 11/15/23 07:03:29.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:03:29.034
Nov 15 07:03:29.034: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename resourcequota 11/15/23 07:03:29.035
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:03:29.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:03:29.102
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 11/15/23 07:03:29.115
STEP: Creating a ResourceQuota 11/15/23 07:03:34.186
STEP: Ensuring resource quota status is calculated 11/15/23 07:03:34.202
STEP: Creating a Service 11/15/23 07:03:36.221
STEP: Creating a NodePort Service 11/15/23 07:03:36.277
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 11/15/23 07:03:36.365
STEP: Ensuring resource quota status captures service creation 11/15/23 07:03:36.442
STEP: Deleting Services 11/15/23 07:03:38.46
STEP: Ensuring resource quota status released usage 11/15/23 07:03:38.592
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 07:03:40.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8821" for this suite. 11/15/23 07:03:40.653
------------------------------
â€¢ [SLOW TEST] [11.647 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:03:29.034
    Nov 15 07:03:29.034: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename resourcequota 11/15/23 07:03:29.035
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:03:29.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:03:29.102
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 11/15/23 07:03:29.115
    STEP: Creating a ResourceQuota 11/15/23 07:03:34.186
    STEP: Ensuring resource quota status is calculated 11/15/23 07:03:34.202
    STEP: Creating a Service 11/15/23 07:03:36.221
    STEP: Creating a NodePort Service 11/15/23 07:03:36.277
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 11/15/23 07:03:36.365
    STEP: Ensuring resource quota status captures service creation 11/15/23 07:03:36.442
    STEP: Deleting Services 11/15/23 07:03:38.46
    STEP: Ensuring resource quota status released usage 11/15/23 07:03:38.592
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:03:40.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8821" for this suite. 11/15/23 07:03:40.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:03:40.682
Nov 15 07:03:40.682: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename statefulset 11/15/23 07:03:40.683
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:03:40.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:03:40.749
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-271 11/15/23 07:03:40.787
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 11/15/23 07:03:40.822
STEP: Creating stateful set ss in namespace statefulset-271 11/15/23 07:03:40.87
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-271 11/15/23 07:03:40.897
Nov 15 07:03:40.917: INFO: Found 0 stateful pods, waiting for 1
Nov 15 07:03:50.937: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 11/15/23 07:03:50.937
Nov 15 07:03:50.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-271 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 07:03:51.227: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 07:03:51.227: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 07:03:51.227: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 15 07:03:51.248: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 15 07:04:01.274: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 07:04:01.275: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 07:04:01.358: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998841s
Nov 15 07:04:02.391: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.981690329s
Nov 15 07:04:03.411: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.948434474s
Nov 15 07:04:04.427: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.929304109s
Nov 15 07:04:05.447: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.913201841s
Nov 15 07:04:06.467: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.892627416s
Nov 15 07:04:07.488: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.870964787s
Nov 15 07:04:08.511: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.851721534s
Nov 15 07:04:09.530: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.829398633s
Nov 15 07:04:10.549: INFO: Verifying statefulset ss doesn't scale past 1 for another 809.637706ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-271 11/15/23 07:04:11.549
Nov 15 07:04:11.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-271 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 15 07:04:11.896: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 15 07:04:11.896: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 15 07:04:11.896: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 15 07:04:11.914: INFO: Found 1 stateful pods, waiting for 3
Nov 15 07:04:21.941: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 07:04:21.941: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 07:04:21.941: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 11/15/23 07:04:21.941
STEP: Scale down will halt with unhealthy stateful pod 11/15/23 07:04:21.941
Nov 15 07:04:21.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-271 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 07:04:22.244: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 07:04:22.244: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 07:04:22.244: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 15 07:04:22.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-271 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 07:04:22.549: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 07:04:22.549: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 07:04:22.549: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 15 07:04:22.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-271 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 07:04:22.833: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 07:04:22.833: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 07:04:22.833: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 15 07:04:22.833: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 07:04:22.851: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Nov 15 07:04:32.892: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 07:04:32.892: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 07:04:32.892: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 07:04:32.957: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998953s
Nov 15 07:04:33.984: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.979562093s
Nov 15 07:04:35.010: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.952043343s
Nov 15 07:04:36.036: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.926435171s
Nov 15 07:04:37.063: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.900422332s
Nov 15 07:04:38.085: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.874674217s
Nov 15 07:04:39.111: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.852079273s
Nov 15 07:04:40.137: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.826486674s
Nov 15 07:04:41.164: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.799844626s
Nov 15 07:04:42.190: INFO: Verifying statefulset ss doesn't scale past 3 for another 773.553207ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-271 11/15/23 07:04:43.191
Nov 15 07:04:43.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-271 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 15 07:04:43.550: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 15 07:04:43.550: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 15 07:04:43.550: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 15 07:04:43.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-271 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 15 07:04:43.860: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 15 07:04:43.860: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 15 07:04:43.860: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 15 07:04:43.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-271 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 15 07:04:44.406: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 15 07:04:44.406: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 15 07:04:44.406: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 15 07:04:44.406: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 11/15/23 07:04:54.492
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 15 07:04:54.492: INFO: Deleting all statefulset in ns statefulset-271
Nov 15 07:04:54.510: INFO: Scaling statefulset ss to 0
Nov 15 07:04:54.565: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 07:04:54.583: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 15 07:04:54.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-271" for this suite. 11/15/23 07:04:54.682
------------------------------
â€¢ [SLOW TEST] [74.027 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:03:40.682
    Nov 15 07:03:40.682: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename statefulset 11/15/23 07:03:40.683
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:03:40.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:03:40.749
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-271 11/15/23 07:03:40.787
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 11/15/23 07:03:40.822
    STEP: Creating stateful set ss in namespace statefulset-271 11/15/23 07:03:40.87
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-271 11/15/23 07:03:40.897
    Nov 15 07:03:40.917: INFO: Found 0 stateful pods, waiting for 1
    Nov 15 07:03:50.937: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 11/15/23 07:03:50.937
    Nov 15 07:03:50.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-271 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 07:03:51.227: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 07:03:51.227: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 07:03:51.227: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 15 07:03:51.248: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Nov 15 07:04:01.274: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Nov 15 07:04:01.275: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 07:04:01.358: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998841s
    Nov 15 07:04:02.391: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.981690329s
    Nov 15 07:04:03.411: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.948434474s
    Nov 15 07:04:04.427: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.929304109s
    Nov 15 07:04:05.447: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.913201841s
    Nov 15 07:04:06.467: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.892627416s
    Nov 15 07:04:07.488: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.870964787s
    Nov 15 07:04:08.511: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.851721534s
    Nov 15 07:04:09.530: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.829398633s
    Nov 15 07:04:10.549: INFO: Verifying statefulset ss doesn't scale past 1 for another 809.637706ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-271 11/15/23 07:04:11.549
    Nov 15 07:04:11.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-271 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 15 07:04:11.896: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 15 07:04:11.896: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 15 07:04:11.896: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 15 07:04:11.914: INFO: Found 1 stateful pods, waiting for 3
    Nov 15 07:04:21.941: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 07:04:21.941: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 07:04:21.941: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 11/15/23 07:04:21.941
    STEP: Scale down will halt with unhealthy stateful pod 11/15/23 07:04:21.941
    Nov 15 07:04:21.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-271 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 07:04:22.244: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 07:04:22.244: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 07:04:22.244: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 15 07:04:22.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-271 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 07:04:22.549: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 07:04:22.549: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 07:04:22.549: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 15 07:04:22.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-271 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 07:04:22.833: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 07:04:22.833: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 07:04:22.833: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 15 07:04:22.833: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 07:04:22.851: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
    Nov 15 07:04:32.892: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Nov 15 07:04:32.892: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Nov 15 07:04:32.892: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Nov 15 07:04:32.957: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998953s
    Nov 15 07:04:33.984: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.979562093s
    Nov 15 07:04:35.010: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.952043343s
    Nov 15 07:04:36.036: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.926435171s
    Nov 15 07:04:37.063: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.900422332s
    Nov 15 07:04:38.085: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.874674217s
    Nov 15 07:04:39.111: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.852079273s
    Nov 15 07:04:40.137: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.826486674s
    Nov 15 07:04:41.164: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.799844626s
    Nov 15 07:04:42.190: INFO: Verifying statefulset ss doesn't scale past 3 for another 773.553207ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-271 11/15/23 07:04:43.191
    Nov 15 07:04:43.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-271 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 15 07:04:43.550: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 15 07:04:43.550: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 15 07:04:43.550: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 15 07:04:43.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-271 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 15 07:04:43.860: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 15 07:04:43.860: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 15 07:04:43.860: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 15 07:04:43.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-271 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 15 07:04:44.406: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 15 07:04:44.406: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 15 07:04:44.406: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 15 07:04:44.406: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 11/15/23 07:04:54.492
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 15 07:04:54.492: INFO: Deleting all statefulset in ns statefulset-271
    Nov 15 07:04:54.510: INFO: Scaling statefulset ss to 0
    Nov 15 07:04:54.565: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 07:04:54.583: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:04:54.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-271" for this suite. 11/15/23 07:04:54.682
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:04:54.709
Nov 15 07:04:54.709: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename watch 11/15/23 07:04:54.71
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:04:54.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:04:54.787
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 11/15/23 07:04:54.804
STEP: modifying the configmap once 11/15/23 07:04:54.823
STEP: modifying the configmap a second time 11/15/23 07:04:54.862
STEP: deleting the configmap 11/15/23 07:04:54.902
STEP: creating a watch on configmaps from the resource version returned by the first update 11/15/23 07:04:54.928
STEP: Expecting to observe notifications for all changes to the configmap after the first update 11/15/23 07:04:54.935
Nov 15 07:04:54.935: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9261  5538a9e8-021b-47d7-aac2-885e232db5c6 112585 0 2023-11-15 07:04:54 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-11-15 07:04:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 07:04:54.936: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9261  5538a9e8-021b-47d7-aac2-885e232db5c6 112587 0 2023-11-15 07:04:54 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-11-15 07:04:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Nov 15 07:04:54.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9261" for this suite. 11/15/23 07:04:54.964
------------------------------
â€¢ [0.283 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:04:54.709
    Nov 15 07:04:54.709: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename watch 11/15/23 07:04:54.71
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:04:54.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:04:54.787
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 11/15/23 07:04:54.804
    STEP: modifying the configmap once 11/15/23 07:04:54.823
    STEP: modifying the configmap a second time 11/15/23 07:04:54.862
    STEP: deleting the configmap 11/15/23 07:04:54.902
    STEP: creating a watch on configmaps from the resource version returned by the first update 11/15/23 07:04:54.928
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 11/15/23 07:04:54.935
    Nov 15 07:04:54.935: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9261  5538a9e8-021b-47d7-aac2-885e232db5c6 112585 0 2023-11-15 07:04:54 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-11-15 07:04:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 07:04:54.936: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9261  5538a9e8-021b-47d7-aac2-885e232db5c6 112587 0 2023-11-15 07:04:54 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-11-15 07:04:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:04:54.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9261" for this suite. 11/15/23 07:04:54.964
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:04:54.996
Nov 15 07:04:54.996: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubectl 11/15/23 07:04:54.996
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:04:55.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:04:55.063
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 11/15/23 07:04:55.076
Nov 15 07:04:55.077: INFO: namespace kubectl-7703
Nov 15 07:04:55.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-7703 create -f -'
Nov 15 07:04:56.485: INFO: stderr: ""
Nov 15 07:04:56.485: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 11/15/23 07:04:56.485
Nov 15 07:04:57.509: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 07:04:57.509: INFO: Found 0 / 1
Nov 15 07:04:58.504: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 07:04:58.504: INFO: Found 0 / 1
Nov 15 07:04:59.503: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 07:04:59.503: INFO: Found 1 / 1
Nov 15 07:04:59.503: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 15 07:04:59.521: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 07:04:59.521: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 15 07:04:59.521: INFO: wait on agnhost-primary startup in kubectl-7703 
Nov 15 07:04:59.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-7703 logs agnhost-primary-jxbwn agnhost-primary'
Nov 15 07:04:59.699: INFO: stderr: ""
Nov 15 07:04:59.699: INFO: stdout: "Paused\n"
STEP: exposing RC 11/15/23 07:04:59.699
Nov 15 07:04:59.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-7703 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Nov 15 07:04:59.983: INFO: stderr: ""
Nov 15 07:04:59.983: INFO: stdout: "service/rm2 exposed\n"
Nov 15 07:05:00.038: INFO: Service rm2 in namespace kubectl-7703 found.
STEP: exposing service 11/15/23 07:05:02.097
Nov 15 07:05:02.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-7703 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Nov 15 07:05:02.235: INFO: stderr: ""
Nov 15 07:05:02.235: INFO: stdout: "service/rm3 exposed\n"
Nov 15 07:05:02.260: INFO: Service rm3 in namespace kubectl-7703 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 07:05:04.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7703" for this suite. 11/15/23 07:05:04.343
------------------------------
â€¢ [SLOW TEST] [9.380 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:04:54.996
    Nov 15 07:04:54.996: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubectl 11/15/23 07:04:54.996
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:04:55.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:04:55.063
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 11/15/23 07:04:55.076
    Nov 15 07:04:55.077: INFO: namespace kubectl-7703
    Nov 15 07:04:55.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-7703 create -f -'
    Nov 15 07:04:56.485: INFO: stderr: ""
    Nov 15 07:04:56.485: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 11/15/23 07:04:56.485
    Nov 15 07:04:57.509: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 07:04:57.509: INFO: Found 0 / 1
    Nov 15 07:04:58.504: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 07:04:58.504: INFO: Found 0 / 1
    Nov 15 07:04:59.503: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 07:04:59.503: INFO: Found 1 / 1
    Nov 15 07:04:59.503: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Nov 15 07:04:59.521: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 07:04:59.521: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Nov 15 07:04:59.521: INFO: wait on agnhost-primary startup in kubectl-7703 
    Nov 15 07:04:59.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-7703 logs agnhost-primary-jxbwn agnhost-primary'
    Nov 15 07:04:59.699: INFO: stderr: ""
    Nov 15 07:04:59.699: INFO: stdout: "Paused\n"
    STEP: exposing RC 11/15/23 07:04:59.699
    Nov 15 07:04:59.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-7703 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Nov 15 07:04:59.983: INFO: stderr: ""
    Nov 15 07:04:59.983: INFO: stdout: "service/rm2 exposed\n"
    Nov 15 07:05:00.038: INFO: Service rm2 in namespace kubectl-7703 found.
    STEP: exposing service 11/15/23 07:05:02.097
    Nov 15 07:05:02.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-7703 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Nov 15 07:05:02.235: INFO: stderr: ""
    Nov 15 07:05:02.235: INFO: stdout: "service/rm3 exposed\n"
    Nov 15 07:05:02.260: INFO: Service rm3 in namespace kubectl-7703 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:05:04.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7703" for this suite. 11/15/23 07:05:04.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:05:04.376
Nov 15 07:05:04.376: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 11/15/23 07:05:04.377
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:05:04.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:05:04.444
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 11/15/23 07:05:04.459
STEP: Creating hostNetwork=false pod 11/15/23 07:05:04.459
Nov 15 07:05:04.508: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-5820" to be "running and ready"
Nov 15 07:05:04.533: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 24.312277ms
Nov 15 07:05:04.533: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:05:06.553: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044673436s
Nov 15 07:05:06.553: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:05:08.561: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.053016874s
Nov 15 07:05:08.561: INFO: The phase of Pod test-pod is Running (Ready = true)
Nov 15 07:05:08.561: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 11/15/23 07:05:08.602
Nov 15 07:05:08.637: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-5820" to be "running and ready"
Nov 15 07:05:08.666: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 29.713586ms
Nov 15 07:05:08.666: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:05:10.684: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.047855909s
Nov 15 07:05:10.685: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Nov 15 07:05:10.685: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 11/15/23 07:05:10.703
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 11/15/23 07:05:10.704
Nov 15 07:05:10.704: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:05:10.704: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:05:10.704: INFO: ExecWithOptions: Clientset creation
Nov 15 07:05:10.704: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Nov 15 07:05:10.914: INFO: Exec stderr: ""
Nov 15 07:05:10.914: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:05:10.914: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:05:10.915: INFO: ExecWithOptions: Clientset creation
Nov 15 07:05:10.915: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Nov 15 07:05:11.189: INFO: Exec stderr: ""
Nov 15 07:05:11.189: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:05:11.189: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:05:11.190: INFO: ExecWithOptions: Clientset creation
Nov 15 07:05:11.190: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Nov 15 07:05:11.407: INFO: Exec stderr: ""
Nov 15 07:05:11.407: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:05:11.407: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:05:11.408: INFO: ExecWithOptions: Clientset creation
Nov 15 07:05:11.408: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Nov 15 07:05:11.688: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 11/15/23 07:05:11.688
Nov 15 07:05:11.688: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:05:11.688: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:05:11.689: INFO: ExecWithOptions: Clientset creation
Nov 15 07:05:11.689: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Nov 15 07:05:11.854: INFO: Exec stderr: ""
Nov 15 07:05:11.854: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:05:11.854: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:05:11.855: INFO: ExecWithOptions: Clientset creation
Nov 15 07:05:11.855: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Nov 15 07:05:12.047: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 11/15/23 07:05:12.047
Nov 15 07:05:12.047: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:05:12.047: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:05:12.048: INFO: ExecWithOptions: Clientset creation
Nov 15 07:05:12.048: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Nov 15 07:05:12.211: INFO: Exec stderr: ""
Nov 15 07:05:12.211: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:05:12.211: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:05:12.212: INFO: ExecWithOptions: Clientset creation
Nov 15 07:05:12.212: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Nov 15 07:05:12.414: INFO: Exec stderr: ""
Nov 15 07:05:12.414: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:05:12.414: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:05:12.414: INFO: ExecWithOptions: Clientset creation
Nov 15 07:05:12.414: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Nov 15 07:05:12.628: INFO: Exec stderr: ""
Nov 15 07:05:12.628: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:05:12.628: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:05:12.629: INFO: ExecWithOptions: Clientset creation
Nov 15 07:05:12.629: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Nov 15 07:05:12.834: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Nov 15 07:05:12.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5820" for this suite. 11/15/23 07:05:12.865
------------------------------
â€¢ [SLOW TEST] [8.516 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:05:04.376
    Nov 15 07:05:04.376: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 11/15/23 07:05:04.377
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:05:04.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:05:04.444
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 11/15/23 07:05:04.459
    STEP: Creating hostNetwork=false pod 11/15/23 07:05:04.459
    Nov 15 07:05:04.508: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-5820" to be "running and ready"
    Nov 15 07:05:04.533: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 24.312277ms
    Nov 15 07:05:04.533: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:05:06.553: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044673436s
    Nov 15 07:05:06.553: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:05:08.561: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.053016874s
    Nov 15 07:05:08.561: INFO: The phase of Pod test-pod is Running (Ready = true)
    Nov 15 07:05:08.561: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 11/15/23 07:05:08.602
    Nov 15 07:05:08.637: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-5820" to be "running and ready"
    Nov 15 07:05:08.666: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 29.713586ms
    Nov 15 07:05:08.666: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:05:10.684: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.047855909s
    Nov 15 07:05:10.685: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Nov 15 07:05:10.685: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 11/15/23 07:05:10.703
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 11/15/23 07:05:10.704
    Nov 15 07:05:10.704: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:05:10.704: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:05:10.704: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:05:10.704: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Nov 15 07:05:10.914: INFO: Exec stderr: ""
    Nov 15 07:05:10.914: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:05:10.914: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:05:10.915: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:05:10.915: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Nov 15 07:05:11.189: INFO: Exec stderr: ""
    Nov 15 07:05:11.189: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:05:11.189: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:05:11.190: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:05:11.190: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Nov 15 07:05:11.407: INFO: Exec stderr: ""
    Nov 15 07:05:11.407: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:05:11.407: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:05:11.408: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:05:11.408: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Nov 15 07:05:11.688: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 11/15/23 07:05:11.688
    Nov 15 07:05:11.688: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:05:11.688: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:05:11.689: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:05:11.689: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Nov 15 07:05:11.854: INFO: Exec stderr: ""
    Nov 15 07:05:11.854: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:05:11.854: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:05:11.855: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:05:11.855: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Nov 15 07:05:12.047: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 11/15/23 07:05:12.047
    Nov 15 07:05:12.047: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:05:12.047: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:05:12.048: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:05:12.048: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Nov 15 07:05:12.211: INFO: Exec stderr: ""
    Nov 15 07:05:12.211: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:05:12.211: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:05:12.212: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:05:12.212: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Nov 15 07:05:12.414: INFO: Exec stderr: ""
    Nov 15 07:05:12.414: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:05:12.414: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:05:12.414: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:05:12.414: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Nov 15 07:05:12.628: INFO: Exec stderr: ""
    Nov 15 07:05:12.628: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5820 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:05:12.628: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:05:12.629: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:05:12.629: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5820/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Nov 15 07:05:12.834: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:05:12.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-5820" for this suite. 11/15/23 07:05:12.865
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:05:12.892
Nov 15 07:05:12.892: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename namespaces 11/15/23 07:05:12.893
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:05:12.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:05:12.961
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 11/15/23 07:05:12.974
Nov 15 07:05:12.991: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 11/15/23 07:05:12.991
Nov 15 07:05:13.019: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 11/15/23 07:05:13.019
Nov 15 07:05:13.060: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:05:13.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1157" for this suite. 11/15/23 07:05:13.089
------------------------------
â€¢ [0.224 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:05:12.892
    Nov 15 07:05:12.892: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename namespaces 11/15/23 07:05:12.893
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:05:12.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:05:12.961
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 11/15/23 07:05:12.974
    Nov 15 07:05:12.991: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 11/15/23 07:05:12.991
    Nov 15 07:05:13.019: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 11/15/23 07:05:13.019
    Nov 15 07:05:13.060: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:05:13.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1157" for this suite. 11/15/23 07:05:13.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:05:13.118
Nov 15 07:05:13.118: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename downward-api 11/15/23 07:05:13.119
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:05:13.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:05:13.218
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 11/15/23 07:05:13.23
Nov 15 07:05:13.266: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27b7584a-30e1-49ec-8bce-3fdad403384e" in namespace "downward-api-2600" to be "Succeeded or Failed"
Nov 15 07:05:13.286: INFO: Pod "downwardapi-volume-27b7584a-30e1-49ec-8bce-3fdad403384e": Phase="Pending", Reason="", readiness=false. Elapsed: 19.981411ms
Nov 15 07:05:15.306: INFO: Pod "downwardapi-volume-27b7584a-30e1-49ec-8bce-3fdad403384e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03970634s
Nov 15 07:05:17.318: INFO: Pod "downwardapi-volume-27b7584a-30e1-49ec-8bce-3fdad403384e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052015165s
STEP: Saw pod success 11/15/23 07:05:17.318
Nov 15 07:05:17.319: INFO: Pod "downwardapi-volume-27b7584a-30e1-49ec-8bce-3fdad403384e" satisfied condition "Succeeded or Failed"
Nov 15 07:05:17.337: INFO: Trying to get logs from node 10.72.152.81 pod downwardapi-volume-27b7584a-30e1-49ec-8bce-3fdad403384e container client-container: <nil>
STEP: delete the pod 11/15/23 07:05:17.445
Nov 15 07:05:17.553: INFO: Waiting for pod downwardapi-volume-27b7584a-30e1-49ec-8bce-3fdad403384e to disappear
Nov 15 07:05:17.610: INFO: Pod downwardapi-volume-27b7584a-30e1-49ec-8bce-3fdad403384e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 07:05:17.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2600" for this suite. 11/15/23 07:05:17.67
------------------------------
â€¢ [4.580 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:05:13.118
    Nov 15 07:05:13.118: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename downward-api 11/15/23 07:05:13.119
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:05:13.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:05:13.218
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 11/15/23 07:05:13.23
    Nov 15 07:05:13.266: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27b7584a-30e1-49ec-8bce-3fdad403384e" in namespace "downward-api-2600" to be "Succeeded or Failed"
    Nov 15 07:05:13.286: INFO: Pod "downwardapi-volume-27b7584a-30e1-49ec-8bce-3fdad403384e": Phase="Pending", Reason="", readiness=false. Elapsed: 19.981411ms
    Nov 15 07:05:15.306: INFO: Pod "downwardapi-volume-27b7584a-30e1-49ec-8bce-3fdad403384e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03970634s
    Nov 15 07:05:17.318: INFO: Pod "downwardapi-volume-27b7584a-30e1-49ec-8bce-3fdad403384e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052015165s
    STEP: Saw pod success 11/15/23 07:05:17.318
    Nov 15 07:05:17.319: INFO: Pod "downwardapi-volume-27b7584a-30e1-49ec-8bce-3fdad403384e" satisfied condition "Succeeded or Failed"
    Nov 15 07:05:17.337: INFO: Trying to get logs from node 10.72.152.81 pod downwardapi-volume-27b7584a-30e1-49ec-8bce-3fdad403384e container client-container: <nil>
    STEP: delete the pod 11/15/23 07:05:17.445
    Nov 15 07:05:17.553: INFO: Waiting for pod downwardapi-volume-27b7584a-30e1-49ec-8bce-3fdad403384e to disappear
    Nov 15 07:05:17.610: INFO: Pod downwardapi-volume-27b7584a-30e1-49ec-8bce-3fdad403384e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:05:17.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2600" for this suite. 11/15/23 07:05:17.67
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:05:17.698
Nov 15 07:05:17.699: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 07:05:17.7
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:05:17.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:05:17.813
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 11/15/23 07:05:17.826
Nov 15 07:05:17.827: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:05:22.629: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:05:40.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5615" for this suite. 11/15/23 07:05:40.31
------------------------------
â€¢ [SLOW TEST] [22.636 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:05:17.698
    Nov 15 07:05:17.699: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 07:05:17.7
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:05:17.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:05:17.813
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 11/15/23 07:05:17.826
    Nov 15 07:05:17.827: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:05:22.629: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:05:40.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5615" for this suite. 11/15/23 07:05:40.31
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:05:40.338
Nov 15 07:05:40.338: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 07:05:40.339
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:05:40.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:05:40.414
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-b35203e1-04fa-4990-92bd-623c918e1528 11/15/23 07:05:40.427
STEP: Creating a pod to test consume secrets 11/15/23 07:05:40.447
Nov 15 07:05:40.482: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f8346ffc-6535-4a31-8712-1b2f41ba7806" in namespace "projected-5971" to be "Succeeded or Failed"
Nov 15 07:05:40.508: INFO: Pod "pod-projected-secrets-f8346ffc-6535-4a31-8712-1b2f41ba7806": Phase="Pending", Reason="", readiness=false. Elapsed: 25.504324ms
Nov 15 07:05:42.524: INFO: Pod "pod-projected-secrets-f8346ffc-6535-4a31-8712-1b2f41ba7806": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041805832s
Nov 15 07:05:44.531: INFO: Pod "pod-projected-secrets-f8346ffc-6535-4a31-8712-1b2f41ba7806": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04866673s
STEP: Saw pod success 11/15/23 07:05:44.531
Nov 15 07:05:44.531: INFO: Pod "pod-projected-secrets-f8346ffc-6535-4a31-8712-1b2f41ba7806" satisfied condition "Succeeded or Failed"
Nov 15 07:05:44.548: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-secrets-f8346ffc-6535-4a31-8712-1b2f41ba7806 container projected-secret-volume-test: <nil>
STEP: delete the pod 11/15/23 07:05:44.585
Nov 15 07:05:44.629: INFO: Waiting for pod pod-projected-secrets-f8346ffc-6535-4a31-8712-1b2f41ba7806 to disappear
Nov 15 07:05:44.644: INFO: Pod pod-projected-secrets-f8346ffc-6535-4a31-8712-1b2f41ba7806 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 15 07:05:44.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5971" for this suite. 11/15/23 07:05:44.668
------------------------------
â€¢ [4.355 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:05:40.338
    Nov 15 07:05:40.338: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 07:05:40.339
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:05:40.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:05:40.414
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-b35203e1-04fa-4990-92bd-623c918e1528 11/15/23 07:05:40.427
    STEP: Creating a pod to test consume secrets 11/15/23 07:05:40.447
    Nov 15 07:05:40.482: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f8346ffc-6535-4a31-8712-1b2f41ba7806" in namespace "projected-5971" to be "Succeeded or Failed"
    Nov 15 07:05:40.508: INFO: Pod "pod-projected-secrets-f8346ffc-6535-4a31-8712-1b2f41ba7806": Phase="Pending", Reason="", readiness=false. Elapsed: 25.504324ms
    Nov 15 07:05:42.524: INFO: Pod "pod-projected-secrets-f8346ffc-6535-4a31-8712-1b2f41ba7806": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041805832s
    Nov 15 07:05:44.531: INFO: Pod "pod-projected-secrets-f8346ffc-6535-4a31-8712-1b2f41ba7806": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04866673s
    STEP: Saw pod success 11/15/23 07:05:44.531
    Nov 15 07:05:44.531: INFO: Pod "pod-projected-secrets-f8346ffc-6535-4a31-8712-1b2f41ba7806" satisfied condition "Succeeded or Failed"
    Nov 15 07:05:44.548: INFO: Trying to get logs from node 10.72.152.86 pod pod-projected-secrets-f8346ffc-6535-4a31-8712-1b2f41ba7806 container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 07:05:44.585
    Nov 15 07:05:44.629: INFO: Waiting for pod pod-projected-secrets-f8346ffc-6535-4a31-8712-1b2f41ba7806 to disappear
    Nov 15 07:05:44.644: INFO: Pod pod-projected-secrets-f8346ffc-6535-4a31-8712-1b2f41ba7806 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:05:44.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5971" for this suite. 11/15/23 07:05:44.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:05:44.694
Nov 15 07:05:44.694: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename init-container 11/15/23 07:05:44.695
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:05:44.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:05:44.775
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 11/15/23 07:05:44.805
Nov 15 07:05:44.805: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:05:50.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3324" for this suite. 11/15/23 07:05:50.105
------------------------------
â€¢ [SLOW TEST] [5.436 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:05:44.694
    Nov 15 07:05:44.694: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename init-container 11/15/23 07:05:44.695
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:05:44.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:05:44.775
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 11/15/23 07:05:44.805
    Nov 15 07:05:44.805: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:05:50.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3324" for this suite. 11/15/23 07:05:50.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:05:50.134
Nov 15 07:05:50.134: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubelet-test 11/15/23 07:05:50.135
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:05:50.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:05:50.208
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Nov 15 07:05:50.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2282" for this suite. 11/15/23 07:05:50.332
------------------------------
â€¢ [0.226 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:05:50.134
    Nov 15 07:05:50.134: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubelet-test 11/15/23 07:05:50.135
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:05:50.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:05:50.208
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:05:50.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2282" for this suite. 11/15/23 07:05:50.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:05:50.362
Nov 15 07:05:50.363: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename container-probe 11/15/23 07:05:50.364
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:05:50.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:05:50.442
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-a0b8c211-574a-44fb-a93b-c4651d0e4c7b in namespace container-probe-1158 11/15/23 07:05:50.453
Nov 15 07:05:50.512: INFO: Waiting up to 5m0s for pod "busybox-a0b8c211-574a-44fb-a93b-c4651d0e4c7b" in namespace "container-probe-1158" to be "not pending"
Nov 15 07:05:50.532: INFO: Pod "busybox-a0b8c211-574a-44fb-a93b-c4651d0e4c7b": Phase="Pending", Reason="", readiness=false. Elapsed: 19.894309ms
Nov 15 07:05:52.549: INFO: Pod "busybox-a0b8c211-574a-44fb-a93b-c4651d0e4c7b": Phase="Running", Reason="", readiness=true. Elapsed: 2.036200926s
Nov 15 07:05:52.549: INFO: Pod "busybox-a0b8c211-574a-44fb-a93b-c4651d0e4c7b" satisfied condition "not pending"
Nov 15 07:05:52.549: INFO: Started pod busybox-a0b8c211-574a-44fb-a93b-c4651d0e4c7b in namespace container-probe-1158
STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 07:05:52.549
Nov 15 07:05:52.564: INFO: Initial restart count of pod busybox-a0b8c211-574a-44fb-a93b-c4651d0e4c7b is 0
Nov 15 07:06:43.084: INFO: Restart count of pod container-probe-1158/busybox-a0b8c211-574a-44fb-a93b-c4651d0e4c7b is now 1 (50.5200683s elapsed)
STEP: deleting the pod 11/15/23 07:06:43.084
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 15 07:06:43.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1158" for this suite. 11/15/23 07:06:43.159
------------------------------
â€¢ [SLOW TEST] [52.821 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:05:50.362
    Nov 15 07:05:50.363: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename container-probe 11/15/23 07:05:50.364
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:05:50.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:05:50.442
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-a0b8c211-574a-44fb-a93b-c4651d0e4c7b in namespace container-probe-1158 11/15/23 07:05:50.453
    Nov 15 07:05:50.512: INFO: Waiting up to 5m0s for pod "busybox-a0b8c211-574a-44fb-a93b-c4651d0e4c7b" in namespace "container-probe-1158" to be "not pending"
    Nov 15 07:05:50.532: INFO: Pod "busybox-a0b8c211-574a-44fb-a93b-c4651d0e4c7b": Phase="Pending", Reason="", readiness=false. Elapsed: 19.894309ms
    Nov 15 07:05:52.549: INFO: Pod "busybox-a0b8c211-574a-44fb-a93b-c4651d0e4c7b": Phase="Running", Reason="", readiness=true. Elapsed: 2.036200926s
    Nov 15 07:05:52.549: INFO: Pod "busybox-a0b8c211-574a-44fb-a93b-c4651d0e4c7b" satisfied condition "not pending"
    Nov 15 07:05:52.549: INFO: Started pod busybox-a0b8c211-574a-44fb-a93b-c4651d0e4c7b in namespace container-probe-1158
    STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 07:05:52.549
    Nov 15 07:05:52.564: INFO: Initial restart count of pod busybox-a0b8c211-574a-44fb-a93b-c4651d0e4c7b is 0
    Nov 15 07:06:43.084: INFO: Restart count of pod container-probe-1158/busybox-a0b8c211-574a-44fb-a93b-c4651d0e4c7b is now 1 (50.5200683s elapsed)
    STEP: deleting the pod 11/15/23 07:06:43.084
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:06:43.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1158" for this suite. 11/15/23 07:06:43.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:06:43.186
Nov 15 07:06:43.187: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename events 11/15/23 07:06:43.188
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:06:43.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:06:43.266
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 11/15/23 07:06:43.276
Nov 15 07:06:43.297: INFO: created test-event-1
Nov 15 07:06:43.335: INFO: created test-event-2
Nov 15 07:06:43.365: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 11/15/23 07:06:43.365
STEP: delete collection of events 11/15/23 07:06:43.383
Nov 15 07:06:43.383: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 11/15/23 07:06:43.481
Nov 15 07:06:43.481: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Nov 15 07:06:43.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4700" for this suite. 11/15/23 07:06:43.518
------------------------------
â€¢ [0.355 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:06:43.186
    Nov 15 07:06:43.187: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename events 11/15/23 07:06:43.188
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:06:43.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:06:43.266
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 11/15/23 07:06:43.276
    Nov 15 07:06:43.297: INFO: created test-event-1
    Nov 15 07:06:43.335: INFO: created test-event-2
    Nov 15 07:06:43.365: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 11/15/23 07:06:43.365
    STEP: delete collection of events 11/15/23 07:06:43.383
    Nov 15 07:06:43.383: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 11/15/23 07:06:43.481
    Nov 15 07:06:43.481: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:06:43.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4700" for this suite. 11/15/23 07:06:43.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:06:43.544
Nov 15 07:06:43.544: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename disruption 11/15/23 07:06:43.545
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:06:43.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:06:43.623
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 11/15/23 07:06:43.634
STEP: Waiting for the pdb to be processed 11/15/23 07:06:43.654
STEP: updating the pdb 11/15/23 07:06:43.68
STEP: Waiting for the pdb to be processed 11/15/23 07:06:43.714
STEP: patching the pdb 11/15/23 07:06:43.735
STEP: Waiting for the pdb to be processed 11/15/23 07:06:43.777
STEP: Waiting for the pdb to be deleted 11/15/23 07:06:43.826
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Nov 15 07:06:43.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1800" for this suite. 11/15/23 07:06:43.867
------------------------------
â€¢ [0.353 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:06:43.544
    Nov 15 07:06:43.544: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename disruption 11/15/23 07:06:43.545
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:06:43.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:06:43.623
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 11/15/23 07:06:43.634
    STEP: Waiting for the pdb to be processed 11/15/23 07:06:43.654
    STEP: updating the pdb 11/15/23 07:06:43.68
    STEP: Waiting for the pdb to be processed 11/15/23 07:06:43.714
    STEP: patching the pdb 11/15/23 07:06:43.735
    STEP: Waiting for the pdb to be processed 11/15/23 07:06:43.777
    STEP: Waiting for the pdb to be deleted 11/15/23 07:06:43.826
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:06:43.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1800" for this suite. 11/15/23 07:06:43.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:06:43.898
Nov 15 07:06:43.898: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename container-probe 11/15/23 07:06:43.899
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:06:43.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:06:44.007
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296 in namespace container-probe-8546 11/15/23 07:06:44.021
Nov 15 07:06:44.060: INFO: Waiting up to 5m0s for pod "liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296" in namespace "container-probe-8546" to be "not pending"
Nov 15 07:06:44.081: INFO: Pod "liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296": Phase="Pending", Reason="", readiness=false. Elapsed: 21.814173ms
Nov 15 07:06:46.098: INFO: Pod "liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03861569s
Nov 15 07:06:48.098: INFO: Pod "liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296": Phase="Running", Reason="", readiness=true. Elapsed: 4.038647987s
Nov 15 07:06:48.098: INFO: Pod "liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296" satisfied condition "not pending"
Nov 15 07:06:48.098: INFO: Started pod liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296 in namespace container-probe-8546
STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 07:06:48.098
Nov 15 07:06:48.114: INFO: Initial restart count of pod liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296 is 0
Nov 15 07:07:06.302: INFO: Restart count of pod container-probe-8546/liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296 is now 1 (18.188373081s elapsed)
Nov 15 07:07:26.495: INFO: Restart count of pod container-probe-8546/liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296 is now 2 (38.381380109s elapsed)
Nov 15 07:07:46.687: INFO: Restart count of pod container-probe-8546/liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296 is now 3 (58.57326428s elapsed)
Nov 15 07:08:06.883: INFO: Restart count of pod container-probe-8546/liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296 is now 4 (1m18.769042309s elapsed)
Nov 15 07:09:15.570: INFO: Restart count of pod container-probe-8546/liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296 is now 5 (2m27.455974608s elapsed)
STEP: deleting the pod 11/15/23 07:09:15.57
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 15 07:09:15.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8546" for this suite. 11/15/23 07:09:15.643
------------------------------
â€¢ [SLOW TEST] [151.768 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:06:43.898
    Nov 15 07:06:43.898: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename container-probe 11/15/23 07:06:43.899
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:06:43.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:06:44.007
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296 in namespace container-probe-8546 11/15/23 07:06:44.021
    Nov 15 07:06:44.060: INFO: Waiting up to 5m0s for pod "liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296" in namespace "container-probe-8546" to be "not pending"
    Nov 15 07:06:44.081: INFO: Pod "liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296": Phase="Pending", Reason="", readiness=false. Elapsed: 21.814173ms
    Nov 15 07:06:46.098: INFO: Pod "liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03861569s
    Nov 15 07:06:48.098: INFO: Pod "liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296": Phase="Running", Reason="", readiness=true. Elapsed: 4.038647987s
    Nov 15 07:06:48.098: INFO: Pod "liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296" satisfied condition "not pending"
    Nov 15 07:06:48.098: INFO: Started pod liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296 in namespace container-probe-8546
    STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 07:06:48.098
    Nov 15 07:06:48.114: INFO: Initial restart count of pod liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296 is 0
    Nov 15 07:07:06.302: INFO: Restart count of pod container-probe-8546/liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296 is now 1 (18.188373081s elapsed)
    Nov 15 07:07:26.495: INFO: Restart count of pod container-probe-8546/liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296 is now 2 (38.381380109s elapsed)
    Nov 15 07:07:46.687: INFO: Restart count of pod container-probe-8546/liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296 is now 3 (58.57326428s elapsed)
    Nov 15 07:08:06.883: INFO: Restart count of pod container-probe-8546/liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296 is now 4 (1m18.769042309s elapsed)
    Nov 15 07:09:15.570: INFO: Restart count of pod container-probe-8546/liveness-35a7d7ec-3ab2-4c49-a620-884e05cb3296 is now 5 (2m27.455974608s elapsed)
    STEP: deleting the pod 11/15/23 07:09:15.57
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:09:15.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8546" for this suite. 11/15/23 07:09:15.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:09:15.669
Nov 15 07:09:15.669: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 07:09:15.67
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:09:15.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:09:15.75
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Nov 15 07:09:15.764: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:09:19.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2365" for this suite. 11/15/23 07:09:19.17
------------------------------
â€¢ [3.524 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:09:15.669
    Nov 15 07:09:15.669: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 07:09:15.67
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:09:15.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:09:15.75
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Nov 15 07:09:15.764: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:09:19.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2365" for this suite. 11/15/23 07:09:19.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:09:19.193
Nov 15 07:09:19.193: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename emptydir 11/15/23 07:09:19.194
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:09:19.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:09:19.304
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 11/15/23 07:09:19.313
W1115 07:09:19.403421      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Nov 15 07:09:19.403: INFO: Waiting up to 5m0s for pod "pod-a5640f1f-be8a-4801-ae5a-36307192bd19" in namespace "emptydir-2682" to be "Succeeded or Failed"
Nov 15 07:09:19.470: INFO: Pod "pod-a5640f1f-be8a-4801-ae5a-36307192bd19": Phase="Pending", Reason="", readiness=false. Elapsed: 66.684204ms
Nov 15 07:09:21.486: INFO: Pod "pod-a5640f1f-be8a-4801-ae5a-36307192bd19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083043479s
Nov 15 07:09:23.486: INFO: Pod "pod-a5640f1f-be8a-4801-ae5a-36307192bd19": Phase="Pending", Reason="", readiness=false. Elapsed: 4.082889551s
Nov 15 07:09:25.519: INFO: Pod "pod-a5640f1f-be8a-4801-ae5a-36307192bd19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.115834966s
STEP: Saw pod success 11/15/23 07:09:25.519
Nov 15 07:09:25.520: INFO: Pod "pod-a5640f1f-be8a-4801-ae5a-36307192bd19" satisfied condition "Succeeded or Failed"
Nov 15 07:09:25.534: INFO: Trying to get logs from node 10.72.152.86 pod pod-a5640f1f-be8a-4801-ae5a-36307192bd19 container test-container: <nil>
STEP: delete the pod 11/15/23 07:09:25.617
Nov 15 07:09:25.658: INFO: Waiting for pod pod-a5640f1f-be8a-4801-ae5a-36307192bd19 to disappear
Nov 15 07:09:25.674: INFO: Pod pod-a5640f1f-be8a-4801-ae5a-36307192bd19 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 07:09:25.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2682" for this suite. 11/15/23 07:09:25.716
------------------------------
â€¢ [SLOW TEST] [6.556 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:09:19.193
    Nov 15 07:09:19.193: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename emptydir 11/15/23 07:09:19.194
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:09:19.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:09:19.304
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 11/15/23 07:09:19.313
    W1115 07:09:19.403421      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Nov 15 07:09:19.403: INFO: Waiting up to 5m0s for pod "pod-a5640f1f-be8a-4801-ae5a-36307192bd19" in namespace "emptydir-2682" to be "Succeeded or Failed"
    Nov 15 07:09:19.470: INFO: Pod "pod-a5640f1f-be8a-4801-ae5a-36307192bd19": Phase="Pending", Reason="", readiness=false. Elapsed: 66.684204ms
    Nov 15 07:09:21.486: INFO: Pod "pod-a5640f1f-be8a-4801-ae5a-36307192bd19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083043479s
    Nov 15 07:09:23.486: INFO: Pod "pod-a5640f1f-be8a-4801-ae5a-36307192bd19": Phase="Pending", Reason="", readiness=false. Elapsed: 4.082889551s
    Nov 15 07:09:25.519: INFO: Pod "pod-a5640f1f-be8a-4801-ae5a-36307192bd19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.115834966s
    STEP: Saw pod success 11/15/23 07:09:25.519
    Nov 15 07:09:25.520: INFO: Pod "pod-a5640f1f-be8a-4801-ae5a-36307192bd19" satisfied condition "Succeeded or Failed"
    Nov 15 07:09:25.534: INFO: Trying to get logs from node 10.72.152.86 pod pod-a5640f1f-be8a-4801-ae5a-36307192bd19 container test-container: <nil>
    STEP: delete the pod 11/15/23 07:09:25.617
    Nov 15 07:09:25.658: INFO: Waiting for pod pod-a5640f1f-be8a-4801-ae5a-36307192bd19 to disappear
    Nov 15 07:09:25.674: INFO: Pod pod-a5640f1f-be8a-4801-ae5a-36307192bd19 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:09:25.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2682" for this suite. 11/15/23 07:09:25.716
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:09:25.749
Nov 15 07:09:25.749: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename replicaset 11/15/23 07:09:25.75
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:09:25.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:09:25.876
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 11/15/23 07:09:25.892
Nov 15 07:09:25.960: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4755" to be "running and ready"
Nov 15 07:09:26.026: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 66.263555ms
Nov 15 07:09:26.026: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:09:28.047: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.086885782s
Nov 15 07:09:28.047: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Nov 15 07:09:28.047: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 11/15/23 07:09:28.063
STEP: Then the orphan pod is adopted 11/15/23 07:09:28.092
STEP: When the matched label of one of its pods change 11/15/23 07:09:29.129
Nov 15 07:09:29.146: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 11/15/23 07:09:29.193
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Nov 15 07:09:30.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4755" for this suite. 11/15/23 07:09:30.254
------------------------------
â€¢ [4.539 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:09:25.749
    Nov 15 07:09:25.749: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename replicaset 11/15/23 07:09:25.75
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:09:25.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:09:25.876
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 11/15/23 07:09:25.892
    Nov 15 07:09:25.960: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4755" to be "running and ready"
    Nov 15 07:09:26.026: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 66.263555ms
    Nov 15 07:09:26.026: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:09:28.047: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.086885782s
    Nov 15 07:09:28.047: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Nov 15 07:09:28.047: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 11/15/23 07:09:28.063
    STEP: Then the orphan pod is adopted 11/15/23 07:09:28.092
    STEP: When the matched label of one of its pods change 11/15/23 07:09:29.129
    Nov 15 07:09:29.146: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 11/15/23 07:09:29.193
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:09:30.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4755" for this suite. 11/15/23 07:09:30.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:09:30.289
Nov 15 07:09:30.290: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 07:09:30.291
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:09:30.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:09:30.382
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 11/15/23 07:09:30.396
Nov 15 07:09:30.428: INFO: Waiting up to 5m0s for pod "annotationupdate82101a39-de29-432c-9046-546cd15f1d42" in namespace "projected-1241" to be "running and ready"
Nov 15 07:09:30.459: INFO: Pod "annotationupdate82101a39-de29-432c-9046-546cd15f1d42": Phase="Pending", Reason="", readiness=false. Elapsed: 31.118239ms
Nov 15 07:09:30.459: INFO: The phase of Pod annotationupdate82101a39-de29-432c-9046-546cd15f1d42 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:09:32.476: INFO: Pod "annotationupdate82101a39-de29-432c-9046-546cd15f1d42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048109571s
Nov 15 07:09:32.476: INFO: The phase of Pod annotationupdate82101a39-de29-432c-9046-546cd15f1d42 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:09:34.476: INFO: Pod "annotationupdate82101a39-de29-432c-9046-546cd15f1d42": Phase="Running", Reason="", readiness=true. Elapsed: 4.047887868s
Nov 15 07:09:34.476: INFO: The phase of Pod annotationupdate82101a39-de29-432c-9046-546cd15f1d42 is Running (Ready = true)
Nov 15 07:09:34.476: INFO: Pod "annotationupdate82101a39-de29-432c-9046-546cd15f1d42" satisfied condition "running and ready"
Nov 15 07:09:35.110: INFO: Successfully updated pod "annotationupdate82101a39-de29-432c-9046-546cd15f1d42"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 07:09:37.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1241" for this suite. 11/15/23 07:09:37.219
------------------------------
â€¢ [SLOW TEST] [6.948 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:09:30.289
    Nov 15 07:09:30.290: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 07:09:30.291
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:09:30.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:09:30.382
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 11/15/23 07:09:30.396
    Nov 15 07:09:30.428: INFO: Waiting up to 5m0s for pod "annotationupdate82101a39-de29-432c-9046-546cd15f1d42" in namespace "projected-1241" to be "running and ready"
    Nov 15 07:09:30.459: INFO: Pod "annotationupdate82101a39-de29-432c-9046-546cd15f1d42": Phase="Pending", Reason="", readiness=false. Elapsed: 31.118239ms
    Nov 15 07:09:30.459: INFO: The phase of Pod annotationupdate82101a39-de29-432c-9046-546cd15f1d42 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:09:32.476: INFO: Pod "annotationupdate82101a39-de29-432c-9046-546cd15f1d42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048109571s
    Nov 15 07:09:32.476: INFO: The phase of Pod annotationupdate82101a39-de29-432c-9046-546cd15f1d42 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:09:34.476: INFO: Pod "annotationupdate82101a39-de29-432c-9046-546cd15f1d42": Phase="Running", Reason="", readiness=true. Elapsed: 4.047887868s
    Nov 15 07:09:34.476: INFO: The phase of Pod annotationupdate82101a39-de29-432c-9046-546cd15f1d42 is Running (Ready = true)
    Nov 15 07:09:34.476: INFO: Pod "annotationupdate82101a39-de29-432c-9046-546cd15f1d42" satisfied condition "running and ready"
    Nov 15 07:09:35.110: INFO: Successfully updated pod "annotationupdate82101a39-de29-432c-9046-546cd15f1d42"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:09:37.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1241" for this suite. 11/15/23 07:09:37.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:09:37.241
Nov 15 07:09:37.241: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename pods 11/15/23 07:09:37.243
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:09:37.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:09:37.312
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 11/15/23 07:09:37.352
Nov 15 07:09:37.385: INFO: created test-pod-1
Nov 15 07:09:37.407: INFO: created test-pod-2
Nov 15 07:09:37.431: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 11/15/23 07:09:37.431
Nov 15 07:09:37.431: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6216' to be running and ready
Nov 15 07:09:37.501: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 15 07:09:37.501: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 15 07:09:37.501: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 15 07:09:37.501: INFO: 0 / 3 pods in namespace 'pods-6216' are running and ready (0 seconds elapsed)
Nov 15 07:09:37.501: INFO: expected 0 pod replicas in namespace 'pods-6216', 0 are Running and Ready.
Nov 15 07:09:37.501: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
Nov 15 07:09:37.501: INFO: test-pod-1  10.72.152.88  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  }]
Nov 15 07:09:37.501: INFO: test-pod-2  10.72.152.88  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  }]
Nov 15 07:09:37.501: INFO: test-pod-3  10.72.152.81  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  }]
Nov 15 07:09:37.501: INFO: 
Nov 15 07:09:39.576: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 15 07:09:39.576: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 15 07:09:39.576: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 15 07:09:39.576: INFO: 0 / 3 pods in namespace 'pods-6216' are running and ready (2 seconds elapsed)
Nov 15 07:09:39.576: INFO: expected 0 pod replicas in namespace 'pods-6216', 0 are Running and Ready.
Nov 15 07:09:39.576: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
Nov 15 07:09:39.576: INFO: test-pod-1  10.72.152.88  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  }]
Nov 15 07:09:39.577: INFO: test-pod-2  10.72.152.88  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  }]
Nov 15 07:09:39.577: INFO: test-pod-3  10.72.152.81  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  }]
Nov 15 07:09:39.577: INFO: 
Nov 15 07:09:41.557: INFO: 3 / 3 pods in namespace 'pods-6216' are running and ready (4 seconds elapsed)
Nov 15 07:09:41.557: INFO: expected 0 pod replicas in namespace 'pods-6216', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 11/15/23 07:09:41.641
Nov 15 07:09:41.659: INFO: Pod quantity 3 is different from expected quantity 0
Nov 15 07:09:42.688: INFO: Pod quantity 3 is different from expected quantity 0
Nov 15 07:09:43.675: INFO: Pod quantity 2 is different from expected quantity 0
Nov 15 07:09:44.676: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 07:09:45.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6216" for this suite. 11/15/23 07:09:45.703
------------------------------
â€¢ [SLOW TEST] [8.485 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:09:37.241
    Nov 15 07:09:37.241: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename pods 11/15/23 07:09:37.243
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:09:37.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:09:37.312
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 11/15/23 07:09:37.352
    Nov 15 07:09:37.385: INFO: created test-pod-1
    Nov 15 07:09:37.407: INFO: created test-pod-2
    Nov 15 07:09:37.431: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 11/15/23 07:09:37.431
    Nov 15 07:09:37.431: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6216' to be running and ready
    Nov 15 07:09:37.501: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Nov 15 07:09:37.501: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Nov 15 07:09:37.501: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Nov 15 07:09:37.501: INFO: 0 / 3 pods in namespace 'pods-6216' are running and ready (0 seconds elapsed)
    Nov 15 07:09:37.501: INFO: expected 0 pod replicas in namespace 'pods-6216', 0 are Running and Ready.
    Nov 15 07:09:37.501: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
    Nov 15 07:09:37.501: INFO: test-pod-1  10.72.152.88  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  }]
    Nov 15 07:09:37.501: INFO: test-pod-2  10.72.152.88  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  }]
    Nov 15 07:09:37.501: INFO: test-pod-3  10.72.152.81  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  }]
    Nov 15 07:09:37.501: INFO: 
    Nov 15 07:09:39.576: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Nov 15 07:09:39.576: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Nov 15 07:09:39.576: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Nov 15 07:09:39.576: INFO: 0 / 3 pods in namespace 'pods-6216' are running and ready (2 seconds elapsed)
    Nov 15 07:09:39.576: INFO: expected 0 pod replicas in namespace 'pods-6216', 0 are Running and Ready.
    Nov 15 07:09:39.576: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
    Nov 15 07:09:39.576: INFO: test-pod-1  10.72.152.88  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  }]
    Nov 15 07:09:39.577: INFO: test-pod-2  10.72.152.88  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  }]
    Nov 15 07:09:39.577: INFO: test-pod-3  10.72.152.81  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 07:09:37 +0000 UTC  }]
    Nov 15 07:09:39.577: INFO: 
    Nov 15 07:09:41.557: INFO: 3 / 3 pods in namespace 'pods-6216' are running and ready (4 seconds elapsed)
    Nov 15 07:09:41.557: INFO: expected 0 pod replicas in namespace 'pods-6216', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 11/15/23 07:09:41.641
    Nov 15 07:09:41.659: INFO: Pod quantity 3 is different from expected quantity 0
    Nov 15 07:09:42.688: INFO: Pod quantity 3 is different from expected quantity 0
    Nov 15 07:09:43.675: INFO: Pod quantity 2 is different from expected quantity 0
    Nov 15 07:09:44.676: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:09:45.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6216" for this suite. 11/15/23 07:09:45.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:09:45.727
Nov 15 07:09:45.727: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename services 11/15/23 07:09:45.728
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:09:45.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:09:45.804
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 11/15/23 07:09:45.815
Nov 15 07:09:45.815: INFO: Creating e2e-svc-a-srpqn
Nov 15 07:09:45.857: INFO: Creating e2e-svc-b-t8sq9
Nov 15 07:09:45.904: INFO: Creating e2e-svc-c-h6gdp
STEP: deleting service collection 11/15/23 07:09:45.958
Nov 15 07:09:46.103: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 07:09:46.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9502" for this suite. 11/15/23 07:09:46.153
------------------------------
â€¢ [0.451 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:09:45.727
    Nov 15 07:09:45.727: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename services 11/15/23 07:09:45.728
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:09:45.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:09:45.804
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 11/15/23 07:09:45.815
    Nov 15 07:09:45.815: INFO: Creating e2e-svc-a-srpqn
    Nov 15 07:09:45.857: INFO: Creating e2e-svc-b-t8sq9
    Nov 15 07:09:45.904: INFO: Creating e2e-svc-c-h6gdp
    STEP: deleting service collection 11/15/23 07:09:45.958
    Nov 15 07:09:46.103: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:09:46.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9502" for this suite. 11/15/23 07:09:46.153
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:09:46.178
Nov 15 07:09:46.178: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubectl 11/15/23 07:09:46.179
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:09:46.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:09:46.264
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 11/15/23 07:09:46.276
Nov 15 07:09:46.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 create -f -'
Nov 15 07:09:48.828: INFO: stderr: ""
Nov 15 07:09:48.828: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 11/15/23 07:09:48.828
Nov 15 07:09:48.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 07:09:49.038: INFO: stderr: ""
Nov 15 07:09:49.038: INFO: stdout: "update-demo-nautilus-94xx9 update-demo-nautilus-lxk8s "
Nov 15 07:09:49.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-94xx9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 07:09:49.194: INFO: stderr: ""
Nov 15 07:09:49.194: INFO: stdout: ""
Nov 15 07:09:49.194: INFO: update-demo-nautilus-94xx9 is created but not running
Nov 15 07:09:54.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 07:09:54.297: INFO: stderr: ""
Nov 15 07:09:54.297: INFO: stdout: "update-demo-nautilus-94xx9 update-demo-nautilus-lxk8s "
Nov 15 07:09:54.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-94xx9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 07:09:54.393: INFO: stderr: ""
Nov 15 07:09:54.393: INFO: stdout: ""
Nov 15 07:09:54.393: INFO: update-demo-nautilus-94xx9 is created but not running
Nov 15 07:09:59.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 07:09:59.507: INFO: stderr: ""
Nov 15 07:09:59.507: INFO: stdout: "update-demo-nautilus-94xx9 update-demo-nautilus-lxk8s "
Nov 15 07:09:59.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-94xx9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 07:09:59.611: INFO: stderr: ""
Nov 15 07:09:59.611: INFO: stdout: "true"
Nov 15 07:09:59.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-94xx9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 15 07:09:59.708: INFO: stderr: ""
Nov 15 07:09:59.708: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 15 07:09:59.708: INFO: validating pod update-demo-nautilus-94xx9
Nov 15 07:09:59.743: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 15 07:09:59.743: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 15 07:09:59.744: INFO: update-demo-nautilus-94xx9 is verified up and running
Nov 15 07:09:59.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-lxk8s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 07:09:59.840: INFO: stderr: ""
Nov 15 07:09:59.840: INFO: stdout: "true"
Nov 15 07:09:59.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-lxk8s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 15 07:09:59.929: INFO: stderr: ""
Nov 15 07:09:59.929: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 15 07:09:59.929: INFO: validating pod update-demo-nautilus-lxk8s
Nov 15 07:09:59.957: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 15 07:09:59.957: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 15 07:09:59.957: INFO: update-demo-nautilus-lxk8s is verified up and running
STEP: scaling down the replication controller 11/15/23 07:09:59.957
Nov 15 07:09:59.961: INFO: scanned /root for discovery docs: <nil>
Nov 15 07:09:59.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Nov 15 07:10:01.128: INFO: stderr: ""
Nov 15 07:10:01.128: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 11/15/23 07:10:01.128
Nov 15 07:10:01.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 07:10:01.283: INFO: stderr: ""
Nov 15 07:10:01.283: INFO: stdout: "update-demo-nautilus-94xx9 update-demo-nautilus-lxk8s "
STEP: Replicas for name=update-demo: expected=1 actual=2 11/15/23 07:10:01.283
Nov 15 07:10:06.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 07:10:06.404: INFO: stderr: ""
Nov 15 07:10:06.404: INFO: stdout: "update-demo-nautilus-94xx9 "
Nov 15 07:10:06.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-94xx9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 07:10:06.510: INFO: stderr: ""
Nov 15 07:10:06.510: INFO: stdout: "true"
Nov 15 07:10:06.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-94xx9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 15 07:10:06.633: INFO: stderr: ""
Nov 15 07:10:06.633: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 15 07:10:06.633: INFO: validating pod update-demo-nautilus-94xx9
Nov 15 07:10:06.662: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 15 07:10:06.662: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 15 07:10:06.662: INFO: update-demo-nautilus-94xx9 is verified up and running
STEP: scaling up the replication controller 11/15/23 07:10:06.662
Nov 15 07:10:06.666: INFO: scanned /root for discovery docs: <nil>
Nov 15 07:10:06.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Nov 15 07:10:07.815: INFO: stderr: ""
Nov 15 07:10:07.815: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 11/15/23 07:10:07.815
Nov 15 07:10:07.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 07:10:07.949: INFO: stderr: ""
Nov 15 07:10:07.949: INFO: stdout: "update-demo-nautilus-5czzz update-demo-nautilus-94xx9 "
Nov 15 07:10:07.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-5czzz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 07:10:08.046: INFO: stderr: ""
Nov 15 07:10:08.046: INFO: stdout: ""
Nov 15 07:10:08.046: INFO: update-demo-nautilus-5czzz is created but not running
Nov 15 07:10:13.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 07:10:13.168: INFO: stderr: ""
Nov 15 07:10:13.168: INFO: stdout: "update-demo-nautilus-5czzz update-demo-nautilus-94xx9 "
Nov 15 07:10:13.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-5czzz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 07:10:13.272: INFO: stderr: ""
Nov 15 07:10:13.272: INFO: stdout: "true"
Nov 15 07:10:13.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-5czzz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 15 07:10:13.385: INFO: stderr: ""
Nov 15 07:10:13.385: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 15 07:10:13.385: INFO: validating pod update-demo-nautilus-5czzz
Nov 15 07:10:13.411: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 15 07:10:13.411: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 15 07:10:13.411: INFO: update-demo-nautilus-5czzz is verified up and running
Nov 15 07:10:13.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-94xx9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 07:10:13.536: INFO: stderr: ""
Nov 15 07:10:13.536: INFO: stdout: "true"
Nov 15 07:10:13.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-94xx9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 15 07:10:13.626: INFO: stderr: ""
Nov 15 07:10:13.626: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 15 07:10:13.626: INFO: validating pod update-demo-nautilus-94xx9
Nov 15 07:10:13.651: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 15 07:10:13.651: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 15 07:10:13.651: INFO: update-demo-nautilus-94xx9 is verified up and running
STEP: using delete to clean up resources 11/15/23 07:10:13.651
Nov 15 07:10:13.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 delete --grace-period=0 --force -f -'
Nov 15 07:10:13.773: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 07:10:13.773: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 15 07:10:13.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get rc,svc -l name=update-demo --no-headers'
Nov 15 07:10:13.910: INFO: stderr: "No resources found in kubectl-1096 namespace.\n"
Nov 15 07:10:13.910: INFO: stdout: ""
Nov 15 07:10:13.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 15 07:10:14.023: INFO: stderr: ""
Nov 15 07:10:14.023: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 07:10:14.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1096" for this suite. 11/15/23 07:10:14.048
------------------------------
â€¢ [SLOW TEST] [27.892 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:09:46.178
    Nov 15 07:09:46.178: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubectl 11/15/23 07:09:46.179
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:09:46.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:09:46.264
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 11/15/23 07:09:46.276
    Nov 15 07:09:46.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 create -f -'
    Nov 15 07:09:48.828: INFO: stderr: ""
    Nov 15 07:09:48.828: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 11/15/23 07:09:48.828
    Nov 15 07:09:48.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 07:09:49.038: INFO: stderr: ""
    Nov 15 07:09:49.038: INFO: stdout: "update-demo-nautilus-94xx9 update-demo-nautilus-lxk8s "
    Nov 15 07:09:49.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-94xx9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 07:09:49.194: INFO: stderr: ""
    Nov 15 07:09:49.194: INFO: stdout: ""
    Nov 15 07:09:49.194: INFO: update-demo-nautilus-94xx9 is created but not running
    Nov 15 07:09:54.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 07:09:54.297: INFO: stderr: ""
    Nov 15 07:09:54.297: INFO: stdout: "update-demo-nautilus-94xx9 update-demo-nautilus-lxk8s "
    Nov 15 07:09:54.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-94xx9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 07:09:54.393: INFO: stderr: ""
    Nov 15 07:09:54.393: INFO: stdout: ""
    Nov 15 07:09:54.393: INFO: update-demo-nautilus-94xx9 is created but not running
    Nov 15 07:09:59.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 07:09:59.507: INFO: stderr: ""
    Nov 15 07:09:59.507: INFO: stdout: "update-demo-nautilus-94xx9 update-demo-nautilus-lxk8s "
    Nov 15 07:09:59.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-94xx9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 07:09:59.611: INFO: stderr: ""
    Nov 15 07:09:59.611: INFO: stdout: "true"
    Nov 15 07:09:59.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-94xx9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 15 07:09:59.708: INFO: stderr: ""
    Nov 15 07:09:59.708: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 15 07:09:59.708: INFO: validating pod update-demo-nautilus-94xx9
    Nov 15 07:09:59.743: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 15 07:09:59.743: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 15 07:09:59.744: INFO: update-demo-nautilus-94xx9 is verified up and running
    Nov 15 07:09:59.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-lxk8s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 07:09:59.840: INFO: stderr: ""
    Nov 15 07:09:59.840: INFO: stdout: "true"
    Nov 15 07:09:59.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-lxk8s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 15 07:09:59.929: INFO: stderr: ""
    Nov 15 07:09:59.929: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 15 07:09:59.929: INFO: validating pod update-demo-nautilus-lxk8s
    Nov 15 07:09:59.957: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 15 07:09:59.957: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 15 07:09:59.957: INFO: update-demo-nautilus-lxk8s is verified up and running
    STEP: scaling down the replication controller 11/15/23 07:09:59.957
    Nov 15 07:09:59.961: INFO: scanned /root for discovery docs: <nil>
    Nov 15 07:09:59.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Nov 15 07:10:01.128: INFO: stderr: ""
    Nov 15 07:10:01.128: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 11/15/23 07:10:01.128
    Nov 15 07:10:01.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 07:10:01.283: INFO: stderr: ""
    Nov 15 07:10:01.283: INFO: stdout: "update-demo-nautilus-94xx9 update-demo-nautilus-lxk8s "
    STEP: Replicas for name=update-demo: expected=1 actual=2 11/15/23 07:10:01.283
    Nov 15 07:10:06.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 07:10:06.404: INFO: stderr: ""
    Nov 15 07:10:06.404: INFO: stdout: "update-demo-nautilus-94xx9 "
    Nov 15 07:10:06.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-94xx9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 07:10:06.510: INFO: stderr: ""
    Nov 15 07:10:06.510: INFO: stdout: "true"
    Nov 15 07:10:06.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-94xx9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 15 07:10:06.633: INFO: stderr: ""
    Nov 15 07:10:06.633: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 15 07:10:06.633: INFO: validating pod update-demo-nautilus-94xx9
    Nov 15 07:10:06.662: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 15 07:10:06.662: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 15 07:10:06.662: INFO: update-demo-nautilus-94xx9 is verified up and running
    STEP: scaling up the replication controller 11/15/23 07:10:06.662
    Nov 15 07:10:06.666: INFO: scanned /root for discovery docs: <nil>
    Nov 15 07:10:06.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Nov 15 07:10:07.815: INFO: stderr: ""
    Nov 15 07:10:07.815: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 11/15/23 07:10:07.815
    Nov 15 07:10:07.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 07:10:07.949: INFO: stderr: ""
    Nov 15 07:10:07.949: INFO: stdout: "update-demo-nautilus-5czzz update-demo-nautilus-94xx9 "
    Nov 15 07:10:07.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-5czzz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 07:10:08.046: INFO: stderr: ""
    Nov 15 07:10:08.046: INFO: stdout: ""
    Nov 15 07:10:08.046: INFO: update-demo-nautilus-5czzz is created but not running
    Nov 15 07:10:13.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 07:10:13.168: INFO: stderr: ""
    Nov 15 07:10:13.168: INFO: stdout: "update-demo-nautilus-5czzz update-demo-nautilus-94xx9 "
    Nov 15 07:10:13.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-5czzz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 07:10:13.272: INFO: stderr: ""
    Nov 15 07:10:13.272: INFO: stdout: "true"
    Nov 15 07:10:13.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-5czzz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 15 07:10:13.385: INFO: stderr: ""
    Nov 15 07:10:13.385: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 15 07:10:13.385: INFO: validating pod update-demo-nautilus-5czzz
    Nov 15 07:10:13.411: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 15 07:10:13.411: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 15 07:10:13.411: INFO: update-demo-nautilus-5czzz is verified up and running
    Nov 15 07:10:13.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-94xx9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 07:10:13.536: INFO: stderr: ""
    Nov 15 07:10:13.536: INFO: stdout: "true"
    Nov 15 07:10:13.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods update-demo-nautilus-94xx9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 15 07:10:13.626: INFO: stderr: ""
    Nov 15 07:10:13.626: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 15 07:10:13.626: INFO: validating pod update-demo-nautilus-94xx9
    Nov 15 07:10:13.651: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 15 07:10:13.651: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 15 07:10:13.651: INFO: update-demo-nautilus-94xx9 is verified up and running
    STEP: using delete to clean up resources 11/15/23 07:10:13.651
    Nov 15 07:10:13.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 delete --grace-period=0 --force -f -'
    Nov 15 07:10:13.773: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 15 07:10:13.773: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Nov 15 07:10:13.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get rc,svc -l name=update-demo --no-headers'
    Nov 15 07:10:13.910: INFO: stderr: "No resources found in kubectl-1096 namespace.\n"
    Nov 15 07:10:13.910: INFO: stdout: ""
    Nov 15 07:10:13.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-1096 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Nov 15 07:10:14.023: INFO: stderr: ""
    Nov 15 07:10:14.023: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:10:14.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1096" for this suite. 11/15/23 07:10:14.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:10:14.078
Nov 15 07:10:14.078: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 07:10:14.079
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:10:14.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:10:14.151
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-21276934-aeaf-4633-af68-8ab7dc313c91 11/15/23 07:10:14.161
STEP: Creating a pod to test consume secrets 11/15/23 07:10:14.18
Nov 15 07:10:14.216: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b0feb616-924b-4d09-9aa1-66c8e8bf23bf" in namespace "projected-288" to be "Succeeded or Failed"
Nov 15 07:10:14.235: INFO: Pod "pod-projected-secrets-b0feb616-924b-4d09-9aa1-66c8e8bf23bf": Phase="Pending", Reason="", readiness=false. Elapsed: 18.922855ms
Nov 15 07:10:16.249: INFO: Pod "pod-projected-secrets-b0feb616-924b-4d09-9aa1-66c8e8bf23bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033619858s
Nov 15 07:10:18.254: INFO: Pod "pod-projected-secrets-b0feb616-924b-4d09-9aa1-66c8e8bf23bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037732023s
STEP: Saw pod success 11/15/23 07:10:18.254
Nov 15 07:10:18.254: INFO: Pod "pod-projected-secrets-b0feb616-924b-4d09-9aa1-66c8e8bf23bf" satisfied condition "Succeeded or Failed"
Nov 15 07:10:18.269: INFO: Trying to get logs from node 10.72.152.88 pod pod-projected-secrets-b0feb616-924b-4d09-9aa1-66c8e8bf23bf container secret-volume-test: <nil>
STEP: delete the pod 11/15/23 07:10:18.347
Nov 15 07:10:18.385: INFO: Waiting for pod pod-projected-secrets-b0feb616-924b-4d09-9aa1-66c8e8bf23bf to disappear
Nov 15 07:10:18.404: INFO: Pod pod-projected-secrets-b0feb616-924b-4d09-9aa1-66c8e8bf23bf no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 15 07:10:18.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-288" for this suite. 11/15/23 07:10:18.43
------------------------------
â€¢ [4.375 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:10:14.078
    Nov 15 07:10:14.078: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 07:10:14.079
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:10:14.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:10:14.151
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-21276934-aeaf-4633-af68-8ab7dc313c91 11/15/23 07:10:14.161
    STEP: Creating a pod to test consume secrets 11/15/23 07:10:14.18
    Nov 15 07:10:14.216: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b0feb616-924b-4d09-9aa1-66c8e8bf23bf" in namespace "projected-288" to be "Succeeded or Failed"
    Nov 15 07:10:14.235: INFO: Pod "pod-projected-secrets-b0feb616-924b-4d09-9aa1-66c8e8bf23bf": Phase="Pending", Reason="", readiness=false. Elapsed: 18.922855ms
    Nov 15 07:10:16.249: INFO: Pod "pod-projected-secrets-b0feb616-924b-4d09-9aa1-66c8e8bf23bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033619858s
    Nov 15 07:10:18.254: INFO: Pod "pod-projected-secrets-b0feb616-924b-4d09-9aa1-66c8e8bf23bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037732023s
    STEP: Saw pod success 11/15/23 07:10:18.254
    Nov 15 07:10:18.254: INFO: Pod "pod-projected-secrets-b0feb616-924b-4d09-9aa1-66c8e8bf23bf" satisfied condition "Succeeded or Failed"
    Nov 15 07:10:18.269: INFO: Trying to get logs from node 10.72.152.88 pod pod-projected-secrets-b0feb616-924b-4d09-9aa1-66c8e8bf23bf container secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 07:10:18.347
    Nov 15 07:10:18.385: INFO: Waiting for pod pod-projected-secrets-b0feb616-924b-4d09-9aa1-66c8e8bf23bf to disappear
    Nov 15 07:10:18.404: INFO: Pod pod-projected-secrets-b0feb616-924b-4d09-9aa1-66c8e8bf23bf no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:10:18.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-288" for this suite. 11/15/23 07:10:18.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:10:18.454
Nov 15 07:10:18.454: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename dns 11/15/23 07:10:18.455
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:10:18.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:10:18.539
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 11/15/23 07:10:18.554
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5528.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local; sleep 1; done
 11/15/23 07:10:18.585
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5528.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local; sleep 1; done
 11/15/23 07:10:18.585
STEP: creating a pod to probe DNS 11/15/23 07:10:18.586
STEP: submitting the pod to kubernetes 11/15/23 07:10:18.586
Nov 15 07:10:18.627: INFO: Waiting up to 15m0s for pod "dns-test-98f5785d-8648-4509-b5c4-d0aacf604625" in namespace "dns-5528" to be "running"
Nov 15 07:10:18.653: INFO: Pod "dns-test-98f5785d-8648-4509-b5c4-d0aacf604625": Phase="Pending", Reason="", readiness=false. Elapsed: 26.239803ms
Nov 15 07:10:20.681: INFO: Pod "dns-test-98f5785d-8648-4509-b5c4-d0aacf604625": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054257649s
Nov 15 07:10:22.671: INFO: Pod "dns-test-98f5785d-8648-4509-b5c4-d0aacf604625": Phase="Running", Reason="", readiness=true. Elapsed: 4.043738941s
Nov 15 07:10:22.671: INFO: Pod "dns-test-98f5785d-8648-4509-b5c4-d0aacf604625" satisfied condition "running"
STEP: retrieving the pod 11/15/23 07:10:22.671
STEP: looking for the results for each expected name from probers 11/15/23 07:10:22.686
Nov 15 07:10:22.738: INFO: DNS probes using dns-test-98f5785d-8648-4509-b5c4-d0aacf604625 succeeded

STEP: deleting the pod 11/15/23 07:10:22.738
STEP: changing the externalName to bar.example.com 11/15/23 07:10:22.782
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5528.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local; sleep 1; done
 11/15/23 07:10:22.817
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5528.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local; sleep 1; done
 11/15/23 07:10:22.817
STEP: creating a second pod to probe DNS 11/15/23 07:10:22.817
STEP: submitting the pod to kubernetes 11/15/23 07:10:22.817
Nov 15 07:10:22.845: INFO: Waiting up to 15m0s for pod "dns-test-1b789bfc-59b3-495f-82b0-ee360fea5985" in namespace "dns-5528" to be "running"
Nov 15 07:10:22.866: INFO: Pod "dns-test-1b789bfc-59b3-495f-82b0-ee360fea5985": Phase="Pending", Reason="", readiness=false. Elapsed: 20.743409ms
Nov 15 07:10:24.884: INFO: Pod "dns-test-1b789bfc-59b3-495f-82b0-ee360fea5985": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03881284s
Nov 15 07:10:26.884: INFO: Pod "dns-test-1b789bfc-59b3-495f-82b0-ee360fea5985": Phase="Running", Reason="", readiness=true. Elapsed: 4.039217051s
Nov 15 07:10:26.884: INFO: Pod "dns-test-1b789bfc-59b3-495f-82b0-ee360fea5985" satisfied condition "running"
STEP: retrieving the pod 11/15/23 07:10:26.884
STEP: looking for the results for each expected name from probers 11/15/23 07:10:26.9
Nov 15 07:10:26.954: INFO: DNS probes using dns-test-1b789bfc-59b3-495f-82b0-ee360fea5985 succeeded

STEP: deleting the pod 11/15/23 07:10:26.954
STEP: changing the service to type=ClusterIP 11/15/23 07:10:27.003
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5528.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local; sleep 1; done
 11/15/23 07:10:27.068
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5528.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local; sleep 1; done
 11/15/23 07:10:27.068
STEP: creating a third pod to probe DNS 11/15/23 07:10:27.068
STEP: submitting the pod to kubernetes 11/15/23 07:10:27.083
Nov 15 07:10:27.105: INFO: Waiting up to 15m0s for pod "dns-test-308d52e2-66b0-4d9a-98d3-44f73b1ea1a4" in namespace "dns-5528" to be "running"
Nov 15 07:10:27.120: INFO: Pod "dns-test-308d52e2-66b0-4d9a-98d3-44f73b1ea1a4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.548958ms
Nov 15 07:10:29.155: INFO: Pod "dns-test-308d52e2-66b0-4d9a-98d3-44f73b1ea1a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.050159771s
Nov 15 07:10:29.155: INFO: Pod "dns-test-308d52e2-66b0-4d9a-98d3-44f73b1ea1a4" satisfied condition "running"
STEP: retrieving the pod 11/15/23 07:10:29.155
STEP: looking for the results for each expected name from probers 11/15/23 07:10:29.176
Nov 15 07:10:29.293: INFO: DNS probes using dns-test-308d52e2-66b0-4d9a-98d3-44f73b1ea1a4 succeeded

STEP: deleting the pod 11/15/23 07:10:29.301
STEP: deleting the test externalName service 11/15/23 07:10:29.348
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 15 07:10:29.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5528" for this suite. 11/15/23 07:10:29.469
------------------------------
â€¢ [SLOW TEST] [11.073 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:10:18.454
    Nov 15 07:10:18.454: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename dns 11/15/23 07:10:18.455
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:10:18.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:10:18.539
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 11/15/23 07:10:18.554
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5528.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local; sleep 1; done
     11/15/23 07:10:18.585
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5528.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local; sleep 1; done
     11/15/23 07:10:18.585
    STEP: creating a pod to probe DNS 11/15/23 07:10:18.586
    STEP: submitting the pod to kubernetes 11/15/23 07:10:18.586
    Nov 15 07:10:18.627: INFO: Waiting up to 15m0s for pod "dns-test-98f5785d-8648-4509-b5c4-d0aacf604625" in namespace "dns-5528" to be "running"
    Nov 15 07:10:18.653: INFO: Pod "dns-test-98f5785d-8648-4509-b5c4-d0aacf604625": Phase="Pending", Reason="", readiness=false. Elapsed: 26.239803ms
    Nov 15 07:10:20.681: INFO: Pod "dns-test-98f5785d-8648-4509-b5c4-d0aacf604625": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054257649s
    Nov 15 07:10:22.671: INFO: Pod "dns-test-98f5785d-8648-4509-b5c4-d0aacf604625": Phase="Running", Reason="", readiness=true. Elapsed: 4.043738941s
    Nov 15 07:10:22.671: INFO: Pod "dns-test-98f5785d-8648-4509-b5c4-d0aacf604625" satisfied condition "running"
    STEP: retrieving the pod 11/15/23 07:10:22.671
    STEP: looking for the results for each expected name from probers 11/15/23 07:10:22.686
    Nov 15 07:10:22.738: INFO: DNS probes using dns-test-98f5785d-8648-4509-b5c4-d0aacf604625 succeeded

    STEP: deleting the pod 11/15/23 07:10:22.738
    STEP: changing the externalName to bar.example.com 11/15/23 07:10:22.782
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5528.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local; sleep 1; done
     11/15/23 07:10:22.817
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5528.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local; sleep 1; done
     11/15/23 07:10:22.817
    STEP: creating a second pod to probe DNS 11/15/23 07:10:22.817
    STEP: submitting the pod to kubernetes 11/15/23 07:10:22.817
    Nov 15 07:10:22.845: INFO: Waiting up to 15m0s for pod "dns-test-1b789bfc-59b3-495f-82b0-ee360fea5985" in namespace "dns-5528" to be "running"
    Nov 15 07:10:22.866: INFO: Pod "dns-test-1b789bfc-59b3-495f-82b0-ee360fea5985": Phase="Pending", Reason="", readiness=false. Elapsed: 20.743409ms
    Nov 15 07:10:24.884: INFO: Pod "dns-test-1b789bfc-59b3-495f-82b0-ee360fea5985": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03881284s
    Nov 15 07:10:26.884: INFO: Pod "dns-test-1b789bfc-59b3-495f-82b0-ee360fea5985": Phase="Running", Reason="", readiness=true. Elapsed: 4.039217051s
    Nov 15 07:10:26.884: INFO: Pod "dns-test-1b789bfc-59b3-495f-82b0-ee360fea5985" satisfied condition "running"
    STEP: retrieving the pod 11/15/23 07:10:26.884
    STEP: looking for the results for each expected name from probers 11/15/23 07:10:26.9
    Nov 15 07:10:26.954: INFO: DNS probes using dns-test-1b789bfc-59b3-495f-82b0-ee360fea5985 succeeded

    STEP: deleting the pod 11/15/23 07:10:26.954
    STEP: changing the service to type=ClusterIP 11/15/23 07:10:27.003
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5528.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local; sleep 1; done
     11/15/23 07:10:27.068
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5528.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local; sleep 1; done
     11/15/23 07:10:27.068
    STEP: creating a third pod to probe DNS 11/15/23 07:10:27.068
    STEP: submitting the pod to kubernetes 11/15/23 07:10:27.083
    Nov 15 07:10:27.105: INFO: Waiting up to 15m0s for pod "dns-test-308d52e2-66b0-4d9a-98d3-44f73b1ea1a4" in namespace "dns-5528" to be "running"
    Nov 15 07:10:27.120: INFO: Pod "dns-test-308d52e2-66b0-4d9a-98d3-44f73b1ea1a4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.548958ms
    Nov 15 07:10:29.155: INFO: Pod "dns-test-308d52e2-66b0-4d9a-98d3-44f73b1ea1a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.050159771s
    Nov 15 07:10:29.155: INFO: Pod "dns-test-308d52e2-66b0-4d9a-98d3-44f73b1ea1a4" satisfied condition "running"
    STEP: retrieving the pod 11/15/23 07:10:29.155
    STEP: looking for the results for each expected name from probers 11/15/23 07:10:29.176
    Nov 15 07:10:29.293: INFO: DNS probes using dns-test-308d52e2-66b0-4d9a-98d3-44f73b1ea1a4 succeeded

    STEP: deleting the pod 11/15/23 07:10:29.301
    STEP: deleting the test externalName service 11/15/23 07:10:29.348
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:10:29.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5528" for this suite. 11/15/23 07:10:29.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:10:29.527
Nov 15 07:10:29.528: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename pod-network-test 11/15/23 07:10:29.529
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:10:29.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:10:29.633
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-9155 11/15/23 07:10:29.643
STEP: creating a selector 11/15/23 07:10:29.643
STEP: Creating the service pods in kubernetes 11/15/23 07:10:29.643
Nov 15 07:10:29.643: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 15 07:10:29.875: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9155" to be "running and ready"
Nov 15 07:10:29.893: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.603325ms
Nov 15 07:10:29.893: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:10:31.908: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.032661601s
Nov 15 07:10:31.908: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:10:33.908: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.032936562s
Nov 15 07:10:33.908: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:10:35.911: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.035831522s
Nov 15 07:10:35.911: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:10:37.911: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.036153282s
Nov 15 07:10:37.911: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:10:39.908: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.033236816s
Nov 15 07:10:39.908: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:10:41.909: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.034195081s
Nov 15 07:10:41.909: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:10:43.926: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.050826698s
Nov 15 07:10:43.926: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:10:45.912: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.037358843s
Nov 15 07:10:45.913: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:10:47.909: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.034228313s
Nov 15 07:10:47.909: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:10:49.907: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.032138032s
Nov 15 07:10:49.907: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:10:51.908: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.033101836s
Nov 15 07:10:51.908: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Nov 15 07:10:51.908: INFO: Pod "netserver-0" satisfied condition "running and ready"
Nov 15 07:10:51.923: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9155" to be "running and ready"
Nov 15 07:10:51.937: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 13.606313ms
Nov 15 07:10:51.937: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Nov 15 07:10:51.937: INFO: Pod "netserver-1" satisfied condition "running and ready"
Nov 15 07:10:51.952: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9155" to be "running and ready"
Nov 15 07:10:51.966: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 14.30933ms
Nov 15 07:10:51.967: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Nov 15 07:10:51.967: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 11/15/23 07:10:51.981
Nov 15 07:10:52.007: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9155" to be "running"
Nov 15 07:10:52.020: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.35155ms
Nov 15 07:10:54.037: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030385052s
Nov 15 07:10:56.039: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.032014875s
Nov 15 07:10:56.039: INFO: Pod "test-container-pod" satisfied condition "running"
Nov 15 07:10:56.052: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 15 07:10:56.052: INFO: Breadth first check of 172.30.214.183 on host 10.72.152.81...
Nov 15 07:10:56.065: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.213.181:9080/dial?request=hostname&protocol=udp&host=172.30.214.183&port=8081&tries=1'] Namespace:pod-network-test-9155 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:10:56.065: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:10:56.065: INFO: ExecWithOptions: Clientset creation
Nov 15 07:10:56.065: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9155/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.213.181%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.214.183%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 15 07:10:56.250: INFO: Waiting for responses: map[]
Nov 15 07:10:56.250: INFO: reached 172.30.214.183 after 0/1 tries
Nov 15 07:10:56.250: INFO: Breadth first check of 172.30.213.191 on host 10.72.152.86...
Nov 15 07:10:56.265: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.213.181:9080/dial?request=hostname&protocol=udp&host=172.30.213.191&port=8081&tries=1'] Namespace:pod-network-test-9155 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:10:56.265: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:10:56.266: INFO: ExecWithOptions: Clientset creation
Nov 15 07:10:56.266: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9155/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.213.181%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.213.191%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 15 07:10:56.484: INFO: Waiting for responses: map[]
Nov 15 07:10:56.484: INFO: reached 172.30.213.191 after 0/1 tries
Nov 15 07:10:56.484: INFO: Breadth first check of 172.30.10.171 on host 10.72.152.88...
Nov 15 07:10:56.503: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.213.181:9080/dial?request=hostname&protocol=udp&host=172.30.10.171&port=8081&tries=1'] Namespace:pod-network-test-9155 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:10:56.503: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:10:56.503: INFO: ExecWithOptions: Clientset creation
Nov 15 07:10:56.503: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9155/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.213.181%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.10.171%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 15 07:10:56.693: INFO: Waiting for responses: map[]
Nov 15 07:10:56.693: INFO: reached 172.30.10.171 after 0/1 tries
Nov 15 07:10:56.693: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Nov 15 07:10:56.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9155" for this suite. 11/15/23 07:10:56.73
------------------------------
â€¢ [SLOW TEST] [27.232 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:10:29.527
    Nov 15 07:10:29.528: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename pod-network-test 11/15/23 07:10:29.529
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:10:29.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:10:29.633
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-9155 11/15/23 07:10:29.643
    STEP: creating a selector 11/15/23 07:10:29.643
    STEP: Creating the service pods in kubernetes 11/15/23 07:10:29.643
    Nov 15 07:10:29.643: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Nov 15 07:10:29.875: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9155" to be "running and ready"
    Nov 15 07:10:29.893: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.603325ms
    Nov 15 07:10:29.893: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:10:31.908: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.032661601s
    Nov 15 07:10:31.908: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:10:33.908: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.032936562s
    Nov 15 07:10:33.908: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:10:35.911: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.035831522s
    Nov 15 07:10:35.911: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:10:37.911: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.036153282s
    Nov 15 07:10:37.911: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:10:39.908: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.033236816s
    Nov 15 07:10:39.908: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:10:41.909: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.034195081s
    Nov 15 07:10:41.909: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:10:43.926: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.050826698s
    Nov 15 07:10:43.926: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:10:45.912: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.037358843s
    Nov 15 07:10:45.913: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:10:47.909: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.034228313s
    Nov 15 07:10:47.909: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:10:49.907: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.032138032s
    Nov 15 07:10:49.907: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:10:51.908: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.033101836s
    Nov 15 07:10:51.908: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Nov 15 07:10:51.908: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Nov 15 07:10:51.923: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9155" to be "running and ready"
    Nov 15 07:10:51.937: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 13.606313ms
    Nov 15 07:10:51.937: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Nov 15 07:10:51.937: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Nov 15 07:10:51.952: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9155" to be "running and ready"
    Nov 15 07:10:51.966: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 14.30933ms
    Nov 15 07:10:51.967: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Nov 15 07:10:51.967: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 11/15/23 07:10:51.981
    Nov 15 07:10:52.007: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9155" to be "running"
    Nov 15 07:10:52.020: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.35155ms
    Nov 15 07:10:54.037: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030385052s
    Nov 15 07:10:56.039: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.032014875s
    Nov 15 07:10:56.039: INFO: Pod "test-container-pod" satisfied condition "running"
    Nov 15 07:10:56.052: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Nov 15 07:10:56.052: INFO: Breadth first check of 172.30.214.183 on host 10.72.152.81...
    Nov 15 07:10:56.065: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.213.181:9080/dial?request=hostname&protocol=udp&host=172.30.214.183&port=8081&tries=1'] Namespace:pod-network-test-9155 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:10:56.065: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:10:56.065: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:10:56.065: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9155/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.213.181%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.214.183%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 15 07:10:56.250: INFO: Waiting for responses: map[]
    Nov 15 07:10:56.250: INFO: reached 172.30.214.183 after 0/1 tries
    Nov 15 07:10:56.250: INFO: Breadth first check of 172.30.213.191 on host 10.72.152.86...
    Nov 15 07:10:56.265: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.213.181:9080/dial?request=hostname&protocol=udp&host=172.30.213.191&port=8081&tries=1'] Namespace:pod-network-test-9155 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:10:56.265: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:10:56.266: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:10:56.266: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9155/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.213.181%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.213.191%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 15 07:10:56.484: INFO: Waiting for responses: map[]
    Nov 15 07:10:56.484: INFO: reached 172.30.213.191 after 0/1 tries
    Nov 15 07:10:56.484: INFO: Breadth first check of 172.30.10.171 on host 10.72.152.88...
    Nov 15 07:10:56.503: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.213.181:9080/dial?request=hostname&protocol=udp&host=172.30.10.171&port=8081&tries=1'] Namespace:pod-network-test-9155 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:10:56.503: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:10:56.503: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:10:56.503: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9155/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.213.181%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.10.171%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 15 07:10:56.693: INFO: Waiting for responses: map[]
    Nov 15 07:10:56.693: INFO: reached 172.30.10.171 after 0/1 tries
    Nov 15 07:10:56.693: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:10:56.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9155" for this suite. 11/15/23 07:10:56.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:10:56.76
Nov 15 07:10:56.760: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename replication-controller 11/15/23 07:10:56.761
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:10:56.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:10:56.908
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218 11/15/23 07:10:56.92
Nov 15 07:10:56.965: INFO: Pod name my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218: Found 0 pods out of 1
Nov 15 07:11:01.982: INFO: Pod name my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218: Found 1 pods out of 1
Nov 15 07:11:01.982: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218" are running
Nov 15 07:11:01.982: INFO: Waiting up to 5m0s for pod "my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218-gbnn8" in namespace "replication-controller-758" to be "running"
Nov 15 07:11:01.999: INFO: Pod "my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218-gbnn8": Phase="Running", Reason="", readiness=true. Elapsed: 16.964597ms
Nov 15 07:11:01.999: INFO: Pod "my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218-gbnn8" satisfied condition "running"
Nov 15 07:11:01.999: INFO: Pod "my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218-gbnn8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 07:10:57 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 07:10:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 07:10:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 07:10:56 +0000 UTC Reason: Message:}])
Nov 15 07:11:01.999: INFO: Trying to dial the pod
Nov 15 07:11:07.057: INFO: Controller my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218: Got expected result from replica 1 [my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218-gbnn8]: "my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218-gbnn8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Nov 15 07:11:07.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-758" for this suite. 11/15/23 07:11:07.083
------------------------------
â€¢ [SLOW TEST] [10.347 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:10:56.76
    Nov 15 07:10:56.760: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename replication-controller 11/15/23 07:10:56.761
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:10:56.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:10:56.908
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218 11/15/23 07:10:56.92
    Nov 15 07:10:56.965: INFO: Pod name my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218: Found 0 pods out of 1
    Nov 15 07:11:01.982: INFO: Pod name my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218: Found 1 pods out of 1
    Nov 15 07:11:01.982: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218" are running
    Nov 15 07:11:01.982: INFO: Waiting up to 5m0s for pod "my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218-gbnn8" in namespace "replication-controller-758" to be "running"
    Nov 15 07:11:01.999: INFO: Pod "my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218-gbnn8": Phase="Running", Reason="", readiness=true. Elapsed: 16.964597ms
    Nov 15 07:11:01.999: INFO: Pod "my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218-gbnn8" satisfied condition "running"
    Nov 15 07:11:01.999: INFO: Pod "my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218-gbnn8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 07:10:57 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 07:10:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 07:10:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 07:10:56 +0000 UTC Reason: Message:}])
    Nov 15 07:11:01.999: INFO: Trying to dial the pod
    Nov 15 07:11:07.057: INFO: Controller my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218: Got expected result from replica 1 [my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218-gbnn8]: "my-hostname-basic-fe0af338-0674-4587-94f1-3e090ad2a218-gbnn8", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:11:07.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-758" for this suite. 11/15/23 07:11:07.083
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:11:07.109
Nov 15 07:11:07.109: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename webhook 11/15/23 07:11:07.11
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:11:07.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:11:07.185
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 07:11:07.254
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 07:11:07.587
STEP: Deploying the webhook pod 11/15/23 07:11:07.63
STEP: Wait for the deployment to be ready 11/15/23 07:11:07.673
Nov 15 07:11:07.707: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/15/23 07:11:09.752
STEP: Verifying the service has paired with the endpoint 11/15/23 07:11:09.795
Nov 15 07:11:10.796: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Nov 15 07:11:10.812: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Registering the custom resource webhook via the AdmissionRegistration API 11/15/23 07:11:11.349
STEP: Creating a custom resource that should be denied by the webhook 11/15/23 07:11:11.442
STEP: Creating a custom resource whose deletion would be denied by the webhook 11/15/23 07:11:13.527
STEP: Updating the custom resource with disallowed data should be denied 11/15/23 07:11:13.557
STEP: Deleting the custom resource should be denied 11/15/23 07:11:13.626
STEP: Remove the offending key and value from the custom resource data 11/15/23 07:11:13.66
STEP: Deleting the updated custom resource should be successful 11/15/23 07:11:13.705
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:11:14.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4122" for this suite. 11/15/23 07:11:14.525
STEP: Destroying namespace "webhook-4122-markers" for this suite. 11/15/23 07:11:14.557
------------------------------
â€¢ [SLOW TEST] [7.486 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:11:07.109
    Nov 15 07:11:07.109: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename webhook 11/15/23 07:11:07.11
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:11:07.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:11:07.185
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 07:11:07.254
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 07:11:07.587
    STEP: Deploying the webhook pod 11/15/23 07:11:07.63
    STEP: Wait for the deployment to be ready 11/15/23 07:11:07.673
    Nov 15 07:11:07.707: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/15/23 07:11:09.752
    STEP: Verifying the service has paired with the endpoint 11/15/23 07:11:09.795
    Nov 15 07:11:10.796: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Nov 15 07:11:10.812: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 11/15/23 07:11:11.349
    STEP: Creating a custom resource that should be denied by the webhook 11/15/23 07:11:11.442
    STEP: Creating a custom resource whose deletion would be denied by the webhook 11/15/23 07:11:13.527
    STEP: Updating the custom resource with disallowed data should be denied 11/15/23 07:11:13.557
    STEP: Deleting the custom resource should be denied 11/15/23 07:11:13.626
    STEP: Remove the offending key and value from the custom resource data 11/15/23 07:11:13.66
    STEP: Deleting the updated custom resource should be successful 11/15/23 07:11:13.705
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:11:14.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4122" for this suite. 11/15/23 07:11:14.525
    STEP: Destroying namespace "webhook-4122-markers" for this suite. 11/15/23 07:11:14.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:11:14.602
Nov 15 07:11:14.602: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename pod-network-test 11/15/23 07:11:14.604
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:11:14.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:11:14.693
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-8264 11/15/23 07:11:14.71
STEP: creating a selector 11/15/23 07:11:14.71
STEP: Creating the service pods in kubernetes 11/15/23 07:11:14.71
Nov 15 07:11:14.710: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 15 07:11:14.886: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8264" to be "running and ready"
Nov 15 07:11:14.916: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 29.901415ms
Nov 15 07:11:14.916: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:11:16.935: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049427986s
Nov 15 07:11:16.936: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:11:18.932: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.046357926s
Nov 15 07:11:18.932: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:11:20.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.046930471s
Nov 15 07:11:20.933: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:11:22.932: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.045982363s
Nov 15 07:11:22.932: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:11:24.942: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.056378657s
Nov 15 07:11:24.942: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:11:26.932: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.046445541s
Nov 15 07:11:26.932: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:11:28.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.046892324s
Nov 15 07:11:28.933: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:11:30.935: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.049411082s
Nov 15 07:11:30.935: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:11:32.937: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.05146884s
Nov 15 07:11:32.937: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:11:34.932: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.046564153s
Nov 15 07:11:34.932: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:11:36.934: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.048473161s
Nov 15 07:11:36.934: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Nov 15 07:11:36.934: INFO: Pod "netserver-0" satisfied condition "running and ready"
Nov 15 07:11:36.950: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8264" to be "running and ready"
Nov 15 07:11:36.967: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 16.778377ms
Nov 15 07:11:36.967: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Nov 15 07:11:36.967: INFO: Pod "netserver-1" satisfied condition "running and ready"
Nov 15 07:11:36.980: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8264" to be "running and ready"
Nov 15 07:11:36.998: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 18.313096ms
Nov 15 07:11:36.998: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Nov 15 07:11:36.999: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 11/15/23 07:11:37.013
Nov 15 07:11:37.059: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8264" to be "running"
Nov 15 07:11:37.076: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.318078ms
Nov 15 07:11:39.090: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.031186453s
Nov 15 07:11:39.090: INFO: Pod "test-container-pod" satisfied condition "running"
Nov 15 07:11:39.104: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8264" to be "running"
Nov 15 07:11:39.119: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 14.185264ms
Nov 15 07:11:39.119: INFO: Pod "host-test-container-pod" satisfied condition "running"
Nov 15 07:11:39.132: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 15 07:11:39.132: INFO: Going to poll 172.30.214.187 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Nov 15 07:11:39.146: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.214.187:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:11:39.146: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:11:39.147: INFO: ExecWithOptions: Clientset creation
Nov 15 07:11:39.147: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.214.187%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 15 07:11:39.359: INFO: Found all 1 expected endpoints: [netserver-0]
Nov 15 07:11:39.359: INFO: Going to poll 172.30.213.188 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Nov 15 07:11:39.380: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.213.188:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:11:39.380: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:11:39.381: INFO: ExecWithOptions: Clientset creation
Nov 15 07:11:39.381: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.213.188%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 15 07:11:39.669: INFO: Found all 1 expected endpoints: [netserver-1]
Nov 15 07:11:39.670: INFO: Going to poll 172.30.10.187 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Nov 15 07:11:39.684: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.10.187:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:11:39.684: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:11:39.685: INFO: ExecWithOptions: Clientset creation
Nov 15 07:11:39.685: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.10.187%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 15 07:11:39.886: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Nov 15 07:11:39.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-8264" for this suite. 11/15/23 07:11:39.912
------------------------------
â€¢ [SLOW TEST] [25.334 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:11:14.602
    Nov 15 07:11:14.602: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename pod-network-test 11/15/23 07:11:14.604
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:11:14.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:11:14.693
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-8264 11/15/23 07:11:14.71
    STEP: creating a selector 11/15/23 07:11:14.71
    STEP: Creating the service pods in kubernetes 11/15/23 07:11:14.71
    Nov 15 07:11:14.710: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Nov 15 07:11:14.886: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8264" to be "running and ready"
    Nov 15 07:11:14.916: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 29.901415ms
    Nov 15 07:11:14.916: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:11:16.935: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049427986s
    Nov 15 07:11:16.936: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:11:18.932: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.046357926s
    Nov 15 07:11:18.932: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:11:20.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.046930471s
    Nov 15 07:11:20.933: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:11:22.932: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.045982363s
    Nov 15 07:11:22.932: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:11:24.942: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.056378657s
    Nov 15 07:11:24.942: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:11:26.932: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.046445541s
    Nov 15 07:11:26.932: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:11:28.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.046892324s
    Nov 15 07:11:28.933: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:11:30.935: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.049411082s
    Nov 15 07:11:30.935: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:11:32.937: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.05146884s
    Nov 15 07:11:32.937: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:11:34.932: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.046564153s
    Nov 15 07:11:34.932: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:11:36.934: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.048473161s
    Nov 15 07:11:36.934: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Nov 15 07:11:36.934: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Nov 15 07:11:36.950: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8264" to be "running and ready"
    Nov 15 07:11:36.967: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 16.778377ms
    Nov 15 07:11:36.967: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Nov 15 07:11:36.967: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Nov 15 07:11:36.980: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8264" to be "running and ready"
    Nov 15 07:11:36.998: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 18.313096ms
    Nov 15 07:11:36.998: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Nov 15 07:11:36.999: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 11/15/23 07:11:37.013
    Nov 15 07:11:37.059: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8264" to be "running"
    Nov 15 07:11:37.076: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.318078ms
    Nov 15 07:11:39.090: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.031186453s
    Nov 15 07:11:39.090: INFO: Pod "test-container-pod" satisfied condition "running"
    Nov 15 07:11:39.104: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8264" to be "running"
    Nov 15 07:11:39.119: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 14.185264ms
    Nov 15 07:11:39.119: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Nov 15 07:11:39.132: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Nov 15 07:11:39.132: INFO: Going to poll 172.30.214.187 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Nov 15 07:11:39.146: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.214.187:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:11:39.146: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:11:39.147: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:11:39.147: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.214.187%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 15 07:11:39.359: INFO: Found all 1 expected endpoints: [netserver-0]
    Nov 15 07:11:39.359: INFO: Going to poll 172.30.213.188 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Nov 15 07:11:39.380: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.213.188:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:11:39.380: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:11:39.381: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:11:39.381: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.213.188%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 15 07:11:39.669: INFO: Found all 1 expected endpoints: [netserver-1]
    Nov 15 07:11:39.670: INFO: Going to poll 172.30.10.187 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Nov 15 07:11:39.684: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.10.187:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:11:39.684: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:11:39.685: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:11:39.685: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.10.187%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 15 07:11:39.886: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:11:39.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-8264" for this suite. 11/15/23 07:11:39.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:11:39.938
Nov 15 07:11:39.938: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename taint-multiple-pods 11/15/23 07:11:39.939
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:11:40.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:11:40.018
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Nov 15 07:11:40.029: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 15 07:12:40.231: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Nov 15 07:12:40.263: INFO: Starting informer...
STEP: Starting pods... 11/15/23 07:12:40.263
Nov 15 07:12:40.538: INFO: Pod1 is running on 10.72.152.81. Tainting Node
Nov 15 07:12:40.573: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-2949" to be "running"
Nov 15 07:12:40.591: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.219281ms
Nov 15 07:12:42.613: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.03943203s
Nov 15 07:12:42.613: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Nov 15 07:12:42.613: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-2949" to be "running"
Nov 15 07:12:42.631: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 18.275559ms
Nov 15 07:12:42.631: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Nov 15 07:12:42.631: INFO: Pod2 is running on 10.72.152.81. Tainting Node
STEP: Trying to apply a taint on the Node 11/15/23 07:12:42.631
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/15/23 07:12:42.678
STEP: Waiting for Pod1 and Pod2 to be deleted 11/15/23 07:12:42.694
Nov 15 07:12:51.622: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Nov 15 07:13:09.385: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/15/23 07:13:09.423
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:13:09.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-2949" for this suite. 11/15/23 07:13:09.478
------------------------------
â€¢ [SLOW TEST] [89.605 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:11:39.938
    Nov 15 07:11:39.938: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename taint-multiple-pods 11/15/23 07:11:39.939
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:11:40.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:11:40.018
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Nov 15 07:11:40.029: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 15 07:12:40.231: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Nov 15 07:12:40.263: INFO: Starting informer...
    STEP: Starting pods... 11/15/23 07:12:40.263
    Nov 15 07:12:40.538: INFO: Pod1 is running on 10.72.152.81. Tainting Node
    Nov 15 07:12:40.573: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-2949" to be "running"
    Nov 15 07:12:40.591: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.219281ms
    Nov 15 07:12:42.613: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.03943203s
    Nov 15 07:12:42.613: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Nov 15 07:12:42.613: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-2949" to be "running"
    Nov 15 07:12:42.631: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 18.275559ms
    Nov 15 07:12:42.631: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Nov 15 07:12:42.631: INFO: Pod2 is running on 10.72.152.81. Tainting Node
    STEP: Trying to apply a taint on the Node 11/15/23 07:12:42.631
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/15/23 07:12:42.678
    STEP: Waiting for Pod1 and Pod2 to be deleted 11/15/23 07:12:42.694
    Nov 15 07:12:51.622: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Nov 15 07:13:09.385: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/15/23 07:13:09.423
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:13:09.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-2949" for this suite. 11/15/23 07:13:09.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:13:09.551
Nov 15 07:13:09.551: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename emptydir 11/15/23 07:13:09.557
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:13:09.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:13:09.646
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 11/15/23 07:13:09.664
Nov 15 07:13:09.709: INFO: Waiting up to 5m0s for pod "pod-ffa40b73-66cb-4417-adc2-174e52665d28" in namespace "emptydir-2103" to be "Succeeded or Failed"
Nov 15 07:13:09.729: INFO: Pod "pod-ffa40b73-66cb-4417-adc2-174e52665d28": Phase="Pending", Reason="", readiness=false. Elapsed: 20.407464ms
Nov 15 07:13:11.785: INFO: Pod "pod-ffa40b73-66cb-4417-adc2-174e52665d28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075920261s
Nov 15 07:13:13.748: INFO: Pod "pod-ffa40b73-66cb-4417-adc2-174e52665d28": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039316136s
Nov 15 07:13:15.746: INFO: Pod "pod-ffa40b73-66cb-4417-adc2-174e52665d28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037309613s
STEP: Saw pod success 11/15/23 07:13:15.746
Nov 15 07:13:15.747: INFO: Pod "pod-ffa40b73-66cb-4417-adc2-174e52665d28" satisfied condition "Succeeded or Failed"
Nov 15 07:13:15.763: INFO: Trying to get logs from node 10.72.152.81 pod pod-ffa40b73-66cb-4417-adc2-174e52665d28 container test-container: <nil>
STEP: delete the pod 11/15/23 07:13:15.846
Nov 15 07:13:15.921: INFO: Waiting for pod pod-ffa40b73-66cb-4417-adc2-174e52665d28 to disappear
Nov 15 07:13:15.935: INFO: Pod pod-ffa40b73-66cb-4417-adc2-174e52665d28 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 07:13:15.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2103" for this suite. 11/15/23 07:13:15.982
------------------------------
â€¢ [SLOW TEST] [6.454 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:13:09.551
    Nov 15 07:13:09.551: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename emptydir 11/15/23 07:13:09.557
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:13:09.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:13:09.646
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 11/15/23 07:13:09.664
    Nov 15 07:13:09.709: INFO: Waiting up to 5m0s for pod "pod-ffa40b73-66cb-4417-adc2-174e52665d28" in namespace "emptydir-2103" to be "Succeeded or Failed"
    Nov 15 07:13:09.729: INFO: Pod "pod-ffa40b73-66cb-4417-adc2-174e52665d28": Phase="Pending", Reason="", readiness=false. Elapsed: 20.407464ms
    Nov 15 07:13:11.785: INFO: Pod "pod-ffa40b73-66cb-4417-adc2-174e52665d28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075920261s
    Nov 15 07:13:13.748: INFO: Pod "pod-ffa40b73-66cb-4417-adc2-174e52665d28": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039316136s
    Nov 15 07:13:15.746: INFO: Pod "pod-ffa40b73-66cb-4417-adc2-174e52665d28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037309613s
    STEP: Saw pod success 11/15/23 07:13:15.746
    Nov 15 07:13:15.747: INFO: Pod "pod-ffa40b73-66cb-4417-adc2-174e52665d28" satisfied condition "Succeeded or Failed"
    Nov 15 07:13:15.763: INFO: Trying to get logs from node 10.72.152.81 pod pod-ffa40b73-66cb-4417-adc2-174e52665d28 container test-container: <nil>
    STEP: delete the pod 11/15/23 07:13:15.846
    Nov 15 07:13:15.921: INFO: Waiting for pod pod-ffa40b73-66cb-4417-adc2-174e52665d28 to disappear
    Nov 15 07:13:15.935: INFO: Pod pod-ffa40b73-66cb-4417-adc2-174e52665d28 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:13:15.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2103" for this suite. 11/15/23 07:13:15.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:13:16.006
Nov 15 07:13:16.007: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename downward-api 11/15/23 07:13:16.007
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:13:16.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:13:16.111
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 11/15/23 07:13:16.123
Nov 15 07:13:16.164: INFO: Waiting up to 5m0s for pod "downward-api-dc21e757-ee55-48b9-9309-ba4ebbbfac89" in namespace "downward-api-3711" to be "Succeeded or Failed"
Nov 15 07:13:16.200: INFO: Pod "downward-api-dc21e757-ee55-48b9-9309-ba4ebbbfac89": Phase="Pending", Reason="", readiness=false. Elapsed: 35.670476ms
Nov 15 07:13:18.223: INFO: Pod "downward-api-dc21e757-ee55-48b9-9309-ba4ebbbfac89": Phase="Running", Reason="", readiness=true. Elapsed: 2.058451029s
Nov 15 07:13:20.218: INFO: Pod "downward-api-dc21e757-ee55-48b9-9309-ba4ebbbfac89": Phase="Running", Reason="", readiness=false. Elapsed: 4.053297942s
Nov 15 07:13:22.227: INFO: Pod "downward-api-dc21e757-ee55-48b9-9309-ba4ebbbfac89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062331402s
STEP: Saw pod success 11/15/23 07:13:22.227
Nov 15 07:13:22.227: INFO: Pod "downward-api-dc21e757-ee55-48b9-9309-ba4ebbbfac89" satisfied condition "Succeeded or Failed"
Nov 15 07:13:22.248: INFO: Trying to get logs from node 10.72.152.81 pod downward-api-dc21e757-ee55-48b9-9309-ba4ebbbfac89 container dapi-container: <nil>
STEP: delete the pod 11/15/23 07:13:22.287
Nov 15 07:13:22.332: INFO: Waiting for pod downward-api-dc21e757-ee55-48b9-9309-ba4ebbbfac89 to disappear
Nov 15 07:13:22.349: INFO: Pod downward-api-dc21e757-ee55-48b9-9309-ba4ebbbfac89 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Nov 15 07:13:22.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3711" for this suite. 11/15/23 07:13:22.372
------------------------------
â€¢ [SLOW TEST] [6.389 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:13:16.006
    Nov 15 07:13:16.007: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename downward-api 11/15/23 07:13:16.007
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:13:16.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:13:16.111
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 11/15/23 07:13:16.123
    Nov 15 07:13:16.164: INFO: Waiting up to 5m0s for pod "downward-api-dc21e757-ee55-48b9-9309-ba4ebbbfac89" in namespace "downward-api-3711" to be "Succeeded or Failed"
    Nov 15 07:13:16.200: INFO: Pod "downward-api-dc21e757-ee55-48b9-9309-ba4ebbbfac89": Phase="Pending", Reason="", readiness=false. Elapsed: 35.670476ms
    Nov 15 07:13:18.223: INFO: Pod "downward-api-dc21e757-ee55-48b9-9309-ba4ebbbfac89": Phase="Running", Reason="", readiness=true. Elapsed: 2.058451029s
    Nov 15 07:13:20.218: INFO: Pod "downward-api-dc21e757-ee55-48b9-9309-ba4ebbbfac89": Phase="Running", Reason="", readiness=false. Elapsed: 4.053297942s
    Nov 15 07:13:22.227: INFO: Pod "downward-api-dc21e757-ee55-48b9-9309-ba4ebbbfac89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062331402s
    STEP: Saw pod success 11/15/23 07:13:22.227
    Nov 15 07:13:22.227: INFO: Pod "downward-api-dc21e757-ee55-48b9-9309-ba4ebbbfac89" satisfied condition "Succeeded or Failed"
    Nov 15 07:13:22.248: INFO: Trying to get logs from node 10.72.152.81 pod downward-api-dc21e757-ee55-48b9-9309-ba4ebbbfac89 container dapi-container: <nil>
    STEP: delete the pod 11/15/23 07:13:22.287
    Nov 15 07:13:22.332: INFO: Waiting for pod downward-api-dc21e757-ee55-48b9-9309-ba4ebbbfac89 to disappear
    Nov 15 07:13:22.349: INFO: Pod downward-api-dc21e757-ee55-48b9-9309-ba4ebbbfac89 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:13:22.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3711" for this suite. 11/15/23 07:13:22.372
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:13:22.401
Nov 15 07:13:22.401: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename downward-api 11/15/23 07:13:22.402
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:13:22.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:13:22.474
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 11/15/23 07:13:22.5
Nov 15 07:13:22.548: INFO: Waiting up to 5m0s for pod "downward-api-9e418d6e-421d-4125-b984-568f49aa75df" in namespace "downward-api-9269" to be "Succeeded or Failed"
Nov 15 07:13:22.573: INFO: Pod "downward-api-9e418d6e-421d-4125-b984-568f49aa75df": Phase="Pending", Reason="", readiness=false. Elapsed: 24.415217ms
Nov 15 07:13:24.600: INFO: Pod "downward-api-9e418d6e-421d-4125-b984-568f49aa75df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052135834s
Nov 15 07:13:26.590: INFO: Pod "downward-api-9e418d6e-421d-4125-b984-568f49aa75df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041752702s
STEP: Saw pod success 11/15/23 07:13:26.59
Nov 15 07:13:26.590: INFO: Pod "downward-api-9e418d6e-421d-4125-b984-568f49aa75df" satisfied condition "Succeeded or Failed"
Nov 15 07:13:26.607: INFO: Trying to get logs from node 10.72.152.81 pod downward-api-9e418d6e-421d-4125-b984-568f49aa75df container dapi-container: <nil>
STEP: delete the pod 11/15/23 07:13:26.651
Nov 15 07:13:26.707: INFO: Waiting for pod downward-api-9e418d6e-421d-4125-b984-568f49aa75df to disappear
Nov 15 07:13:26.722: INFO: Pod downward-api-9e418d6e-421d-4125-b984-568f49aa75df no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Nov 15 07:13:26.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9269" for this suite. 11/15/23 07:13:26.744
------------------------------
â€¢ [4.368 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:13:22.401
    Nov 15 07:13:22.401: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename downward-api 11/15/23 07:13:22.402
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:13:22.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:13:22.474
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 11/15/23 07:13:22.5
    Nov 15 07:13:22.548: INFO: Waiting up to 5m0s for pod "downward-api-9e418d6e-421d-4125-b984-568f49aa75df" in namespace "downward-api-9269" to be "Succeeded or Failed"
    Nov 15 07:13:22.573: INFO: Pod "downward-api-9e418d6e-421d-4125-b984-568f49aa75df": Phase="Pending", Reason="", readiness=false. Elapsed: 24.415217ms
    Nov 15 07:13:24.600: INFO: Pod "downward-api-9e418d6e-421d-4125-b984-568f49aa75df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052135834s
    Nov 15 07:13:26.590: INFO: Pod "downward-api-9e418d6e-421d-4125-b984-568f49aa75df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041752702s
    STEP: Saw pod success 11/15/23 07:13:26.59
    Nov 15 07:13:26.590: INFO: Pod "downward-api-9e418d6e-421d-4125-b984-568f49aa75df" satisfied condition "Succeeded or Failed"
    Nov 15 07:13:26.607: INFO: Trying to get logs from node 10.72.152.81 pod downward-api-9e418d6e-421d-4125-b984-568f49aa75df container dapi-container: <nil>
    STEP: delete the pod 11/15/23 07:13:26.651
    Nov 15 07:13:26.707: INFO: Waiting for pod downward-api-9e418d6e-421d-4125-b984-568f49aa75df to disappear
    Nov 15 07:13:26.722: INFO: Pod downward-api-9e418d6e-421d-4125-b984-568f49aa75df no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:13:26.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9269" for this suite. 11/15/23 07:13:26.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:13:26.77
Nov 15 07:13:26.770: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubectl 11/15/23 07:13:26.771
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:13:26.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:13:26.875
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/15/23 07:13:26.887
Nov 15 07:13:26.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-911 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Nov 15 07:13:27.091: INFO: stderr: ""
Nov 15 07:13:27.091: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 11/15/23 07:13:27.091
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Nov 15 07:13:27.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-911 delete pods e2e-test-httpd-pod'
Nov 15 07:13:29.835: INFO: stderr: ""
Nov 15 07:13:29.835: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 07:13:29.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-911" for this suite. 11/15/23 07:13:29.86
------------------------------
â€¢ [3.116 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:13:26.77
    Nov 15 07:13:26.770: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubectl 11/15/23 07:13:26.771
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:13:26.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:13:26.875
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/15/23 07:13:26.887
    Nov 15 07:13:26.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-911 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Nov 15 07:13:27.091: INFO: stderr: ""
    Nov 15 07:13:27.091: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 11/15/23 07:13:27.091
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Nov 15 07:13:27.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-911 delete pods e2e-test-httpd-pod'
    Nov 15 07:13:29.835: INFO: stderr: ""
    Nov 15 07:13:29.835: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:13:29.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-911" for this suite. 11/15/23 07:13:29.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:13:29.887
Nov 15 07:13:29.888: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 07:13:29.889
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:13:29.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:13:29.97
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-918a35fd-bba8-41bb-b36c-5f410d8c549c 11/15/23 07:13:30.093
STEP: Creating a pod to test consume secrets 11/15/23 07:13:30.123
Nov 15 07:13:30.159: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4332d8d2-c25a-45d4-ab35-89140c928bbf" in namespace "projected-2400" to be "Succeeded or Failed"
Nov 15 07:13:30.176: INFO: Pod "pod-projected-secrets-4332d8d2-c25a-45d4-ab35-89140c928bbf": Phase="Pending", Reason="", readiness=false. Elapsed: 15.966153ms
Nov 15 07:13:32.209: INFO: Pod "pod-projected-secrets-4332d8d2-c25a-45d4-ab35-89140c928bbf": Phase="Running", Reason="", readiness=true. Elapsed: 2.049492539s
Nov 15 07:13:34.196: INFO: Pod "pod-projected-secrets-4332d8d2-c25a-45d4-ab35-89140c928bbf": Phase="Running", Reason="", readiness=false. Elapsed: 4.036739861s
Nov 15 07:13:36.198: INFO: Pod "pod-projected-secrets-4332d8d2-c25a-45d4-ab35-89140c928bbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038070455s
STEP: Saw pod success 11/15/23 07:13:36.198
Nov 15 07:13:36.198: INFO: Pod "pod-projected-secrets-4332d8d2-c25a-45d4-ab35-89140c928bbf" satisfied condition "Succeeded or Failed"
Nov 15 07:13:36.215: INFO: Trying to get logs from node 10.72.152.81 pod pod-projected-secrets-4332d8d2-c25a-45d4-ab35-89140c928bbf container projected-secret-volume-test: <nil>
STEP: delete the pod 11/15/23 07:13:36.279
Nov 15 07:13:36.322: INFO: Waiting for pod pod-projected-secrets-4332d8d2-c25a-45d4-ab35-89140c928bbf to disappear
Nov 15 07:13:36.356: INFO: Pod pod-projected-secrets-4332d8d2-c25a-45d4-ab35-89140c928bbf no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 15 07:13:36.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2400" for this suite. 11/15/23 07:13:36.38
------------------------------
â€¢ [SLOW TEST] [6.526 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:13:29.887
    Nov 15 07:13:29.888: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 07:13:29.889
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:13:29.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:13:29.97
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-918a35fd-bba8-41bb-b36c-5f410d8c549c 11/15/23 07:13:30.093
    STEP: Creating a pod to test consume secrets 11/15/23 07:13:30.123
    Nov 15 07:13:30.159: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4332d8d2-c25a-45d4-ab35-89140c928bbf" in namespace "projected-2400" to be "Succeeded or Failed"
    Nov 15 07:13:30.176: INFO: Pod "pod-projected-secrets-4332d8d2-c25a-45d4-ab35-89140c928bbf": Phase="Pending", Reason="", readiness=false. Elapsed: 15.966153ms
    Nov 15 07:13:32.209: INFO: Pod "pod-projected-secrets-4332d8d2-c25a-45d4-ab35-89140c928bbf": Phase="Running", Reason="", readiness=true. Elapsed: 2.049492539s
    Nov 15 07:13:34.196: INFO: Pod "pod-projected-secrets-4332d8d2-c25a-45d4-ab35-89140c928bbf": Phase="Running", Reason="", readiness=false. Elapsed: 4.036739861s
    Nov 15 07:13:36.198: INFO: Pod "pod-projected-secrets-4332d8d2-c25a-45d4-ab35-89140c928bbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038070455s
    STEP: Saw pod success 11/15/23 07:13:36.198
    Nov 15 07:13:36.198: INFO: Pod "pod-projected-secrets-4332d8d2-c25a-45d4-ab35-89140c928bbf" satisfied condition "Succeeded or Failed"
    Nov 15 07:13:36.215: INFO: Trying to get logs from node 10.72.152.81 pod pod-projected-secrets-4332d8d2-c25a-45d4-ab35-89140c928bbf container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 07:13:36.279
    Nov 15 07:13:36.322: INFO: Waiting for pod pod-projected-secrets-4332d8d2-c25a-45d4-ab35-89140c928bbf to disappear
    Nov 15 07:13:36.356: INFO: Pod pod-projected-secrets-4332d8d2-c25a-45d4-ab35-89140c928bbf no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:13:36.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2400" for this suite. 11/15/23 07:13:36.38
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:13:36.421
Nov 15 07:13:36.421: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename statefulset 11/15/23 07:13:36.422
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:13:36.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:13:36.507
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6689 11/15/23 07:13:36.528
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 11/15/23 07:13:36.583
Nov 15 07:13:36.638: INFO: Found 0 stateful pods, waiting for 3
Nov 15 07:13:46.662: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 07:13:46.662: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 07:13:46.662: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 11/15/23 07:13:46.721
Nov 15 07:13:46.768: INFO: Updating stateful set ss2
STEP: Creating a new revision 11/15/23 07:13:46.768
STEP: Not applying an update when the partition is greater than the number of replicas 11/15/23 07:13:56.847
STEP: Performing a canary update 11/15/23 07:13:56.847
Nov 15 07:13:56.892: INFO: Updating stateful set ss2
Nov 15 07:13:56.933: INFO: Waiting for Pod statefulset-6689/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 11/15/23 07:14:06.97
Nov 15 07:14:07.092: INFO: Found 2 stateful pods, waiting for 3
Nov 15 07:14:17.111: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 07:14:17.111: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 07:14:17.111: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 11/15/23 07:14:17.142
Nov 15 07:14:17.190: INFO: Updating stateful set ss2
Nov 15 07:14:17.225: INFO: Waiting for Pod statefulset-6689/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Nov 15 07:14:27.317: INFO: Updating stateful set ss2
Nov 15 07:14:27.366: INFO: Waiting for StatefulSet statefulset-6689/ss2 to complete update
Nov 15 07:14:27.366: INFO: Waiting for Pod statefulset-6689/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 15 07:14:37.415: INFO: Deleting all statefulset in ns statefulset-6689
Nov 15 07:14:37.428: INFO: Scaling statefulset ss2 to 0
Nov 15 07:14:47.527: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 07:14:47.542: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 15 07:14:47.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6689" for this suite. 11/15/23 07:14:47.649
------------------------------
â€¢ [SLOW TEST] [71.253 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:13:36.421
    Nov 15 07:13:36.421: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename statefulset 11/15/23 07:13:36.422
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:13:36.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:13:36.507
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6689 11/15/23 07:13:36.528
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 11/15/23 07:13:36.583
    Nov 15 07:13:36.638: INFO: Found 0 stateful pods, waiting for 3
    Nov 15 07:13:46.662: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 07:13:46.662: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 07:13:46.662: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 11/15/23 07:13:46.721
    Nov 15 07:13:46.768: INFO: Updating stateful set ss2
    STEP: Creating a new revision 11/15/23 07:13:46.768
    STEP: Not applying an update when the partition is greater than the number of replicas 11/15/23 07:13:56.847
    STEP: Performing a canary update 11/15/23 07:13:56.847
    Nov 15 07:13:56.892: INFO: Updating stateful set ss2
    Nov 15 07:13:56.933: INFO: Waiting for Pod statefulset-6689/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 11/15/23 07:14:06.97
    Nov 15 07:14:07.092: INFO: Found 2 stateful pods, waiting for 3
    Nov 15 07:14:17.111: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 07:14:17.111: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 07:14:17.111: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 11/15/23 07:14:17.142
    Nov 15 07:14:17.190: INFO: Updating stateful set ss2
    Nov 15 07:14:17.225: INFO: Waiting for Pod statefulset-6689/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Nov 15 07:14:27.317: INFO: Updating stateful set ss2
    Nov 15 07:14:27.366: INFO: Waiting for StatefulSet statefulset-6689/ss2 to complete update
    Nov 15 07:14:27.366: INFO: Waiting for Pod statefulset-6689/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 15 07:14:37.415: INFO: Deleting all statefulset in ns statefulset-6689
    Nov 15 07:14:37.428: INFO: Scaling statefulset ss2 to 0
    Nov 15 07:14:47.527: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 07:14:47.542: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:14:47.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6689" for this suite. 11/15/23 07:14:47.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:14:47.675
Nov 15 07:14:47.675: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename replicaset 11/15/23 07:14:47.676
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:14:47.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:14:47.768
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 11/15/23 07:14:47.817
STEP: Verify that the required pods have come up. 11/15/23 07:14:47.839
Nov 15 07:14:47.855: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 15 07:14:52.873: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/15/23 07:14:52.873
STEP: Getting /status 11/15/23 07:14:52.878
Nov 15 07:14:52.898: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 11/15/23 07:14:52.898
Nov 15 07:14:52.939: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 11/15/23 07:14:52.939
Nov 15 07:14:52.945: INFO: Observed &ReplicaSet event: ADDED
Nov 15 07:14:52.946: INFO: Observed &ReplicaSet event: MODIFIED
Nov 15 07:14:52.946: INFO: Observed &ReplicaSet event: MODIFIED
Nov 15 07:14:52.946: INFO: Observed &ReplicaSet event: MODIFIED
Nov 15 07:14:52.946: INFO: Found replicaset test-rs in namespace replicaset-8037 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 15 07:14:52.946: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 11/15/23 07:14:52.946
Nov 15 07:14:52.946: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 15 07:14:52.969: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 11/15/23 07:14:52.969
Nov 15 07:14:52.975: INFO: Observed &ReplicaSet event: ADDED
Nov 15 07:14:52.975: INFO: Observed &ReplicaSet event: MODIFIED
Nov 15 07:14:52.976: INFO: Observed &ReplicaSet event: MODIFIED
Nov 15 07:14:52.976: INFO: Observed &ReplicaSet event: MODIFIED
Nov 15 07:14:52.976: INFO: Observed replicaset test-rs in namespace replicaset-8037 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 15 07:14:52.976: INFO: Observed &ReplicaSet event: MODIFIED
Nov 15 07:14:52.976: INFO: Found replicaset test-rs in namespace replicaset-8037 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Nov 15 07:14:52.976: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Nov 15 07:14:52.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8037" for this suite. 11/15/23 07:14:53
------------------------------
â€¢ [SLOW TEST] [5.349 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:14:47.675
    Nov 15 07:14:47.675: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename replicaset 11/15/23 07:14:47.676
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:14:47.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:14:47.768
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 11/15/23 07:14:47.817
    STEP: Verify that the required pods have come up. 11/15/23 07:14:47.839
    Nov 15 07:14:47.855: INFO: Pod name sample-pod: Found 0 pods out of 1
    Nov 15 07:14:52.873: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/15/23 07:14:52.873
    STEP: Getting /status 11/15/23 07:14:52.878
    Nov 15 07:14:52.898: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 11/15/23 07:14:52.898
    Nov 15 07:14:52.939: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 11/15/23 07:14:52.939
    Nov 15 07:14:52.945: INFO: Observed &ReplicaSet event: ADDED
    Nov 15 07:14:52.946: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 15 07:14:52.946: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 15 07:14:52.946: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 15 07:14:52.946: INFO: Found replicaset test-rs in namespace replicaset-8037 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Nov 15 07:14:52.946: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 11/15/23 07:14:52.946
    Nov 15 07:14:52.946: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Nov 15 07:14:52.969: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 11/15/23 07:14:52.969
    Nov 15 07:14:52.975: INFO: Observed &ReplicaSet event: ADDED
    Nov 15 07:14:52.975: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 15 07:14:52.976: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 15 07:14:52.976: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 15 07:14:52.976: INFO: Observed replicaset test-rs in namespace replicaset-8037 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Nov 15 07:14:52.976: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 15 07:14:52.976: INFO: Found replicaset test-rs in namespace replicaset-8037 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Nov 15 07:14:52.976: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:14:52.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8037" for this suite. 11/15/23 07:14:53
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:14:53.027
Nov 15 07:14:53.027: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename secrets 11/15/23 07:14:53.028
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:14:53.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:14:53.109
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-a8f03003-1962-48c8-b681-c759d794c769 11/15/23 07:14:53.135
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 07:14:53.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8609" for this suite. 11/15/23 07:14:53.163
------------------------------
â€¢ [0.160 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:14:53.027
    Nov 15 07:14:53.027: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename secrets 11/15/23 07:14:53.028
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:14:53.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:14:53.109
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-a8f03003-1962-48c8-b681-c759d794c769 11/15/23 07:14:53.135
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:14:53.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8609" for this suite. 11/15/23 07:14:53.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:14:53.189
Nov 15 07:14:53.189: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename services 11/15/23 07:14:53.19
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:14:53.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:14:53.27
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-2136 11/15/23 07:14:53.281
STEP: creating service affinity-nodeport-transition in namespace services-2136 11/15/23 07:14:53.282
STEP: creating replication controller affinity-nodeport-transition in namespace services-2136 11/15/23 07:14:53.366
I1115 07:14:53.390645      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2136, replica count: 3
I1115 07:14:56.442957      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 07:14:56.531: INFO: Creating new exec pod
Nov 15 07:14:56.577: INFO: Waiting up to 5m0s for pod "execpod-affinityv9km8" in namespace "services-2136" to be "running"
Nov 15 07:14:56.602: INFO: Pod "execpod-affinityv9km8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.215607ms
Nov 15 07:14:58.619: INFO: Pod "execpod-affinityv9km8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041983447s
Nov 15 07:15:00.621: INFO: Pod "execpod-affinityv9km8": Phase="Running", Reason="", readiness=true. Elapsed: 4.043305693s
Nov 15 07:15:00.621: INFO: Pod "execpod-affinityv9km8" satisfied condition "running"
Nov 15 07:15:01.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2136 exec execpod-affinityv9km8 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Nov 15 07:15:02.260: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Nov 15 07:15:02.260: INFO: stdout: ""
Nov 15 07:15:02.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2136 exec execpod-affinityv9km8 -- /bin/sh -x -c nc -v -z -w 2 172.21.166.117 80'
Nov 15 07:15:02.660: INFO: stderr: "+ nc -v -z -w 2 172.21.166.117 80\nConnection to 172.21.166.117 80 port [tcp/http] succeeded!\n"
Nov 15 07:15:02.660: INFO: stdout: ""
Nov 15 07:15:02.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2136 exec execpod-affinityv9km8 -- /bin/sh -x -c nc -v -z -w 2 10.72.152.86 30279'
Nov 15 07:15:02.984: INFO: stderr: "+ nc -v -z -w 2 10.72.152.86 30279\nConnection to 10.72.152.86 30279 port [tcp/*] succeeded!\n"
Nov 15 07:15:02.984: INFO: stdout: ""
Nov 15 07:15:02.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2136 exec execpod-affinityv9km8 -- /bin/sh -x -c nc -v -z -w 2 10.72.152.81 30279'
Nov 15 07:15:03.372: INFO: stderr: "+ nc -v -z -w 2 10.72.152.81 30279\nConnection to 10.72.152.81 30279 port [tcp/*] succeeded!\n"
Nov 15 07:15:03.372: INFO: stdout: ""
Nov 15 07:15:03.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2136 exec execpod-affinityv9km8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.72.152.81:30279/ ; done'
Nov 15 07:15:03.869: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n"
Nov 15 07:15:03.870: INFO: stdout: "\naffinity-nodeport-transition-wlf8k\naffinity-nodeport-transition-j4djd\naffinity-nodeport-transition-wlf8k\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-wlf8k\naffinity-nodeport-transition-j4djd\naffinity-nodeport-transition-wlf8k\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-j4djd\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-wlf8k\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-j4djd\naffinity-nodeport-transition-wlf8k\naffinity-nodeport-transition-j4djd\naffinity-nodeport-transition-j4djd"
Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-wlf8k
Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-j4djd
Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-wlf8k
Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-wlf8k
Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-j4djd
Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-wlf8k
Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-j4djd
Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-wlf8k
Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-j4djd
Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-wlf8k
Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-j4djd
Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-j4djd
Nov 15 07:15:03.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2136 exec execpod-affinityv9km8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.72.152.81:30279/ ; done'
Nov 15 07:15:04.336: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n"
Nov 15 07:15:04.336: INFO: stdout: "\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6"
Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
Nov 15 07:15:04.336: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2136, will wait for the garbage collector to delete the pods 11/15/23 07:15:04.373
Nov 15 07:15:04.485: INFO: Deleting ReplicationController affinity-nodeport-transition took: 37.122966ms
Nov 15 07:15:04.586: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.723689ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 07:15:07.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2136" for this suite. 11/15/23 07:15:07.56
------------------------------
â€¢ [SLOW TEST] [14.406 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:14:53.189
    Nov 15 07:14:53.189: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename services 11/15/23 07:14:53.19
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:14:53.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:14:53.27
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-2136 11/15/23 07:14:53.281
    STEP: creating service affinity-nodeport-transition in namespace services-2136 11/15/23 07:14:53.282
    STEP: creating replication controller affinity-nodeport-transition in namespace services-2136 11/15/23 07:14:53.366
    I1115 07:14:53.390645      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2136, replica count: 3
    I1115 07:14:56.442957      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 15 07:14:56.531: INFO: Creating new exec pod
    Nov 15 07:14:56.577: INFO: Waiting up to 5m0s for pod "execpod-affinityv9km8" in namespace "services-2136" to be "running"
    Nov 15 07:14:56.602: INFO: Pod "execpod-affinityv9km8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.215607ms
    Nov 15 07:14:58.619: INFO: Pod "execpod-affinityv9km8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041983447s
    Nov 15 07:15:00.621: INFO: Pod "execpod-affinityv9km8": Phase="Running", Reason="", readiness=true. Elapsed: 4.043305693s
    Nov 15 07:15:00.621: INFO: Pod "execpod-affinityv9km8" satisfied condition "running"
    Nov 15 07:15:01.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2136 exec execpod-affinityv9km8 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Nov 15 07:15:02.260: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Nov 15 07:15:02.260: INFO: stdout: ""
    Nov 15 07:15:02.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2136 exec execpod-affinityv9km8 -- /bin/sh -x -c nc -v -z -w 2 172.21.166.117 80'
    Nov 15 07:15:02.660: INFO: stderr: "+ nc -v -z -w 2 172.21.166.117 80\nConnection to 172.21.166.117 80 port [tcp/http] succeeded!\n"
    Nov 15 07:15:02.660: INFO: stdout: ""
    Nov 15 07:15:02.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2136 exec execpod-affinityv9km8 -- /bin/sh -x -c nc -v -z -w 2 10.72.152.86 30279'
    Nov 15 07:15:02.984: INFO: stderr: "+ nc -v -z -w 2 10.72.152.86 30279\nConnection to 10.72.152.86 30279 port [tcp/*] succeeded!\n"
    Nov 15 07:15:02.984: INFO: stdout: ""
    Nov 15 07:15:02.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2136 exec execpod-affinityv9km8 -- /bin/sh -x -c nc -v -z -w 2 10.72.152.81 30279'
    Nov 15 07:15:03.372: INFO: stderr: "+ nc -v -z -w 2 10.72.152.81 30279\nConnection to 10.72.152.81 30279 port [tcp/*] succeeded!\n"
    Nov 15 07:15:03.372: INFO: stdout: ""
    Nov 15 07:15:03.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2136 exec execpod-affinityv9km8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.72.152.81:30279/ ; done'
    Nov 15 07:15:03.869: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n"
    Nov 15 07:15:03.870: INFO: stdout: "\naffinity-nodeport-transition-wlf8k\naffinity-nodeport-transition-j4djd\naffinity-nodeport-transition-wlf8k\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-wlf8k\naffinity-nodeport-transition-j4djd\naffinity-nodeport-transition-wlf8k\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-j4djd\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-wlf8k\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-j4djd\naffinity-nodeport-transition-wlf8k\naffinity-nodeport-transition-j4djd\naffinity-nodeport-transition-j4djd"
    Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-wlf8k
    Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-j4djd
    Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-wlf8k
    Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-wlf8k
    Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-j4djd
    Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-wlf8k
    Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-j4djd
    Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-wlf8k
    Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-j4djd
    Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-wlf8k
    Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-j4djd
    Nov 15 07:15:03.870: INFO: Received response from host: affinity-nodeport-transition-j4djd
    Nov 15 07:15:03.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-2136 exec execpod-affinityv9km8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.72.152.81:30279/ ; done'
    Nov 15 07:15:04.336: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.72.152.81:30279/\n"
    Nov 15 07:15:04.336: INFO: stdout: "\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6\naffinity-nodeport-transition-xc7j6"
    Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:04.336: INFO: Received response from host: affinity-nodeport-transition-xc7j6
    Nov 15 07:15:04.336: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2136, will wait for the garbage collector to delete the pods 11/15/23 07:15:04.373
    Nov 15 07:15:04.485: INFO: Deleting ReplicationController affinity-nodeport-transition took: 37.122966ms
    Nov 15 07:15:04.586: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.723689ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:15:07.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2136" for this suite. 11/15/23 07:15:07.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:15:07.597
Nov 15 07:15:07.597: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename downward-api 11/15/23 07:15:07.598
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:15:07.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:15:07.731
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 11/15/23 07:15:07.745
Nov 15 07:15:07.777: INFO: Waiting up to 5m0s for pod "downwardapi-volume-29754a3d-377d-431f-9174-a4c70ac19132" in namespace "downward-api-8392" to be "Succeeded or Failed"
Nov 15 07:15:07.799: INFO: Pod "downwardapi-volume-29754a3d-377d-431f-9174-a4c70ac19132": Phase="Pending", Reason="", readiness=false. Elapsed: 21.907445ms
Nov 15 07:15:09.814: INFO: Pod "downwardapi-volume-29754a3d-377d-431f-9174-a4c70ac19132": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036705541s
Nov 15 07:15:11.820: INFO: Pod "downwardapi-volume-29754a3d-377d-431f-9174-a4c70ac19132": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043077928s
Nov 15 07:15:13.817: INFO: Pod "downwardapi-volume-29754a3d-377d-431f-9174-a4c70ac19132": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039890536s
STEP: Saw pod success 11/15/23 07:15:13.817
Nov 15 07:15:13.817: INFO: Pod "downwardapi-volume-29754a3d-377d-431f-9174-a4c70ac19132" satisfied condition "Succeeded or Failed"
Nov 15 07:15:13.837: INFO: Trying to get logs from node 10.72.152.81 pod downwardapi-volume-29754a3d-377d-431f-9174-a4c70ac19132 container client-container: <nil>
STEP: delete the pod 11/15/23 07:15:13.911
Nov 15 07:15:13.963: INFO: Waiting for pod downwardapi-volume-29754a3d-377d-431f-9174-a4c70ac19132 to disappear
Nov 15 07:15:13.982: INFO: Pod downwardapi-volume-29754a3d-377d-431f-9174-a4c70ac19132 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 07:15:13.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8392" for this suite. 11/15/23 07:15:14.038
------------------------------
â€¢ [SLOW TEST] [6.482 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:15:07.597
    Nov 15 07:15:07.597: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename downward-api 11/15/23 07:15:07.598
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:15:07.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:15:07.731
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 11/15/23 07:15:07.745
    Nov 15 07:15:07.777: INFO: Waiting up to 5m0s for pod "downwardapi-volume-29754a3d-377d-431f-9174-a4c70ac19132" in namespace "downward-api-8392" to be "Succeeded or Failed"
    Nov 15 07:15:07.799: INFO: Pod "downwardapi-volume-29754a3d-377d-431f-9174-a4c70ac19132": Phase="Pending", Reason="", readiness=false. Elapsed: 21.907445ms
    Nov 15 07:15:09.814: INFO: Pod "downwardapi-volume-29754a3d-377d-431f-9174-a4c70ac19132": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036705541s
    Nov 15 07:15:11.820: INFO: Pod "downwardapi-volume-29754a3d-377d-431f-9174-a4c70ac19132": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043077928s
    Nov 15 07:15:13.817: INFO: Pod "downwardapi-volume-29754a3d-377d-431f-9174-a4c70ac19132": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039890536s
    STEP: Saw pod success 11/15/23 07:15:13.817
    Nov 15 07:15:13.817: INFO: Pod "downwardapi-volume-29754a3d-377d-431f-9174-a4c70ac19132" satisfied condition "Succeeded or Failed"
    Nov 15 07:15:13.837: INFO: Trying to get logs from node 10.72.152.81 pod downwardapi-volume-29754a3d-377d-431f-9174-a4c70ac19132 container client-container: <nil>
    STEP: delete the pod 11/15/23 07:15:13.911
    Nov 15 07:15:13.963: INFO: Waiting for pod downwardapi-volume-29754a3d-377d-431f-9174-a4c70ac19132 to disappear
    Nov 15 07:15:13.982: INFO: Pod downwardapi-volume-29754a3d-377d-431f-9174-a4c70ac19132 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:15:13.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8392" for this suite. 11/15/23 07:15:14.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:15:14.08
Nov 15 07:15:14.080: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename emptydir 11/15/23 07:15:14.081
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:15:14.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:15:14.159
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 11/15/23 07:15:14.17
Nov 15 07:15:14.207: INFO: Waiting up to 5m0s for pod "pod-10ba5242-f470-4f6a-bdee-da425f65bd78" in namespace "emptydir-4479" to be "Succeeded or Failed"
Nov 15 07:15:14.225: INFO: Pod "pod-10ba5242-f470-4f6a-bdee-da425f65bd78": Phase="Pending", Reason="", readiness=false. Elapsed: 17.693796ms
Nov 15 07:15:16.247: INFO: Pod "pod-10ba5242-f470-4f6a-bdee-da425f65bd78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039654634s
Nov 15 07:15:18.241: INFO: Pod "pod-10ba5242-f470-4f6a-bdee-da425f65bd78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033182754s
STEP: Saw pod success 11/15/23 07:15:18.241
Nov 15 07:15:18.241: INFO: Pod "pod-10ba5242-f470-4f6a-bdee-da425f65bd78" satisfied condition "Succeeded or Failed"
Nov 15 07:15:18.256: INFO: Trying to get logs from node 10.72.152.81 pod pod-10ba5242-f470-4f6a-bdee-da425f65bd78 container test-container: <nil>
STEP: delete the pod 11/15/23 07:15:18.291
Nov 15 07:15:18.340: INFO: Waiting for pod pod-10ba5242-f470-4f6a-bdee-da425f65bd78 to disappear
Nov 15 07:15:18.357: INFO: Pod pod-10ba5242-f470-4f6a-bdee-da425f65bd78 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 07:15:18.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4479" for this suite. 11/15/23 07:15:18.378
------------------------------
â€¢ [4.322 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:15:14.08
    Nov 15 07:15:14.080: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename emptydir 11/15/23 07:15:14.081
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:15:14.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:15:14.159
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 11/15/23 07:15:14.17
    Nov 15 07:15:14.207: INFO: Waiting up to 5m0s for pod "pod-10ba5242-f470-4f6a-bdee-da425f65bd78" in namespace "emptydir-4479" to be "Succeeded or Failed"
    Nov 15 07:15:14.225: INFO: Pod "pod-10ba5242-f470-4f6a-bdee-da425f65bd78": Phase="Pending", Reason="", readiness=false. Elapsed: 17.693796ms
    Nov 15 07:15:16.247: INFO: Pod "pod-10ba5242-f470-4f6a-bdee-da425f65bd78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039654634s
    Nov 15 07:15:18.241: INFO: Pod "pod-10ba5242-f470-4f6a-bdee-da425f65bd78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033182754s
    STEP: Saw pod success 11/15/23 07:15:18.241
    Nov 15 07:15:18.241: INFO: Pod "pod-10ba5242-f470-4f6a-bdee-da425f65bd78" satisfied condition "Succeeded or Failed"
    Nov 15 07:15:18.256: INFO: Trying to get logs from node 10.72.152.81 pod pod-10ba5242-f470-4f6a-bdee-da425f65bd78 container test-container: <nil>
    STEP: delete the pod 11/15/23 07:15:18.291
    Nov 15 07:15:18.340: INFO: Waiting for pod pod-10ba5242-f470-4f6a-bdee-da425f65bd78 to disappear
    Nov 15 07:15:18.357: INFO: Pod pod-10ba5242-f470-4f6a-bdee-da425f65bd78 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:15:18.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4479" for this suite. 11/15/23 07:15:18.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:15:18.406
Nov 15 07:15:18.406: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename configmap 11/15/23 07:15:18.406
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:15:18.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:15:18.514
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
Nov 15 07:15:18.564: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-fd549dd6-4e47-4a49-8701-37bd4b9e1e66 11/15/23 07:15:18.564
STEP: Creating configMap with name cm-test-opt-upd-b3c5d8f3-57bc-44a7-b2f6-cfb9c067b9ab 11/15/23 07:15:18.585
STEP: Creating the pod 11/15/23 07:15:18.607
Nov 15 07:15:18.643: INFO: Waiting up to 5m0s for pod "pod-configmaps-02885eb9-a279-4918-88b2-b580c297284e" in namespace "configmap-8473" to be "running and ready"
Nov 15 07:15:18.672: INFO: Pod "pod-configmaps-02885eb9-a279-4918-88b2-b580c297284e": Phase="Pending", Reason="", readiness=false. Elapsed: 28.150416ms
Nov 15 07:15:18.672: INFO: The phase of Pod pod-configmaps-02885eb9-a279-4918-88b2-b580c297284e is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:15:20.689: INFO: Pod "pod-configmaps-02885eb9-a279-4918-88b2-b580c297284e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045526151s
Nov 15 07:15:20.689: INFO: The phase of Pod pod-configmaps-02885eb9-a279-4918-88b2-b580c297284e is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:15:22.690: INFO: Pod "pod-configmaps-02885eb9-a279-4918-88b2-b580c297284e": Phase="Running", Reason="", readiness=true. Elapsed: 4.046627773s
Nov 15 07:15:22.690: INFO: The phase of Pod pod-configmaps-02885eb9-a279-4918-88b2-b580c297284e is Running (Ready = true)
Nov 15 07:15:22.690: INFO: Pod "pod-configmaps-02885eb9-a279-4918-88b2-b580c297284e" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-fd549dd6-4e47-4a49-8701-37bd4b9e1e66 11/15/23 07:15:22.842
STEP: Updating configmap cm-test-opt-upd-b3c5d8f3-57bc-44a7-b2f6-cfb9c067b9ab 11/15/23 07:15:22.869
STEP: Creating configMap with name cm-test-opt-create-4aad3c09-2129-4e86-9e85-782f7d923981 11/15/23 07:15:22.89
STEP: waiting to observe update in volume 11/15/23 07:15:22.91
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 07:16:38.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8473" for this suite. 11/15/23 07:16:38.867
------------------------------
â€¢ [SLOW TEST] [80.486 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:15:18.406
    Nov 15 07:15:18.406: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename configmap 11/15/23 07:15:18.406
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:15:18.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:15:18.514
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    Nov 15 07:15:18.564: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name cm-test-opt-del-fd549dd6-4e47-4a49-8701-37bd4b9e1e66 11/15/23 07:15:18.564
    STEP: Creating configMap with name cm-test-opt-upd-b3c5d8f3-57bc-44a7-b2f6-cfb9c067b9ab 11/15/23 07:15:18.585
    STEP: Creating the pod 11/15/23 07:15:18.607
    Nov 15 07:15:18.643: INFO: Waiting up to 5m0s for pod "pod-configmaps-02885eb9-a279-4918-88b2-b580c297284e" in namespace "configmap-8473" to be "running and ready"
    Nov 15 07:15:18.672: INFO: Pod "pod-configmaps-02885eb9-a279-4918-88b2-b580c297284e": Phase="Pending", Reason="", readiness=false. Elapsed: 28.150416ms
    Nov 15 07:15:18.672: INFO: The phase of Pod pod-configmaps-02885eb9-a279-4918-88b2-b580c297284e is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:15:20.689: INFO: Pod "pod-configmaps-02885eb9-a279-4918-88b2-b580c297284e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045526151s
    Nov 15 07:15:20.689: INFO: The phase of Pod pod-configmaps-02885eb9-a279-4918-88b2-b580c297284e is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:15:22.690: INFO: Pod "pod-configmaps-02885eb9-a279-4918-88b2-b580c297284e": Phase="Running", Reason="", readiness=true. Elapsed: 4.046627773s
    Nov 15 07:15:22.690: INFO: The phase of Pod pod-configmaps-02885eb9-a279-4918-88b2-b580c297284e is Running (Ready = true)
    Nov 15 07:15:22.690: INFO: Pod "pod-configmaps-02885eb9-a279-4918-88b2-b580c297284e" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-fd549dd6-4e47-4a49-8701-37bd4b9e1e66 11/15/23 07:15:22.842
    STEP: Updating configmap cm-test-opt-upd-b3c5d8f3-57bc-44a7-b2f6-cfb9c067b9ab 11/15/23 07:15:22.869
    STEP: Creating configMap with name cm-test-opt-create-4aad3c09-2129-4e86-9e85-782f7d923981 11/15/23 07:15:22.89
    STEP: waiting to observe update in volume 11/15/23 07:15:22.91
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:16:38.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8473" for this suite. 11/15/23 07:16:38.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:16:38.893
Nov 15 07:16:38.893: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename watch 11/15/23 07:16:38.894
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:16:38.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:16:38.969
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 11/15/23 07:16:38.991
STEP: starting a background goroutine to produce watch events 11/15/23 07:16:39.039
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 11/15/23 07:16:39.039
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Nov 15 07:16:41.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2613" for this suite. 11/15/23 07:16:41.766
------------------------------
â€¢ [2.936 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:16:38.893
    Nov 15 07:16:38.893: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename watch 11/15/23 07:16:38.894
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:16:38.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:16:38.969
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 11/15/23 07:16:38.991
    STEP: starting a background goroutine to produce watch events 11/15/23 07:16:39.039
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 11/15/23 07:16:39.039
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:16:41.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2613" for this suite. 11/15/23 07:16:41.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:16:41.832
Nov 15 07:16:41.832: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename services 11/15/23 07:16:41.834
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:16:41.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:16:41.909
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-7908 11/15/23 07:16:41.93
STEP: creating service affinity-clusterip in namespace services-7908 11/15/23 07:16:41.93
STEP: creating replication controller affinity-clusterip in namespace services-7908 11/15/23 07:16:42.015
I1115 07:16:42.037291      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7908, replica count: 3
I1115 07:16:45.089426      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 07:16:45.119: INFO: Creating new exec pod
Nov 15 07:16:45.157: INFO: Waiting up to 5m0s for pod "execpod-affinitydlb4p" in namespace "services-7908" to be "running"
Nov 15 07:16:45.180: INFO: Pod "execpod-affinitydlb4p": Phase="Pending", Reason="", readiness=false. Elapsed: 22.831774ms
Nov 15 07:16:47.197: INFO: Pod "execpod-affinitydlb4p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039288203s
Nov 15 07:16:49.197: INFO: Pod "execpod-affinitydlb4p": Phase="Running", Reason="", readiness=true. Elapsed: 4.03943588s
Nov 15 07:16:49.197: INFO: Pod "execpod-affinitydlb4p" satisfied condition "running"
Nov 15 07:16:50.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7908 exec execpod-affinitydlb4p -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Nov 15 07:16:50.501: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Nov 15 07:16:50.501: INFO: stdout: ""
Nov 15 07:16:50.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7908 exec execpod-affinitydlb4p -- /bin/sh -x -c nc -v -z -w 2 172.21.153.201 80'
Nov 15 07:16:50.812: INFO: stderr: "+ nc -v -z -w 2 172.21.153.201 80\nConnection to 172.21.153.201 80 port [tcp/http] succeeded!\n"
Nov 15 07:16:50.812: INFO: stdout: ""
Nov 15 07:16:50.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7908 exec execpod-affinitydlb4p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.153.201:80/ ; done'
Nov 15 07:16:51.149: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n"
Nov 15 07:16:51.149: INFO: stdout: "\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx"
Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
Nov 15 07:16:51.149: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7908, will wait for the garbage collector to delete the pods 11/15/23 07:16:51.195
Nov 15 07:16:51.296: INFO: Deleting ReplicationController affinity-clusterip took: 32.75032ms
Nov 15 07:16:51.497: INFO: Terminating ReplicationController affinity-clusterip pods took: 200.233308ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 07:16:54.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7908" for this suite. 11/15/23 07:16:54.104
------------------------------
â€¢ [SLOW TEST] [12.295 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:16:41.832
    Nov 15 07:16:41.832: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename services 11/15/23 07:16:41.834
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:16:41.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:16:41.909
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-7908 11/15/23 07:16:41.93
    STEP: creating service affinity-clusterip in namespace services-7908 11/15/23 07:16:41.93
    STEP: creating replication controller affinity-clusterip in namespace services-7908 11/15/23 07:16:42.015
    I1115 07:16:42.037291      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7908, replica count: 3
    I1115 07:16:45.089426      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 15 07:16:45.119: INFO: Creating new exec pod
    Nov 15 07:16:45.157: INFO: Waiting up to 5m0s for pod "execpod-affinitydlb4p" in namespace "services-7908" to be "running"
    Nov 15 07:16:45.180: INFO: Pod "execpod-affinitydlb4p": Phase="Pending", Reason="", readiness=false. Elapsed: 22.831774ms
    Nov 15 07:16:47.197: INFO: Pod "execpod-affinitydlb4p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039288203s
    Nov 15 07:16:49.197: INFO: Pod "execpod-affinitydlb4p": Phase="Running", Reason="", readiness=true. Elapsed: 4.03943588s
    Nov 15 07:16:49.197: INFO: Pod "execpod-affinitydlb4p" satisfied condition "running"
    Nov 15 07:16:50.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7908 exec execpod-affinitydlb4p -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Nov 15 07:16:50.501: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Nov 15 07:16:50.501: INFO: stdout: ""
    Nov 15 07:16:50.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7908 exec execpod-affinitydlb4p -- /bin/sh -x -c nc -v -z -w 2 172.21.153.201 80'
    Nov 15 07:16:50.812: INFO: stderr: "+ nc -v -z -w 2 172.21.153.201 80\nConnection to 172.21.153.201 80 port [tcp/http] succeeded!\n"
    Nov 15 07:16:50.812: INFO: stdout: ""
    Nov 15 07:16:50.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-7908 exec execpod-affinitydlb4p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.153.201:80/ ; done'
    Nov 15 07:16:51.149: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.201:80/\n"
    Nov 15 07:16:51.149: INFO: stdout: "\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx\naffinity-clusterip-66vvx"
    Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
    Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
    Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
    Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
    Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
    Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
    Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
    Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
    Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
    Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
    Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
    Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
    Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
    Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
    Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
    Nov 15 07:16:51.149: INFO: Received response from host: affinity-clusterip-66vvx
    Nov 15 07:16:51.149: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-7908, will wait for the garbage collector to delete the pods 11/15/23 07:16:51.195
    Nov 15 07:16:51.296: INFO: Deleting ReplicationController affinity-clusterip took: 32.75032ms
    Nov 15 07:16:51.497: INFO: Terminating ReplicationController affinity-clusterip pods took: 200.233308ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:16:54.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7908" for this suite. 11/15/23 07:16:54.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:16:54.126
Nov 15 07:16:54.127: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename proxy 11/15/23 07:16:54.127
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:16:54.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:16:54.217
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Nov 15 07:16:54.226: INFO: Creating pod...
Nov 15 07:16:54.255: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5421" to be "running"
Nov 15 07:16:54.275: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 20.298074ms
Nov 15 07:16:56.294: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038777751s
Nov 15 07:16:58.291: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.03648739s
Nov 15 07:16:58.291: INFO: Pod "agnhost" satisfied condition "running"
Nov 15 07:16:58.291: INFO: Creating service...
Nov 15 07:16:58.329: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/pods/agnhost/proxy/some/path/with/DELETE
Nov 15 07:16:58.363: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 15 07:16:58.363: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/pods/agnhost/proxy/some/path/with/GET
Nov 15 07:16:58.389: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Nov 15 07:16:58.390: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/pods/agnhost/proxy/some/path/with/HEAD
Nov 15 07:16:58.412: INFO: http.Client request:HEAD | StatusCode:200
Nov 15 07:16:58.412: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/pods/agnhost/proxy/some/path/with/OPTIONS
Nov 15 07:16:58.437: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 15 07:16:58.437: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/pods/agnhost/proxy/some/path/with/PATCH
Nov 15 07:16:58.461: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 15 07:16:58.461: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/pods/agnhost/proxy/some/path/with/POST
Nov 15 07:16:58.484: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 15 07:16:58.484: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/pods/agnhost/proxy/some/path/with/PUT
Nov 15 07:16:58.513: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Nov 15 07:16:58.513: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/services/test-service/proxy/some/path/with/DELETE
Nov 15 07:16:58.548: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 15 07:16:58.548: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/services/test-service/proxy/some/path/with/GET
Nov 15 07:16:58.582: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Nov 15 07:16:58.582: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/services/test-service/proxy/some/path/with/HEAD
Nov 15 07:16:58.615: INFO: http.Client request:HEAD | StatusCode:200
Nov 15 07:16:58.615: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/services/test-service/proxy/some/path/with/OPTIONS
Nov 15 07:16:58.651: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 15 07:16:58.651: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/services/test-service/proxy/some/path/with/PATCH
Nov 15 07:16:58.680: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 15 07:16:58.680: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/services/test-service/proxy/some/path/with/POST
Nov 15 07:16:58.711: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 15 07:16:58.711: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/services/test-service/proxy/some/path/with/PUT
Nov 15 07:16:58.743: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Nov 15 07:16:58.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-5421" for this suite. 11/15/23 07:16:58.765
------------------------------
â€¢ [4.663 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:16:54.126
    Nov 15 07:16:54.127: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename proxy 11/15/23 07:16:54.127
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:16:54.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:16:54.217
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Nov 15 07:16:54.226: INFO: Creating pod...
    Nov 15 07:16:54.255: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5421" to be "running"
    Nov 15 07:16:54.275: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 20.298074ms
    Nov 15 07:16:56.294: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038777751s
    Nov 15 07:16:58.291: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.03648739s
    Nov 15 07:16:58.291: INFO: Pod "agnhost" satisfied condition "running"
    Nov 15 07:16:58.291: INFO: Creating service...
    Nov 15 07:16:58.329: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/pods/agnhost/proxy/some/path/with/DELETE
    Nov 15 07:16:58.363: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Nov 15 07:16:58.363: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/pods/agnhost/proxy/some/path/with/GET
    Nov 15 07:16:58.389: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Nov 15 07:16:58.390: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/pods/agnhost/proxy/some/path/with/HEAD
    Nov 15 07:16:58.412: INFO: http.Client request:HEAD | StatusCode:200
    Nov 15 07:16:58.412: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/pods/agnhost/proxy/some/path/with/OPTIONS
    Nov 15 07:16:58.437: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Nov 15 07:16:58.437: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/pods/agnhost/proxy/some/path/with/PATCH
    Nov 15 07:16:58.461: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Nov 15 07:16:58.461: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/pods/agnhost/proxy/some/path/with/POST
    Nov 15 07:16:58.484: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Nov 15 07:16:58.484: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/pods/agnhost/proxy/some/path/with/PUT
    Nov 15 07:16:58.513: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Nov 15 07:16:58.513: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/services/test-service/proxy/some/path/with/DELETE
    Nov 15 07:16:58.548: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Nov 15 07:16:58.548: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/services/test-service/proxy/some/path/with/GET
    Nov 15 07:16:58.582: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Nov 15 07:16:58.582: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/services/test-service/proxy/some/path/with/HEAD
    Nov 15 07:16:58.615: INFO: http.Client request:HEAD | StatusCode:200
    Nov 15 07:16:58.615: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/services/test-service/proxy/some/path/with/OPTIONS
    Nov 15 07:16:58.651: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Nov 15 07:16:58.651: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/services/test-service/proxy/some/path/with/PATCH
    Nov 15 07:16:58.680: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Nov 15 07:16:58.680: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/services/test-service/proxy/some/path/with/POST
    Nov 15 07:16:58.711: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Nov 15 07:16:58.711: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5421/services/test-service/proxy/some/path/with/PUT
    Nov 15 07:16:58.743: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:16:58.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-5421" for this suite. 11/15/23 07:16:58.765
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:16:58.79
Nov 15 07:16:58.790: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename taint-single-pod 11/15/23 07:16:58.79
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:16:58.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:16:58.887
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Nov 15 07:16:58.901: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 15 07:17:59.112: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Nov 15 07:17:59.143: INFO: Starting informer...
STEP: Starting pod... 11/15/23 07:17:59.143
Nov 15 07:17:59.407: INFO: Pod is running on 10.72.152.81. Tainting Node
STEP: Trying to apply a taint on the Node 11/15/23 07:17:59.407
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/15/23 07:17:59.452
STEP: Waiting short time to make sure Pod is queued for deletion 11/15/23 07:17:59.474
Nov 15 07:17:59.474: INFO: Pod wasn't evicted. Proceeding
Nov 15 07:17:59.474: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/15/23 07:17:59.524
STEP: Waiting some time to make sure that toleration time passed. 11/15/23 07:17:59.546
Nov 15 07:19:14.548: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:19:14.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-2992" for this suite. 11/15/23 07:19:14.572
------------------------------
â€¢ [SLOW TEST] [135.808 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:16:58.79
    Nov 15 07:16:58.790: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename taint-single-pod 11/15/23 07:16:58.79
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:16:58.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:16:58.887
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Nov 15 07:16:58.901: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 15 07:17:59.112: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Nov 15 07:17:59.143: INFO: Starting informer...
    STEP: Starting pod... 11/15/23 07:17:59.143
    Nov 15 07:17:59.407: INFO: Pod is running on 10.72.152.81. Tainting Node
    STEP: Trying to apply a taint on the Node 11/15/23 07:17:59.407
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/15/23 07:17:59.452
    STEP: Waiting short time to make sure Pod is queued for deletion 11/15/23 07:17:59.474
    Nov 15 07:17:59.474: INFO: Pod wasn't evicted. Proceeding
    Nov 15 07:17:59.474: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/15/23 07:17:59.524
    STEP: Waiting some time to make sure that toleration time passed. 11/15/23 07:17:59.546
    Nov 15 07:19:14.548: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:19:14.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-2992" for this suite. 11/15/23 07:19:14.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:19:14.601
Nov 15 07:19:14.601: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename replicaset 11/15/23 07:19:14.603
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:19:14.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:19:14.756
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Nov 15 07:19:14.842: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 15 07:19:19.857: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/15/23 07:19:19.857
STEP: Scaling up "test-rs" replicaset  11/15/23 07:19:19.858
Nov 15 07:19:19.896: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 11/15/23 07:19:19.896
W1115 07:19:19.926015      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Nov 15 07:19:19.936: INFO: observed ReplicaSet test-rs in namespace replicaset-5938 with ReadyReplicas 1, AvailableReplicas 1
Nov 15 07:19:19.970: INFO: observed ReplicaSet test-rs in namespace replicaset-5938 with ReadyReplicas 1, AvailableReplicas 1
Nov 15 07:19:20.032: INFO: observed ReplicaSet test-rs in namespace replicaset-5938 with ReadyReplicas 1, AvailableReplicas 1
Nov 15 07:19:20.050: INFO: observed ReplicaSet test-rs in namespace replicaset-5938 with ReadyReplicas 1, AvailableReplicas 1
Nov 15 07:19:21.636: INFO: observed ReplicaSet test-rs in namespace replicaset-5938 with ReadyReplicas 2, AvailableReplicas 2
Nov 15 07:19:21.823: INFO: observed Replicaset test-rs in namespace replicaset-5938 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Nov 15 07:19:21.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5938" for this suite. 11/15/23 07:19:21.849
------------------------------
â€¢ [SLOW TEST] [7.273 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:19:14.601
    Nov 15 07:19:14.601: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename replicaset 11/15/23 07:19:14.603
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:19:14.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:19:14.756
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Nov 15 07:19:14.842: INFO: Pod name sample-pod: Found 0 pods out of 1
    Nov 15 07:19:19.857: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/15/23 07:19:19.857
    STEP: Scaling up "test-rs" replicaset  11/15/23 07:19:19.858
    Nov 15 07:19:19.896: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 11/15/23 07:19:19.896
    W1115 07:19:19.926015      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Nov 15 07:19:19.936: INFO: observed ReplicaSet test-rs in namespace replicaset-5938 with ReadyReplicas 1, AvailableReplicas 1
    Nov 15 07:19:19.970: INFO: observed ReplicaSet test-rs in namespace replicaset-5938 with ReadyReplicas 1, AvailableReplicas 1
    Nov 15 07:19:20.032: INFO: observed ReplicaSet test-rs in namespace replicaset-5938 with ReadyReplicas 1, AvailableReplicas 1
    Nov 15 07:19:20.050: INFO: observed ReplicaSet test-rs in namespace replicaset-5938 with ReadyReplicas 1, AvailableReplicas 1
    Nov 15 07:19:21.636: INFO: observed ReplicaSet test-rs in namespace replicaset-5938 with ReadyReplicas 2, AvailableReplicas 2
    Nov 15 07:19:21.823: INFO: observed Replicaset test-rs in namespace replicaset-5938 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:19:21.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5938" for this suite. 11/15/23 07:19:21.849
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:19:21.899
Nov 15 07:19:21.899: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename ephemeral-containers-test 11/15/23 07:19:21.901
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:19:21.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:19:21.98
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 11/15/23 07:19:21.993
Nov 15 07:19:22.028: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1509" to be "running and ready"
Nov 15 07:19:22.047: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.654034ms
Nov 15 07:19:22.047: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:19:24.064: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.036710271s
Nov 15 07:19:24.064: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Nov 15 07:19:24.065: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 11/15/23 07:19:24.08
Nov 15 07:19:24.112: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1509" to be "container debugger running"
Nov 15 07:19:24.144: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 31.647648ms
Nov 15 07:19:26.167: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.05477422s
Nov 15 07:19:28.161: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.048686491s
Nov 15 07:19:28.161: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 11/15/23 07:19:28.161
Nov 15 07:19:28.161: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-1509 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:19:28.162: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:19:28.162: INFO: ExecWithOptions: Clientset creation
Nov 15 07:19:28.162: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-1509/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Nov 15 07:19:28.396: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:19:28.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-1509" for this suite. 11/15/23 07:19:28.504
------------------------------
â€¢ [SLOW TEST] [6.630 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:19:21.899
    Nov 15 07:19:21.899: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename ephemeral-containers-test 11/15/23 07:19:21.901
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:19:21.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:19:21.98
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 11/15/23 07:19:21.993
    Nov 15 07:19:22.028: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1509" to be "running and ready"
    Nov 15 07:19:22.047: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.654034ms
    Nov 15 07:19:22.047: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:19:24.064: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.036710271s
    Nov 15 07:19:24.064: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Nov 15 07:19:24.065: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 11/15/23 07:19:24.08
    Nov 15 07:19:24.112: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1509" to be "container debugger running"
    Nov 15 07:19:24.144: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 31.647648ms
    Nov 15 07:19:26.167: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.05477422s
    Nov 15 07:19:28.161: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.048686491s
    Nov 15 07:19:28.161: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 11/15/23 07:19:28.161
    Nov 15 07:19:28.161: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-1509 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:19:28.162: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:19:28.162: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:19:28.162: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-1509/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Nov 15 07:19:28.396: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:19:28.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-1509" for this suite. 11/15/23 07:19:28.504
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:19:28.532
Nov 15 07:19:28.532: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename sched-preemption 11/15/23 07:19:28.533
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:19:28.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:19:28.613
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Nov 15 07:19:28.693: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 15 07:20:29.018: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 11/15/23 07:20:29.077
Nov 15 07:20:29.188: INFO: Created pod: pod0-0-sched-preemption-low-priority
Nov 15 07:20:29.222: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Nov 15 07:20:29.345: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Nov 15 07:20:29.408: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Nov 15 07:20:29.485: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Nov 15 07:20:29.528: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 11/15/23 07:20:29.528
Nov 15 07:20:29.528: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-2532" to be "running"
Nov 15 07:20:29.556: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 28.413682ms
Nov 15 07:20:31.596: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.068226566s
Nov 15 07:20:31.596: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Nov 15 07:20:31.596: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-2532" to be "running"
Nov 15 07:20:31.635: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 38.815563ms
Nov 15 07:20:31.635: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Nov 15 07:20:31.635: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-2532" to be "running"
Nov 15 07:20:31.659: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 24.128376ms
Nov 15 07:20:31.659: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Nov 15 07:20:31.659: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-2532" to be "running"
Nov 15 07:20:31.691: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 31.331115ms
Nov 15 07:20:31.691: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Nov 15 07:20:31.691: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-2532" to be "running"
Nov 15 07:20:31.706: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 15.577464ms
Nov 15 07:20:33.724: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.033361243s
Nov 15 07:20:33.724: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Nov 15 07:20:33.724: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-2532" to be "running"
Nov 15 07:20:33.741: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.232571ms
Nov 15 07:20:33.741: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 11/15/23 07:20:33.741
Nov 15 07:20:33.784: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Nov 15 07:20:33.798: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.244376ms
Nov 15 07:20:35.815: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030823873s
Nov 15 07:20:37.814: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.030110284s
Nov 15 07:20:37.814: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:20:37.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-2532" for this suite. 11/15/23 07:20:38.19
------------------------------
â€¢ [SLOW TEST] [69.679 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:19:28.532
    Nov 15 07:19:28.532: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename sched-preemption 11/15/23 07:19:28.533
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:19:28.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:19:28.613
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Nov 15 07:19:28.693: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 15 07:20:29.018: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 11/15/23 07:20:29.077
    Nov 15 07:20:29.188: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Nov 15 07:20:29.222: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Nov 15 07:20:29.345: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Nov 15 07:20:29.408: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Nov 15 07:20:29.485: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Nov 15 07:20:29.528: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 11/15/23 07:20:29.528
    Nov 15 07:20:29.528: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-2532" to be "running"
    Nov 15 07:20:29.556: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 28.413682ms
    Nov 15 07:20:31.596: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.068226566s
    Nov 15 07:20:31.596: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Nov 15 07:20:31.596: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-2532" to be "running"
    Nov 15 07:20:31.635: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 38.815563ms
    Nov 15 07:20:31.635: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Nov 15 07:20:31.635: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-2532" to be "running"
    Nov 15 07:20:31.659: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 24.128376ms
    Nov 15 07:20:31.659: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Nov 15 07:20:31.659: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-2532" to be "running"
    Nov 15 07:20:31.691: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 31.331115ms
    Nov 15 07:20:31.691: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Nov 15 07:20:31.691: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-2532" to be "running"
    Nov 15 07:20:31.706: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 15.577464ms
    Nov 15 07:20:33.724: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.033361243s
    Nov 15 07:20:33.724: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Nov 15 07:20:33.724: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-2532" to be "running"
    Nov 15 07:20:33.741: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.232571ms
    Nov 15 07:20:33.741: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 11/15/23 07:20:33.741
    Nov 15 07:20:33.784: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Nov 15 07:20:33.798: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.244376ms
    Nov 15 07:20:35.815: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030823873s
    Nov 15 07:20:37.814: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.030110284s
    Nov 15 07:20:37.814: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:20:37.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-2532" for this suite. 11/15/23 07:20:38.19
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:20:38.212
Nov 15 07:20:38.212: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename daemonsets 11/15/23 07:20:38.213
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:20:38.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:20:38.288
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
Nov 15 07:20:38.422: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 11/15/23 07:20:38.437
Nov 15 07:20:38.478: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 07:20:38.478: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 11/15/23 07:20:38.478
Nov 15 07:20:38.620: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 07:20:38.620: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 07:20:39.638: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 07:20:39.638: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 07:20:40.635: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 15 07:20:40.636: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 11/15/23 07:20:40.651
Nov 15 07:20:40.721: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 15 07:20:40.721: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Nov 15 07:20:41.739: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 07:20:41.739: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 11/15/23 07:20:41.739
Nov 15 07:20:41.779: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 07:20:41.779: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 07:20:42.796: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 07:20:42.796: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 07:20:43.797: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 07:20:43.797: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 07:20:44.798: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 07:20:44.798: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
Nov 15 07:20:45.796: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 15 07:20:45.796: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 11/15/23 07:20:45.827
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4523, will wait for the garbage collector to delete the pods 11/15/23 07:20:45.827
Nov 15 07:20:45.922: INFO: Deleting DaemonSet.extensions daemon-set took: 28.98929ms
Nov 15 07:20:46.125: INFO: Terminating DaemonSet.extensions daemon-set pods took: 202.907273ms
Nov 15 07:20:48.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 07:20:48.340: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 15 07:20:48.354: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"123927"},"items":null}

Nov 15 07:20:48.368: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"123927"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:20:48.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4523" for this suite. 11/15/23 07:20:48.498
------------------------------
â€¢ [SLOW TEST] [10.313 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:20:38.212
    Nov 15 07:20:38.212: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename daemonsets 11/15/23 07:20:38.213
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:20:38.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:20:38.288
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:205
    Nov 15 07:20:38.422: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 11/15/23 07:20:38.437
    Nov 15 07:20:38.478: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 07:20:38.478: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 11/15/23 07:20:38.478
    Nov 15 07:20:38.620: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 07:20:38.620: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 07:20:39.638: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 07:20:39.638: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 07:20:40.635: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 15 07:20:40.636: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 11/15/23 07:20:40.651
    Nov 15 07:20:40.721: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 15 07:20:40.721: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Nov 15 07:20:41.739: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 07:20:41.739: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 11/15/23 07:20:41.739
    Nov 15 07:20:41.779: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 07:20:41.779: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 07:20:42.796: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 07:20:42.796: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 07:20:43.797: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 07:20:43.797: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 07:20:44.798: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 07:20:44.798: INFO: Node 10.72.152.81 is running 0 daemon pod, expected 1
    Nov 15 07:20:45.796: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 15 07:20:45.796: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 11/15/23 07:20:45.827
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4523, will wait for the garbage collector to delete the pods 11/15/23 07:20:45.827
    Nov 15 07:20:45.922: INFO: Deleting DaemonSet.extensions daemon-set took: 28.98929ms
    Nov 15 07:20:46.125: INFO: Terminating DaemonSet.extensions daemon-set pods took: 202.907273ms
    Nov 15 07:20:48.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 07:20:48.340: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 15 07:20:48.354: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"123927"},"items":null}

    Nov 15 07:20:48.368: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"123927"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:20:48.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4523" for this suite. 11/15/23 07:20:48.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:20:48.526
Nov 15 07:20:48.526: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename namespaces 11/15/23 07:20:48.526
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:20:48.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:20:48.603
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-6910" 11/15/23 07:20:48.613
Nov 15 07:20:48.645: INFO: Namespace "namespaces-6910" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"ecce4ee9-09c9-4c3e-878b-b9821315fe37", "kubernetes.io/metadata.name":"namespaces-6910", "namespaces-6910":"updated", "pod-security.kubernetes.io/audit":"privileged", "pod-security.kubernetes.io/audit-version":"v1.24", "pod-security.kubernetes.io/enforce":"baseline", "pod-security.kubernetes.io/warn":"privileged", "pod-security.kubernetes.io/warn-version":"v1.24"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:20:48.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6910" for this suite. 11/15/23 07:20:48.667
------------------------------
â€¢ [0.177 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:20:48.526
    Nov 15 07:20:48.526: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename namespaces 11/15/23 07:20:48.526
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:20:48.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:20:48.603
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-6910" 11/15/23 07:20:48.613
    Nov 15 07:20:48.645: INFO: Namespace "namespaces-6910" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"ecce4ee9-09c9-4c3e-878b-b9821315fe37", "kubernetes.io/metadata.name":"namespaces-6910", "namespaces-6910":"updated", "pod-security.kubernetes.io/audit":"privileged", "pod-security.kubernetes.io/audit-version":"v1.24", "pod-security.kubernetes.io/enforce":"baseline", "pod-security.kubernetes.io/warn":"privileged", "pod-security.kubernetes.io/warn-version":"v1.24"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:20:48.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6910" for this suite. 11/15/23 07:20:48.667
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:20:48.705
Nov 15 07:20:48.705: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename svcaccounts 11/15/23 07:20:48.706
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:20:48.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:20:48.788
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-jkmt4"  11/15/23 07:20:48.8
Nov 15 07:20:48.819: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-jkmt4"  11/15/23 07:20:48.819
Nov 15 07:20:48.856: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 15 07:20:48.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7394" for this suite. 11/15/23 07:20:48.881
------------------------------
â€¢ [0.198 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:20:48.705
    Nov 15 07:20:48.705: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename svcaccounts 11/15/23 07:20:48.706
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:20:48.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:20:48.788
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-jkmt4"  11/15/23 07:20:48.8
    Nov 15 07:20:48.819: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-jkmt4"  11/15/23 07:20:48.819
    Nov 15 07:20:48.856: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:20:48.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7394" for this suite. 11/15/23 07:20:48.881
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:20:48.903
Nov 15 07:20:48.903: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename aggregator 11/15/23 07:20:48.904
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:20:48.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:20:48.979
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Nov 15 07:20:48.989: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 11/15/23 07:20:48.99
Nov 15 07:20:49.525: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Nov 15 07:20:51.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 07:20:53.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 07:20:55.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 07:20:57.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 07:20:59.911: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 07:21:01.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 07:21:03.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 07:21:05.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 07:21:07.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 07:21:09.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 07:21:11.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 07:21:13.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 07:21:16.119: INFO: Waited 180.989707ms for the sample-apiserver to be ready to handle requests.
I1115 07:21:17.304830      22 request.go:690] Waited for 1.034249957s due to client-side throttling, not priority and fairness, request: GET:https://172.21.0.1:443/apis/operators.coreos.com/v1alpha2
STEP: Read Status for v1alpha1.wardle.example.com 11/15/23 07:21:17.564
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 11/15/23 07:21:17.579
STEP: List APIServices 11/15/23 07:21:17.626
Nov 15 07:21:17.704: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Nov 15 07:21:18.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-2997" for this suite. 11/15/23 07:21:18.529
------------------------------
â€¢ [SLOW TEST] [29.681 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:20:48.903
    Nov 15 07:20:48.903: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename aggregator 11/15/23 07:20:48.904
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:20:48.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:20:48.979
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Nov 15 07:20:48.989: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 11/15/23 07:20:48.99
    Nov 15 07:20:49.525: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Nov 15 07:20:51.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 07:20:53.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 07:20:55.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 07:20:57.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 07:20:59.911: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 07:21:01.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 07:21:03.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 07:21:05.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 07:21:07.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 07:21:09.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 07:21:11.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 07:21:13.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 20, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 07:21:16.119: INFO: Waited 180.989707ms for the sample-apiserver to be ready to handle requests.
    I1115 07:21:17.304830      22 request.go:690] Waited for 1.034249957s due to client-side throttling, not priority and fairness, request: GET:https://172.21.0.1:443/apis/operators.coreos.com/v1alpha2
    STEP: Read Status for v1alpha1.wardle.example.com 11/15/23 07:21:17.564
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 11/15/23 07:21:17.579
    STEP: List APIServices 11/15/23 07:21:17.626
    Nov 15 07:21:17.704: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:21:18.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-2997" for this suite. 11/15/23 07:21:18.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:21:18.585
Nov 15 07:21:18.585: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename emptydir 11/15/23 07:21:18.586
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:21:18.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:21:18.672
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 11/15/23 07:21:18.684
Nov 15 07:21:18.718: INFO: Waiting up to 5m0s for pod "pod-96fd52cc-5674-45fb-9b51-8451b86c453f" in namespace "emptydir-5937" to be "Succeeded or Failed"
Nov 15 07:21:18.734: INFO: Pod "pod-96fd52cc-5674-45fb-9b51-8451b86c453f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.817007ms
Nov 15 07:21:20.752: INFO: Pod "pod-96fd52cc-5674-45fb-9b51-8451b86c453f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033621335s
Nov 15 07:21:22.748: INFO: Pod "pod-96fd52cc-5674-45fb-9b51-8451b86c453f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030039062s
STEP: Saw pod success 11/15/23 07:21:22.749
Nov 15 07:21:22.749: INFO: Pod "pod-96fd52cc-5674-45fb-9b51-8451b86c453f" satisfied condition "Succeeded or Failed"
Nov 15 07:21:22.762: INFO: Trying to get logs from node 10.72.152.81 pod pod-96fd52cc-5674-45fb-9b51-8451b86c453f container test-container: <nil>
STEP: delete the pod 11/15/23 07:21:22.873
Nov 15 07:21:22.947: INFO: Waiting for pod pod-96fd52cc-5674-45fb-9b51-8451b86c453f to disappear
Nov 15 07:21:22.967: INFO: Pod pod-96fd52cc-5674-45fb-9b51-8451b86c453f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 07:21:22.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5937" for this suite. 11/15/23 07:21:23
------------------------------
â€¢ [4.453 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:21:18.585
    Nov 15 07:21:18.585: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename emptydir 11/15/23 07:21:18.586
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:21:18.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:21:18.672
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 11/15/23 07:21:18.684
    Nov 15 07:21:18.718: INFO: Waiting up to 5m0s for pod "pod-96fd52cc-5674-45fb-9b51-8451b86c453f" in namespace "emptydir-5937" to be "Succeeded or Failed"
    Nov 15 07:21:18.734: INFO: Pod "pod-96fd52cc-5674-45fb-9b51-8451b86c453f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.817007ms
    Nov 15 07:21:20.752: INFO: Pod "pod-96fd52cc-5674-45fb-9b51-8451b86c453f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033621335s
    Nov 15 07:21:22.748: INFO: Pod "pod-96fd52cc-5674-45fb-9b51-8451b86c453f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030039062s
    STEP: Saw pod success 11/15/23 07:21:22.749
    Nov 15 07:21:22.749: INFO: Pod "pod-96fd52cc-5674-45fb-9b51-8451b86c453f" satisfied condition "Succeeded or Failed"
    Nov 15 07:21:22.762: INFO: Trying to get logs from node 10.72.152.81 pod pod-96fd52cc-5674-45fb-9b51-8451b86c453f container test-container: <nil>
    STEP: delete the pod 11/15/23 07:21:22.873
    Nov 15 07:21:22.947: INFO: Waiting for pod pod-96fd52cc-5674-45fb-9b51-8451b86c453f to disappear
    Nov 15 07:21:22.967: INFO: Pod pod-96fd52cc-5674-45fb-9b51-8451b86c453f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:21:22.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5937" for this suite. 11/15/23 07:21:23
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:21:23.039
Nov 15 07:21:23.039: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename webhook 11/15/23 07:21:23.043
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:21:23.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:21:23.158
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 07:21:23.253
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 07:21:24.387
STEP: Deploying the webhook pod 11/15/23 07:21:24.445
STEP: Wait for the deployment to be ready 11/15/23 07:21:24.479
Nov 15 07:21:24.520: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/15/23 07:21:26.567
STEP: Verifying the service has paired with the endpoint 11/15/23 07:21:26.61
Nov 15 07:21:27.611: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Nov 15 07:21:27.627: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4134-crds.webhook.example.com via the AdmissionRegistration API 11/15/23 07:21:28.161
STEP: Creating a custom resource that should be mutated by the webhook 11/15/23 07:21:28.246
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:21:30.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5507" for this suite. 11/15/23 07:21:31.119
STEP: Destroying namespace "webhook-5507-markers" for this suite. 11/15/23 07:21:31.146
------------------------------
â€¢ [SLOW TEST] [8.129 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:21:23.039
    Nov 15 07:21:23.039: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename webhook 11/15/23 07:21:23.043
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:21:23.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:21:23.158
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 07:21:23.253
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 07:21:24.387
    STEP: Deploying the webhook pod 11/15/23 07:21:24.445
    STEP: Wait for the deployment to be ready 11/15/23 07:21:24.479
    Nov 15 07:21:24.520: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/15/23 07:21:26.567
    STEP: Verifying the service has paired with the endpoint 11/15/23 07:21:26.61
    Nov 15 07:21:27.611: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Nov 15 07:21:27.627: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4134-crds.webhook.example.com via the AdmissionRegistration API 11/15/23 07:21:28.161
    STEP: Creating a custom resource that should be mutated by the webhook 11/15/23 07:21:28.246
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:21:30.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5507" for this suite. 11/15/23 07:21:31.119
    STEP: Destroying namespace "webhook-5507-markers" for this suite. 11/15/23 07:21:31.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:21:31.169
Nov 15 07:21:31.169: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename services 11/15/23 07:21:31.171
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:21:31.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:21:31.335
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-4051 11/15/23 07:21:31.391
STEP: creating service affinity-clusterip-transition in namespace services-4051 11/15/23 07:21:31.392
STEP: creating replication controller affinity-clusterip-transition in namespace services-4051 11/15/23 07:21:31.446
I1115 07:21:31.473809      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4051, replica count: 3
I1115 07:21:34.524588      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 07:21:34.560: INFO: Creating new exec pod
Nov 15 07:21:34.594: INFO: Waiting up to 5m0s for pod "execpod-affinityzr7tr" in namespace "services-4051" to be "running"
Nov 15 07:21:34.610: INFO: Pod "execpod-affinityzr7tr": Phase="Pending", Reason="", readiness=false. Elapsed: 16.091675ms
Nov 15 07:21:36.629: INFO: Pod "execpod-affinityzr7tr": Phase="Running", Reason="", readiness=true. Elapsed: 2.034868309s
Nov 15 07:21:36.629: INFO: Pod "execpod-affinityzr7tr" satisfied condition "running"
Nov 15 07:21:37.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-4051 exec execpod-affinityzr7tr -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Nov 15 07:21:37.966: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Nov 15 07:21:37.966: INFO: stdout: ""
Nov 15 07:21:37.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-4051 exec execpod-affinityzr7tr -- /bin/sh -x -c nc -v -z -w 2 172.21.69.195 80'
Nov 15 07:21:38.291: INFO: stderr: "+ nc -v -z -w 2 172.21.69.195 80\nConnection to 172.21.69.195 80 port [tcp/http] succeeded!\n"
Nov 15 07:21:38.291: INFO: stdout: ""
Nov 15 07:21:38.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-4051 exec execpod-affinityzr7tr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.69.195:80/ ; done'
Nov 15 07:21:38.724: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n"
Nov 15 07:21:38.724: INFO: stdout: "\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-g4lnp\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-g4lnp\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-g4lnp\naffinity-clusterip-transition-g4lnp\naffinity-clusterip-transition-g4lnp\naffinity-clusterip-transition-l7j26\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-g4lnp\naffinity-clusterip-transition-vmz5m"
Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-g4lnp
Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-g4lnp
Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-g4lnp
Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-g4lnp
Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-g4lnp
Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-l7j26
Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-g4lnp
Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:38.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-4051 exec execpod-affinityzr7tr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.69.195:80/ ; done'
Nov 15 07:21:39.159: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n"
Nov 15 07:21:39.159: INFO: stdout: "\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m"
Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
Nov 15 07:21:39.159: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4051, will wait for the garbage collector to delete the pods 11/15/23 07:21:39.209
Nov 15 07:21:39.300: INFO: Deleting ReplicationController affinity-clusterip-transition took: 22.974779ms
Nov 15 07:21:39.401: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.638729ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 07:21:42.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4051" for this suite. 11/15/23 07:21:42.701
------------------------------
â€¢ [SLOW TEST] [11.556 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:21:31.169
    Nov 15 07:21:31.169: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename services 11/15/23 07:21:31.171
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:21:31.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:21:31.335
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-4051 11/15/23 07:21:31.391
    STEP: creating service affinity-clusterip-transition in namespace services-4051 11/15/23 07:21:31.392
    STEP: creating replication controller affinity-clusterip-transition in namespace services-4051 11/15/23 07:21:31.446
    I1115 07:21:31.473809      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4051, replica count: 3
    I1115 07:21:34.524588      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 15 07:21:34.560: INFO: Creating new exec pod
    Nov 15 07:21:34.594: INFO: Waiting up to 5m0s for pod "execpod-affinityzr7tr" in namespace "services-4051" to be "running"
    Nov 15 07:21:34.610: INFO: Pod "execpod-affinityzr7tr": Phase="Pending", Reason="", readiness=false. Elapsed: 16.091675ms
    Nov 15 07:21:36.629: INFO: Pod "execpod-affinityzr7tr": Phase="Running", Reason="", readiness=true. Elapsed: 2.034868309s
    Nov 15 07:21:36.629: INFO: Pod "execpod-affinityzr7tr" satisfied condition "running"
    Nov 15 07:21:37.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-4051 exec execpod-affinityzr7tr -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Nov 15 07:21:37.966: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Nov 15 07:21:37.966: INFO: stdout: ""
    Nov 15 07:21:37.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-4051 exec execpod-affinityzr7tr -- /bin/sh -x -c nc -v -z -w 2 172.21.69.195 80'
    Nov 15 07:21:38.291: INFO: stderr: "+ nc -v -z -w 2 172.21.69.195 80\nConnection to 172.21.69.195 80 port [tcp/http] succeeded!\n"
    Nov 15 07:21:38.291: INFO: stdout: ""
    Nov 15 07:21:38.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-4051 exec execpod-affinityzr7tr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.69.195:80/ ; done'
    Nov 15 07:21:38.724: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n"
    Nov 15 07:21:38.724: INFO: stdout: "\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-g4lnp\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-g4lnp\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-g4lnp\naffinity-clusterip-transition-g4lnp\naffinity-clusterip-transition-g4lnp\naffinity-clusterip-transition-l7j26\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-g4lnp\naffinity-clusterip-transition-vmz5m"
    Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-g4lnp
    Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-g4lnp
    Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-g4lnp
    Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-g4lnp
    Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-g4lnp
    Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-l7j26
    Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-g4lnp
    Nov 15 07:21:38.724: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:38.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-4051 exec execpod-affinityzr7tr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.69.195:80/ ; done'
    Nov 15 07:21:39.159: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.69.195:80/\n"
    Nov 15 07:21:39.159: INFO: stdout: "\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m\naffinity-clusterip-transition-vmz5m"
    Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:39.159: INFO: Received response from host: affinity-clusterip-transition-vmz5m
    Nov 15 07:21:39.159: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4051, will wait for the garbage collector to delete the pods 11/15/23 07:21:39.209
    Nov 15 07:21:39.300: INFO: Deleting ReplicationController affinity-clusterip-transition took: 22.974779ms
    Nov 15 07:21:39.401: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.638729ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:21:42.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4051" for this suite. 11/15/23 07:21:42.701
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:21:42.726
Nov 15 07:21:42.726: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename container-runtime 11/15/23 07:21:42.727
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:21:42.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:21:42.832
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 11/15/23 07:21:42.843
STEP: wait for the container to reach Succeeded 11/15/23 07:21:42.881
STEP: get the container status 11/15/23 07:21:46.991
STEP: the container should be terminated 11/15/23 07:21:47.007
STEP: the termination message should be set 11/15/23 07:21:47.007
Nov 15 07:21:47.007: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 11/15/23 07:21:47.007
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Nov 15 07:21:47.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8569" for this suite. 11/15/23 07:21:47.093
------------------------------
â€¢ [4.390 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:21:42.726
    Nov 15 07:21:42.726: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename container-runtime 11/15/23 07:21:42.727
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:21:42.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:21:42.832
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 11/15/23 07:21:42.843
    STEP: wait for the container to reach Succeeded 11/15/23 07:21:42.881
    STEP: get the container status 11/15/23 07:21:46.991
    STEP: the container should be terminated 11/15/23 07:21:47.007
    STEP: the termination message should be set 11/15/23 07:21:47.007
    Nov 15 07:21:47.007: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 11/15/23 07:21:47.007
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:21:47.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8569" for this suite. 11/15/23 07:21:47.093
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:21:47.116
Nov 15 07:21:47.116: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename svcaccounts 11/15/23 07:21:47.119
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:21:47.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:21:47.209
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 11/15/23 07:21:47.22
STEP: watching for the ServiceAccount to be added 11/15/23 07:21:47.252
STEP: patching the ServiceAccount 11/15/23 07:21:47.257
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 11/15/23 07:21:47.281
STEP: deleting the ServiceAccount 11/15/23 07:21:47.314
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 15 07:21:47.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6465" for this suite. 11/15/23 07:21:47.452
------------------------------
â€¢ [0.357 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:21:47.116
    Nov 15 07:21:47.116: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename svcaccounts 11/15/23 07:21:47.119
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:21:47.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:21:47.209
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 11/15/23 07:21:47.22
    STEP: watching for the ServiceAccount to be added 11/15/23 07:21:47.252
    STEP: patching the ServiceAccount 11/15/23 07:21:47.257
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 11/15/23 07:21:47.281
    STEP: deleting the ServiceAccount 11/15/23 07:21:47.314
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:21:47.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6465" for this suite. 11/15/23 07:21:47.452
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:21:47.474
Nov 15 07:21:47.475: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubectl 11/15/23 07:21:47.475
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:21:47.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:21:47.546
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 11/15/23 07:21:47.56
Nov 15 07:21:47.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2500 create -f -'
Nov 15 07:21:48.541: INFO: stderr: ""
Nov 15 07:21:48.541: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 11/15/23 07:21:48.541
Nov 15 07:21:48.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2500 diff -f -'
Nov 15 07:21:49.615: INFO: rc: 1
Nov 15 07:21:49.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2500 delete -f -'
Nov 15 07:21:49.722: INFO: stderr: ""
Nov 15 07:21:49.722: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 07:21:49.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2500" for this suite. 11/15/23 07:21:49.751
------------------------------
â€¢ [2.301 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:21:47.474
    Nov 15 07:21:47.475: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubectl 11/15/23 07:21:47.475
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:21:47.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:21:47.546
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 11/15/23 07:21:47.56
    Nov 15 07:21:47.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2500 create -f -'
    Nov 15 07:21:48.541: INFO: stderr: ""
    Nov 15 07:21:48.541: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 11/15/23 07:21:48.541
    Nov 15 07:21:48.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2500 diff -f -'
    Nov 15 07:21:49.615: INFO: rc: 1
    Nov 15 07:21:49.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-2500 delete -f -'
    Nov 15 07:21:49.722: INFO: stderr: ""
    Nov 15 07:21:49.722: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:21:49.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2500" for this suite. 11/15/23 07:21:49.751
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:21:49.776
Nov 15 07:21:49.776: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename container-lifecycle-hook 11/15/23 07:21:49.777
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:21:49.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:21:49.855
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 11/15/23 07:21:49.891
Nov 15 07:21:49.928: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6022" to be "running and ready"
Nov 15 07:21:49.949: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 21.364395ms
Nov 15 07:21:49.949: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:21:51.970: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042147584s
Nov 15 07:21:51.970: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:21:53.994: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.066580203s
Nov 15 07:21:53.994: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Nov 15 07:21:53.994: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 11/15/23 07:21:54.027
Nov 15 07:21:54.062: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6022" to be "running and ready"
Nov 15 07:21:54.093: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 30.730449ms
Nov 15 07:21:54.093: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:21:56.113: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.051049819s
Nov 15 07:21:56.113: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Nov 15 07:21:56.113: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 11/15/23 07:21:56.137
Nov 15 07:21:56.166: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 07:21:56.183: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 15 07:21:58.186: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 07:21:58.202: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 15 07:22:00.184: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 07:22:00.200: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 11/15/23 07:22:00.2
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Nov 15 07:22:00.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6022" for this suite. 11/15/23 07:22:00.296
------------------------------
â€¢ [SLOW TEST] [10.546 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:21:49.776
    Nov 15 07:21:49.776: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename container-lifecycle-hook 11/15/23 07:21:49.777
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:21:49.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:21:49.855
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 11/15/23 07:21:49.891
    Nov 15 07:21:49.928: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6022" to be "running and ready"
    Nov 15 07:21:49.949: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 21.364395ms
    Nov 15 07:21:49.949: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:21:51.970: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042147584s
    Nov 15 07:21:51.970: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:21:53.994: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.066580203s
    Nov 15 07:21:53.994: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Nov 15 07:21:53.994: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 11/15/23 07:21:54.027
    Nov 15 07:21:54.062: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6022" to be "running and ready"
    Nov 15 07:21:54.093: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 30.730449ms
    Nov 15 07:21:54.093: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:21:56.113: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.051049819s
    Nov 15 07:21:56.113: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Nov 15 07:21:56.113: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 11/15/23 07:21:56.137
    Nov 15 07:21:56.166: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Nov 15 07:21:56.183: INFO: Pod pod-with-prestop-exec-hook still exists
    Nov 15 07:21:58.186: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Nov 15 07:21:58.202: INFO: Pod pod-with-prestop-exec-hook still exists
    Nov 15 07:22:00.184: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Nov 15 07:22:00.200: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 11/15/23 07:22:00.2
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:22:00.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6022" for this suite. 11/15/23 07:22:00.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:22:00.326
Nov 15 07:22:00.327: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename prestop 11/15/23 07:22:00.328
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:22:00.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:22:00.401
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-2353 11/15/23 07:22:00.411
STEP: Waiting for pods to come up. 11/15/23 07:22:00.443
Nov 15 07:22:00.443: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2353" to be "running"
Nov 15 07:22:00.460: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 16.310542ms
Nov 15 07:22:02.484: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040687961s
Nov 15 07:22:04.534: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.090508052s
Nov 15 07:22:04.534: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-2353 11/15/23 07:22:04.577
Nov 15 07:22:04.627: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2353" to be "running"
Nov 15 07:22:04.669: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 41.816002ms
Nov 15 07:22:06.692: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.064410694s
Nov 15 07:22:06.692: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 11/15/23 07:22:06.692
Nov 15 07:22:11.849: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 11/15/23 07:22:11.852
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Nov 15 07:22:11.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-2353" for this suite. 11/15/23 07:22:11.936
------------------------------
â€¢ [SLOW TEST] [11.634 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:22:00.326
    Nov 15 07:22:00.327: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename prestop 11/15/23 07:22:00.328
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:22:00.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:22:00.401
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-2353 11/15/23 07:22:00.411
    STEP: Waiting for pods to come up. 11/15/23 07:22:00.443
    Nov 15 07:22:00.443: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2353" to be "running"
    Nov 15 07:22:00.460: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 16.310542ms
    Nov 15 07:22:02.484: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040687961s
    Nov 15 07:22:04.534: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.090508052s
    Nov 15 07:22:04.534: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-2353 11/15/23 07:22:04.577
    Nov 15 07:22:04.627: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2353" to be "running"
    Nov 15 07:22:04.669: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 41.816002ms
    Nov 15 07:22:06.692: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.064410694s
    Nov 15 07:22:06.692: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 11/15/23 07:22:06.692
    Nov 15 07:22:11.849: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 11/15/23 07:22:11.852
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:22:11.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-2353" for this suite. 11/15/23 07:22:11.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:22:11.961
Nov 15 07:22:11.961: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename container-runtime 11/15/23 07:22:11.962
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:22:12.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:22:12.036
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 11/15/23 07:22:12.05
STEP: wait for the container to reach Failed 11/15/23 07:22:12.094
STEP: get the container status 11/15/23 07:22:16.186
STEP: the container should be terminated 11/15/23 07:22:16.203
STEP: the termination message should be set 11/15/23 07:22:16.203
Nov 15 07:22:16.203: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 11/15/23 07:22:16.204
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Nov 15 07:22:16.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4317" for this suite. 11/15/23 07:22:16.287
------------------------------
â€¢ [4.351 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:22:11.961
    Nov 15 07:22:11.961: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename container-runtime 11/15/23 07:22:11.962
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:22:12.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:22:12.036
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 11/15/23 07:22:12.05
    STEP: wait for the container to reach Failed 11/15/23 07:22:12.094
    STEP: get the container status 11/15/23 07:22:16.186
    STEP: the container should be terminated 11/15/23 07:22:16.203
    STEP: the termination message should be set 11/15/23 07:22:16.203
    Nov 15 07:22:16.203: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 11/15/23 07:22:16.204
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:22:16.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4317" for this suite. 11/15/23 07:22:16.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:22:16.318
Nov 15 07:22:16.318: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename lease-test 11/15/23 07:22:16.319
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:22:16.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:22:16.398
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Nov 15 07:22:16.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-8452" for this suite. 11/15/23 07:22:16.715
------------------------------
â€¢ [0.423 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:22:16.318
    Nov 15 07:22:16.318: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename lease-test 11/15/23 07:22:16.319
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:22:16.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:22:16.398
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:22:16.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-8452" for this suite. 11/15/23 07:22:16.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:22:16.744
Nov 15 07:22:16.744: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 07:22:16.744
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:22:16.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:22:16.841
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Nov 15 07:22:16.855: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 11/15/23 07:22:22.09
Nov 15 07:22:22.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 --namespace=crd-publish-openapi-3014 create -f -'
Nov 15 07:22:24.144: INFO: stderr: ""
Nov 15 07:22:24.144: INFO: stdout: "e2e-test-crd-publish-openapi-3083-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 15 07:22:24.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 --namespace=crd-publish-openapi-3014 delete e2e-test-crd-publish-openapi-3083-crds test-foo'
Nov 15 07:22:24.307: INFO: stderr: ""
Nov 15 07:22:24.307: INFO: stdout: "e2e-test-crd-publish-openapi-3083-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Nov 15 07:22:24.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 --namespace=crd-publish-openapi-3014 apply -f -'
Nov 15 07:22:26.244: INFO: stderr: ""
Nov 15 07:22:26.244: INFO: stdout: "e2e-test-crd-publish-openapi-3083-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 15 07:22:26.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 --namespace=crd-publish-openapi-3014 delete e2e-test-crd-publish-openapi-3083-crds test-foo'
Nov 15 07:22:26.404: INFO: stderr: ""
Nov 15 07:22:26.404: INFO: stdout: "e2e-test-crd-publish-openapi-3083-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 11/15/23 07:22:26.404
Nov 15 07:22:26.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 --namespace=crd-publish-openapi-3014 create -f -'
Nov 15 07:22:26.955: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 11/15/23 07:22:26.955
Nov 15 07:22:26.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 --namespace=crd-publish-openapi-3014 create -f -'
Nov 15 07:22:28.686: INFO: rc: 1
Nov 15 07:22:28.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 --namespace=crd-publish-openapi-3014 apply -f -'
Nov 15 07:22:29.773: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 11/15/23 07:22:29.773
Nov 15 07:22:29.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 --namespace=crd-publish-openapi-3014 create -f -'
Nov 15 07:22:30.378: INFO: rc: 1
Nov 15 07:22:30.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 --namespace=crd-publish-openapi-3014 apply -f -'
Nov 15 07:22:30.927: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 11/15/23 07:22:30.927
Nov 15 07:22:30.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 explain e2e-test-crd-publish-openapi-3083-crds'
Nov 15 07:22:31.517: INFO: stderr: ""
Nov 15 07:22:31.517: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3083-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 11/15/23 07:22:31.517
Nov 15 07:22:31.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 explain e2e-test-crd-publish-openapi-3083-crds.metadata'
Nov 15 07:22:32.265: INFO: stderr: ""
Nov 15 07:22:32.265: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3083-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Nov 15 07:22:32.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 explain e2e-test-crd-publish-openapi-3083-crds.spec'
Nov 15 07:22:32.765: INFO: stderr: ""
Nov 15 07:22:32.765: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3083-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Nov 15 07:22:32.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 explain e2e-test-crd-publish-openapi-3083-crds.spec.bars'
Nov 15 07:22:33.480: INFO: stderr: ""
Nov 15 07:22:33.480: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3083-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 11/15/23 07:22:33.48
Nov 15 07:22:33.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 explain e2e-test-crd-publish-openapi-3083-crds.spec.bars2'
Nov 15 07:22:34.028: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:22:38.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3014" for this suite. 11/15/23 07:22:38.12
------------------------------
â€¢ [SLOW TEST] [21.403 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:22:16.744
    Nov 15 07:22:16.744: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 07:22:16.744
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:22:16.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:22:16.841
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Nov 15 07:22:16.855: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 11/15/23 07:22:22.09
    Nov 15 07:22:22.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 --namespace=crd-publish-openapi-3014 create -f -'
    Nov 15 07:22:24.144: INFO: stderr: ""
    Nov 15 07:22:24.144: INFO: stdout: "e2e-test-crd-publish-openapi-3083-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Nov 15 07:22:24.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 --namespace=crd-publish-openapi-3014 delete e2e-test-crd-publish-openapi-3083-crds test-foo'
    Nov 15 07:22:24.307: INFO: stderr: ""
    Nov 15 07:22:24.307: INFO: stdout: "e2e-test-crd-publish-openapi-3083-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Nov 15 07:22:24.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 --namespace=crd-publish-openapi-3014 apply -f -'
    Nov 15 07:22:26.244: INFO: stderr: ""
    Nov 15 07:22:26.244: INFO: stdout: "e2e-test-crd-publish-openapi-3083-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Nov 15 07:22:26.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 --namespace=crd-publish-openapi-3014 delete e2e-test-crd-publish-openapi-3083-crds test-foo'
    Nov 15 07:22:26.404: INFO: stderr: ""
    Nov 15 07:22:26.404: INFO: stdout: "e2e-test-crd-publish-openapi-3083-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 11/15/23 07:22:26.404
    Nov 15 07:22:26.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 --namespace=crd-publish-openapi-3014 create -f -'
    Nov 15 07:22:26.955: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 11/15/23 07:22:26.955
    Nov 15 07:22:26.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 --namespace=crd-publish-openapi-3014 create -f -'
    Nov 15 07:22:28.686: INFO: rc: 1
    Nov 15 07:22:28.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 --namespace=crd-publish-openapi-3014 apply -f -'
    Nov 15 07:22:29.773: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 11/15/23 07:22:29.773
    Nov 15 07:22:29.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 --namespace=crd-publish-openapi-3014 create -f -'
    Nov 15 07:22:30.378: INFO: rc: 1
    Nov 15 07:22:30.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 --namespace=crd-publish-openapi-3014 apply -f -'
    Nov 15 07:22:30.927: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 11/15/23 07:22:30.927
    Nov 15 07:22:30.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 explain e2e-test-crd-publish-openapi-3083-crds'
    Nov 15 07:22:31.517: INFO: stderr: ""
    Nov 15 07:22:31.517: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3083-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 11/15/23 07:22:31.517
    Nov 15 07:22:31.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 explain e2e-test-crd-publish-openapi-3083-crds.metadata'
    Nov 15 07:22:32.265: INFO: stderr: ""
    Nov 15 07:22:32.265: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3083-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Nov 15 07:22:32.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 explain e2e-test-crd-publish-openapi-3083-crds.spec'
    Nov 15 07:22:32.765: INFO: stderr: ""
    Nov 15 07:22:32.765: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3083-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Nov 15 07:22:32.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 explain e2e-test-crd-publish-openapi-3083-crds.spec.bars'
    Nov 15 07:22:33.480: INFO: stderr: ""
    Nov 15 07:22:33.480: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3083-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 11/15/23 07:22:33.48
    Nov 15 07:22:33.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=crd-publish-openapi-3014 explain e2e-test-crd-publish-openapi-3083-crds.spec.bars2'
    Nov 15 07:22:34.028: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:22:38.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3014" for this suite. 11/15/23 07:22:38.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:22:38.149
Nov 15 07:22:38.149: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename secrets 11/15/23 07:22:38.15
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:22:38.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:22:38.212
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-b2450870-9866-49d2-bd5e-b42bc4196517 11/15/23 07:22:38.221
STEP: Creating a pod to test consume secrets 11/15/23 07:22:38.234
Nov 15 07:22:38.263: INFO: Waiting up to 5m0s for pod "pod-secrets-5b0b2b4c-be94-4a7b-a76e-27fe6d9fa663" in namespace "secrets-9245" to be "Succeeded or Failed"
Nov 15 07:22:38.272: INFO: Pod "pod-secrets-5b0b2b4c-be94-4a7b-a76e-27fe6d9fa663": Phase="Pending", Reason="", readiness=false. Elapsed: 8.798149ms
Nov 15 07:22:40.283: INFO: Pod "pod-secrets-5b0b2b4c-be94-4a7b-a76e-27fe6d9fa663": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02001369s
Nov 15 07:22:42.283: INFO: Pod "pod-secrets-5b0b2b4c-be94-4a7b-a76e-27fe6d9fa663": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020061124s
Nov 15 07:22:44.288: INFO: Pod "pod-secrets-5b0b2b4c-be94-4a7b-a76e-27fe6d9fa663": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024518754s
STEP: Saw pod success 11/15/23 07:22:44.288
Nov 15 07:22:44.288: INFO: Pod "pod-secrets-5b0b2b4c-be94-4a7b-a76e-27fe6d9fa663" satisfied condition "Succeeded or Failed"
Nov 15 07:22:44.299: INFO: Trying to get logs from node 10.72.152.81 pod pod-secrets-5b0b2b4c-be94-4a7b-a76e-27fe6d9fa663 container secret-volume-test: <nil>
STEP: delete the pod 11/15/23 07:22:44.357
Nov 15 07:22:44.382: INFO: Waiting for pod pod-secrets-5b0b2b4c-be94-4a7b-a76e-27fe6d9fa663 to disappear
Nov 15 07:22:44.391: INFO: Pod pod-secrets-5b0b2b4c-be94-4a7b-a76e-27fe6d9fa663 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 07:22:44.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9245" for this suite. 11/15/23 07:22:44.408
------------------------------
â€¢ [SLOW TEST] [6.281 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:22:38.149
    Nov 15 07:22:38.149: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename secrets 11/15/23 07:22:38.15
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:22:38.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:22:38.212
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-b2450870-9866-49d2-bd5e-b42bc4196517 11/15/23 07:22:38.221
    STEP: Creating a pod to test consume secrets 11/15/23 07:22:38.234
    Nov 15 07:22:38.263: INFO: Waiting up to 5m0s for pod "pod-secrets-5b0b2b4c-be94-4a7b-a76e-27fe6d9fa663" in namespace "secrets-9245" to be "Succeeded or Failed"
    Nov 15 07:22:38.272: INFO: Pod "pod-secrets-5b0b2b4c-be94-4a7b-a76e-27fe6d9fa663": Phase="Pending", Reason="", readiness=false. Elapsed: 8.798149ms
    Nov 15 07:22:40.283: INFO: Pod "pod-secrets-5b0b2b4c-be94-4a7b-a76e-27fe6d9fa663": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02001369s
    Nov 15 07:22:42.283: INFO: Pod "pod-secrets-5b0b2b4c-be94-4a7b-a76e-27fe6d9fa663": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020061124s
    Nov 15 07:22:44.288: INFO: Pod "pod-secrets-5b0b2b4c-be94-4a7b-a76e-27fe6d9fa663": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024518754s
    STEP: Saw pod success 11/15/23 07:22:44.288
    Nov 15 07:22:44.288: INFO: Pod "pod-secrets-5b0b2b4c-be94-4a7b-a76e-27fe6d9fa663" satisfied condition "Succeeded or Failed"
    Nov 15 07:22:44.299: INFO: Trying to get logs from node 10.72.152.81 pod pod-secrets-5b0b2b4c-be94-4a7b-a76e-27fe6d9fa663 container secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 07:22:44.357
    Nov 15 07:22:44.382: INFO: Waiting for pod pod-secrets-5b0b2b4c-be94-4a7b-a76e-27fe6d9fa663 to disappear
    Nov 15 07:22:44.391: INFO: Pod pod-secrets-5b0b2b4c-be94-4a7b-a76e-27fe6d9fa663 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:22:44.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9245" for this suite. 11/15/23 07:22:44.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:22:44.431
Nov 15 07:22:44.431: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename container-probe 11/15/23 07:22:44.432
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:22:44.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:22:44.486
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-938878f6-ca59-4a92-8238-8ca07141ffd0 in namespace container-probe-7347 11/15/23 07:22:44.492
Nov 15 07:22:44.515: INFO: Waiting up to 5m0s for pod "busybox-938878f6-ca59-4a92-8238-8ca07141ffd0" in namespace "container-probe-7347" to be "not pending"
Nov 15 07:22:44.527: INFO: Pod "busybox-938878f6-ca59-4a92-8238-8ca07141ffd0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.804315ms
Nov 15 07:22:46.544: INFO: Pod "busybox-938878f6-ca59-4a92-8238-8ca07141ffd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029141093s
Nov 15 07:22:48.539: INFO: Pod "busybox-938878f6-ca59-4a92-8238-8ca07141ffd0": Phase="Running", Reason="", readiness=true. Elapsed: 4.023802191s
Nov 15 07:22:48.539: INFO: Pod "busybox-938878f6-ca59-4a92-8238-8ca07141ffd0" satisfied condition "not pending"
Nov 15 07:22:48.539: INFO: Started pod busybox-938878f6-ca59-4a92-8238-8ca07141ffd0 in namespace container-probe-7347
STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 07:22:48.539
Nov 15 07:22:48.559: INFO: Initial restart count of pod busybox-938878f6-ca59-4a92-8238-8ca07141ffd0 is 0
STEP: deleting the pod 11/15/23 07:26:50.232
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 15 07:26:50.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7347" for this suite. 11/15/23 07:26:50.308
------------------------------
â€¢ [SLOW TEST] [245.896 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:22:44.431
    Nov 15 07:22:44.431: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename container-probe 11/15/23 07:22:44.432
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:22:44.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:22:44.486
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-938878f6-ca59-4a92-8238-8ca07141ffd0 in namespace container-probe-7347 11/15/23 07:22:44.492
    Nov 15 07:22:44.515: INFO: Waiting up to 5m0s for pod "busybox-938878f6-ca59-4a92-8238-8ca07141ffd0" in namespace "container-probe-7347" to be "not pending"
    Nov 15 07:22:44.527: INFO: Pod "busybox-938878f6-ca59-4a92-8238-8ca07141ffd0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.804315ms
    Nov 15 07:22:46.544: INFO: Pod "busybox-938878f6-ca59-4a92-8238-8ca07141ffd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029141093s
    Nov 15 07:22:48.539: INFO: Pod "busybox-938878f6-ca59-4a92-8238-8ca07141ffd0": Phase="Running", Reason="", readiness=true. Elapsed: 4.023802191s
    Nov 15 07:22:48.539: INFO: Pod "busybox-938878f6-ca59-4a92-8238-8ca07141ffd0" satisfied condition "not pending"
    Nov 15 07:22:48.539: INFO: Started pod busybox-938878f6-ca59-4a92-8238-8ca07141ffd0 in namespace container-probe-7347
    STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 07:22:48.539
    Nov 15 07:22:48.559: INFO: Initial restart count of pod busybox-938878f6-ca59-4a92-8238-8ca07141ffd0 is 0
    STEP: deleting the pod 11/15/23 07:26:50.232
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:26:50.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7347" for this suite. 11/15/23 07:26:50.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:26:50.328
Nov 15 07:26:50.328: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename downward-api 11/15/23 07:26:50.329
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:26:50.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:26:50.43
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 11/15/23 07:26:50.45
W1115 07:26:50.491612      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Nov 15 07:26:50.491: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80b4587e-bad6-4f28-a3e5-2b30e149db81" in namespace "downward-api-8130" to be "Succeeded or Failed"
Nov 15 07:26:50.504: INFO: Pod "downwardapi-volume-80b4587e-bad6-4f28-a3e5-2b30e149db81": Phase="Pending", Reason="", readiness=false. Elapsed: 12.776448ms
Nov 15 07:26:52.541: INFO: Pod "downwardapi-volume-80b4587e-bad6-4f28-a3e5-2b30e149db81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050026087s
Nov 15 07:26:54.517: INFO: Pod "downwardapi-volume-80b4587e-bad6-4f28-a3e5-2b30e149db81": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026113089s
Nov 15 07:26:56.520: INFO: Pod "downwardapi-volume-80b4587e-bad6-4f28-a3e5-2b30e149db81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02871745s
STEP: Saw pod success 11/15/23 07:26:56.52
Nov 15 07:26:56.520: INFO: Pod "downwardapi-volume-80b4587e-bad6-4f28-a3e5-2b30e149db81" satisfied condition "Succeeded or Failed"
Nov 15 07:26:56.532: INFO: Trying to get logs from node 10.72.152.81 pod downwardapi-volume-80b4587e-bad6-4f28-a3e5-2b30e149db81 container client-container: <nil>
STEP: delete the pod 11/15/23 07:26:56.604
Nov 15 07:26:56.630: INFO: Waiting for pod downwardapi-volume-80b4587e-bad6-4f28-a3e5-2b30e149db81 to disappear
Nov 15 07:26:56.640: INFO: Pod downwardapi-volume-80b4587e-bad6-4f28-a3e5-2b30e149db81 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 07:26:56.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8130" for this suite. 11/15/23 07:26:56.655
------------------------------
â€¢ [SLOW TEST] [6.346 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:26:50.328
    Nov 15 07:26:50.328: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename downward-api 11/15/23 07:26:50.329
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:26:50.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:26:50.43
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 11/15/23 07:26:50.45
    W1115 07:26:50.491612      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Nov 15 07:26:50.491: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80b4587e-bad6-4f28-a3e5-2b30e149db81" in namespace "downward-api-8130" to be "Succeeded or Failed"
    Nov 15 07:26:50.504: INFO: Pod "downwardapi-volume-80b4587e-bad6-4f28-a3e5-2b30e149db81": Phase="Pending", Reason="", readiness=false. Elapsed: 12.776448ms
    Nov 15 07:26:52.541: INFO: Pod "downwardapi-volume-80b4587e-bad6-4f28-a3e5-2b30e149db81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050026087s
    Nov 15 07:26:54.517: INFO: Pod "downwardapi-volume-80b4587e-bad6-4f28-a3e5-2b30e149db81": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026113089s
    Nov 15 07:26:56.520: INFO: Pod "downwardapi-volume-80b4587e-bad6-4f28-a3e5-2b30e149db81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02871745s
    STEP: Saw pod success 11/15/23 07:26:56.52
    Nov 15 07:26:56.520: INFO: Pod "downwardapi-volume-80b4587e-bad6-4f28-a3e5-2b30e149db81" satisfied condition "Succeeded or Failed"
    Nov 15 07:26:56.532: INFO: Trying to get logs from node 10.72.152.81 pod downwardapi-volume-80b4587e-bad6-4f28-a3e5-2b30e149db81 container client-container: <nil>
    STEP: delete the pod 11/15/23 07:26:56.604
    Nov 15 07:26:56.630: INFO: Waiting for pod downwardapi-volume-80b4587e-bad6-4f28-a3e5-2b30e149db81 to disappear
    Nov 15 07:26:56.640: INFO: Pod downwardapi-volume-80b4587e-bad6-4f28-a3e5-2b30e149db81 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:26:56.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8130" for this suite. 11/15/23 07:26:56.655
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:26:56.674
Nov 15 07:26:56.674: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename statefulset 11/15/23 07:26:56.676
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:26:56.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:26:56.735
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7933 11/15/23 07:26:56.741
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 11/15/23 07:26:56.751
Nov 15 07:26:56.816: INFO: Found 0 stateful pods, waiting for 3
Nov 15 07:27:06.830: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 07:27:06.830: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 07:27:06.830: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 07:27:06.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7933 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 07:27:07.174: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 07:27:07.174: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 07:27:07.174: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 11/15/23 07:27:17.225
Nov 15 07:27:17.265: INFO: Updating stateful set ss2
STEP: Creating a new revision 11/15/23 07:27:17.265
STEP: Updating Pods in reverse ordinal order 11/15/23 07:27:27.318
Nov 15 07:27:27.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7933 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 15 07:27:27.684: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 15 07:27:27.684: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 15 07:27:27.684: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 15 07:27:37.761: INFO: Waiting for StatefulSet statefulset-7933/ss2 to complete update
Nov 15 07:27:37.761: INFO: Waiting for Pod statefulset-7933/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Rolling back to a previous revision 11/15/23 07:27:47.787
Nov 15 07:27:47.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7933 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 07:27:48.101: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 07:27:48.101: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 07:27:48.101: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 15 07:27:58.219: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 11/15/23 07:28:08.277
Nov 15 07:28:08.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7933 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 15 07:28:08.565: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 15 07:28:08.565: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 15 07:28:08.565: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 15 07:28:18.693: INFO: Deleting all statefulset in ns statefulset-7933
Nov 15 07:28:18.712: INFO: Scaling statefulset ss2 to 0
Nov 15 07:28:28.798: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 07:28:28.812: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 15 07:28:28.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7933" for this suite. 11/15/23 07:28:28.91
------------------------------
â€¢ [SLOW TEST] [92.259 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:26:56.674
    Nov 15 07:26:56.674: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename statefulset 11/15/23 07:26:56.676
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:26:56.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:26:56.735
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7933 11/15/23 07:26:56.741
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 11/15/23 07:26:56.751
    Nov 15 07:26:56.816: INFO: Found 0 stateful pods, waiting for 3
    Nov 15 07:27:06.830: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 07:27:06.830: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 07:27:06.830: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 07:27:06.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7933 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 07:27:07.174: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 07:27:07.174: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 07:27:07.174: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 11/15/23 07:27:17.225
    Nov 15 07:27:17.265: INFO: Updating stateful set ss2
    STEP: Creating a new revision 11/15/23 07:27:17.265
    STEP: Updating Pods in reverse ordinal order 11/15/23 07:27:27.318
    Nov 15 07:27:27.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7933 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 15 07:27:27.684: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 15 07:27:27.684: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 15 07:27:27.684: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 15 07:27:37.761: INFO: Waiting for StatefulSet statefulset-7933/ss2 to complete update
    Nov 15 07:27:37.761: INFO: Waiting for Pod statefulset-7933/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Rolling back to a previous revision 11/15/23 07:27:47.787
    Nov 15 07:27:47.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7933 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 07:27:48.101: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 07:27:48.101: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 07:27:48.101: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 15 07:27:58.219: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 11/15/23 07:28:08.277
    Nov 15 07:28:08.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=statefulset-7933 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 15 07:28:08.565: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 15 07:28:08.565: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 15 07:28:08.565: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 15 07:28:18.693: INFO: Deleting all statefulset in ns statefulset-7933
    Nov 15 07:28:18.712: INFO: Scaling statefulset ss2 to 0
    Nov 15 07:28:28.798: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 07:28:28.812: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:28:28.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7933" for this suite. 11/15/23 07:28:28.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:28:28.935
Nov 15 07:28:28.935: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename webhook 11/15/23 07:28:28.937
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:28:29.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:28:29.016
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 07:28:29.076
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 07:28:29.666
STEP: Deploying the webhook pod 11/15/23 07:28:29.707
STEP: Wait for the deployment to be ready 11/15/23 07:28:29.732
Nov 15 07:28:29.760: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/15/23 07:28:31.799
STEP: Verifying the service has paired with the endpoint 11/15/23 07:28:31.837
Nov 15 07:28:32.838: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Nov 15 07:28:32.849: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1079-crds.webhook.example.com via the AdmissionRegistration API 11/15/23 07:28:33.372
STEP: Creating a custom resource while v1 is storage version 11/15/23 07:28:33.462
STEP: Patching Custom Resource Definition to set v2 as storage 11/15/23 07:28:35.589
STEP: Patching the custom resource while v2 is storage version 11/15/23 07:28:35.604
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:28:36.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6824" for this suite. 11/15/23 07:28:36.443
STEP: Destroying namespace "webhook-6824-markers" for this suite. 11/15/23 07:28:36.46
------------------------------
â€¢ [SLOW TEST] [7.547 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:28:28.935
    Nov 15 07:28:28.935: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename webhook 11/15/23 07:28:28.937
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:28:29.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:28:29.016
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 07:28:29.076
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 07:28:29.666
    STEP: Deploying the webhook pod 11/15/23 07:28:29.707
    STEP: Wait for the deployment to be ready 11/15/23 07:28:29.732
    Nov 15 07:28:29.760: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/15/23 07:28:31.799
    STEP: Verifying the service has paired with the endpoint 11/15/23 07:28:31.837
    Nov 15 07:28:32.838: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Nov 15 07:28:32.849: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1079-crds.webhook.example.com via the AdmissionRegistration API 11/15/23 07:28:33.372
    STEP: Creating a custom resource while v1 is storage version 11/15/23 07:28:33.462
    STEP: Patching Custom Resource Definition to set v2 as storage 11/15/23 07:28:35.589
    STEP: Patching the custom resource while v2 is storage version 11/15/23 07:28:35.604
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:28:36.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6824" for this suite. 11/15/23 07:28:36.443
    STEP: Destroying namespace "webhook-6824-markers" for this suite. 11/15/23 07:28:36.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:28:36.483
Nov 15 07:28:36.483: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename pods 11/15/23 07:28:36.484
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:28:36.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:28:36.562
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 11/15/23 07:28:36.574
STEP: setting up watch 11/15/23 07:28:36.574
STEP: submitting the pod to kubernetes 11/15/23 07:28:36.689
STEP: verifying the pod is in kubernetes 11/15/23 07:28:36.714
STEP: verifying pod creation was observed 11/15/23 07:28:36.735
Nov 15 07:28:36.737: INFO: Waiting up to 5m0s for pod "pod-submit-remove-955b5620-ab47-456a-b7d0-1a7ebb39d4fa" in namespace "pods-3295" to be "running"
Nov 15 07:28:36.754: INFO: Pod "pod-submit-remove-955b5620-ab47-456a-b7d0-1a7ebb39d4fa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.617029ms
Nov 15 07:28:38.767: INFO: Pod "pod-submit-remove-955b5620-ab47-456a-b7d0-1a7ebb39d4fa": Phase="Running", Reason="", readiness=true. Elapsed: 2.03034733s
Nov 15 07:28:38.767: INFO: Pod "pod-submit-remove-955b5620-ab47-456a-b7d0-1a7ebb39d4fa" satisfied condition "running"
STEP: deleting the pod gracefully 11/15/23 07:28:38.778
STEP: verifying pod deletion was observed 11/15/23 07:28:38.798
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 07:28:41.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3295" for this suite. 11/15/23 07:28:41.224
------------------------------
â€¢ [4.758 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:28:36.483
    Nov 15 07:28:36.483: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename pods 11/15/23 07:28:36.484
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:28:36.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:28:36.562
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 11/15/23 07:28:36.574
    STEP: setting up watch 11/15/23 07:28:36.574
    STEP: submitting the pod to kubernetes 11/15/23 07:28:36.689
    STEP: verifying the pod is in kubernetes 11/15/23 07:28:36.714
    STEP: verifying pod creation was observed 11/15/23 07:28:36.735
    Nov 15 07:28:36.737: INFO: Waiting up to 5m0s for pod "pod-submit-remove-955b5620-ab47-456a-b7d0-1a7ebb39d4fa" in namespace "pods-3295" to be "running"
    Nov 15 07:28:36.754: INFO: Pod "pod-submit-remove-955b5620-ab47-456a-b7d0-1a7ebb39d4fa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.617029ms
    Nov 15 07:28:38.767: INFO: Pod "pod-submit-remove-955b5620-ab47-456a-b7d0-1a7ebb39d4fa": Phase="Running", Reason="", readiness=true. Elapsed: 2.03034733s
    Nov 15 07:28:38.767: INFO: Pod "pod-submit-remove-955b5620-ab47-456a-b7d0-1a7ebb39d4fa" satisfied condition "running"
    STEP: deleting the pod gracefully 11/15/23 07:28:38.778
    STEP: verifying pod deletion was observed 11/15/23 07:28:38.798
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:28:41.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3295" for this suite. 11/15/23 07:28:41.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:28:41.243
Nov 15 07:28:41.243: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename services 11/15/23 07:28:41.245
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:28:41.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:28:41.303
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5298 11/15/23 07:28:41.31
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 11/15/23 07:28:41.358
STEP: creating service externalsvc in namespace services-5298 11/15/23 07:28:41.359
STEP: creating replication controller externalsvc in namespace services-5298 11/15/23 07:28:41.392
I1115 07:28:41.416070      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5298, replica count: 2
I1115 07:28:44.468441      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 11/15/23 07:28:44.478
Nov 15 07:28:44.519: INFO: Creating new exec pod
Nov 15 07:28:44.541: INFO: Waiting up to 5m0s for pod "execpodjdpbq" in namespace "services-5298" to be "running"
Nov 15 07:28:44.550: INFO: Pod "execpodjdpbq": Phase="Pending", Reason="", readiness=false. Elapsed: 8.705992ms
Nov 15 07:28:46.562: INFO: Pod "execpodjdpbq": Phase="Running", Reason="", readiness=true. Elapsed: 2.020654184s
Nov 15 07:28:46.562: INFO: Pod "execpodjdpbq" satisfied condition "running"
Nov 15 07:28:46.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-5298 exec execpodjdpbq -- /bin/sh -x -c nslookup clusterip-service.services-5298.svc.cluster.local'
Nov 15 07:28:46.864: INFO: stderr: "+ nslookup clusterip-service.services-5298.svc.cluster.local\n"
Nov 15 07:28:46.864: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-5298.svc.cluster.local\tcanonical name = externalsvc.services-5298.svc.cluster.local.\nName:\texternalsvc.services-5298.svc.cluster.local\nAddress: 172.21.158.219\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5298, will wait for the garbage collector to delete the pods 11/15/23 07:28:46.864
Nov 15 07:28:46.958: INFO: Deleting ReplicationController externalsvc took: 25.289265ms
Nov 15 07:28:47.058: INFO: Terminating ReplicationController externalsvc pods took: 100.534409ms
Nov 15 07:28:50.019: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 07:28:50.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5298" for this suite. 11/15/23 07:28:50.094
------------------------------
â€¢ [SLOW TEST] [8.868 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:28:41.243
    Nov 15 07:28:41.243: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename services 11/15/23 07:28:41.245
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:28:41.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:28:41.303
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5298 11/15/23 07:28:41.31
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 11/15/23 07:28:41.358
    STEP: creating service externalsvc in namespace services-5298 11/15/23 07:28:41.359
    STEP: creating replication controller externalsvc in namespace services-5298 11/15/23 07:28:41.392
    I1115 07:28:41.416070      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5298, replica count: 2
    I1115 07:28:44.468441      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 11/15/23 07:28:44.478
    Nov 15 07:28:44.519: INFO: Creating new exec pod
    Nov 15 07:28:44.541: INFO: Waiting up to 5m0s for pod "execpodjdpbq" in namespace "services-5298" to be "running"
    Nov 15 07:28:44.550: INFO: Pod "execpodjdpbq": Phase="Pending", Reason="", readiness=false. Elapsed: 8.705992ms
    Nov 15 07:28:46.562: INFO: Pod "execpodjdpbq": Phase="Running", Reason="", readiness=true. Elapsed: 2.020654184s
    Nov 15 07:28:46.562: INFO: Pod "execpodjdpbq" satisfied condition "running"
    Nov 15 07:28:46.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=services-5298 exec execpodjdpbq -- /bin/sh -x -c nslookup clusterip-service.services-5298.svc.cluster.local'
    Nov 15 07:28:46.864: INFO: stderr: "+ nslookup clusterip-service.services-5298.svc.cluster.local\n"
    Nov 15 07:28:46.864: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-5298.svc.cluster.local\tcanonical name = externalsvc.services-5298.svc.cluster.local.\nName:\texternalsvc.services-5298.svc.cluster.local\nAddress: 172.21.158.219\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-5298, will wait for the garbage collector to delete the pods 11/15/23 07:28:46.864
    Nov 15 07:28:46.958: INFO: Deleting ReplicationController externalsvc took: 25.289265ms
    Nov 15 07:28:47.058: INFO: Terminating ReplicationController externalsvc pods took: 100.534409ms
    Nov 15 07:28:50.019: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:28:50.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5298" for this suite. 11/15/23 07:28:50.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:28:50.112
Nov 15 07:28:50.112: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename replication-controller 11/15/23 07:28:50.113
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:28:50.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:28:50.18
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 11/15/23 07:28:50.188
Nov 15 07:28:50.210: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-250" to be "running and ready"
Nov 15 07:28:50.222: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 12.000894ms
Nov 15 07:28:50.222: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:28:52.233: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.023117094s
Nov 15 07:28:52.233: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Nov 15 07:28:52.233: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 11/15/23 07:28:52.242
STEP: Then the orphan pod is adopted 11/15/23 07:28:52.258
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Nov 15 07:28:53.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-250" for this suite. 11/15/23 07:28:53.294
------------------------------
â€¢ [3.200 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:28:50.112
    Nov 15 07:28:50.112: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename replication-controller 11/15/23 07:28:50.113
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:28:50.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:28:50.18
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 11/15/23 07:28:50.188
    Nov 15 07:28:50.210: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-250" to be "running and ready"
    Nov 15 07:28:50.222: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 12.000894ms
    Nov 15 07:28:50.222: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:28:52.233: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.023117094s
    Nov 15 07:28:52.233: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Nov 15 07:28:52.233: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 11/15/23 07:28:52.242
    STEP: Then the orphan pod is adopted 11/15/23 07:28:52.258
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:28:53.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-250" for this suite. 11/15/23 07:28:53.294
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:28:53.313
Nov 15 07:28:53.313: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubectl 11/15/23 07:28:53.314
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:28:53.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:28:53.381
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 11/15/23 07:28:53.401
Nov 15 07:28:53.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-458 api-versions'
Nov 15 07:28:53.556: INFO: stderr: ""
Nov 15 07:28:53.556: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 07:28:53.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-458" for this suite. 11/15/23 07:28:53.57
------------------------------
â€¢ [0.278 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:28:53.313
    Nov 15 07:28:53.313: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubectl 11/15/23 07:28:53.314
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:28:53.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:28:53.381
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 11/15/23 07:28:53.401
    Nov 15 07:28:53.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-458 api-versions'
    Nov 15 07:28:53.556: INFO: stderr: ""
    Nov 15 07:28:53.556: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:28:53.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-458" for this suite. 11/15/23 07:28:53.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:28:53.592
Nov 15 07:28:53.592: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename gc 11/15/23 07:28:53.593
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:28:53.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:28:53.66
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 11/15/23 07:28:53.686
STEP: delete the rc 11/15/23 07:28:58.765
STEP: wait for the rc to be deleted 11/15/23 07:28:58.836
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 11/15/23 07:29:03.891
STEP: Gathering metrics 11/15/23 07:29:33.92
W1115 07:29:33.936677      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 15 07:29:33.936: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Nov 15 07:29:33.936: INFO: Deleting pod "simpletest.rc-244t5" in namespace "gc-6322"
Nov 15 07:29:33.977: INFO: Deleting pod "simpletest.rc-2ljlm" in namespace "gc-6322"
Nov 15 07:29:34.001: INFO: Deleting pod "simpletest.rc-46lsw" in namespace "gc-6322"
Nov 15 07:29:34.032: INFO: Deleting pod "simpletest.rc-482kr" in namespace "gc-6322"
Nov 15 07:29:34.060: INFO: Deleting pod "simpletest.rc-4mzgp" in namespace "gc-6322"
Nov 15 07:29:34.084: INFO: Deleting pod "simpletest.rc-4sqps" in namespace "gc-6322"
Nov 15 07:29:34.107: INFO: Deleting pod "simpletest.rc-526ll" in namespace "gc-6322"
Nov 15 07:29:34.136: INFO: Deleting pod "simpletest.rc-56t7g" in namespace "gc-6322"
Nov 15 07:29:34.169: INFO: Deleting pod "simpletest.rc-5bbhr" in namespace "gc-6322"
Nov 15 07:29:34.201: INFO: Deleting pod "simpletest.rc-5h4qt" in namespace "gc-6322"
Nov 15 07:29:34.226: INFO: Deleting pod "simpletest.rc-5hmkv" in namespace "gc-6322"
Nov 15 07:29:34.260: INFO: Deleting pod "simpletest.rc-64dmw" in namespace "gc-6322"
Nov 15 07:29:34.308: INFO: Deleting pod "simpletest.rc-6lckm" in namespace "gc-6322"
Nov 15 07:29:34.356: INFO: Deleting pod "simpletest.rc-6qlzj" in namespace "gc-6322"
Nov 15 07:29:34.409: INFO: Deleting pod "simpletest.rc-6rjqw" in namespace "gc-6322"
Nov 15 07:29:34.439: INFO: Deleting pod "simpletest.rc-724tm" in namespace "gc-6322"
Nov 15 07:29:34.489: INFO: Deleting pod "simpletest.rc-77cww" in namespace "gc-6322"
Nov 15 07:29:34.553: INFO: Deleting pod "simpletest.rc-7s54c" in namespace "gc-6322"
Nov 15 07:29:34.590: INFO: Deleting pod "simpletest.rc-8bskg" in namespace "gc-6322"
Nov 15 07:29:34.616: INFO: Deleting pod "simpletest.rc-8zws9" in namespace "gc-6322"
Nov 15 07:29:34.701: INFO: Deleting pod "simpletest.rc-95dcf" in namespace "gc-6322"
Nov 15 07:29:34.726: INFO: Deleting pod "simpletest.rc-95svf" in namespace "gc-6322"
Nov 15 07:29:34.773: INFO: Deleting pod "simpletest.rc-98dgs" in namespace "gc-6322"
Nov 15 07:29:34.802: INFO: Deleting pod "simpletest.rc-b2895" in namespace "gc-6322"
Nov 15 07:29:34.828: INFO: Deleting pod "simpletest.rc-bg2mr" in namespace "gc-6322"
Nov 15 07:29:34.859: INFO: Deleting pod "simpletest.rc-brdj6" in namespace "gc-6322"
Nov 15 07:29:34.885: INFO: Deleting pod "simpletest.rc-bzwx7" in namespace "gc-6322"
Nov 15 07:29:34.919: INFO: Deleting pod "simpletest.rc-cnx9z" in namespace "gc-6322"
Nov 15 07:29:34.990: INFO: Deleting pod "simpletest.rc-cqspq" in namespace "gc-6322"
Nov 15 07:29:35.087: INFO: Deleting pod "simpletest.rc-f9szq" in namespace "gc-6322"
Nov 15 07:29:35.122: INFO: Deleting pod "simpletest.rc-fk8j8" in namespace "gc-6322"
Nov 15 07:29:35.148: INFO: Deleting pod "simpletest.rc-fqwwc" in namespace "gc-6322"
Nov 15 07:29:35.174: INFO: Deleting pod "simpletest.rc-frf8f" in namespace "gc-6322"
Nov 15 07:29:35.201: INFO: Deleting pod "simpletest.rc-fslzv" in namespace "gc-6322"
Nov 15 07:29:35.241: INFO: Deleting pod "simpletest.rc-fv6g5" in namespace "gc-6322"
Nov 15 07:29:35.299: INFO: Deleting pod "simpletest.rc-fwt4x" in namespace "gc-6322"
Nov 15 07:29:35.404: INFO: Deleting pod "simpletest.rc-g9q7p" in namespace "gc-6322"
Nov 15 07:29:35.453: INFO: Deleting pod "simpletest.rc-gclw2" in namespace "gc-6322"
Nov 15 07:29:35.512: INFO: Deleting pod "simpletest.rc-gh7r8" in namespace "gc-6322"
Nov 15 07:29:35.541: INFO: Deleting pod "simpletest.rc-glxhr" in namespace "gc-6322"
Nov 15 07:29:35.580: INFO: Deleting pod "simpletest.rc-gn8m2" in namespace "gc-6322"
Nov 15 07:29:35.610: INFO: Deleting pod "simpletest.rc-gr545" in namespace "gc-6322"
Nov 15 07:29:35.639: INFO: Deleting pod "simpletest.rc-h2dgr" in namespace "gc-6322"
Nov 15 07:29:35.670: INFO: Deleting pod "simpletest.rc-h92sh" in namespace "gc-6322"
Nov 15 07:29:35.718: INFO: Deleting pod "simpletest.rc-hb69r" in namespace "gc-6322"
Nov 15 07:29:35.749: INFO: Deleting pod "simpletest.rc-hsn5q" in namespace "gc-6322"
Nov 15 07:29:35.808: INFO: Deleting pod "simpletest.rc-jcgxq" in namespace "gc-6322"
Nov 15 07:29:35.883: INFO: Deleting pod "simpletest.rc-jdrtw" in namespace "gc-6322"
Nov 15 07:29:35.925: INFO: Deleting pod "simpletest.rc-jhbh2" in namespace "gc-6322"
Nov 15 07:29:35.965: INFO: Deleting pod "simpletest.rc-jkc8m" in namespace "gc-6322"
Nov 15 07:29:35.997: INFO: Deleting pod "simpletest.rc-js9sp" in namespace "gc-6322"
Nov 15 07:29:36.187: INFO: Deleting pod "simpletest.rc-jwsr9" in namespace "gc-6322"
Nov 15 07:29:36.253: INFO: Deleting pod "simpletest.rc-kgwdk" in namespace "gc-6322"
Nov 15 07:29:36.291: INFO: Deleting pod "simpletest.rc-kl52l" in namespace "gc-6322"
Nov 15 07:29:36.315: INFO: Deleting pod "simpletest.rc-l67ls" in namespace "gc-6322"
Nov 15 07:29:36.349: INFO: Deleting pod "simpletest.rc-lhwj8" in namespace "gc-6322"
Nov 15 07:29:36.470: INFO: Deleting pod "simpletest.rc-lrfw7" in namespace "gc-6322"
Nov 15 07:29:36.509: INFO: Deleting pod "simpletest.rc-ls2j7" in namespace "gc-6322"
Nov 15 07:29:36.557: INFO: Deleting pod "simpletest.rc-lsb4q" in namespace "gc-6322"
Nov 15 07:29:36.597: INFO: Deleting pod "simpletest.rc-mc9tl" in namespace "gc-6322"
Nov 15 07:29:36.645: INFO: Deleting pod "simpletest.rc-mdlzj" in namespace "gc-6322"
Nov 15 07:29:36.679: INFO: Deleting pod "simpletest.rc-mjpwk" in namespace "gc-6322"
Nov 15 07:29:36.726: INFO: Deleting pod "simpletest.rc-mkx55" in namespace "gc-6322"
Nov 15 07:29:36.753: INFO: Deleting pod "simpletest.rc-mlpnw" in namespace "gc-6322"
Nov 15 07:29:36.782: INFO: Deleting pod "simpletest.rc-p4xsp" in namespace "gc-6322"
Nov 15 07:29:36.813: INFO: Deleting pod "simpletest.rc-p6rwt" in namespace "gc-6322"
Nov 15 07:29:36.862: INFO: Deleting pod "simpletest.rc-pcfv9" in namespace "gc-6322"
Nov 15 07:29:36.903: INFO: Deleting pod "simpletest.rc-phjz8" in namespace "gc-6322"
Nov 15 07:29:36.959: INFO: Deleting pod "simpletest.rc-q2zdk" in namespace "gc-6322"
Nov 15 07:29:37.101: INFO: Deleting pod "simpletest.rc-qfgpx" in namespace "gc-6322"
Nov 15 07:29:37.158: INFO: Deleting pod "simpletest.rc-qfv9b" in namespace "gc-6322"
Nov 15 07:29:37.267: INFO: Deleting pod "simpletest.rc-qnsd4" in namespace "gc-6322"
Nov 15 07:29:37.297: INFO: Deleting pod "simpletest.rc-qq7w2" in namespace "gc-6322"
Nov 15 07:29:37.367: INFO: Deleting pod "simpletest.rc-qt9rg" in namespace "gc-6322"
Nov 15 07:29:37.463: INFO: Deleting pod "simpletest.rc-rdk2g" in namespace "gc-6322"
Nov 15 07:29:37.537: INFO: Deleting pod "simpletest.rc-rfjcv" in namespace "gc-6322"
Nov 15 07:29:37.572: INFO: Deleting pod "simpletest.rc-rhsgj" in namespace "gc-6322"
Nov 15 07:29:37.611: INFO: Deleting pod "simpletest.rc-rrmbt" in namespace "gc-6322"
Nov 15 07:29:37.645: INFO: Deleting pod "simpletest.rc-rxps5" in namespace "gc-6322"
Nov 15 07:29:37.701: INFO: Deleting pod "simpletest.rc-s9dtx" in namespace "gc-6322"
Nov 15 07:29:37.740: INFO: Deleting pod "simpletest.rc-sg9t7" in namespace "gc-6322"
Nov 15 07:29:37.964: INFO: Deleting pod "simpletest.rc-skkcr" in namespace "gc-6322"
Nov 15 07:29:38.045: INFO: Deleting pod "simpletest.rc-spkh2" in namespace "gc-6322"
Nov 15 07:29:38.122: INFO: Deleting pod "simpletest.rc-sql2z" in namespace "gc-6322"
Nov 15 07:29:38.171: INFO: Deleting pod "simpletest.rc-thhvp" in namespace "gc-6322"
Nov 15 07:29:38.335: INFO: Deleting pod "simpletest.rc-tnbgg" in namespace "gc-6322"
Nov 15 07:29:38.389: INFO: Deleting pod "simpletest.rc-tw9zw" in namespace "gc-6322"
Nov 15 07:29:38.512: INFO: Deleting pod "simpletest.rc-v2bcb" in namespace "gc-6322"
Nov 15 07:29:38.770: INFO: Deleting pod "simpletest.rc-vp2z7" in namespace "gc-6322"
Nov 15 07:29:38.809: INFO: Deleting pod "simpletest.rc-vtkgq" in namespace "gc-6322"
Nov 15 07:29:38.833: INFO: Deleting pod "simpletest.rc-wfppj" in namespace "gc-6322"
Nov 15 07:29:38.861: INFO: Deleting pod "simpletest.rc-wsmgx" in namespace "gc-6322"
Nov 15 07:29:39.152: INFO: Deleting pod "simpletest.rc-wxh8k" in namespace "gc-6322"
Nov 15 07:29:39.193: INFO: Deleting pod "simpletest.rc-xc8jg" in namespace "gc-6322"
Nov 15 07:29:39.217: INFO: Deleting pod "simpletest.rc-xksgk" in namespace "gc-6322"
Nov 15 07:29:39.242: INFO: Deleting pod "simpletest.rc-xlzjg" in namespace "gc-6322"
Nov 15 07:29:39.274: INFO: Deleting pod "simpletest.rc-xqs5b" in namespace "gc-6322"
Nov 15 07:29:39.306: INFO: Deleting pod "simpletest.rc-xz57v" in namespace "gc-6322"
Nov 15 07:29:39.344: INFO: Deleting pod "simpletest.rc-zgdx9" in namespace "gc-6322"
Nov 15 07:29:39.397: INFO: Deleting pod "simpletest.rc-zpqjz" in namespace "gc-6322"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 15 07:29:39.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6322" for this suite. 11/15/23 07:29:39.442
------------------------------
â€¢ [SLOW TEST] [45.876 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:28:53.592
    Nov 15 07:28:53.592: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename gc 11/15/23 07:28:53.593
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:28:53.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:28:53.66
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 11/15/23 07:28:53.686
    STEP: delete the rc 11/15/23 07:28:58.765
    STEP: wait for the rc to be deleted 11/15/23 07:28:58.836
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 11/15/23 07:29:03.891
    STEP: Gathering metrics 11/15/23 07:29:33.92
    W1115 07:29:33.936677      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Nov 15 07:29:33.936: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Nov 15 07:29:33.936: INFO: Deleting pod "simpletest.rc-244t5" in namespace "gc-6322"
    Nov 15 07:29:33.977: INFO: Deleting pod "simpletest.rc-2ljlm" in namespace "gc-6322"
    Nov 15 07:29:34.001: INFO: Deleting pod "simpletest.rc-46lsw" in namespace "gc-6322"
    Nov 15 07:29:34.032: INFO: Deleting pod "simpletest.rc-482kr" in namespace "gc-6322"
    Nov 15 07:29:34.060: INFO: Deleting pod "simpletest.rc-4mzgp" in namespace "gc-6322"
    Nov 15 07:29:34.084: INFO: Deleting pod "simpletest.rc-4sqps" in namespace "gc-6322"
    Nov 15 07:29:34.107: INFO: Deleting pod "simpletest.rc-526ll" in namespace "gc-6322"
    Nov 15 07:29:34.136: INFO: Deleting pod "simpletest.rc-56t7g" in namespace "gc-6322"
    Nov 15 07:29:34.169: INFO: Deleting pod "simpletest.rc-5bbhr" in namespace "gc-6322"
    Nov 15 07:29:34.201: INFO: Deleting pod "simpletest.rc-5h4qt" in namespace "gc-6322"
    Nov 15 07:29:34.226: INFO: Deleting pod "simpletest.rc-5hmkv" in namespace "gc-6322"
    Nov 15 07:29:34.260: INFO: Deleting pod "simpletest.rc-64dmw" in namespace "gc-6322"
    Nov 15 07:29:34.308: INFO: Deleting pod "simpletest.rc-6lckm" in namespace "gc-6322"
    Nov 15 07:29:34.356: INFO: Deleting pod "simpletest.rc-6qlzj" in namespace "gc-6322"
    Nov 15 07:29:34.409: INFO: Deleting pod "simpletest.rc-6rjqw" in namespace "gc-6322"
    Nov 15 07:29:34.439: INFO: Deleting pod "simpletest.rc-724tm" in namespace "gc-6322"
    Nov 15 07:29:34.489: INFO: Deleting pod "simpletest.rc-77cww" in namespace "gc-6322"
    Nov 15 07:29:34.553: INFO: Deleting pod "simpletest.rc-7s54c" in namespace "gc-6322"
    Nov 15 07:29:34.590: INFO: Deleting pod "simpletest.rc-8bskg" in namespace "gc-6322"
    Nov 15 07:29:34.616: INFO: Deleting pod "simpletest.rc-8zws9" in namespace "gc-6322"
    Nov 15 07:29:34.701: INFO: Deleting pod "simpletest.rc-95dcf" in namespace "gc-6322"
    Nov 15 07:29:34.726: INFO: Deleting pod "simpletest.rc-95svf" in namespace "gc-6322"
    Nov 15 07:29:34.773: INFO: Deleting pod "simpletest.rc-98dgs" in namespace "gc-6322"
    Nov 15 07:29:34.802: INFO: Deleting pod "simpletest.rc-b2895" in namespace "gc-6322"
    Nov 15 07:29:34.828: INFO: Deleting pod "simpletest.rc-bg2mr" in namespace "gc-6322"
    Nov 15 07:29:34.859: INFO: Deleting pod "simpletest.rc-brdj6" in namespace "gc-6322"
    Nov 15 07:29:34.885: INFO: Deleting pod "simpletest.rc-bzwx7" in namespace "gc-6322"
    Nov 15 07:29:34.919: INFO: Deleting pod "simpletest.rc-cnx9z" in namespace "gc-6322"
    Nov 15 07:29:34.990: INFO: Deleting pod "simpletest.rc-cqspq" in namespace "gc-6322"
    Nov 15 07:29:35.087: INFO: Deleting pod "simpletest.rc-f9szq" in namespace "gc-6322"
    Nov 15 07:29:35.122: INFO: Deleting pod "simpletest.rc-fk8j8" in namespace "gc-6322"
    Nov 15 07:29:35.148: INFO: Deleting pod "simpletest.rc-fqwwc" in namespace "gc-6322"
    Nov 15 07:29:35.174: INFO: Deleting pod "simpletest.rc-frf8f" in namespace "gc-6322"
    Nov 15 07:29:35.201: INFO: Deleting pod "simpletest.rc-fslzv" in namespace "gc-6322"
    Nov 15 07:29:35.241: INFO: Deleting pod "simpletest.rc-fv6g5" in namespace "gc-6322"
    Nov 15 07:29:35.299: INFO: Deleting pod "simpletest.rc-fwt4x" in namespace "gc-6322"
    Nov 15 07:29:35.404: INFO: Deleting pod "simpletest.rc-g9q7p" in namespace "gc-6322"
    Nov 15 07:29:35.453: INFO: Deleting pod "simpletest.rc-gclw2" in namespace "gc-6322"
    Nov 15 07:29:35.512: INFO: Deleting pod "simpletest.rc-gh7r8" in namespace "gc-6322"
    Nov 15 07:29:35.541: INFO: Deleting pod "simpletest.rc-glxhr" in namespace "gc-6322"
    Nov 15 07:29:35.580: INFO: Deleting pod "simpletest.rc-gn8m2" in namespace "gc-6322"
    Nov 15 07:29:35.610: INFO: Deleting pod "simpletest.rc-gr545" in namespace "gc-6322"
    Nov 15 07:29:35.639: INFO: Deleting pod "simpletest.rc-h2dgr" in namespace "gc-6322"
    Nov 15 07:29:35.670: INFO: Deleting pod "simpletest.rc-h92sh" in namespace "gc-6322"
    Nov 15 07:29:35.718: INFO: Deleting pod "simpletest.rc-hb69r" in namespace "gc-6322"
    Nov 15 07:29:35.749: INFO: Deleting pod "simpletest.rc-hsn5q" in namespace "gc-6322"
    Nov 15 07:29:35.808: INFO: Deleting pod "simpletest.rc-jcgxq" in namespace "gc-6322"
    Nov 15 07:29:35.883: INFO: Deleting pod "simpletest.rc-jdrtw" in namespace "gc-6322"
    Nov 15 07:29:35.925: INFO: Deleting pod "simpletest.rc-jhbh2" in namespace "gc-6322"
    Nov 15 07:29:35.965: INFO: Deleting pod "simpletest.rc-jkc8m" in namespace "gc-6322"
    Nov 15 07:29:35.997: INFO: Deleting pod "simpletest.rc-js9sp" in namespace "gc-6322"
    Nov 15 07:29:36.187: INFO: Deleting pod "simpletest.rc-jwsr9" in namespace "gc-6322"
    Nov 15 07:29:36.253: INFO: Deleting pod "simpletest.rc-kgwdk" in namespace "gc-6322"
    Nov 15 07:29:36.291: INFO: Deleting pod "simpletest.rc-kl52l" in namespace "gc-6322"
    Nov 15 07:29:36.315: INFO: Deleting pod "simpletest.rc-l67ls" in namespace "gc-6322"
    Nov 15 07:29:36.349: INFO: Deleting pod "simpletest.rc-lhwj8" in namespace "gc-6322"
    Nov 15 07:29:36.470: INFO: Deleting pod "simpletest.rc-lrfw7" in namespace "gc-6322"
    Nov 15 07:29:36.509: INFO: Deleting pod "simpletest.rc-ls2j7" in namespace "gc-6322"
    Nov 15 07:29:36.557: INFO: Deleting pod "simpletest.rc-lsb4q" in namespace "gc-6322"
    Nov 15 07:29:36.597: INFO: Deleting pod "simpletest.rc-mc9tl" in namespace "gc-6322"
    Nov 15 07:29:36.645: INFO: Deleting pod "simpletest.rc-mdlzj" in namespace "gc-6322"
    Nov 15 07:29:36.679: INFO: Deleting pod "simpletest.rc-mjpwk" in namespace "gc-6322"
    Nov 15 07:29:36.726: INFO: Deleting pod "simpletest.rc-mkx55" in namespace "gc-6322"
    Nov 15 07:29:36.753: INFO: Deleting pod "simpletest.rc-mlpnw" in namespace "gc-6322"
    Nov 15 07:29:36.782: INFO: Deleting pod "simpletest.rc-p4xsp" in namespace "gc-6322"
    Nov 15 07:29:36.813: INFO: Deleting pod "simpletest.rc-p6rwt" in namespace "gc-6322"
    Nov 15 07:29:36.862: INFO: Deleting pod "simpletest.rc-pcfv9" in namespace "gc-6322"
    Nov 15 07:29:36.903: INFO: Deleting pod "simpletest.rc-phjz8" in namespace "gc-6322"
    Nov 15 07:29:36.959: INFO: Deleting pod "simpletest.rc-q2zdk" in namespace "gc-6322"
    Nov 15 07:29:37.101: INFO: Deleting pod "simpletest.rc-qfgpx" in namespace "gc-6322"
    Nov 15 07:29:37.158: INFO: Deleting pod "simpletest.rc-qfv9b" in namespace "gc-6322"
    Nov 15 07:29:37.267: INFO: Deleting pod "simpletest.rc-qnsd4" in namespace "gc-6322"
    Nov 15 07:29:37.297: INFO: Deleting pod "simpletest.rc-qq7w2" in namespace "gc-6322"
    Nov 15 07:29:37.367: INFO: Deleting pod "simpletest.rc-qt9rg" in namespace "gc-6322"
    Nov 15 07:29:37.463: INFO: Deleting pod "simpletest.rc-rdk2g" in namespace "gc-6322"
    Nov 15 07:29:37.537: INFO: Deleting pod "simpletest.rc-rfjcv" in namespace "gc-6322"
    Nov 15 07:29:37.572: INFO: Deleting pod "simpletest.rc-rhsgj" in namespace "gc-6322"
    Nov 15 07:29:37.611: INFO: Deleting pod "simpletest.rc-rrmbt" in namespace "gc-6322"
    Nov 15 07:29:37.645: INFO: Deleting pod "simpletest.rc-rxps5" in namespace "gc-6322"
    Nov 15 07:29:37.701: INFO: Deleting pod "simpletest.rc-s9dtx" in namespace "gc-6322"
    Nov 15 07:29:37.740: INFO: Deleting pod "simpletest.rc-sg9t7" in namespace "gc-6322"
    Nov 15 07:29:37.964: INFO: Deleting pod "simpletest.rc-skkcr" in namespace "gc-6322"
    Nov 15 07:29:38.045: INFO: Deleting pod "simpletest.rc-spkh2" in namespace "gc-6322"
    Nov 15 07:29:38.122: INFO: Deleting pod "simpletest.rc-sql2z" in namespace "gc-6322"
    Nov 15 07:29:38.171: INFO: Deleting pod "simpletest.rc-thhvp" in namespace "gc-6322"
    Nov 15 07:29:38.335: INFO: Deleting pod "simpletest.rc-tnbgg" in namespace "gc-6322"
    Nov 15 07:29:38.389: INFO: Deleting pod "simpletest.rc-tw9zw" in namespace "gc-6322"
    Nov 15 07:29:38.512: INFO: Deleting pod "simpletest.rc-v2bcb" in namespace "gc-6322"
    Nov 15 07:29:38.770: INFO: Deleting pod "simpletest.rc-vp2z7" in namespace "gc-6322"
    Nov 15 07:29:38.809: INFO: Deleting pod "simpletest.rc-vtkgq" in namespace "gc-6322"
    Nov 15 07:29:38.833: INFO: Deleting pod "simpletest.rc-wfppj" in namespace "gc-6322"
    Nov 15 07:29:38.861: INFO: Deleting pod "simpletest.rc-wsmgx" in namespace "gc-6322"
    Nov 15 07:29:39.152: INFO: Deleting pod "simpletest.rc-wxh8k" in namespace "gc-6322"
    Nov 15 07:29:39.193: INFO: Deleting pod "simpletest.rc-xc8jg" in namespace "gc-6322"
    Nov 15 07:29:39.217: INFO: Deleting pod "simpletest.rc-xksgk" in namespace "gc-6322"
    Nov 15 07:29:39.242: INFO: Deleting pod "simpletest.rc-xlzjg" in namespace "gc-6322"
    Nov 15 07:29:39.274: INFO: Deleting pod "simpletest.rc-xqs5b" in namespace "gc-6322"
    Nov 15 07:29:39.306: INFO: Deleting pod "simpletest.rc-xz57v" in namespace "gc-6322"
    Nov 15 07:29:39.344: INFO: Deleting pod "simpletest.rc-zgdx9" in namespace "gc-6322"
    Nov 15 07:29:39.397: INFO: Deleting pod "simpletest.rc-zpqjz" in namespace "gc-6322"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:29:39.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6322" for this suite. 11/15/23 07:29:39.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:29:39.473
Nov 15 07:29:39.474: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename containers 11/15/23 07:29:39.475
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:29:39.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:29:39.534
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 11/15/23 07:29:39.543
W1115 07:29:39.584481      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Nov 15 07:29:39.585: INFO: Waiting up to 5m0s for pod "client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df" in namespace "containers-6138" to be "Succeeded or Failed"
Nov 15 07:29:39.681: INFO: Pod "client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df": Phase="Pending", Reason="", readiness=false. Elapsed: 95.611886ms
Nov 15 07:29:41.704: INFO: Pod "client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.118575211s
Nov 15 07:29:43.691: INFO: Pod "client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.106155633s
Nov 15 07:29:45.694: INFO: Pod "client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.108879236s
Nov 15 07:29:47.693: INFO: Pod "client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.108485361s
STEP: Saw pod success 11/15/23 07:29:47.694
Nov 15 07:29:47.694: INFO: Pod "client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df" satisfied condition "Succeeded or Failed"
Nov 15 07:29:47.703: INFO: Trying to get logs from node 10.72.152.81 pod client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df container agnhost-container: <nil>
STEP: delete the pod 11/15/23 07:29:47.772
Nov 15 07:29:47.804: INFO: Waiting for pod client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df to disappear
Nov 15 07:29:47.819: INFO: Pod client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Nov 15 07:29:47.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6138" for this suite. 11/15/23 07:29:47.837
------------------------------
â€¢ [SLOW TEST] [8.387 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:29:39.473
    Nov 15 07:29:39.474: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename containers 11/15/23 07:29:39.475
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:29:39.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:29:39.534
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 11/15/23 07:29:39.543
    W1115 07:29:39.584481      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Nov 15 07:29:39.585: INFO: Waiting up to 5m0s for pod "client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df" in namespace "containers-6138" to be "Succeeded or Failed"
    Nov 15 07:29:39.681: INFO: Pod "client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df": Phase="Pending", Reason="", readiness=false. Elapsed: 95.611886ms
    Nov 15 07:29:41.704: INFO: Pod "client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.118575211s
    Nov 15 07:29:43.691: INFO: Pod "client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.106155633s
    Nov 15 07:29:45.694: INFO: Pod "client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.108879236s
    Nov 15 07:29:47.693: INFO: Pod "client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.108485361s
    STEP: Saw pod success 11/15/23 07:29:47.694
    Nov 15 07:29:47.694: INFO: Pod "client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df" satisfied condition "Succeeded or Failed"
    Nov 15 07:29:47.703: INFO: Trying to get logs from node 10.72.152.81 pod client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 07:29:47.772
    Nov 15 07:29:47.804: INFO: Waiting for pod client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df to disappear
    Nov 15 07:29:47.819: INFO: Pod client-containers-8216f455-96a6-4e11-9e6c-e69a73cf92df no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:29:47.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6138" for this suite. 11/15/23 07:29:47.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:29:47.862
Nov 15 07:29:47.862: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename podtemplate 11/15/23 07:29:47.863
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:29:47.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:29:47.938
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 11/15/23 07:29:47.945
W1115 07:29:47.971086      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Nov 15 07:29:47.971: INFO: created test-podtemplate-1
Nov 15 07:29:47.997: INFO: created test-podtemplate-2
Nov 15 07:29:48.011: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 11/15/23 07:29:48.011
STEP: delete collection of pod templates 11/15/23 07:29:48.035
Nov 15 07:29:48.036: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 11/15/23 07:29:48.114
Nov 15 07:29:48.114: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Nov 15 07:29:48.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-1160" for this suite. 11/15/23 07:29:48.148
------------------------------
â€¢ [0.314 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:29:47.862
    Nov 15 07:29:47.862: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename podtemplate 11/15/23 07:29:47.863
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:29:47.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:29:47.938
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 11/15/23 07:29:47.945
    W1115 07:29:47.971086      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Nov 15 07:29:47.971: INFO: created test-podtemplate-1
    Nov 15 07:29:47.997: INFO: created test-podtemplate-2
    Nov 15 07:29:48.011: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 11/15/23 07:29:48.011
    STEP: delete collection of pod templates 11/15/23 07:29:48.035
    Nov 15 07:29:48.036: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 11/15/23 07:29:48.114
    Nov 15 07:29:48.114: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:29:48.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-1160" for this suite. 11/15/23 07:29:48.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:29:48.178
Nov 15 07:29:48.178: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename emptydir 11/15/23 07:29:48.179
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:29:48.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:29:48.242
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 11/15/23 07:29:48.247
Nov 15 07:29:48.275: INFO: Waiting up to 5m0s for pod "pod-bd929cf5-e35a-49dc-90c8-90e32051c54a" in namespace "emptydir-5609" to be "Succeeded or Failed"
Nov 15 07:29:48.286: INFO: Pod "pod-bd929cf5-e35a-49dc-90c8-90e32051c54a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.036382ms
Nov 15 07:29:50.298: INFO: Pod "pod-bd929cf5-e35a-49dc-90c8-90e32051c54a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022730746s
Nov 15 07:29:52.296: INFO: Pod "pod-bd929cf5-e35a-49dc-90c8-90e32051c54a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021519434s
Nov 15 07:29:54.299: INFO: Pod "pod-bd929cf5-e35a-49dc-90c8-90e32051c54a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024088527s
STEP: Saw pod success 11/15/23 07:29:54.299
Nov 15 07:29:54.299: INFO: Pod "pod-bd929cf5-e35a-49dc-90c8-90e32051c54a" satisfied condition "Succeeded or Failed"
Nov 15 07:29:54.308: INFO: Trying to get logs from node 10.72.152.81 pod pod-bd929cf5-e35a-49dc-90c8-90e32051c54a container test-container: <nil>
STEP: delete the pod 11/15/23 07:29:54.344
Nov 15 07:29:54.377: INFO: Waiting for pod pod-bd929cf5-e35a-49dc-90c8-90e32051c54a to disappear
Nov 15 07:29:54.388: INFO: Pod pod-bd929cf5-e35a-49dc-90c8-90e32051c54a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 07:29:54.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5609" for this suite. 11/15/23 07:29:54.405
------------------------------
â€¢ [SLOW TEST] [6.246 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:29:48.178
    Nov 15 07:29:48.178: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename emptydir 11/15/23 07:29:48.179
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:29:48.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:29:48.242
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 11/15/23 07:29:48.247
    Nov 15 07:29:48.275: INFO: Waiting up to 5m0s for pod "pod-bd929cf5-e35a-49dc-90c8-90e32051c54a" in namespace "emptydir-5609" to be "Succeeded or Failed"
    Nov 15 07:29:48.286: INFO: Pod "pod-bd929cf5-e35a-49dc-90c8-90e32051c54a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.036382ms
    Nov 15 07:29:50.298: INFO: Pod "pod-bd929cf5-e35a-49dc-90c8-90e32051c54a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022730746s
    Nov 15 07:29:52.296: INFO: Pod "pod-bd929cf5-e35a-49dc-90c8-90e32051c54a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021519434s
    Nov 15 07:29:54.299: INFO: Pod "pod-bd929cf5-e35a-49dc-90c8-90e32051c54a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024088527s
    STEP: Saw pod success 11/15/23 07:29:54.299
    Nov 15 07:29:54.299: INFO: Pod "pod-bd929cf5-e35a-49dc-90c8-90e32051c54a" satisfied condition "Succeeded or Failed"
    Nov 15 07:29:54.308: INFO: Trying to get logs from node 10.72.152.81 pod pod-bd929cf5-e35a-49dc-90c8-90e32051c54a container test-container: <nil>
    STEP: delete the pod 11/15/23 07:29:54.344
    Nov 15 07:29:54.377: INFO: Waiting for pod pod-bd929cf5-e35a-49dc-90c8-90e32051c54a to disappear
    Nov 15 07:29:54.388: INFO: Pod pod-bd929cf5-e35a-49dc-90c8-90e32051c54a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:29:54.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5609" for this suite. 11/15/23 07:29:54.405
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:29:54.424
Nov 15 07:29:54.424: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename sysctl 11/15/23 07:29:54.425
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:29:54.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:29:54.495
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 11/15/23 07:29:54.5
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:29:54.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-5954" for this suite. 11/15/23 07:29:54.533
------------------------------
â€¢ [0.134 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:29:54.424
    Nov 15 07:29:54.424: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename sysctl 11/15/23 07:29:54.425
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:29:54.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:29:54.495
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 11/15/23 07:29:54.5
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:29:54.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-5954" for this suite. 11/15/23 07:29:54.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:29:54.56
Nov 15 07:29:54.560: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename emptydir 11/15/23 07:29:54.561
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:29:54.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:29:54.633
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 11/15/23 07:29:54.639
Nov 15 07:29:54.663: INFO: Waiting up to 5m0s for pod "pod-f5044d55-75e2-4a61-a468-fc607de663ac" in namespace "emptydir-9427" to be "Succeeded or Failed"
Nov 15 07:29:54.674: INFO: Pod "pod-f5044d55-75e2-4a61-a468-fc607de663ac": Phase="Pending", Reason="", readiness=false. Elapsed: 10.956669ms
Nov 15 07:29:56.684: INFO: Pod "pod-f5044d55-75e2-4a61-a468-fc607de663ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021419553s
Nov 15 07:29:58.687: INFO: Pod "pod-f5044d55-75e2-4a61-a468-fc607de663ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023825509s
STEP: Saw pod success 11/15/23 07:29:58.687
Nov 15 07:29:58.687: INFO: Pod "pod-f5044d55-75e2-4a61-a468-fc607de663ac" satisfied condition "Succeeded or Failed"
Nov 15 07:29:58.704: INFO: Trying to get logs from node 10.72.152.81 pod pod-f5044d55-75e2-4a61-a468-fc607de663ac container test-container: <nil>
STEP: delete the pod 11/15/23 07:29:58.743
Nov 15 07:29:58.772: INFO: Waiting for pod pod-f5044d55-75e2-4a61-a468-fc607de663ac to disappear
Nov 15 07:29:58.791: INFO: Pod pod-f5044d55-75e2-4a61-a468-fc607de663ac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 07:29:58.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9427" for this suite. 11/15/23 07:29:58.807
------------------------------
â€¢ [4.269 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:29:54.56
    Nov 15 07:29:54.560: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename emptydir 11/15/23 07:29:54.561
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:29:54.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:29:54.633
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 11/15/23 07:29:54.639
    Nov 15 07:29:54.663: INFO: Waiting up to 5m0s for pod "pod-f5044d55-75e2-4a61-a468-fc607de663ac" in namespace "emptydir-9427" to be "Succeeded or Failed"
    Nov 15 07:29:54.674: INFO: Pod "pod-f5044d55-75e2-4a61-a468-fc607de663ac": Phase="Pending", Reason="", readiness=false. Elapsed: 10.956669ms
    Nov 15 07:29:56.684: INFO: Pod "pod-f5044d55-75e2-4a61-a468-fc607de663ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021419553s
    Nov 15 07:29:58.687: INFO: Pod "pod-f5044d55-75e2-4a61-a468-fc607de663ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023825509s
    STEP: Saw pod success 11/15/23 07:29:58.687
    Nov 15 07:29:58.687: INFO: Pod "pod-f5044d55-75e2-4a61-a468-fc607de663ac" satisfied condition "Succeeded or Failed"
    Nov 15 07:29:58.704: INFO: Trying to get logs from node 10.72.152.81 pod pod-f5044d55-75e2-4a61-a468-fc607de663ac container test-container: <nil>
    STEP: delete the pod 11/15/23 07:29:58.743
    Nov 15 07:29:58.772: INFO: Waiting for pod pod-f5044d55-75e2-4a61-a468-fc607de663ac to disappear
    Nov 15 07:29:58.791: INFO: Pod pod-f5044d55-75e2-4a61-a468-fc607de663ac no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:29:58.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9427" for this suite. 11/15/23 07:29:58.807
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:29:58.829
Nov 15 07:29:58.829: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename job 11/15/23 07:29:58.832
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:29:58.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:29:58.894
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 11/15/23 07:29:58.899
W1115 07:29:58.914102      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions 11/15/23 07:29:58.914
STEP: Ensuring pods with index for job exist 11/15/23 07:30:08.929
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Nov 15 07:30:08.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4230" for this suite. 11/15/23 07:30:08.961
------------------------------
â€¢ [SLOW TEST] [10.150 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:29:58.829
    Nov 15 07:29:58.829: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename job 11/15/23 07:29:58.832
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:29:58.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:29:58.894
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 11/15/23 07:29:58.899
    W1115 07:29:58.914102      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring job reaches completions 11/15/23 07:29:58.914
    STEP: Ensuring pods with index for job exist 11/15/23 07:30:08.929
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:30:08.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4230" for this suite. 11/15/23 07:30:08.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:30:08.981
Nov 15 07:30:08.982: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename ingress 11/15/23 07:30:08.983
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:30:09.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:30:09.047
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 11/15/23 07:30:09.054
STEP: getting /apis/networking.k8s.io 11/15/23 07:30:09.06
STEP: getting /apis/networking.k8s.iov1 11/15/23 07:30:09.062
STEP: creating 11/15/23 07:30:09.067
STEP: getting 11/15/23 07:30:09.106
STEP: listing 11/15/23 07:30:09.119
STEP: watching 11/15/23 07:30:09.141
Nov 15 07:30:09.141: INFO: starting watch
STEP: cluster-wide listing 11/15/23 07:30:09.145
STEP: cluster-wide watching 11/15/23 07:30:09.158
Nov 15 07:30:09.158: INFO: starting watch
STEP: patching 11/15/23 07:30:09.16
STEP: updating 11/15/23 07:30:09.171
Nov 15 07:30:09.210: INFO: waiting for watch events with expected annotations
Nov 15 07:30:09.210: INFO: saw patched and updated annotations
STEP: patching /status 11/15/23 07:30:09.211
STEP: updating /status 11/15/23 07:30:09.223
STEP: get /status 11/15/23 07:30:09.248
STEP: deleting 11/15/23 07:30:09.26
STEP: deleting a collection 11/15/23 07:30:09.299
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Nov 15 07:30:09.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-5793" for this suite. 11/15/23 07:30:09.367
------------------------------
â€¢ [0.414 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:30:08.981
    Nov 15 07:30:08.982: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename ingress 11/15/23 07:30:08.983
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:30:09.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:30:09.047
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 11/15/23 07:30:09.054
    STEP: getting /apis/networking.k8s.io 11/15/23 07:30:09.06
    STEP: getting /apis/networking.k8s.iov1 11/15/23 07:30:09.062
    STEP: creating 11/15/23 07:30:09.067
    STEP: getting 11/15/23 07:30:09.106
    STEP: listing 11/15/23 07:30:09.119
    STEP: watching 11/15/23 07:30:09.141
    Nov 15 07:30:09.141: INFO: starting watch
    STEP: cluster-wide listing 11/15/23 07:30:09.145
    STEP: cluster-wide watching 11/15/23 07:30:09.158
    Nov 15 07:30:09.158: INFO: starting watch
    STEP: patching 11/15/23 07:30:09.16
    STEP: updating 11/15/23 07:30:09.171
    Nov 15 07:30:09.210: INFO: waiting for watch events with expected annotations
    Nov 15 07:30:09.210: INFO: saw patched and updated annotations
    STEP: patching /status 11/15/23 07:30:09.211
    STEP: updating /status 11/15/23 07:30:09.223
    STEP: get /status 11/15/23 07:30:09.248
    STEP: deleting 11/15/23 07:30:09.26
    STEP: deleting a collection 11/15/23 07:30:09.299
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:30:09.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-5793" for this suite. 11/15/23 07:30:09.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:30:09.397
Nov 15 07:30:09.397: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename secrets 11/15/23 07:30:09.398
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:30:09.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:30:09.462
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
Nov 15 07:30:09.514: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-2aefd0f8-dec7-4506-823b-75665d647679 11/15/23 07:30:09.514
STEP: Creating secret with name s-test-opt-upd-546d0d3e-0f46-46ba-a0a9-af9c128bb723 11/15/23 07:30:09.525
STEP: Creating the pod 11/15/23 07:30:09.536
Nov 15 07:30:09.564: INFO: Waiting up to 5m0s for pod "pod-secrets-dcc9d57d-28e3-492e-906e-1abce3aefd92" in namespace "secrets-9649" to be "running and ready"
Nov 15 07:30:09.578: INFO: Pod "pod-secrets-dcc9d57d-28e3-492e-906e-1abce3aefd92": Phase="Pending", Reason="", readiness=false. Elapsed: 13.893453ms
Nov 15 07:30:09.578: INFO: The phase of Pod pod-secrets-dcc9d57d-28e3-492e-906e-1abce3aefd92 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:30:11.622: INFO: Pod "pod-secrets-dcc9d57d-28e3-492e-906e-1abce3aefd92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05735022s
Nov 15 07:30:11.622: INFO: The phase of Pod pod-secrets-dcc9d57d-28e3-492e-906e-1abce3aefd92 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:30:13.597: INFO: Pod "pod-secrets-dcc9d57d-28e3-492e-906e-1abce3aefd92": Phase="Running", Reason="", readiness=true. Elapsed: 4.032610094s
Nov 15 07:30:13.597: INFO: The phase of Pod pod-secrets-dcc9d57d-28e3-492e-906e-1abce3aefd92 is Running (Ready = true)
Nov 15 07:30:13.597: INFO: Pod "pod-secrets-dcc9d57d-28e3-492e-906e-1abce3aefd92" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-2aefd0f8-dec7-4506-823b-75665d647679 11/15/23 07:30:13.708
STEP: Updating secret s-test-opt-upd-546d0d3e-0f46-46ba-a0a9-af9c128bb723 11/15/23 07:30:13.757
STEP: Creating secret with name s-test-opt-create-34aaa627-a607-47c2-9795-8d99bf1dc868 11/15/23 07:30:13.77
STEP: waiting to observe update in volume 11/15/23 07:30:13.803
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 07:31:39.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9649" for this suite. 11/15/23 07:31:39.361
------------------------------
â€¢ [SLOW TEST] [89.982 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:30:09.397
    Nov 15 07:30:09.397: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename secrets 11/15/23 07:30:09.398
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:30:09.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:30:09.462
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    Nov 15 07:30:09.514: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating secret with name s-test-opt-del-2aefd0f8-dec7-4506-823b-75665d647679 11/15/23 07:30:09.514
    STEP: Creating secret with name s-test-opt-upd-546d0d3e-0f46-46ba-a0a9-af9c128bb723 11/15/23 07:30:09.525
    STEP: Creating the pod 11/15/23 07:30:09.536
    Nov 15 07:30:09.564: INFO: Waiting up to 5m0s for pod "pod-secrets-dcc9d57d-28e3-492e-906e-1abce3aefd92" in namespace "secrets-9649" to be "running and ready"
    Nov 15 07:30:09.578: INFO: Pod "pod-secrets-dcc9d57d-28e3-492e-906e-1abce3aefd92": Phase="Pending", Reason="", readiness=false. Elapsed: 13.893453ms
    Nov 15 07:30:09.578: INFO: The phase of Pod pod-secrets-dcc9d57d-28e3-492e-906e-1abce3aefd92 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:30:11.622: INFO: Pod "pod-secrets-dcc9d57d-28e3-492e-906e-1abce3aefd92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05735022s
    Nov 15 07:30:11.622: INFO: The phase of Pod pod-secrets-dcc9d57d-28e3-492e-906e-1abce3aefd92 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:30:13.597: INFO: Pod "pod-secrets-dcc9d57d-28e3-492e-906e-1abce3aefd92": Phase="Running", Reason="", readiness=true. Elapsed: 4.032610094s
    Nov 15 07:30:13.597: INFO: The phase of Pod pod-secrets-dcc9d57d-28e3-492e-906e-1abce3aefd92 is Running (Ready = true)
    Nov 15 07:30:13.597: INFO: Pod "pod-secrets-dcc9d57d-28e3-492e-906e-1abce3aefd92" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-2aefd0f8-dec7-4506-823b-75665d647679 11/15/23 07:30:13.708
    STEP: Updating secret s-test-opt-upd-546d0d3e-0f46-46ba-a0a9-af9c128bb723 11/15/23 07:30:13.757
    STEP: Creating secret with name s-test-opt-create-34aaa627-a607-47c2-9795-8d99bf1dc868 11/15/23 07:30:13.77
    STEP: waiting to observe update in volume 11/15/23 07:30:13.803
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:31:39.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9649" for this suite. 11/15/23 07:31:39.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:31:39.381
Nov 15 07:31:39.382: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename deployment 11/15/23 07:31:39.383
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:31:39.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:31:39.457
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Nov 15 07:31:39.465: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
W1115 07:31:39.481534      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Nov 15 07:31:39.490: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 15 07:31:44.501: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/15/23 07:31:44.501
Nov 15 07:31:44.502: INFO: Creating deployment "test-rolling-update-deployment"
Nov 15 07:31:44.514: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Nov 15 07:31:44.540: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Nov 15 07:31:46.563: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Nov 15 07:31:46.573: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 31, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 31, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 31, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 31, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 07:31:48.584: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 15 07:31:48.624: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4672  3020ac3e-9159-42ad-b10c-4ad3497e2eba 133010 1 2023-11-15 07:31:44 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-11-15 07:31:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 07:31:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b45b538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-11-15 07:31:44 +0000 UTC,LastTransitionTime:2023-11-15 07:31:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-11-15 07:31:47 +0000 UTC,LastTransitionTime:2023-11-15 07:31:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 15 07:31:48.634: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-4672  a5f0be7c-f584-4620-8726-80cf5612cb28 132997 1 2023-11-15 07:31:44 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3020ac3e-9159-42ad-b10c-4ad3497e2eba 0xc00412c307 0xc00412c308}] [] [{kube-controller-manager Update apps/v1 2023-11-15 07:31:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3020ac3e-9159-42ad-b10c-4ad3497e2eba\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 07:31:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00412c3b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 15 07:31:48.634: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Nov 15 07:31:48.634: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4672  c994d781-5528-4542-a274-55ba0ef675ce 133008 2 2023-11-15 07:31:39 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3020ac3e-9159-42ad-b10c-4ad3497e2eba 0xc00412c1d7 0xc00412c1d8}] [] [{e2e.test Update apps/v1 2023-11-15 07:31:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 07:31:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3020ac3e-9159-42ad-b10c-4ad3497e2eba\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-11-15 07:31:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00412c298 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 15 07:31:48.644: INFO: Pod "test-rolling-update-deployment-7549d9f46d-wdb4v" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-wdb4v test-rolling-update-deployment-7549d9f46d- deployment-4672  9b48fbe8-46e6-4988-be12-a216350f18cb 132995 0 2023-11-15 07:31:44 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:15fa85fb202d89805f2d8aaa2441b3cde1bdcb3acd80ef6beeab97a6800d5248 cni.projectcalico.org/podIP:172.30.214.176/32 cni.projectcalico.org/podIPs:172.30.214.176/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.214.176"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d a5f0be7c-f584-4620-8726-80cf5612cb28 0xc00b45b917 0xc00b45b918}] [] [{kube-controller-manager Update v1 2023-11-15 07:31:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5f0be7c-f584-4620-8726-80cf5612cb28\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 07:31:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 07:31:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 07:31:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.214.176\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-znpl7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-znpl7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c68,c12,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dsxhw,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 07:31:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 07:31:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 07:31:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 07:31:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:172.30.214.176,StartTime:2023-11-15 07:31:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 07:31:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://3baeb665dae9a05f12401f2e3b9598469d65805f1098ab5cc3aee39146dc884c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.214.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 15 07:31:48.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4672" for this suite. 11/15/23 07:31:48.659
------------------------------
â€¢ [SLOW TEST] [9.295 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:31:39.381
    Nov 15 07:31:39.382: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename deployment 11/15/23 07:31:39.383
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:31:39.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:31:39.457
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Nov 15 07:31:39.465: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    W1115 07:31:39.481534      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Nov 15 07:31:39.490: INFO: Pod name sample-pod: Found 0 pods out of 1
    Nov 15 07:31:44.501: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/15/23 07:31:44.501
    Nov 15 07:31:44.502: INFO: Creating deployment "test-rolling-update-deployment"
    Nov 15 07:31:44.514: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Nov 15 07:31:44.540: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Nov 15 07:31:46.563: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Nov 15 07:31:46.573: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 31, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 31, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 31, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 31, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 07:31:48.584: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 15 07:31:48.624: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4672  3020ac3e-9159-42ad-b10c-4ad3497e2eba 133010 1 2023-11-15 07:31:44 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-11-15 07:31:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 07:31:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b45b538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-11-15 07:31:44 +0000 UTC,LastTransitionTime:2023-11-15 07:31:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-11-15 07:31:47 +0000 UTC,LastTransitionTime:2023-11-15 07:31:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Nov 15 07:31:48.634: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-4672  a5f0be7c-f584-4620-8726-80cf5612cb28 132997 1 2023-11-15 07:31:44 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3020ac3e-9159-42ad-b10c-4ad3497e2eba 0xc00412c307 0xc00412c308}] [] [{kube-controller-manager Update apps/v1 2023-11-15 07:31:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3020ac3e-9159-42ad-b10c-4ad3497e2eba\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 07:31:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00412c3b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 07:31:48.634: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Nov 15 07:31:48.634: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4672  c994d781-5528-4542-a274-55ba0ef675ce 133008 2 2023-11-15 07:31:39 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3020ac3e-9159-42ad-b10c-4ad3497e2eba 0xc00412c1d7 0xc00412c1d8}] [] [{e2e.test Update apps/v1 2023-11-15 07:31:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 07:31:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3020ac3e-9159-42ad-b10c-4ad3497e2eba\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-11-15 07:31:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00412c298 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 07:31:48.644: INFO: Pod "test-rolling-update-deployment-7549d9f46d-wdb4v" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-wdb4v test-rolling-update-deployment-7549d9f46d- deployment-4672  9b48fbe8-46e6-4988-be12-a216350f18cb 132995 0 2023-11-15 07:31:44 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:15fa85fb202d89805f2d8aaa2441b3cde1bdcb3acd80ef6beeab97a6800d5248 cni.projectcalico.org/podIP:172.30.214.176/32 cni.projectcalico.org/podIPs:172.30.214.176/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.214.176"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d a5f0be7c-f584-4620-8726-80cf5612cb28 0xc00b45b917 0xc00b45b918}] [] [{kube-controller-manager Update v1 2023-11-15 07:31:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5f0be7c-f584-4620-8726-80cf5612cb28\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 07:31:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-11-15 07:31:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-11-15 07:31:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.214.176\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-znpl7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-znpl7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.152.81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c68,c12,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dsxhw,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 07:31:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 07:31:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 07:31:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 07:31:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.152.81,PodIP:172.30.214.176,StartTime:2023-11-15 07:31:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 07:31:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://3baeb665dae9a05f12401f2e3b9598469d65805f1098ab5cc3aee39146dc884c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.214.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:31:48.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4672" for this suite. 11/15/23 07:31:48.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:31:48.68
Nov 15 07:31:48.680: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename replication-controller 11/15/23 07:31:48.681
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:31:48.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:31:48.753
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 11/15/23 07:31:48.759
W1115 07:31:48.793020      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: When the matched label of one of its pods change 11/15/23 07:31:48.793
Nov 15 07:31:48.801: INFO: Pod name pod-release: Found 0 pods out of 1
Nov 15 07:31:53.812: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 11/15/23 07:31:53.839
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Nov 15 07:31:54.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2700" for this suite. 11/15/23 07:31:54.875
------------------------------
â€¢ [SLOW TEST] [6.213 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:31:48.68
    Nov 15 07:31:48.680: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename replication-controller 11/15/23 07:31:48.681
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:31:48.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:31:48.753
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 11/15/23 07:31:48.759
    W1115 07:31:48.793020      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: When the matched label of one of its pods change 11/15/23 07:31:48.793
    Nov 15 07:31:48.801: INFO: Pod name pod-release: Found 0 pods out of 1
    Nov 15 07:31:53.812: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 11/15/23 07:31:53.839
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:31:54.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2700" for this suite. 11/15/23 07:31:54.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:31:54.894
Nov 15 07:31:54.894: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename pod-network-test 11/15/23 07:31:54.896
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:31:54.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:31:54.96
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-7282 11/15/23 07:31:54.967
STEP: creating a selector 11/15/23 07:31:54.968
STEP: Creating the service pods in kubernetes 11/15/23 07:31:54.968
Nov 15 07:31:54.968: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 15 07:31:55.053: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7282" to be "running and ready"
Nov 15 07:31:55.063: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.676195ms
Nov 15 07:31:55.063: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:31:57.076: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022169418s
Nov 15 07:31:57.076: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 07:31:59.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.020391035s
Nov 15 07:31:59.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:32:01.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.020242769s
Nov 15 07:32:01.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:32:03.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.022067537s
Nov 15 07:32:03.075: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:32:05.076: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.022654159s
Nov 15 07:32:05.076: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:32:07.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.021336309s
Nov 15 07:32:07.075: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:32:09.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.020240117s
Nov 15 07:32:09.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:32:11.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.020207621s
Nov 15 07:32:11.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:32:13.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.021927456s
Nov 15 07:32:13.075: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:32:15.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.020160701s
Nov 15 07:32:15.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 07:32:17.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.021846413s
Nov 15 07:32:17.075: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Nov 15 07:32:17.075: INFO: Pod "netserver-0" satisfied condition "running and ready"
Nov 15 07:32:17.086: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7282" to be "running and ready"
Nov 15 07:32:17.096: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.118885ms
Nov 15 07:32:17.096: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Nov 15 07:32:17.096: INFO: Pod "netserver-1" satisfied condition "running and ready"
Nov 15 07:32:17.105: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7282" to be "running and ready"
Nov 15 07:32:17.115: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 9.43698ms
Nov 15 07:32:17.115: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Nov 15 07:32:17.115: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 11/15/23 07:32:17.124
Nov 15 07:32:17.141: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7282" to be "running"
Nov 15 07:32:17.163: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 21.300951ms
Nov 15 07:32:19.177: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035624855s
Nov 15 07:32:21.177: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.035286506s
Nov 15 07:32:21.177: INFO: Pod "test-container-pod" satisfied condition "running"
Nov 15 07:32:21.186: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 15 07:32:21.186: INFO: Breadth first check of 172.30.214.161 on host 10.72.152.81...
Nov 15 07:32:21.199: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.214.155:9080/dial?request=hostname&protocol=http&host=172.30.214.161&port=8083&tries=1'] Namespace:pod-network-test-7282 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:32:21.199: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:32:21.199: INFO: ExecWithOptions: Clientset creation
Nov 15 07:32:21.200: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7282/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.214.155%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.214.161%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 15 07:32:21.422: INFO: Waiting for responses: map[]
Nov 15 07:32:21.422: INFO: reached 172.30.214.161 after 0/1 tries
Nov 15 07:32:21.422: INFO: Breadth first check of 172.30.213.176 on host 10.72.152.86...
Nov 15 07:32:21.433: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.214.155:9080/dial?request=hostname&protocol=http&host=172.30.213.176&port=8083&tries=1'] Namespace:pod-network-test-7282 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:32:21.433: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:32:21.434: INFO: ExecWithOptions: Clientset creation
Nov 15 07:32:21.434: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7282/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.214.155%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.213.176%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 15 07:32:21.611: INFO: Waiting for responses: map[]
Nov 15 07:32:21.611: INFO: reached 172.30.213.176 after 0/1 tries
Nov 15 07:32:21.611: INFO: Breadth first check of 172.30.10.173 on host 10.72.152.88...
Nov 15 07:32:21.625: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.214.155:9080/dial?request=hostname&protocol=http&host=172.30.10.173&port=8083&tries=1'] Namespace:pod-network-test-7282 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 07:32:21.625: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
Nov 15 07:32:21.626: INFO: ExecWithOptions: Clientset creation
Nov 15 07:32:21.626: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7282/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.214.155%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.10.173%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 15 07:32:21.854: INFO: Waiting for responses: map[]
Nov 15 07:32:21.854: INFO: reached 172.30.10.173 after 0/1 tries
Nov 15 07:32:21.854: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Nov 15 07:32:21.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7282" for this suite. 11/15/23 07:32:21.87
------------------------------
â€¢ [SLOW TEST] [26.995 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:31:54.894
    Nov 15 07:31:54.894: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename pod-network-test 11/15/23 07:31:54.896
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:31:54.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:31:54.96
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-7282 11/15/23 07:31:54.967
    STEP: creating a selector 11/15/23 07:31:54.968
    STEP: Creating the service pods in kubernetes 11/15/23 07:31:54.968
    Nov 15 07:31:54.968: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Nov 15 07:31:55.053: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7282" to be "running and ready"
    Nov 15 07:31:55.063: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.676195ms
    Nov 15 07:31:55.063: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:31:57.076: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022169418s
    Nov 15 07:31:57.076: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 07:31:59.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.020391035s
    Nov 15 07:31:59.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:32:01.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.020242769s
    Nov 15 07:32:01.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:32:03.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.022067537s
    Nov 15 07:32:03.075: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:32:05.076: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.022654159s
    Nov 15 07:32:05.076: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:32:07.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.021336309s
    Nov 15 07:32:07.075: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:32:09.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.020240117s
    Nov 15 07:32:09.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:32:11.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.020207621s
    Nov 15 07:32:11.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:32:13.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.021927456s
    Nov 15 07:32:13.075: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:32:15.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.020160701s
    Nov 15 07:32:15.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 07:32:17.075: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.021846413s
    Nov 15 07:32:17.075: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Nov 15 07:32:17.075: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Nov 15 07:32:17.086: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7282" to be "running and ready"
    Nov 15 07:32:17.096: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.118885ms
    Nov 15 07:32:17.096: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Nov 15 07:32:17.096: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Nov 15 07:32:17.105: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7282" to be "running and ready"
    Nov 15 07:32:17.115: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 9.43698ms
    Nov 15 07:32:17.115: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Nov 15 07:32:17.115: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 11/15/23 07:32:17.124
    Nov 15 07:32:17.141: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7282" to be "running"
    Nov 15 07:32:17.163: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 21.300951ms
    Nov 15 07:32:19.177: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035624855s
    Nov 15 07:32:21.177: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.035286506s
    Nov 15 07:32:21.177: INFO: Pod "test-container-pod" satisfied condition "running"
    Nov 15 07:32:21.186: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Nov 15 07:32:21.186: INFO: Breadth first check of 172.30.214.161 on host 10.72.152.81...
    Nov 15 07:32:21.199: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.214.155:9080/dial?request=hostname&protocol=http&host=172.30.214.161&port=8083&tries=1'] Namespace:pod-network-test-7282 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:32:21.199: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:32:21.199: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:32:21.200: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7282/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.214.155%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.214.161%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 15 07:32:21.422: INFO: Waiting for responses: map[]
    Nov 15 07:32:21.422: INFO: reached 172.30.214.161 after 0/1 tries
    Nov 15 07:32:21.422: INFO: Breadth first check of 172.30.213.176 on host 10.72.152.86...
    Nov 15 07:32:21.433: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.214.155:9080/dial?request=hostname&protocol=http&host=172.30.213.176&port=8083&tries=1'] Namespace:pod-network-test-7282 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:32:21.433: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:32:21.434: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:32:21.434: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7282/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.214.155%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.213.176%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 15 07:32:21.611: INFO: Waiting for responses: map[]
    Nov 15 07:32:21.611: INFO: reached 172.30.213.176 after 0/1 tries
    Nov 15 07:32:21.611: INFO: Breadth first check of 172.30.10.173 on host 10.72.152.88...
    Nov 15 07:32:21.625: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.214.155:9080/dial?request=hostname&protocol=http&host=172.30.10.173&port=8083&tries=1'] Namespace:pod-network-test-7282 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 07:32:21.625: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    Nov 15 07:32:21.626: INFO: ExecWithOptions: Clientset creation
    Nov 15 07:32:21.626: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7282/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.214.155%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.10.173%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 15 07:32:21.854: INFO: Waiting for responses: map[]
    Nov 15 07:32:21.854: INFO: reached 172.30.10.173 after 0/1 tries
    Nov 15 07:32:21.854: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:32:21.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7282" for this suite. 11/15/23 07:32:21.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:32:21.891
Nov 15 07:32:21.891: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubectl 11/15/23 07:32:21.892
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:32:21.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:32:21.954
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 11/15/23 07:32:21.961
Nov 15 07:32:21.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 create -f -'
Nov 15 07:32:25.038: INFO: stderr: ""
Nov 15 07:32:25.038: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 11/15/23 07:32:25.038
Nov 15 07:32:25.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 07:32:25.211: INFO: stderr: ""
Nov 15 07:32:25.211: INFO: stdout: "update-demo-nautilus-f75b8 update-demo-nautilus-f8d8c "
Nov 15 07:32:25.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods update-demo-nautilus-f75b8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 07:32:25.327: INFO: stderr: ""
Nov 15 07:32:25.327: INFO: stdout: ""
Nov 15 07:32:25.327: INFO: update-demo-nautilus-f75b8 is created but not running
Nov 15 07:32:30.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 07:32:30.500: INFO: stderr: ""
Nov 15 07:32:30.500: INFO: stdout: "update-demo-nautilus-f75b8 update-demo-nautilus-f8d8c "
Nov 15 07:32:30.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods update-demo-nautilus-f75b8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 07:32:30.657: INFO: stderr: ""
Nov 15 07:32:30.657: INFO: stdout: "true"
Nov 15 07:32:30.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods update-demo-nautilus-f75b8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 15 07:32:30.815: INFO: stderr: ""
Nov 15 07:32:30.815: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 15 07:32:30.815: INFO: validating pod update-demo-nautilus-f75b8
Nov 15 07:32:30.847: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 15 07:32:30.847: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 15 07:32:30.847: INFO: update-demo-nautilus-f75b8 is verified up and running
Nov 15 07:32:30.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods update-demo-nautilus-f8d8c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 07:32:30.962: INFO: stderr: ""
Nov 15 07:32:30.962: INFO: stdout: ""
Nov 15 07:32:30.962: INFO: update-demo-nautilus-f8d8c is created but not running
Nov 15 07:32:35.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 07:32:36.077: INFO: stderr: ""
Nov 15 07:32:36.077: INFO: stdout: "update-demo-nautilus-f75b8 update-demo-nautilus-f8d8c "
Nov 15 07:32:36.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods update-demo-nautilus-f75b8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 07:32:36.187: INFO: stderr: ""
Nov 15 07:32:36.187: INFO: stdout: "true"
Nov 15 07:32:36.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods update-demo-nautilus-f75b8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 15 07:32:36.296: INFO: stderr: ""
Nov 15 07:32:36.296: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 15 07:32:36.296: INFO: validating pod update-demo-nautilus-f75b8
Nov 15 07:32:36.315: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 15 07:32:36.315: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 15 07:32:36.315: INFO: update-demo-nautilus-f75b8 is verified up and running
Nov 15 07:32:36.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods update-demo-nautilus-f8d8c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 07:32:36.418: INFO: stderr: ""
Nov 15 07:32:36.418: INFO: stdout: "true"
Nov 15 07:32:36.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods update-demo-nautilus-f8d8c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 15 07:32:36.539: INFO: stderr: ""
Nov 15 07:32:36.539: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 15 07:32:36.539: INFO: validating pod update-demo-nautilus-f8d8c
Nov 15 07:32:36.569: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 15 07:32:36.569: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 15 07:32:36.569: INFO: update-demo-nautilus-f8d8c is verified up and running
STEP: using delete to clean up resources 11/15/23 07:32:36.569
Nov 15 07:32:36.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 delete --grace-period=0 --force -f -'
Nov 15 07:32:36.689: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 07:32:36.689: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 15 07:32:36.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get rc,svc -l name=update-demo --no-headers'
Nov 15 07:32:36.840: INFO: stderr: "No resources found in kubectl-5400 namespace.\n"
Nov 15 07:32:36.840: INFO: stdout: ""
Nov 15 07:32:36.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 15 07:32:36.956: INFO: stderr: ""
Nov 15 07:32:36.956: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 07:32:36.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5400" for this suite. 11/15/23 07:32:36.97
------------------------------
â€¢ [SLOW TEST] [15.100 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:32:21.891
    Nov 15 07:32:21.891: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubectl 11/15/23 07:32:21.892
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:32:21.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:32:21.954
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 11/15/23 07:32:21.961
    Nov 15 07:32:21.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 create -f -'
    Nov 15 07:32:25.038: INFO: stderr: ""
    Nov 15 07:32:25.038: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 11/15/23 07:32:25.038
    Nov 15 07:32:25.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 07:32:25.211: INFO: stderr: ""
    Nov 15 07:32:25.211: INFO: stdout: "update-demo-nautilus-f75b8 update-demo-nautilus-f8d8c "
    Nov 15 07:32:25.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods update-demo-nautilus-f75b8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 07:32:25.327: INFO: stderr: ""
    Nov 15 07:32:25.327: INFO: stdout: ""
    Nov 15 07:32:25.327: INFO: update-demo-nautilus-f75b8 is created but not running
    Nov 15 07:32:30.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 07:32:30.500: INFO: stderr: ""
    Nov 15 07:32:30.500: INFO: stdout: "update-demo-nautilus-f75b8 update-demo-nautilus-f8d8c "
    Nov 15 07:32:30.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods update-demo-nautilus-f75b8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 07:32:30.657: INFO: stderr: ""
    Nov 15 07:32:30.657: INFO: stdout: "true"
    Nov 15 07:32:30.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods update-demo-nautilus-f75b8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 15 07:32:30.815: INFO: stderr: ""
    Nov 15 07:32:30.815: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 15 07:32:30.815: INFO: validating pod update-demo-nautilus-f75b8
    Nov 15 07:32:30.847: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 15 07:32:30.847: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 15 07:32:30.847: INFO: update-demo-nautilus-f75b8 is verified up and running
    Nov 15 07:32:30.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods update-demo-nautilus-f8d8c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 07:32:30.962: INFO: stderr: ""
    Nov 15 07:32:30.962: INFO: stdout: ""
    Nov 15 07:32:30.962: INFO: update-demo-nautilus-f8d8c is created but not running
    Nov 15 07:32:35.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 07:32:36.077: INFO: stderr: ""
    Nov 15 07:32:36.077: INFO: stdout: "update-demo-nautilus-f75b8 update-demo-nautilus-f8d8c "
    Nov 15 07:32:36.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods update-demo-nautilus-f75b8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 07:32:36.187: INFO: stderr: ""
    Nov 15 07:32:36.187: INFO: stdout: "true"
    Nov 15 07:32:36.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods update-demo-nautilus-f75b8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 15 07:32:36.296: INFO: stderr: ""
    Nov 15 07:32:36.296: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 15 07:32:36.296: INFO: validating pod update-demo-nautilus-f75b8
    Nov 15 07:32:36.315: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 15 07:32:36.315: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 15 07:32:36.315: INFO: update-demo-nautilus-f75b8 is verified up and running
    Nov 15 07:32:36.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods update-demo-nautilus-f8d8c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 07:32:36.418: INFO: stderr: ""
    Nov 15 07:32:36.418: INFO: stdout: "true"
    Nov 15 07:32:36.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods update-demo-nautilus-f8d8c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 15 07:32:36.539: INFO: stderr: ""
    Nov 15 07:32:36.539: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 15 07:32:36.539: INFO: validating pod update-demo-nautilus-f8d8c
    Nov 15 07:32:36.569: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 15 07:32:36.569: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 15 07:32:36.569: INFO: update-demo-nautilus-f8d8c is verified up and running
    STEP: using delete to clean up resources 11/15/23 07:32:36.569
    Nov 15 07:32:36.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 delete --grace-period=0 --force -f -'
    Nov 15 07:32:36.689: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 15 07:32:36.689: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Nov 15 07:32:36.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get rc,svc -l name=update-demo --no-headers'
    Nov 15 07:32:36.840: INFO: stderr: "No resources found in kubectl-5400 namespace.\n"
    Nov 15 07:32:36.840: INFO: stdout: ""
    Nov 15 07:32:36.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-5400 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Nov 15 07:32:36.956: INFO: stderr: ""
    Nov 15 07:32:36.956: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:32:36.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5400" for this suite. 11/15/23 07:32:36.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:32:36.994
Nov 15 07:32:36.994: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename var-expansion 11/15/23 07:32:36.995
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:32:37.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:32:37.059
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 11/15/23 07:32:37.065
Nov 15 07:32:37.095: INFO: Waiting up to 5m0s for pod "var-expansion-b21a4b95-9196-4433-8101-8b25f875a93d" in namespace "var-expansion-7479" to be "Succeeded or Failed"
Nov 15 07:32:37.110: INFO: Pod "var-expansion-b21a4b95-9196-4433-8101-8b25f875a93d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.087026ms
Nov 15 07:32:39.120: INFO: Pod "var-expansion-b21a4b95-9196-4433-8101-8b25f875a93d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025573958s
Nov 15 07:32:41.121: INFO: Pod "var-expansion-b21a4b95-9196-4433-8101-8b25f875a93d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026679482s
Nov 15 07:32:43.121: INFO: Pod "var-expansion-b21a4b95-9196-4433-8101-8b25f875a93d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026006686s
STEP: Saw pod success 11/15/23 07:32:43.121
Nov 15 07:32:43.121: INFO: Pod "var-expansion-b21a4b95-9196-4433-8101-8b25f875a93d" satisfied condition "Succeeded or Failed"
Nov 15 07:32:43.130: INFO: Trying to get logs from node 10.72.152.81 pod var-expansion-b21a4b95-9196-4433-8101-8b25f875a93d container dapi-container: <nil>
STEP: delete the pod 11/15/23 07:32:43.167
Nov 15 07:32:43.200: INFO: Waiting for pod var-expansion-b21a4b95-9196-4433-8101-8b25f875a93d to disappear
Nov 15 07:32:43.215: INFO: Pod var-expansion-b21a4b95-9196-4433-8101-8b25f875a93d no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 15 07:32:43.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7479" for this suite. 11/15/23 07:32:43.227
------------------------------
â€¢ [SLOW TEST] [6.260 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:32:36.994
    Nov 15 07:32:36.994: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename var-expansion 11/15/23 07:32:36.995
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:32:37.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:32:37.059
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 11/15/23 07:32:37.065
    Nov 15 07:32:37.095: INFO: Waiting up to 5m0s for pod "var-expansion-b21a4b95-9196-4433-8101-8b25f875a93d" in namespace "var-expansion-7479" to be "Succeeded or Failed"
    Nov 15 07:32:37.110: INFO: Pod "var-expansion-b21a4b95-9196-4433-8101-8b25f875a93d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.087026ms
    Nov 15 07:32:39.120: INFO: Pod "var-expansion-b21a4b95-9196-4433-8101-8b25f875a93d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025573958s
    Nov 15 07:32:41.121: INFO: Pod "var-expansion-b21a4b95-9196-4433-8101-8b25f875a93d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026679482s
    Nov 15 07:32:43.121: INFO: Pod "var-expansion-b21a4b95-9196-4433-8101-8b25f875a93d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026006686s
    STEP: Saw pod success 11/15/23 07:32:43.121
    Nov 15 07:32:43.121: INFO: Pod "var-expansion-b21a4b95-9196-4433-8101-8b25f875a93d" satisfied condition "Succeeded or Failed"
    Nov 15 07:32:43.130: INFO: Trying to get logs from node 10.72.152.81 pod var-expansion-b21a4b95-9196-4433-8101-8b25f875a93d container dapi-container: <nil>
    STEP: delete the pod 11/15/23 07:32:43.167
    Nov 15 07:32:43.200: INFO: Waiting for pod var-expansion-b21a4b95-9196-4433-8101-8b25f875a93d to disappear
    Nov 15 07:32:43.215: INFO: Pod var-expansion-b21a4b95-9196-4433-8101-8b25f875a93d no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:32:43.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7479" for this suite. 11/15/23 07:32:43.227
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:32:43.253
Nov 15 07:32:43.253: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename services 11/15/23 07:32:43.254
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:32:43.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:32:43.324
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 11/15/23 07:32:43.352
STEP: watching for the Service to be added 11/15/23 07:32:43.409
Nov 15 07:32:43.412: INFO: Found Service test-service-xbt5f in namespace services-8130 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Nov 15 07:32:43.412: INFO: Service test-service-xbt5f created
STEP: Getting /status 11/15/23 07:32:43.412
Nov 15 07:32:43.423: INFO: Service test-service-xbt5f has LoadBalancer: {[]}
STEP: patching the ServiceStatus 11/15/23 07:32:43.423
STEP: watching for the Service to be patched 11/15/23 07:32:43.465
Nov 15 07:32:43.472: INFO: observed Service test-service-xbt5f in namespace services-8130 with annotations: map[] & LoadBalancer: {[]}
Nov 15 07:32:43.472: INFO: Found Service test-service-xbt5f in namespace services-8130 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Nov 15 07:32:43.472: INFO: Service test-service-xbt5f has service status patched
STEP: updating the ServiceStatus 11/15/23 07:32:43.472
Nov 15 07:32:43.503: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 11/15/23 07:32:43.503
Nov 15 07:32:43.506: INFO: Observed Service test-service-xbt5f in namespace services-8130 with annotations: map[] & Conditions: {[]}
Nov 15 07:32:43.506: INFO: Observed event: &Service{ObjectMeta:{test-service-xbt5f  services-8130  45a5b34e-2b97-4a12-bf64-e56e323ad831 133760 0 2023-11-15 07:32:43 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-11-15 07:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-11-15 07:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.64.39,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.64.39],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Nov 15 07:32:43.506: INFO: Found Service test-service-xbt5f in namespace services-8130 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 15 07:32:43.506: INFO: Service test-service-xbt5f has service status updated
STEP: patching the service 11/15/23 07:32:43.506
STEP: watching for the Service to be patched 11/15/23 07:32:43.519
Nov 15 07:32:43.524: INFO: observed Service test-service-xbt5f in namespace services-8130 with labels: map[test-service-static:true]
Nov 15 07:32:43.524: INFO: observed Service test-service-xbt5f in namespace services-8130 with labels: map[test-service-static:true]
Nov 15 07:32:43.524: INFO: observed Service test-service-xbt5f in namespace services-8130 with labels: map[test-service-static:true]
Nov 15 07:32:43.524: INFO: Found Service test-service-xbt5f in namespace services-8130 with labels: map[test-service:patched test-service-static:true]
Nov 15 07:32:43.524: INFO: Service test-service-xbt5f patched
STEP: deleting the service 11/15/23 07:32:43.525
STEP: watching for the Service to be deleted 11/15/23 07:32:43.602
Nov 15 07:32:43.605: INFO: Observed event: ADDED
Nov 15 07:32:43.606: INFO: Observed event: MODIFIED
Nov 15 07:32:43.606: INFO: Observed event: MODIFIED
Nov 15 07:32:43.606: INFO: Observed event: MODIFIED
Nov 15 07:32:43.606: INFO: Found Service test-service-xbt5f in namespace services-8130 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Nov 15 07:32:43.606: INFO: Service test-service-xbt5f deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 07:32:43.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8130" for this suite. 11/15/23 07:32:43.622
------------------------------
â€¢ [0.410 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:32:43.253
    Nov 15 07:32:43.253: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename services 11/15/23 07:32:43.254
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:32:43.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:32:43.324
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 11/15/23 07:32:43.352
    STEP: watching for the Service to be added 11/15/23 07:32:43.409
    Nov 15 07:32:43.412: INFO: Found Service test-service-xbt5f in namespace services-8130 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Nov 15 07:32:43.412: INFO: Service test-service-xbt5f created
    STEP: Getting /status 11/15/23 07:32:43.412
    Nov 15 07:32:43.423: INFO: Service test-service-xbt5f has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 11/15/23 07:32:43.423
    STEP: watching for the Service to be patched 11/15/23 07:32:43.465
    Nov 15 07:32:43.472: INFO: observed Service test-service-xbt5f in namespace services-8130 with annotations: map[] & LoadBalancer: {[]}
    Nov 15 07:32:43.472: INFO: Found Service test-service-xbt5f in namespace services-8130 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Nov 15 07:32:43.472: INFO: Service test-service-xbt5f has service status patched
    STEP: updating the ServiceStatus 11/15/23 07:32:43.472
    Nov 15 07:32:43.503: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 11/15/23 07:32:43.503
    Nov 15 07:32:43.506: INFO: Observed Service test-service-xbt5f in namespace services-8130 with annotations: map[] & Conditions: {[]}
    Nov 15 07:32:43.506: INFO: Observed event: &Service{ObjectMeta:{test-service-xbt5f  services-8130  45a5b34e-2b97-4a12-bf64-e56e323ad831 133760 0 2023-11-15 07:32:43 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-11-15 07:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-11-15 07:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.64.39,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.64.39],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Nov 15 07:32:43.506: INFO: Found Service test-service-xbt5f in namespace services-8130 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Nov 15 07:32:43.506: INFO: Service test-service-xbt5f has service status updated
    STEP: patching the service 11/15/23 07:32:43.506
    STEP: watching for the Service to be patched 11/15/23 07:32:43.519
    Nov 15 07:32:43.524: INFO: observed Service test-service-xbt5f in namespace services-8130 with labels: map[test-service-static:true]
    Nov 15 07:32:43.524: INFO: observed Service test-service-xbt5f in namespace services-8130 with labels: map[test-service-static:true]
    Nov 15 07:32:43.524: INFO: observed Service test-service-xbt5f in namespace services-8130 with labels: map[test-service-static:true]
    Nov 15 07:32:43.524: INFO: Found Service test-service-xbt5f in namespace services-8130 with labels: map[test-service:patched test-service-static:true]
    Nov 15 07:32:43.524: INFO: Service test-service-xbt5f patched
    STEP: deleting the service 11/15/23 07:32:43.525
    STEP: watching for the Service to be deleted 11/15/23 07:32:43.602
    Nov 15 07:32:43.605: INFO: Observed event: ADDED
    Nov 15 07:32:43.606: INFO: Observed event: MODIFIED
    Nov 15 07:32:43.606: INFO: Observed event: MODIFIED
    Nov 15 07:32:43.606: INFO: Observed event: MODIFIED
    Nov 15 07:32:43.606: INFO: Found Service test-service-xbt5f in namespace services-8130 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Nov 15 07:32:43.606: INFO: Service test-service-xbt5f deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:32:43.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8130" for this suite. 11/15/23 07:32:43.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:32:43.667
Nov 15 07:32:43.667: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename events 11/15/23 07:32:43.668
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:32:43.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:32:43.746
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 11/15/23 07:32:43.754
STEP: listing all events in all namespaces 11/15/23 07:32:43.775
STEP: patching the test event 11/15/23 07:32:43.816
STEP: fetching the test event 11/15/23 07:32:43.838
STEP: updating the test event 11/15/23 07:32:43.848
STEP: getting the test event 11/15/23 07:32:43.881
STEP: deleting the test event 11/15/23 07:32:43.895
STEP: listing all events in all namespaces 11/15/23 07:32:43.927
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Nov 15 07:32:43.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-3005" for this suite. 11/15/23 07:32:43.978
------------------------------
â€¢ [0.336 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:32:43.667
    Nov 15 07:32:43.667: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename events 11/15/23 07:32:43.668
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:32:43.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:32:43.746
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 11/15/23 07:32:43.754
    STEP: listing all events in all namespaces 11/15/23 07:32:43.775
    STEP: patching the test event 11/15/23 07:32:43.816
    STEP: fetching the test event 11/15/23 07:32:43.838
    STEP: updating the test event 11/15/23 07:32:43.848
    STEP: getting the test event 11/15/23 07:32:43.881
    STEP: deleting the test event 11/15/23 07:32:43.895
    STEP: listing all events in all namespaces 11/15/23 07:32:43.927
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:32:43.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-3005" for this suite. 11/15/23 07:32:43.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:32:44.005
Nov 15 07:32:44.005: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename containers 11/15/23 07:32:44.006
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:32:44.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:32:44.068
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 11/15/23 07:32:44.073
Nov 15 07:32:44.100: INFO: Waiting up to 5m0s for pod "client-containers-810de6d2-a28a-43e8-a001-0022f30d8de0" in namespace "containers-3964" to be "Succeeded or Failed"
Nov 15 07:32:44.110: INFO: Pod "client-containers-810de6d2-a28a-43e8-a001-0022f30d8de0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.47867ms
Nov 15 07:32:46.121: INFO: Pod "client-containers-810de6d2-a28a-43e8-a001-0022f30d8de0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021119481s
Nov 15 07:32:48.123: INFO: Pod "client-containers-810de6d2-a28a-43e8-a001-0022f30d8de0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022946062s
Nov 15 07:32:50.121: INFO: Pod "client-containers-810de6d2-a28a-43e8-a001-0022f30d8de0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021356457s
STEP: Saw pod success 11/15/23 07:32:50.121
Nov 15 07:32:50.121: INFO: Pod "client-containers-810de6d2-a28a-43e8-a001-0022f30d8de0" satisfied condition "Succeeded or Failed"
Nov 15 07:32:50.130: INFO: Trying to get logs from node 10.72.152.81 pod client-containers-810de6d2-a28a-43e8-a001-0022f30d8de0 container agnhost-container: <nil>
STEP: delete the pod 11/15/23 07:32:50.163
Nov 15 07:32:50.189: INFO: Waiting for pod client-containers-810de6d2-a28a-43e8-a001-0022f30d8de0 to disappear
Nov 15 07:32:50.201: INFO: Pod client-containers-810de6d2-a28a-43e8-a001-0022f30d8de0 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Nov 15 07:32:50.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3964" for this suite. 11/15/23 07:32:50.215
------------------------------
â€¢ [SLOW TEST] [6.227 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:32:44.005
    Nov 15 07:32:44.005: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename containers 11/15/23 07:32:44.006
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:32:44.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:32:44.068
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 11/15/23 07:32:44.073
    Nov 15 07:32:44.100: INFO: Waiting up to 5m0s for pod "client-containers-810de6d2-a28a-43e8-a001-0022f30d8de0" in namespace "containers-3964" to be "Succeeded or Failed"
    Nov 15 07:32:44.110: INFO: Pod "client-containers-810de6d2-a28a-43e8-a001-0022f30d8de0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.47867ms
    Nov 15 07:32:46.121: INFO: Pod "client-containers-810de6d2-a28a-43e8-a001-0022f30d8de0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021119481s
    Nov 15 07:32:48.123: INFO: Pod "client-containers-810de6d2-a28a-43e8-a001-0022f30d8de0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022946062s
    Nov 15 07:32:50.121: INFO: Pod "client-containers-810de6d2-a28a-43e8-a001-0022f30d8de0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021356457s
    STEP: Saw pod success 11/15/23 07:32:50.121
    Nov 15 07:32:50.121: INFO: Pod "client-containers-810de6d2-a28a-43e8-a001-0022f30d8de0" satisfied condition "Succeeded or Failed"
    Nov 15 07:32:50.130: INFO: Trying to get logs from node 10.72.152.81 pod client-containers-810de6d2-a28a-43e8-a001-0022f30d8de0 container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 07:32:50.163
    Nov 15 07:32:50.189: INFO: Waiting for pod client-containers-810de6d2-a28a-43e8-a001-0022f30d8de0 to disappear
    Nov 15 07:32:50.201: INFO: Pod client-containers-810de6d2-a28a-43e8-a001-0022f30d8de0 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:32:50.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3964" for this suite. 11/15/23 07:32:50.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:32:50.24
Nov 15 07:32:50.240: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename secrets 11/15/23 07:32:50.241
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:32:50.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:32:50.305
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-b66fb59b-9a17-4811-91f8-07ba998857fa 11/15/23 07:32:50.311
STEP: Creating a pod to test consume secrets 11/15/23 07:32:50.324
Nov 15 07:32:50.360: INFO: Waiting up to 5m0s for pod "pod-secrets-e9017939-8817-4bb9-bfa7-a016573a730e" in namespace "secrets-8066" to be "Succeeded or Failed"
Nov 15 07:32:50.373: INFO: Pod "pod-secrets-e9017939-8817-4bb9-bfa7-a016573a730e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.154978ms
Nov 15 07:32:52.385: INFO: Pod "pod-secrets-e9017939-8817-4bb9-bfa7-a016573a730e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025689258s
Nov 15 07:32:54.384: INFO: Pod "pod-secrets-e9017939-8817-4bb9-bfa7-a016573a730e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023740724s
Nov 15 07:32:56.391: INFO: Pod "pod-secrets-e9017939-8817-4bb9-bfa7-a016573a730e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031642009s
STEP: Saw pod success 11/15/23 07:32:56.391
Nov 15 07:32:56.392: INFO: Pod "pod-secrets-e9017939-8817-4bb9-bfa7-a016573a730e" satisfied condition "Succeeded or Failed"
Nov 15 07:32:56.403: INFO: Trying to get logs from node 10.72.152.81 pod pod-secrets-e9017939-8817-4bb9-bfa7-a016573a730e container secret-volume-test: <nil>
STEP: delete the pod 11/15/23 07:32:56.451
Nov 15 07:32:56.495: INFO: Waiting for pod pod-secrets-e9017939-8817-4bb9-bfa7-a016573a730e to disappear
Nov 15 07:32:56.532: INFO: Pod pod-secrets-e9017939-8817-4bb9-bfa7-a016573a730e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 07:32:56.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8066" for this suite. 11/15/23 07:32:56.553
------------------------------
â€¢ [SLOW TEST] [6.348 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:32:50.24
    Nov 15 07:32:50.240: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename secrets 11/15/23 07:32:50.241
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:32:50.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:32:50.305
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-b66fb59b-9a17-4811-91f8-07ba998857fa 11/15/23 07:32:50.311
    STEP: Creating a pod to test consume secrets 11/15/23 07:32:50.324
    Nov 15 07:32:50.360: INFO: Waiting up to 5m0s for pod "pod-secrets-e9017939-8817-4bb9-bfa7-a016573a730e" in namespace "secrets-8066" to be "Succeeded or Failed"
    Nov 15 07:32:50.373: INFO: Pod "pod-secrets-e9017939-8817-4bb9-bfa7-a016573a730e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.154978ms
    Nov 15 07:32:52.385: INFO: Pod "pod-secrets-e9017939-8817-4bb9-bfa7-a016573a730e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025689258s
    Nov 15 07:32:54.384: INFO: Pod "pod-secrets-e9017939-8817-4bb9-bfa7-a016573a730e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023740724s
    Nov 15 07:32:56.391: INFO: Pod "pod-secrets-e9017939-8817-4bb9-bfa7-a016573a730e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031642009s
    STEP: Saw pod success 11/15/23 07:32:56.391
    Nov 15 07:32:56.392: INFO: Pod "pod-secrets-e9017939-8817-4bb9-bfa7-a016573a730e" satisfied condition "Succeeded or Failed"
    Nov 15 07:32:56.403: INFO: Trying to get logs from node 10.72.152.81 pod pod-secrets-e9017939-8817-4bb9-bfa7-a016573a730e container secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 07:32:56.451
    Nov 15 07:32:56.495: INFO: Waiting for pod pod-secrets-e9017939-8817-4bb9-bfa7-a016573a730e to disappear
    Nov 15 07:32:56.532: INFO: Pod pod-secrets-e9017939-8817-4bb9-bfa7-a016573a730e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:32:56.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8066" for this suite. 11/15/23 07:32:56.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:32:56.589
Nov 15 07:32:56.589: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename endpointslice 11/15/23 07:32:56.59
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:32:56.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:32:56.662
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 11/15/23 07:33:01.955
STEP: referencing matching pods with named port 11/15/23 07:33:06.98
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 11/15/23 07:33:12.004
STEP: recreating EndpointSlices after they've been deleted 11/15/23 07:33:17.028
Nov 15 07:33:17.086: INFO: EndpointSlice for Service endpointslice-2706/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Nov 15 07:33:27.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2706" for this suite. 11/15/23 07:33:27.13
------------------------------
â€¢ [SLOW TEST] [30.559 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:32:56.589
    Nov 15 07:32:56.589: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename endpointslice 11/15/23 07:32:56.59
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:32:56.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:32:56.662
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 11/15/23 07:33:01.955
    STEP: referencing matching pods with named port 11/15/23 07:33:06.98
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 11/15/23 07:33:12.004
    STEP: recreating EndpointSlices after they've been deleted 11/15/23 07:33:17.028
    Nov 15 07:33:17.086: INFO: EndpointSlice for Service endpointslice-2706/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:33:27.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2706" for this suite. 11/15/23 07:33:27.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:33:27.149
Nov 15 07:33:27.149: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename server-version 11/15/23 07:33:27.15
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:33:27.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:33:27.213
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 11/15/23 07:33:27.221
STEP: Confirm major version 11/15/23 07:33:27.224
Nov 15 07:33:27.224: INFO: Major version: 1
STEP: Confirm minor version 11/15/23 07:33:27.224
Nov 15 07:33:27.224: INFO: cleanMinorVersion: 26
Nov 15 07:33:27.224: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Nov 15 07:33:27.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-4104" for this suite. 11/15/23 07:33:27.243
------------------------------
â€¢ [0.117 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:33:27.149
    Nov 15 07:33:27.149: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename server-version 11/15/23 07:33:27.15
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:33:27.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:33:27.213
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 11/15/23 07:33:27.221
    STEP: Confirm major version 11/15/23 07:33:27.224
    Nov 15 07:33:27.224: INFO: Major version: 1
    STEP: Confirm minor version 11/15/23 07:33:27.224
    Nov 15 07:33:27.224: INFO: cleanMinorVersion: 26
    Nov 15 07:33:27.224: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:33:27.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-4104" for this suite. 11/15/23 07:33:27.243
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:33:27.266
Nov 15 07:33:27.267: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubelet-test 11/15/23 07:33:27.268
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:33:27.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:33:27.33
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 11/15/23 07:33:27.367
Nov 15 07:33:27.367: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesc5b7de8e-4674-48f7-84a0-d24a33d51b8a" in namespace "kubelet-test-9252" to be "completed"
Nov 15 07:33:27.382: INFO: Pod "agnhost-host-aliasesc5b7de8e-4674-48f7-84a0-d24a33d51b8a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.721463ms
Nov 15 07:33:29.393: INFO: Pod "agnhost-host-aliasesc5b7de8e-4674-48f7-84a0-d24a33d51b8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02618071s
Nov 15 07:33:31.393: INFO: Pod "agnhost-host-aliasesc5b7de8e-4674-48f7-84a0-d24a33d51b8a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026218971s
Nov 15 07:33:33.396: INFO: Pod "agnhost-host-aliasesc5b7de8e-4674-48f7-84a0-d24a33d51b8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028765553s
Nov 15 07:33:33.396: INFO: Pod "agnhost-host-aliasesc5b7de8e-4674-48f7-84a0-d24a33d51b8a" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Nov 15 07:33:33.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9252" for this suite. 11/15/23 07:33:33.453
------------------------------
â€¢ [SLOW TEST] [6.215 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:33:27.266
    Nov 15 07:33:27.267: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubelet-test 11/15/23 07:33:27.268
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:33:27.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:33:27.33
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 11/15/23 07:33:27.367
    Nov 15 07:33:27.367: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesc5b7de8e-4674-48f7-84a0-d24a33d51b8a" in namespace "kubelet-test-9252" to be "completed"
    Nov 15 07:33:27.382: INFO: Pod "agnhost-host-aliasesc5b7de8e-4674-48f7-84a0-d24a33d51b8a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.721463ms
    Nov 15 07:33:29.393: INFO: Pod "agnhost-host-aliasesc5b7de8e-4674-48f7-84a0-d24a33d51b8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02618071s
    Nov 15 07:33:31.393: INFO: Pod "agnhost-host-aliasesc5b7de8e-4674-48f7-84a0-d24a33d51b8a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026218971s
    Nov 15 07:33:33.396: INFO: Pod "agnhost-host-aliasesc5b7de8e-4674-48f7-84a0-d24a33d51b8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028765553s
    Nov 15 07:33:33.396: INFO: Pod "agnhost-host-aliasesc5b7de8e-4674-48f7-84a0-d24a33d51b8a" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:33:33.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9252" for this suite. 11/15/23 07:33:33.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:33:33.484
Nov 15 07:33:33.484: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename runtimeclass 11/15/23 07:33:33.485
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:33:33.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:33:33.541
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Nov 15 07:33:33.631: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-834 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Nov 15 07:33:33.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-834" for this suite. 11/15/23 07:33:33.705
------------------------------
â€¢ [0.241 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:33:33.484
    Nov 15 07:33:33.484: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename runtimeclass 11/15/23 07:33:33.485
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:33:33.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:33:33.541
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Nov 15 07:33:33.631: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-834 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:33:33.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-834" for this suite. 11/15/23 07:33:33.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:33:33.73
Nov 15 07:33:33.730: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename downward-api 11/15/23 07:33:33.732
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:33:33.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:33:33.792
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 11/15/23 07:33:33.802
Nov 15 07:33:33.849: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d7ce7496-a8d2-4df6-a0c3-f316807cd278" in namespace "downward-api-6790" to be "Succeeded or Failed"
Nov 15 07:33:33.866: INFO: Pod "downwardapi-volume-d7ce7496-a8d2-4df6-a0c3-f316807cd278": Phase="Pending", Reason="", readiness=false. Elapsed: 16.732121ms
Nov 15 07:33:35.879: INFO: Pod "downwardapi-volume-d7ce7496-a8d2-4df6-a0c3-f316807cd278": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028963498s
Nov 15 07:33:37.877: INFO: Pod "downwardapi-volume-d7ce7496-a8d2-4df6-a0c3-f316807cd278": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027273414s
STEP: Saw pod success 11/15/23 07:33:37.877
Nov 15 07:33:37.877: INFO: Pod "downwardapi-volume-d7ce7496-a8d2-4df6-a0c3-f316807cd278" satisfied condition "Succeeded or Failed"
Nov 15 07:33:37.887: INFO: Trying to get logs from node 10.72.152.81 pod downwardapi-volume-d7ce7496-a8d2-4df6-a0c3-f316807cd278 container client-container: <nil>
STEP: delete the pod 11/15/23 07:33:37.922
Nov 15 07:33:37.948: INFO: Waiting for pod downwardapi-volume-d7ce7496-a8d2-4df6-a0c3-f316807cd278 to disappear
Nov 15 07:33:37.957: INFO: Pod downwardapi-volume-d7ce7496-a8d2-4df6-a0c3-f316807cd278 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 07:33:37.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6790" for this suite. 11/15/23 07:33:37.97
------------------------------
â€¢ [4.257 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:33:33.73
    Nov 15 07:33:33.730: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename downward-api 11/15/23 07:33:33.732
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:33:33.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:33:33.792
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 11/15/23 07:33:33.802
    Nov 15 07:33:33.849: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d7ce7496-a8d2-4df6-a0c3-f316807cd278" in namespace "downward-api-6790" to be "Succeeded or Failed"
    Nov 15 07:33:33.866: INFO: Pod "downwardapi-volume-d7ce7496-a8d2-4df6-a0c3-f316807cd278": Phase="Pending", Reason="", readiness=false. Elapsed: 16.732121ms
    Nov 15 07:33:35.879: INFO: Pod "downwardapi-volume-d7ce7496-a8d2-4df6-a0c3-f316807cd278": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028963498s
    Nov 15 07:33:37.877: INFO: Pod "downwardapi-volume-d7ce7496-a8d2-4df6-a0c3-f316807cd278": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027273414s
    STEP: Saw pod success 11/15/23 07:33:37.877
    Nov 15 07:33:37.877: INFO: Pod "downwardapi-volume-d7ce7496-a8d2-4df6-a0c3-f316807cd278" satisfied condition "Succeeded or Failed"
    Nov 15 07:33:37.887: INFO: Trying to get logs from node 10.72.152.81 pod downwardapi-volume-d7ce7496-a8d2-4df6-a0c3-f316807cd278 container client-container: <nil>
    STEP: delete the pod 11/15/23 07:33:37.922
    Nov 15 07:33:37.948: INFO: Waiting for pod downwardapi-volume-d7ce7496-a8d2-4df6-a0c3-f316807cd278 to disappear
    Nov 15 07:33:37.957: INFO: Pod downwardapi-volume-d7ce7496-a8d2-4df6-a0c3-f316807cd278 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:33:37.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6790" for this suite. 11/15/23 07:33:37.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:33:37.989
Nov 15 07:33:37.989: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename resourcequota 11/15/23 07:33:37.99
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:33:38.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:33:38.051
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 11/15/23 07:33:55.067
STEP: Creating a ResourceQuota 11/15/23 07:34:00.082
STEP: Ensuring resource quota status is calculated 11/15/23 07:34:00.104
STEP: Creating a ConfigMap 11/15/23 07:34:02.125
STEP: Ensuring resource quota status captures configMap creation 11/15/23 07:34:02.149
STEP: Deleting a ConfigMap 11/15/23 07:34:04.161
STEP: Ensuring resource quota status released usage 11/15/23 07:34:04.178
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 07:34:06.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8788" for this suite. 11/15/23 07:34:06.204
------------------------------
â€¢ [SLOW TEST] [28.233 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:33:37.989
    Nov 15 07:33:37.989: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename resourcequota 11/15/23 07:33:37.99
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:33:38.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:33:38.051
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 11/15/23 07:33:55.067
    STEP: Creating a ResourceQuota 11/15/23 07:34:00.082
    STEP: Ensuring resource quota status is calculated 11/15/23 07:34:00.104
    STEP: Creating a ConfigMap 11/15/23 07:34:02.125
    STEP: Ensuring resource quota status captures configMap creation 11/15/23 07:34:02.149
    STEP: Deleting a ConfigMap 11/15/23 07:34:04.161
    STEP: Ensuring resource quota status released usage 11/15/23 07:34:04.178
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:34:06.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8788" for this suite. 11/15/23 07:34:06.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:34:06.224
Nov 15 07:34:06.224: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename webhook 11/15/23 07:34:06.225
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:34:06.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:34:06.291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 07:34:06.365
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 07:34:06.654
STEP: Deploying the webhook pod 11/15/23 07:34:06.69
STEP: Wait for the deployment to be ready 11/15/23 07:34:06.714
Nov 15 07:34:06.735: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 07:34:08.769: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 34, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 34, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 34, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 34, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 07:34:10.78
STEP: Verifying the service has paired with the endpoint 11/15/23 07:34:10.826
Nov 15 07:34:11.827: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 11/15/23 07:34:11.853
STEP: Updating a mutating webhook configuration's rules to not include the create operation 11/15/23 07:34:11.929
STEP: Creating a configMap that should not be mutated 11/15/23 07:34:11.945
STEP: Patching a mutating webhook configuration's rules to include the create operation 11/15/23 07:34:11.976
STEP: Creating a configMap that should be mutated 11/15/23 07:34:11.993
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:34:12.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4366" for this suite. 11/15/23 07:34:12.254
STEP: Destroying namespace "webhook-4366-markers" for this suite. 11/15/23 07:34:12.271
------------------------------
â€¢ [SLOW TEST] [6.074 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:34:06.224
    Nov 15 07:34:06.224: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename webhook 11/15/23 07:34:06.225
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:34:06.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:34:06.291
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 07:34:06.365
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 07:34:06.654
    STEP: Deploying the webhook pod 11/15/23 07:34:06.69
    STEP: Wait for the deployment to be ready 11/15/23 07:34:06.714
    Nov 15 07:34:06.735: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 07:34:08.769: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 7, 34, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 34, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 7, 34, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 7, 34, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 07:34:10.78
    STEP: Verifying the service has paired with the endpoint 11/15/23 07:34:10.826
    Nov 15 07:34:11.827: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 11/15/23 07:34:11.853
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 11/15/23 07:34:11.929
    STEP: Creating a configMap that should not be mutated 11/15/23 07:34:11.945
    STEP: Patching a mutating webhook configuration's rules to include the create operation 11/15/23 07:34:11.976
    STEP: Creating a configMap that should be mutated 11/15/23 07:34:11.993
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:34:12.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4366" for this suite. 11/15/23 07:34:12.254
    STEP: Destroying namespace "webhook-4366-markers" for this suite. 11/15/23 07:34:12.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:34:12.3
Nov 15 07:34:12.300: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename watch 11/15/23 07:34:12.301
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:34:12.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:34:12.374
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 11/15/23 07:34:12.383
STEP: creating a new configmap 11/15/23 07:34:12.386
STEP: modifying the configmap once 11/15/23 07:34:12.402
STEP: changing the label value of the configmap 11/15/23 07:34:12.426
STEP: Expecting to observe a delete notification for the watched object 11/15/23 07:34:12.45
Nov 15 07:34:12.450: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-641  c7313b05-9755-428c-96c0-1b16366e81bc 134986 0 2023-11-15 07:34:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 07:34:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 07:34:12.450: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-641  c7313b05-9755-428c-96c0-1b16366e81bc 134993 0 2023-11-15 07:34:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 07:34:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 07:34:12.450: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-641  c7313b05-9755-428c-96c0-1b16366e81bc 135000 0 2023-11-15 07:34:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 07:34:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 11/15/23 07:34:12.45
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 11/15/23 07:34:12.477
STEP: changing the label value of the configmap back 11/15/23 07:34:22.478
STEP: modifying the configmap a third time 11/15/23 07:34:22.532
STEP: deleting the configmap 11/15/23 07:34:22.554
STEP: Expecting to observe an add notification for the watched object when the label value was restored 11/15/23 07:34:22.571
Nov 15 07:34:22.571: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-641  c7313b05-9755-428c-96c0-1b16366e81bc 135117 0 2023-11-15 07:34:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 07:34:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 07:34:22.571: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-641  c7313b05-9755-428c-96c0-1b16366e81bc 135118 0 2023-11-15 07:34:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 07:34:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 07:34:22.571: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-641  c7313b05-9755-428c-96c0-1b16366e81bc 135119 0 2023-11-15 07:34:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 07:34:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Nov 15 07:34:22.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-641" for this suite. 11/15/23 07:34:22.585
------------------------------
â€¢ [SLOW TEST] [10.303 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:34:12.3
    Nov 15 07:34:12.300: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename watch 11/15/23 07:34:12.301
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:34:12.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:34:12.374
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 11/15/23 07:34:12.383
    STEP: creating a new configmap 11/15/23 07:34:12.386
    STEP: modifying the configmap once 11/15/23 07:34:12.402
    STEP: changing the label value of the configmap 11/15/23 07:34:12.426
    STEP: Expecting to observe a delete notification for the watched object 11/15/23 07:34:12.45
    Nov 15 07:34:12.450: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-641  c7313b05-9755-428c-96c0-1b16366e81bc 134986 0 2023-11-15 07:34:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 07:34:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 07:34:12.450: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-641  c7313b05-9755-428c-96c0-1b16366e81bc 134993 0 2023-11-15 07:34:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 07:34:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 07:34:12.450: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-641  c7313b05-9755-428c-96c0-1b16366e81bc 135000 0 2023-11-15 07:34:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 07:34:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 11/15/23 07:34:12.45
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 11/15/23 07:34:12.477
    STEP: changing the label value of the configmap back 11/15/23 07:34:22.478
    STEP: modifying the configmap a third time 11/15/23 07:34:22.532
    STEP: deleting the configmap 11/15/23 07:34:22.554
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 11/15/23 07:34:22.571
    Nov 15 07:34:22.571: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-641  c7313b05-9755-428c-96c0-1b16366e81bc 135117 0 2023-11-15 07:34:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 07:34:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 07:34:22.571: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-641  c7313b05-9755-428c-96c0-1b16366e81bc 135118 0 2023-11-15 07:34:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 07:34:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 07:34:22.571: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-641  c7313b05-9755-428c-96c0-1b16366e81bc 135119 0 2023-11-15 07:34:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 07:34:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:34:22.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-641" for this suite. 11/15/23 07:34:22.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:34:22.606
Nov 15 07:34:22.606: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename webhook 11/15/23 07:34:22.607
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:34:22.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:34:22.668
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 07:34:22.719
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 07:34:23.105
STEP: Deploying the webhook pod 11/15/23 07:34:23.127
STEP: Wait for the deployment to be ready 11/15/23 07:34:23.153
Nov 15 07:34:23.173: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/15/23 07:34:25.205
STEP: Verifying the service has paired with the endpoint 11/15/23 07:34:25.239
Nov 15 07:34:26.240: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 11/15/23 07:34:26.435
STEP: Creating a configMap that should be mutated 11/15/23 07:34:26.498
STEP: Deleting the collection of validation webhooks 11/15/23 07:34:26.642
STEP: Creating a configMap that should not be mutated 11/15/23 07:34:26.885
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:34:26.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-50" for this suite. 11/15/23 07:34:27.062
STEP: Destroying namespace "webhook-50-markers" for this suite. 11/15/23 07:34:27.094
------------------------------
â€¢ [4.515 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:34:22.606
    Nov 15 07:34:22.606: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename webhook 11/15/23 07:34:22.607
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:34:22.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:34:22.668
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 07:34:22.719
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 07:34:23.105
    STEP: Deploying the webhook pod 11/15/23 07:34:23.127
    STEP: Wait for the deployment to be ready 11/15/23 07:34:23.153
    Nov 15 07:34:23.173: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/15/23 07:34:25.205
    STEP: Verifying the service has paired with the endpoint 11/15/23 07:34:25.239
    Nov 15 07:34:26.240: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 11/15/23 07:34:26.435
    STEP: Creating a configMap that should be mutated 11/15/23 07:34:26.498
    STEP: Deleting the collection of validation webhooks 11/15/23 07:34:26.642
    STEP: Creating a configMap that should not be mutated 11/15/23 07:34:26.885
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:34:26.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-50" for this suite. 11/15/23 07:34:27.062
    STEP: Destroying namespace "webhook-50-markers" for this suite. 11/15/23 07:34:27.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:34:27.122
Nov 15 07:34:27.122: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename sched-pred 11/15/23 07:34:27.123
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:34:27.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:34:27.202
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Nov 15 07:34:27.208: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 15 07:34:27.244: INFO: Waiting for terminating namespaces to be deleted...
Nov 15 07:34:27.270: INFO: 
Logging pods the apiserver thinks is on node 10.72.152.81 before test
Nov 15 07:34:27.310: INFO: calico-node-p5wd4 from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 07:34:27.310: INFO: calico-typha-76d9767bd5-985rd from calico-system started at 2023-11-15 03:39:41 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 07:34:27.310: INFO: ibm-keepalived-watcher-qb8hn from kube-system started at 2023-11-15 03:38:31 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 07:34:27.310: INFO: ibm-master-proxy-static-10.72.152.81 from kube-system started at 2023-11-15 03:38:18 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 07:34:27.310: INFO: 	Container pause ready: true, restart count 0
Nov 15 07:34:27.310: INFO: ibmcloud-block-storage-driver-z7fmw from kube-system started at 2023-11-15 03:38:36 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 07:34:27.310: INFO: tuned-rb6qn from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container tuned ready: true, restart count 0
Nov 15 07:34:27.310: INFO: dns-default-ttdsk from openshift-dns started at 2023-11-15 07:18:20 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container dns ready: true, restart count 0
Nov 15 07:34:27.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.310: INFO: node-resolver-6lxcm from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 15 07:34:27.310: INFO: node-ca-897k2 from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container node-ca ready: true, restart count 0
Nov 15 07:34:27.310: INFO: registry-pvc-permissions-lw2bj from openshift-image-registry started at 2023-11-15 03:46:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container pvc-permissions ready: false, restart count 0
Nov 15 07:34:27.310: INFO: ingress-canary-b5m5d from openshift-ingress-canary started at 2023-11-15 07:18:01 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 15 07:34:27.310: INFO: openshift-kube-proxy-p8p2b from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 15 07:34:27.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.310: INFO: node-exporter-67458 from openshift-monitoring started at 2023-11-15 03:44:49 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.310: INFO: 	Container node-exporter ready: true, restart count 0
Nov 15 07:34:27.310: INFO: multus-9tc6x from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container kube-multus ready: true, restart count 0
Nov 15 07:34:27.310: INFO: multus-additional-cni-plugins-j2jvc from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 15 07:34:27.310: INFO: network-metrics-daemon-g5fgz from openshift-multus started at 2023-11-15 03:38:47 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.310: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 15 07:34:27.310: INFO: network-check-target-6pv9x from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 15 07:34:27.310: INFO: collect-profiles-28333890-zjsdd from openshift-operator-lifecycle-manager started at 2023-11-15 07:30:00 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container collect-profiles ready: false, restart count 0
Nov 15 07:34:27.310: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-qvnnx from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.310: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 07:34:27.310: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 07:34:27.310: INFO: 
Logging pods the apiserver thinks is on node 10.72.152.86 before test
Nov 15 07:34:27.356: INFO: calico-node-cmgfg from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 07:34:27.356: INFO: calico-typha-76d9767bd5-tx2mp from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 07:34:27.356: INFO: ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-7shbf from ibm-system started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container ibm-cloud-provider-ip-158-176-89-155 ready: true, restart count 0
Nov 15 07:34:27.356: INFO: ibm-keepalived-watcher-qclxn from kube-system started at 2023-11-15 03:38:36 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 07:34:27.356: INFO: ibm-master-proxy-static-10.72.152.86 from kube-system started at 2023-11-15 03:38:24 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 07:34:27.356: INFO: 	Container pause ready: true, restart count 0
Nov 15 07:34:27.356: INFO: ibmcloud-block-storage-driver-m8gq4 from kube-system started at 2023-11-15 03:38:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 07:34:27.356: INFO: vpn-56cd75f85d-zwp8p from kube-system started at 2023-11-15 07:12:42 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container vpn ready: true, restart count 0
Nov 15 07:34:27.356: INFO: cluster-node-tuning-operator-5f77b58f7-t8gf6 from openshift-cluster-node-tuning-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Nov 15 07:34:27.356: INFO: tuned-5qzzn from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container tuned ready: true, restart count 0
Nov 15 07:34:27.356: INFO: cluster-samples-operator-65684cb854-h7n8t from openshift-cluster-samples-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Nov 15 07:34:27.356: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Nov 15 07:34:27.356: INFO: cluster-storage-operator-6cf6b595c7-v8gqp from openshift-cluster-storage-operator started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Nov 15 07:34:27.356: INFO: csi-snapshot-controller-857d54544d-qbhf6 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 07:34:27.356: INFO: csi-snapshot-controller-operator-56df7685c7-rlkr6 from openshift-cluster-storage-operator started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Nov 15 07:34:27.356: INFO: csi-snapshot-webhook-586f5c484d-vkpg9 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container webhook ready: true, restart count 0
Nov 15 07:34:27.356: INFO: console-operator-769f9748fb-7tfcc from openshift-console-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container console-operator ready: true, restart count 1
Nov 15 07:34:27.356: INFO: 	Container conversion-webhook-server ready: true, restart count 3
Nov 15 07:34:27.356: INFO: console-68d6458867-x7kqp from openshift-console started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container console ready: true, restart count 0
Nov 15 07:34:27.356: INFO: downloads-7bb648f846-sr7nb from openshift-console started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container download-server ready: true, restart count 0
Nov 15 07:34:27.356: INFO: dns-operator-dd9c9c896-9gwtp from openshift-dns-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container dns-operator ready: true, restart count 0
Nov 15 07:34:27.356: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.356: INFO: dns-default-24qc6 from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container dns ready: true, restart count 0
Nov 15 07:34:27.356: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.356: INFO: node-resolver-d54kt from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 15 07:34:27.356: INFO: cluster-image-registry-operator-64994bbb4-h9pdh from openshift-image-registry started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.356: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Nov 15 07:34:27.356: INFO: node-ca-w7vhg from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container node-ca ready: true, restart count 0
Nov 15 07:34:27.357: INFO: ingress-canary-ttx6m from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 15 07:34:27.357: INFO: ingress-operator-6d4d6975f7-qtm2n from openshift-ingress-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container ingress-operator ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.357: INFO: router-default-56777c97d6-snr75 from openshift-ingress started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container router ready: true, restart count 0
Nov 15 07:34:27.357: INFO: openshift-kube-proxy-x5hq9 from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.357: INFO: kube-storage-version-migrator-operator-5d88b7484-8m2cc from openshift-kube-storage-version-migrator-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Nov 15 07:34:27.357: INFO: community-operators-6lk9v from openshift-marketplace started at 2023-11-15 07:12:53 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container registry-server ready: true, restart count 0
Nov 15 07:34:27.357: INFO: marketplace-operator-55cc9f5b6b-vsggj from openshift-marketplace started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container marketplace-operator ready: true, restart count 0
Nov 15 07:34:27.357: INFO: redhat-marketplace-97626 from openshift-marketplace started at 2023-11-15 07:12:53 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container registry-server ready: true, restart count 0
Nov 15 07:34:27.357: INFO: redhat-operators-cg2t4 from openshift-marketplace started at 2023-11-15 07:12:52 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container registry-server ready: true, restart count 0
Nov 15 07:34:27.357: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-11-15 03:46:47 +0000 UTC (6 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container alertmanager ready: true, restart count 1
Nov 15 07:34:27.357: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container config-reloader ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 15 07:34:27.357: INFO: cluster-monitoring-operator-868f9b56cf-xrfz7 from openshift-monitoring started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Nov 15 07:34:27.357: INFO: kube-state-metrics-f8d796647-f4rgj from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (3 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 15 07:34:27.357: INFO: node-exporter-n68zb from openshift-monitoring started at 2023-11-15 03:44:49 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container node-exporter ready: true, restart count 0
Nov 15 07:34:27.357: INFO: openshift-state-metrics-69bb697b65-6bcg7 from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (3 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Nov 15 07:34:27.357: INFO: prometheus-adapter-5f5bb574db-569xs from openshift-monitoring started at 2023-11-15 03:44:53 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 15 07:34:27.357: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-11-15 03:46:38 +0000 UTC (6 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container config-reloader ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container prometheus ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 15 07:34:27.357: INFO: prometheus-operator-6fcb4d4c46-t74rt from openshift-monitoring started at 2023-11-15 03:44:41 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 15 07:34:27.357: INFO: prometheus-operator-admission-webhook-c78bf8f99-pswrk from openshift-monitoring started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Nov 15 07:34:27.357: INFO: telemeter-client-77c946bb95-hw2qc from openshift-monitoring started at 2023-11-15 07:12:43 +0000 UTC (3 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container reload ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container telemeter-client ready: true, restart count 0
Nov 15 07:34:27.357: INFO: thanos-querier-56b7586647-8m7mp from openshift-monitoring started at 2023-11-15 03:44:57 +0000 UTC (6 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container thanos-query ready: true, restart count 0
Nov 15 07:34:27.357: INFO: multus-additional-cni-plugins-z6xx7 from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 15 07:34:27.357: INFO: multus-admission-controller-6b76dd856b-6qmnp from openshift-multus started at 2023-11-15 03:44:37 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 15 07:34:27.357: INFO: multus-cltwv from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container kube-multus ready: true, restart count 0
Nov 15 07:34:27.357: INFO: network-metrics-daemon-zbz9n from openshift-multus started at 2023-11-15 03:38:47 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 15 07:34:27.357: INFO: network-check-target-gnph7 from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 15 07:34:27.357: INFO: catalog-operator-798697959c-w4rjl from openshift-operator-lifecycle-manager started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container catalog-operator ready: true, restart count 0
Nov 15 07:34:27.357: INFO: olm-operator-846bf6bd78-9bfgx from openshift-operator-lifecycle-manager started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container olm-operator ready: true, restart count 0
Nov 15 07:34:27.357: INFO: package-server-manager-5b666bf8fd-bnm55 from openshift-operator-lifecycle-manager started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container package-server-manager ready: true, restart count 0
Nov 15 07:34:27.357: INFO: packageserver-758b547fc-65qc5 from openshift-operator-lifecycle-manager started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container packageserver ready: true, restart count 0
Nov 15 07:34:27.357: INFO: service-ca-78fb97bb77-jgxfq from openshift-service-ca started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container service-ca-controller ready: false, restart count 0
Nov 15 07:34:27.357: INFO: sonobuoy from sonobuoy started at 2023-11-15 05:52:01 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 15 07:34:27.357: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-m6885 from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 07:34:27.357: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-11-15 03:41:49 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.357: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 15 07:34:27.357: INFO: 
Logging pods the apiserver thinks is on node 10.72.152.88 before test
Nov 15 07:34:27.402: INFO: calico-kube-controllers-5dd9d87465-759pm from calico-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 15 07:34:27.402: INFO: calico-node-lmf6x from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 07:34:27.402: INFO: managed-storage-validation-webhooks-7457bf6687-9ds5w from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Nov 15 07:34:27.402: INFO: managed-storage-validation-webhooks-7457bf6687-h522x from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Nov 15 07:34:27.402: INFO: managed-storage-validation-webhooks-7457bf6687-phdgp from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Nov 15 07:34:27.402: INFO: ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-vbxww from ibm-system started at 2023-11-15 03:48:09 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container ibm-cloud-provider-ip-158-176-89-155 ready: true, restart count 0
Nov 15 07:34:27.402: INFO: ibm-file-plugin-5fcf7fb495-xmdts from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 15 07:34:27.402: INFO: ibm-keepalived-watcher-5jx26 from kube-system started at 2023-11-15 03:38:12 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 07:34:27.402: INFO: ibm-master-proxy-static-10.72.152.88 from kube-system started at 2023-11-15 03:38:04 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container pause ready: true, restart count 0
Nov 15 07:34:27.402: INFO: ibm-storage-metrics-agent-84fbdc746-5sv68 from kube-system started at 2023-11-15 03:39:51 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Nov 15 07:34:27.402: INFO: ibm-storage-watcher-7445c988b-8ngdm from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 15 07:34:27.402: INFO: ibmcloud-block-storage-driver-9t8rj from kube-system started at 2023-11-15 03:38:17 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 07:34:27.402: INFO: ibmcloud-block-storage-plugin-5774687565-gj9xn from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Nov 15 07:34:27.402: INFO: tuned-q894n from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container tuned ready: true, restart count 0
Nov 15 07:34:27.402: INFO: csi-snapshot-controller-857d54544d-hk655 from openshift-cluster-storage-operator started at 2023-11-15 07:12:42 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 07:34:27.402: INFO: csi-snapshot-webhook-586f5c484d-qs722 from openshift-cluster-storage-operator started at 2023-11-15 07:12:42 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container webhook ready: true, restart count 0
Nov 15 07:34:27.402: INFO: console-68d6458867-krfqd from openshift-console started at 2023-11-15 03:47:51 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container console ready: true, restart count 0
Nov 15 07:34:27.402: INFO: downloads-7bb648f846-tdcnt from openshift-console started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container download-server ready: true, restart count 0
Nov 15 07:34:27.402: INFO: dns-default-lmngx from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container dns ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.402: INFO: node-resolver-hwp5f from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov 15 07:34:27.402: INFO: image-registry-f74f764d8-w48k4 from openshift-image-registry started at 2023-11-15 03:46:35 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container registry ready: true, restart count 0
Nov 15 07:34:27.402: INFO: node-ca-wrqvd from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container node-ca ready: true, restart count 0
Nov 15 07:34:27.402: INFO: ingress-canary-wvl4p from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Nov 15 07:34:27.402: INFO: router-default-56777c97d6-4bnqq from openshift-ingress started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container router ready: true, restart count 0
Nov 15 07:34:27.402: INFO: insights-operator-85b688b59d-65p84 from openshift-insights started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container insights-operator ready: true, restart count 0
Nov 15 07:34:27.402: INFO: openshift-kube-proxy-tzknx from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.402: INFO: migrator-697dd4cbc5-tb2vc from openshift-kube-storage-version-migrator started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container migrator ready: true, restart count 0
Nov 15 07:34:27.402: INFO: certified-operators-8hf46 from openshift-marketplace started at 2023-11-15 07:12:51 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container registry-server ready: true, restart count 0
Nov 15 07:34:27.402: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-11-15 07:12:54 +0000 UTC (6 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container alertmanager ready: true, restart count 1
Nov 15 07:34:27.402: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container config-reloader ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 15 07:34:27.402: INFO: node-exporter-2452k from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container node-exporter ready: true, restart count 0
Nov 15 07:34:27.402: INFO: prometheus-adapter-5f5bb574db-bmrsf from openshift-monitoring started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov 15 07:34:27.402: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-11-15 07:12:51 +0000 UTC (6 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container config-reloader ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container prometheus ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container thanos-sidecar ready: true, restart count 0
Nov 15 07:34:27.402: INFO: prometheus-operator-admission-webhook-c78bf8f99-x8vcz from openshift-monitoring started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Nov 15 07:34:27.402: INFO: thanos-querier-56b7586647-qr75b from openshift-monitoring started at 2023-11-15 03:44:58 +0000 UTC (6 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container oauth-proxy ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container thanos-query ready: true, restart count 0
Nov 15 07:34:27.402: INFO: multus-additional-cni-plugins-lcnxr from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Nov 15 07:34:27.402: INFO: multus-admission-controller-6b76dd856b-6zft2 from openshift-multus started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container multus-admission-controller ready: true, restart count 0
Nov 15 07:34:27.402: INFO: multus-vqg9w from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container kube-multus ready: true, restart count 0
Nov 15 07:34:27.402: INFO: network-metrics-daemon-9mrwp from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Nov 15 07:34:27.402: INFO: network-check-source-5f9c5566b6-grf5l from openshift-network-diagnostics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container check-endpoints ready: true, restart count 0
Nov 15 07:34:27.402: INFO: network-check-target-6rz7p from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container network-check-target-container ready: true, restart count 0
Nov 15 07:34:27.402: INFO: network-operator-847f47449c-j9glm from openshift-network-operator started at 2023-11-15 03:38:21 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container network-operator ready: true, restart count 1
Nov 15 07:34:27.402: INFO: packageserver-758b547fc-m7x8j from openshift-operator-lifecycle-manager started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container packageserver ready: true, restart count 0
Nov 15 07:34:27.402: INFO: metrics-667b585fc4-d5fdk from openshift-roks-metrics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container metrics ready: true, restart count 5
Nov 15 07:34:27.402: INFO: push-gateway-7f9447c646-5mjcp from openshift-roks-metrics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container push-gateway ready: true, restart count 0
Nov 15 07:34:27.402: INFO: service-ca-operator-74cb5c9cf5-fksk5 from openshift-service-ca-operator started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container service-ca-operator ready: true, restart count 0
Nov 15 07:34:27.402: INFO: sonobuoy-e2e-job-4fb3cc1d32834aa9 from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container e2e ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 07:34:27.402: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-ddv8z from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 07:34:27.402: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 07:34:27.402: INFO: tigera-operator-7dbcb4fb45-rn78j from tigera-operator started at 2023-11-15 03:38:21 +0000 UTC (1 container statuses recorded)
Nov 15 07:34:27.402: INFO: 	Container tigera-operator ready: true, restart count 2
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 11/15/23 07:34:27.402
Nov 15 07:34:27.428: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6946" to be "running"
Nov 15 07:34:27.437: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.916826ms
Nov 15 07:34:29.448: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.019903319s
Nov 15 07:34:29.448: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 11/15/23 07:34:29.46
STEP: Trying to apply a random label on the found node. 11/15/23 07:34:29.496
STEP: verifying the node has the label kubernetes.io/e2e-c280b846-1d79-4d03-8492-80404730a5df 42 11/15/23 07:34:29.519
STEP: Trying to relaunch the pod, now with labels. 11/15/23 07:34:29.53
Nov 15 07:34:29.548: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6946" to be "not pending"
Nov 15 07:34:29.557: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 9.261996ms
Nov 15 07:34:31.576: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028450061s
Nov 15 07:34:33.568: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.019733242s
Nov 15 07:34:33.568: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-c280b846-1d79-4d03-8492-80404730a5df off the node 10.72.152.81 11/15/23 07:34:33.588
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c280b846-1d79-4d03-8492-80404730a5df 11/15/23 07:34:33.629
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 07:34:33.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6946" for this suite. 11/15/23 07:34:33.683
------------------------------
â€¢ [SLOW TEST] [6.617 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:34:27.122
    Nov 15 07:34:27.122: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename sched-pred 11/15/23 07:34:27.123
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:34:27.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:34:27.202
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Nov 15 07:34:27.208: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Nov 15 07:34:27.244: INFO: Waiting for terminating namespaces to be deleted...
    Nov 15 07:34:27.270: INFO: 
    Logging pods the apiserver thinks is on node 10.72.152.81 before test
    Nov 15 07:34:27.310: INFO: calico-node-p5wd4 from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: calico-typha-76d9767bd5-985rd from calico-system started at 2023-11-15 03:39:41 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: ibm-keepalived-watcher-qb8hn from kube-system started at 2023-11-15 03:38:31 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: ibm-master-proxy-static-10.72.152.81 from kube-system started at 2023-11-15 03:38:18 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: 	Container pause ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: ibmcloud-block-storage-driver-z7fmw from kube-system started at 2023-11-15 03:38:36 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: tuned-rb6qn from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container tuned ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: dns-default-ttdsk from openshift-dns started at 2023-11-15 07:18:20 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container dns ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: node-resolver-6lxcm from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: node-ca-897k2 from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container node-ca ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: registry-pvc-permissions-lw2bj from openshift-image-registry started at 2023-11-15 03:46:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container pvc-permissions ready: false, restart count 0
    Nov 15 07:34:27.310: INFO: ingress-canary-b5m5d from openshift-ingress-canary started at 2023-11-15 07:18:01 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: openshift-kube-proxy-p8p2b from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: node-exporter-67458 from openshift-monitoring started at 2023-11-15 03:44:49 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: multus-9tc6x from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: multus-additional-cni-plugins-j2jvc from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: network-metrics-daemon-g5fgz from openshift-multus started at 2023-11-15 03:38:47 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: network-check-target-6pv9x from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container network-check-target-container ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: collect-profiles-28333890-zjsdd from openshift-operator-lifecycle-manager started at 2023-11-15 07:30:00 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container collect-profiles ready: false, restart count 0
    Nov 15 07:34:27.310: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-qvnnx from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.310: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 07:34:27.310: INFO: 
    Logging pods the apiserver thinks is on node 10.72.152.86 before test
    Nov 15 07:34:27.356: INFO: calico-node-cmgfg from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: calico-typha-76d9767bd5-tx2mp from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-7shbf from ibm-system started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container ibm-cloud-provider-ip-158-176-89-155 ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: ibm-keepalived-watcher-qclxn from kube-system started at 2023-11-15 03:38:36 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: ibm-master-proxy-static-10.72.152.86 from kube-system started at 2023-11-15 03:38:24 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: 	Container pause ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: ibmcloud-block-storage-driver-m8gq4 from kube-system started at 2023-11-15 03:38:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: vpn-56cd75f85d-zwp8p from kube-system started at 2023-11-15 07:12:42 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container vpn ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: cluster-node-tuning-operator-5f77b58f7-t8gf6 from openshift-cluster-node-tuning-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: tuned-5qzzn from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container tuned ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: cluster-samples-operator-65684cb854-h7n8t from openshift-cluster-samples-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: cluster-storage-operator-6cf6b595c7-v8gqp from openshift-cluster-storage-operator started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container cluster-storage-operator ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: csi-snapshot-controller-857d54544d-qbhf6 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: csi-snapshot-controller-operator-56df7685c7-rlkr6 from openshift-cluster-storage-operator started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: csi-snapshot-webhook-586f5c484d-vkpg9 from openshift-cluster-storage-operator started at 2023-11-15 03:41:33 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container webhook ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: console-operator-769f9748fb-7tfcc from openshift-console-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container console-operator ready: true, restart count 1
    Nov 15 07:34:27.356: INFO: 	Container conversion-webhook-server ready: true, restart count 3
    Nov 15 07:34:27.356: INFO: console-68d6458867-x7kqp from openshift-console started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container console ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: downloads-7bb648f846-sr7nb from openshift-console started at 2023-11-15 03:41:32 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container download-server ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: dns-operator-dd9c9c896-9gwtp from openshift-dns-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container dns-operator ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: dns-default-24qc6 from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container dns ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: node-resolver-d54kt from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: cluster-image-registry-operator-64994bbb4-h9pdh from openshift-image-registry started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.356: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Nov 15 07:34:27.356: INFO: node-ca-w7vhg from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container node-ca ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: ingress-canary-ttx6m from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: ingress-operator-6d4d6975f7-qtm2n from openshift-ingress-operator started at 2023-11-15 03:41:22 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container ingress-operator ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: router-default-56777c97d6-snr75 from openshift-ingress started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container router ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: openshift-kube-proxy-x5hq9 from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: kube-storage-version-migrator-operator-5d88b7484-8m2cc from openshift-kube-storage-version-migrator-operator started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Nov 15 07:34:27.357: INFO: community-operators-6lk9v from openshift-marketplace started at 2023-11-15 07:12:53 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container registry-server ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: marketplace-operator-55cc9f5b6b-vsggj from openshift-marketplace started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container marketplace-operator ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: redhat-marketplace-97626 from openshift-marketplace started at 2023-11-15 07:12:53 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container registry-server ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: redhat-operators-cg2t4 from openshift-marketplace started at 2023-11-15 07:12:52 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container registry-server ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-11-15 03:46:47 +0000 UTC (6 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container alertmanager ready: true, restart count 1
    Nov 15 07:34:27.357: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: cluster-monitoring-operator-868f9b56cf-xrfz7 from openshift-monitoring started at 2023-11-15 03:41:22 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: kube-state-metrics-f8d796647-f4rgj from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (3 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: node-exporter-n68zb from openshift-monitoring started at 2023-11-15 03:44:49 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: openshift-state-metrics-69bb697b65-6bcg7 from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (3 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: prometheus-adapter-5f5bb574db-569xs from openshift-monitoring started at 2023-11-15 03:44:53 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-11-15 03:46:38 +0000 UTC (6 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container prometheus ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: prometheus-operator-6fcb4d4c46-t74rt from openshift-monitoring started at 2023-11-15 03:44:41 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container prometheus-operator ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: prometheus-operator-admission-webhook-c78bf8f99-pswrk from openshift-monitoring started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: telemeter-client-77c946bb95-hw2qc from openshift-monitoring started at 2023-11-15 07:12:43 +0000 UTC (3 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container reload ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container telemeter-client ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: thanos-querier-56b7586647-8m7mp from openshift-monitoring started at 2023-11-15 03:44:57 +0000 UTC (6 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container oauth-proxy ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container thanos-query ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: multus-additional-cni-plugins-z6xx7 from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: multus-admission-controller-6b76dd856b-6qmnp from openshift-multus started at 2023-11-15 03:44:37 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: multus-cltwv from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: network-metrics-daemon-zbz9n from openshift-multus started at 2023-11-15 03:38:47 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: network-check-target-gnph7 from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container network-check-target-container ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: catalog-operator-798697959c-w4rjl from openshift-operator-lifecycle-manager started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container catalog-operator ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: olm-operator-846bf6bd78-9bfgx from openshift-operator-lifecycle-manager started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container olm-operator ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: package-server-manager-5b666bf8fd-bnm55 from openshift-operator-lifecycle-manager started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container package-server-manager ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: packageserver-758b547fc-65qc5 from openshift-operator-lifecycle-manager started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container packageserver ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: service-ca-78fb97bb77-jgxfq from openshift-service-ca started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container service-ca-controller ready: false, restart count 0
    Nov 15 07:34:27.357: INFO: sonobuoy from sonobuoy started at 2023-11-15 05:52:01 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-m6885 from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-11-15 03:41:49 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.357: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    Nov 15 07:34:27.357: INFO: 
    Logging pods the apiserver thinks is on node 10.72.152.88 before test
    Nov 15 07:34:27.402: INFO: calico-kube-controllers-5dd9d87465-759pm from calico-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: calico-node-lmf6x from calico-system started at 2023-11-15 03:39:33 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: managed-storage-validation-webhooks-7457bf6687-9ds5w from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: managed-storage-validation-webhooks-7457bf6687-h522x from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: managed-storage-validation-webhooks-7457bf6687-phdgp from ibm-odf-validation-webhook started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: ibm-cloud-provider-ip-158-176-89-155-5b8c577c77-vbxww from ibm-system started at 2023-11-15 03:48:09 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container ibm-cloud-provider-ip-158-176-89-155 ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: ibm-file-plugin-5fcf7fb495-xmdts from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: ibm-keepalived-watcher-5jx26 from kube-system started at 2023-11-15 03:38:12 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: ibm-master-proxy-static-10.72.152.88 from kube-system started at 2023-11-15 03:38:04 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container pause ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: ibm-storage-metrics-agent-84fbdc746-5sv68 from kube-system started at 2023-11-15 03:39:51 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: ibm-storage-watcher-7445c988b-8ngdm from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: ibmcloud-block-storage-driver-9t8rj from kube-system started at 2023-11-15 03:38:17 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: ibmcloud-block-storage-plugin-5774687565-gj9xn from kube-system started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: tuned-q894n from openshift-cluster-node-tuning-operator started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container tuned ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: csi-snapshot-controller-857d54544d-hk655 from openshift-cluster-storage-operator started at 2023-11-15 07:12:42 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: csi-snapshot-webhook-586f5c484d-qs722 from openshift-cluster-storage-operator started at 2023-11-15 07:12:42 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container webhook ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: console-68d6458867-krfqd from openshift-console started at 2023-11-15 03:47:51 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container console ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: downloads-7bb648f846-tdcnt from openshift-console started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container download-server ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: dns-default-lmngx from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container dns ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: node-resolver-hwp5f from openshift-dns started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: image-registry-f74f764d8-w48k4 from openshift-image-registry started at 2023-11-15 03:46:35 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container registry ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: node-ca-wrqvd from openshift-image-registry started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container node-ca ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: ingress-canary-wvl4p from openshift-ingress-canary started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: router-default-56777c97d6-4bnqq from openshift-ingress started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container router ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: insights-operator-85b688b59d-65p84 from openshift-insights started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container insights-operator ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: openshift-kube-proxy-tzknx from openshift-kube-proxy started at 2023-11-15 03:38:51 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: migrator-697dd4cbc5-tb2vc from openshift-kube-storage-version-migrator started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container migrator ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: certified-operators-8hf46 from openshift-marketplace started at 2023-11-15 07:12:51 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container registry-server ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-11-15 07:12:54 +0000 UTC (6 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container alertmanager ready: true, restart count 1
    Nov 15 07:34:27.402: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: node-exporter-2452k from openshift-monitoring started at 2023-11-15 03:44:48 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: prometheus-adapter-5f5bb574db-bmrsf from openshift-monitoring started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-11-15 07:12:51 +0000 UTC (6 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container prometheus ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: prometheus-operator-admission-webhook-c78bf8f99-x8vcz from openshift-monitoring started at 2023-11-15 03:44:30 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: thanos-querier-56b7586647-qr75b from openshift-monitoring started at 2023-11-15 03:44:58 +0000 UTC (6 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container oauth-proxy ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container thanos-query ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: multus-additional-cni-plugins-lcnxr from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: multus-admission-controller-6b76dd856b-6zft2 from openshift-multus started at 2023-11-15 03:44:30 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: multus-vqg9w from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container kube-multus ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: network-metrics-daemon-9mrwp from openshift-multus started at 2023-11-15 03:38:46 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: network-check-source-5f9c5566b6-grf5l from openshift-network-diagnostics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container check-endpoints ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: network-check-target-6rz7p from openshift-network-diagnostics started at 2023-11-15 03:38:54 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container network-check-target-container ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: network-operator-847f47449c-j9glm from openshift-network-operator started at 2023-11-15 03:38:21 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container network-operator ready: true, restart count 1
    Nov 15 07:34:27.402: INFO: packageserver-758b547fc-m7x8j from openshift-operator-lifecycle-manager started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container packageserver ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: metrics-667b585fc4-d5fdk from openshift-roks-metrics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container metrics ready: true, restart count 5
    Nov 15 07:34:27.402: INFO: push-gateway-7f9447c646-5mjcp from openshift-roks-metrics started at 2023-11-15 03:39:51 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container push-gateway ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: service-ca-operator-74cb5c9cf5-fksk5 from openshift-service-ca-operator started at 2023-11-15 07:12:43 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container service-ca-operator ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: sonobuoy-e2e-job-4fb3cc1d32834aa9 from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container e2e ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: sonobuoy-systemd-logs-daemon-set-997738c51257402b-ddv8z from sonobuoy started at 2023-11-15 05:52:08 +0000 UTC (2 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 07:34:27.402: INFO: tigera-operator-7dbcb4fb45-rn78j from tigera-operator started at 2023-11-15 03:38:21 +0000 UTC (1 container statuses recorded)
    Nov 15 07:34:27.402: INFO: 	Container tigera-operator ready: true, restart count 2
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 11/15/23 07:34:27.402
    Nov 15 07:34:27.428: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6946" to be "running"
    Nov 15 07:34:27.437: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.916826ms
    Nov 15 07:34:29.448: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.019903319s
    Nov 15 07:34:29.448: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 11/15/23 07:34:29.46
    STEP: Trying to apply a random label on the found node. 11/15/23 07:34:29.496
    STEP: verifying the node has the label kubernetes.io/e2e-c280b846-1d79-4d03-8492-80404730a5df 42 11/15/23 07:34:29.519
    STEP: Trying to relaunch the pod, now with labels. 11/15/23 07:34:29.53
    Nov 15 07:34:29.548: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6946" to be "not pending"
    Nov 15 07:34:29.557: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 9.261996ms
    Nov 15 07:34:31.576: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028450061s
    Nov 15 07:34:33.568: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.019733242s
    Nov 15 07:34:33.568: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-c280b846-1d79-4d03-8492-80404730a5df off the node 10.72.152.81 11/15/23 07:34:33.588
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-c280b846-1d79-4d03-8492-80404730a5df 11/15/23 07:34:33.629
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:34:33.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6946" for this suite. 11/15/23 07:34:33.683
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:34:33.74
Nov 15 07:34:33.740: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename security-context-test 11/15/23 07:34:33.741
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:34:33.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:34:33.865
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Nov 15 07:34:33.953: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-907b7053-4c03-4c75-b30e-48f9bdc482fe" in namespace "security-context-test-9176" to be "Succeeded or Failed"
Nov 15 07:34:33.964: INFO: Pod "busybox-privileged-false-907b7053-4c03-4c75-b30e-48f9bdc482fe": Phase="Pending", Reason="", readiness=false. Elapsed: 9.82241ms
Nov 15 07:34:35.977: INFO: Pod "busybox-privileged-false-907b7053-4c03-4c75-b30e-48f9bdc482fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023143975s
Nov 15 07:34:37.974: INFO: Pod "busybox-privileged-false-907b7053-4c03-4c75-b30e-48f9bdc482fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020524105s
Nov 15 07:34:37.974: INFO: Pod "busybox-privileged-false-907b7053-4c03-4c75-b30e-48f9bdc482fe" satisfied condition "Succeeded or Failed"
Nov 15 07:34:38.013: INFO: Got logs for pod "busybox-privileged-false-907b7053-4c03-4c75-b30e-48f9bdc482fe": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Nov 15 07:34:38.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-9176" for this suite. 11/15/23 07:34:38.027
------------------------------
â€¢ [4.305 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:34:33.74
    Nov 15 07:34:33.740: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename security-context-test 11/15/23 07:34:33.741
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:34:33.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:34:33.865
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Nov 15 07:34:33.953: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-907b7053-4c03-4c75-b30e-48f9bdc482fe" in namespace "security-context-test-9176" to be "Succeeded or Failed"
    Nov 15 07:34:33.964: INFO: Pod "busybox-privileged-false-907b7053-4c03-4c75-b30e-48f9bdc482fe": Phase="Pending", Reason="", readiness=false. Elapsed: 9.82241ms
    Nov 15 07:34:35.977: INFO: Pod "busybox-privileged-false-907b7053-4c03-4c75-b30e-48f9bdc482fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023143975s
    Nov 15 07:34:37.974: INFO: Pod "busybox-privileged-false-907b7053-4c03-4c75-b30e-48f9bdc482fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020524105s
    Nov 15 07:34:37.974: INFO: Pod "busybox-privileged-false-907b7053-4c03-4c75-b30e-48f9bdc482fe" satisfied condition "Succeeded or Failed"
    Nov 15 07:34:38.013: INFO: Got logs for pod "busybox-privileged-false-907b7053-4c03-4c75-b30e-48f9bdc482fe": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:34:38.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-9176" for this suite. 11/15/23 07:34:38.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:34:38.048
Nov 15 07:34:38.048: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename projected 11/15/23 07:34:38.049
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:34:38.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:34:38.109
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 11/15/23 07:34:38.115
W1115 07:34:38.151849      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Nov 15 07:34:38.152: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68a2097a-9cc4-4fd5-83fc-edd430c81f2d" in namespace "projected-987" to be "Succeeded or Failed"
Nov 15 07:34:38.177: INFO: Pod "downwardapi-volume-68a2097a-9cc4-4fd5-83fc-edd430c81f2d": Phase="Pending", Reason="", readiness=false. Elapsed: 25.146318ms
Nov 15 07:34:40.187: INFO: Pod "downwardapi-volume-68a2097a-9cc4-4fd5-83fc-edd430c81f2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035743401s
Nov 15 07:34:42.192: INFO: Pod "downwardapi-volume-68a2097a-9cc4-4fd5-83fc-edd430c81f2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040876496s
STEP: Saw pod success 11/15/23 07:34:42.193
Nov 15 07:34:42.193: INFO: Pod "downwardapi-volume-68a2097a-9cc4-4fd5-83fc-edd430c81f2d" satisfied condition "Succeeded or Failed"
Nov 15 07:34:42.207: INFO: Trying to get logs from node 10.72.152.81 pod downwardapi-volume-68a2097a-9cc4-4fd5-83fc-edd430c81f2d container client-container: <nil>
STEP: delete the pod 11/15/23 07:34:42.233
Nov 15 07:34:42.265: INFO: Waiting for pod downwardapi-volume-68a2097a-9cc4-4fd5-83fc-edd430c81f2d to disappear
Nov 15 07:34:42.274: INFO: Pod downwardapi-volume-68a2097a-9cc4-4fd5-83fc-edd430c81f2d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 07:34:42.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-987" for this suite. 11/15/23 07:34:42.291
------------------------------
â€¢ [4.260 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:34:38.048
    Nov 15 07:34:38.048: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename projected 11/15/23 07:34:38.049
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:34:38.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:34:38.109
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 11/15/23 07:34:38.115
    W1115 07:34:38.151849      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Nov 15 07:34:38.152: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68a2097a-9cc4-4fd5-83fc-edd430c81f2d" in namespace "projected-987" to be "Succeeded or Failed"
    Nov 15 07:34:38.177: INFO: Pod "downwardapi-volume-68a2097a-9cc4-4fd5-83fc-edd430c81f2d": Phase="Pending", Reason="", readiness=false. Elapsed: 25.146318ms
    Nov 15 07:34:40.187: INFO: Pod "downwardapi-volume-68a2097a-9cc4-4fd5-83fc-edd430c81f2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035743401s
    Nov 15 07:34:42.192: INFO: Pod "downwardapi-volume-68a2097a-9cc4-4fd5-83fc-edd430c81f2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040876496s
    STEP: Saw pod success 11/15/23 07:34:42.193
    Nov 15 07:34:42.193: INFO: Pod "downwardapi-volume-68a2097a-9cc4-4fd5-83fc-edd430c81f2d" satisfied condition "Succeeded or Failed"
    Nov 15 07:34:42.207: INFO: Trying to get logs from node 10.72.152.81 pod downwardapi-volume-68a2097a-9cc4-4fd5-83fc-edd430c81f2d container client-container: <nil>
    STEP: delete the pod 11/15/23 07:34:42.233
    Nov 15 07:34:42.265: INFO: Waiting for pod downwardapi-volume-68a2097a-9cc4-4fd5-83fc-edd430c81f2d to disappear
    Nov 15 07:34:42.274: INFO: Pod downwardapi-volume-68a2097a-9cc4-4fd5-83fc-edd430c81f2d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:34:42.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-987" for this suite. 11/15/23 07:34:42.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:34:42.317
Nov 15 07:34:42.317: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename kubectl 11/15/23 07:34:42.318
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:34:42.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:34:42.38
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 11/15/23 07:34:42.386
Nov 15 07:34:42.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-3742 cluster-info'
Nov 15 07:34:42.505: INFO: stderr: ""
Nov 15 07:34:42.505: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 07:34:42.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3742" for this suite. 11/15/23 07:34:42.522
------------------------------
â€¢ [0.228 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:34:42.317
    Nov 15 07:34:42.317: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename kubectl 11/15/23 07:34:42.318
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:34:42.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:34:42.38
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 11/15/23 07:34:42.386
    Nov 15 07:34:42.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4133354737 --namespace=kubectl-3742 cluster-info'
    Nov 15 07:34:42.505: INFO: stderr: ""
    Nov 15 07:34:42.505: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:34:42.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3742" for this suite. 11/15/23 07:34:42.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:34:42.547
Nov 15 07:34:42.547: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename podtemplate 11/15/23 07:34:42.548
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:34:42.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:34:42.617
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Nov 15 07:34:42.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-121" for this suite. 11/15/23 07:34:42.767
------------------------------
â€¢ [0.242 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:34:42.547
    Nov 15 07:34:42.547: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename podtemplate 11/15/23 07:34:42.548
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:34:42.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:34:42.617
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:34:42.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-121" for this suite. 11/15/23 07:34:42.767
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 07:34:42.789
Nov 15 07:34:42.789: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
STEP: Building a namespace api object, basename container-runtime 11/15/23 07:34:42.791
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:34:42.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:34:42.864
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 11/15/23 07:34:42.871
STEP: wait for the container to reach Succeeded 11/15/23 07:34:42.907
STEP: get the container status 11/15/23 07:34:46.979
STEP: the container should be terminated 11/15/23 07:34:46.989
STEP: the termination message should be set 11/15/23 07:34:46.989
Nov 15 07:34:46.989: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 11/15/23 07:34:46.989
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Nov 15 07:34:47.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9814" for this suite. 11/15/23 07:34:47.044
------------------------------
â€¢ [4.272 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 07:34:42.789
    Nov 15 07:34:42.789: INFO: >>> kubeConfig: /tmp/kubeconfig-4133354737
    STEP: Building a namespace api object, basename container-runtime 11/15/23 07:34:42.791
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 07:34:42.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 07:34:42.864
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 11/15/23 07:34:42.871
    STEP: wait for the container to reach Succeeded 11/15/23 07:34:42.907
    STEP: get the container status 11/15/23 07:34:46.979
    STEP: the container should be terminated 11/15/23 07:34:46.989
    STEP: the termination message should be set 11/15/23 07:34:46.989
    Nov 15 07:34:46.989: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 11/15/23 07:34:46.989
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Nov 15 07:34:47.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9814" for this suite. 11/15/23 07:34:47.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Nov 15 07:34:47.064: INFO: Running AfterSuite actions on node 1
Nov 15 07:34:47.064: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Nov 15 07:34:47.064: INFO: Running AfterSuite actions on node 1
    Nov 15 07:34:47.064: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.100 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 6144.600 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h42m25.040688773s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

